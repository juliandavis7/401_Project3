{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 401 Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "russia_df = pd.read_csv(\"russian_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing to perform cross validation \n",
    "ind = list(russia_df.index)\n",
    "random.shuffle(ind)\n",
    "\n",
    "splits = int(len(russia_df)/5)\n",
    "inds = []\n",
    "for i in range(5):\n",
    "    inds.append(ind[i*splits:(i+1)*splits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1))\n",
    "y = russia_df[\"price_doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(y).plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decided to use the log of y as the results are more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(y, y_pred):\n",
    "    return np.mean(np.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(y, y_pred):\n",
    "    return np.mean(np.absolute(y-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # create vector of ones...\n",
    "        ones = np.ones(shape=len(X_train))[..., None]\n",
    "        #...and add to feature matrix\n",
    "        X = np.concatenate((ones, X_train), 1)\n",
    "        #calculate coefficients using closed-form solution\n",
    "        self.coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        ones = np.ones(shape=len(X_test))[..., None]\n",
    "        X_test = np.concatenate((ones, X_test), 1)\n",
    "        y_hat = X_test.dot(self.coeffs)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing cross validation using the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Mean absolute error: $0.34\n",
      "Root mean squared error: $0.26\n",
      "Iteration: 1\n",
      "Mean absolute error: $0.34\n",
      "Root mean squared error: $0.59\n",
      "Iteration: 2\n",
      "Mean absolute error: $0.35\n",
      "Root mean squared error: $0.28\n",
      "Iteration: 3\n",
      "Mean absolute error: $0.49\n",
      "Root mean squared error: $0.50\n",
      "Iteration: 4\n",
      "Mean absolute error: $0.34\n",
      "Root mean squared error: $0.66\n"
     ]
    }
   ],
   "source": [
    "scalers = [StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler()]\n",
    "\n",
    "lr_mses = []\n",
    "lr_maes = []\n",
    "non_log_lr_maes = []\n",
    "\n",
    "for i in range(len(scalers)):\n",
    "    copies = inds.copy()\n",
    "    test_ind = inds[i]\n",
    "    copies.remove(copies[i])\n",
    "    train_ind = []\n",
    "    for c in copies:\n",
    "        for num in c:\n",
    "            train_ind.append(num)\n",
    "    scaler = scalers[i]\n",
    "    \n",
    "    X_train_std = scaler.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[train_ind])\n",
    "    X_test_std = scaler.transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[test_ind])\n",
    "    y_train = russia_df[\"price_doc\"].iloc[train_ind]\n",
    "    y_test = russia_df[\"price_doc\"].iloc[test_ind]\n",
    "    y_train_log = np.log(y_train)\n",
    "    y_test_log = np.log(y_test)\n",
    "    \n",
    "    print(\"Iteration:\", i)\n",
    "    lr = LR()\n",
    "    lr.fit(X_train_std, y_train_log)\n",
    "    y_pred = lr.predict(X_test_std)\n",
    "    mae = get_mae(y_test_log, y_pred)\n",
    "    lr_maes.append(mae)\n",
    "    non_log_mae = get_mae(y_test, np.exp(lr.predict(X_test_std)))\n",
    "    non_log_lr_maes.append(non_log_mae)\n",
    "    mse = get_mse(y_test_log, y_pred)\n",
    "    lr_mses.append(mse)\n",
    "    \n",
    "    print('Mean absolute error: $%0.2f'%(mae))\n",
    "    print('Root mean squared error: $%0.2f'%(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3096369533338666e+22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(non_log_lr_maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3729101722711884"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lr_maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4596382013012306"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lr_mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our baseline from the linear regression, it is time to develop a neural network implementations to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers=None, nodes=None, nnodes=None, \n",
    "                 activations=[], activationFn=\"relu\", batchSize=50, \n",
    "                 lr=.001, lr_type=\"constant\", power_t=.5,\n",
    "                 annealing_rate=.999, max_epoch=200, momentum=.9, \n",
    "                 tol=0.0001, alpha=.0001, shuffle=False, \n",
    "                 early_stopping=False, num_epochs_stop=50, verbose=True):\n",
    "        \n",
    "        if layers != None:\n",
    "            self.layers = layers # total number of hidden layers\n",
    "        else:\n",
    "            self.layers = len(nodes)\n",
    "\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        self.nodes = nodes\n",
    "        if nodes != None:\n",
    "            self.nodes.insert(0, batchSize)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        self.nnodes = nnodes\n",
    "        if nnodes != None:\n",
    "            self.nodes = []\n",
    "            self.nodes.append(batchSize)\n",
    "            for i in range(layers):\n",
    "                self.nodes.append(nnodes)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        self.activations = activations\n",
    "        self.activationFn = activationFn\n",
    "        if activationFn != \"\":\n",
    "            self.activations = [activationFn] * self.layers\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.power_t = power_t\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.mu = momentum\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if early_stopping == False:\n",
    "            self.num_epochs_stop = max_epoch\n",
    "        else:\n",
    "            self.num_epochs_stop = num_epochs_stop\n",
    "    \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        self.epochs = 0\n",
    "                \n",
    "    def validateHyperParams(self):\n",
    "        \n",
    "        if self.layers != (len(self.nodes) - 2):\n",
    "            raise ValueError(\"layers must be equal to the number of hidden layers, got %s.\" % self.layers)\n",
    "        if self.nnodes != None and self.nnodes <= 0:\n",
    "            raise ValueError(\"nnodes must be > 0, got %s.\" % self.nnodes)\n",
    "        if self.lr <= 0 or self.lr > 1:\n",
    "            raise ValueError(\"lr must be in (0, 1], got %s.\" % self.lr)\n",
    "            \n",
    "        if self.lr_type not in [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]:\n",
    "            raise ValueError(\"lr_type is not valid\" % self.lr_type\n",
    "                            + \"\\nAvailable lr types: constant, invscaling, adaptive\")\n",
    "            \n",
    "        if self.max_epoch <= 0:\n",
    "            raise ValueError(\"max_iter must be > 0, got %s.\" % self.max_epoch)\n",
    "               \n",
    "        activation_functions = list(ACTIVATIONS.keys())\n",
    "        if self.activationFn != \"\":\n",
    "            if self.activationFn not in activation_functions:\n",
    "                raise ValueError(\"%s is not an activation function\" % self.activationFn\n",
    "                                + \"\\nAvailable activation functions: relu, leaky_relu, sigmoid, tanh\")\n",
    "    \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            \n",
    "            # Xavier (Glorot) Initialization\n",
    "            if self.activationFn == \"tanh\":\n",
    "                target_variance = 2 / (input_size + output_size)\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # He Initialization\n",
    "            elif self.activationFn == \"relu\":\n",
    "                target_variance = 2 / input_size\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # Random Uniform\n",
    "            else:\n",
    "                w_i = np.random.uniform(-1/np.sqrt(input_size), 1/np.sqrt(input_size))\n",
    "                #w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "    \n",
    "    # returns the weight term for L2 regularization\n",
    "    def get_weight_term(self):\n",
    "        weight_term = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_term = np.sum(self.weights[i] ** 2)\n",
    "        return weight_term\n",
    "        \n",
    "    def forward_pass(self, X_batch, y_batch):\n",
    "        \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(self.layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = 0\n",
    "        # only need predictions once we have fit the data\n",
    "        if isinstance(y_batch, np.ndarray): \n",
    "            l2_loss = squared_loss(y_pred, y_batch) # l2\n",
    "            weight_term = self.get_weight_term()\n",
    "            l2_loss += self.alpha * weight_term # l2 regularization\n",
    "            self.layer_values[self.layers + 1] = l2_loss\n",
    "        \n",
    "        return l2_loss, y_pred\n",
    "    \n",
    "    \n",
    "    def backward_pass(self, y_pred, y_batch):\n",
    "        \n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(y_pred, y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (self.layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        w_t = np.delete(w_t, w_t.shape[1] - 1, 1) # take out the bias\n",
    "        J = np.dot(J, w_t)\n",
    "        zeros = [0] * len(J)\n",
    "        zeros = np.reshape(zeros, (len(J), 1))\n",
    "        J = np.append(J, zeros, axis=1)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(self.layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "            \n",
    "            # jacobian w.r.t. inputs\n",
    "            w_t = self.weights[i - 1].T\n",
    "            w_t = np.delete(w_t, w_t.shape[1] - 1, 1)\n",
    "            J = np.dot(J, w_t)\n",
    "            zeros = [0] * len(J)\n",
    "            zeros = np.reshape(zeros, (len(J), 1))\n",
    "            J = np.append(J, zeros, axis=1)\n",
    "            \n",
    "            \n",
    "        # initialize velocity to 0\n",
    "        if self.epochs == 0 and self.iters == 0:\n",
    "            self.velocity = []\n",
    "            for i in range(len(J_weights)):\n",
    "                n_rows = J_weights[i].shape[0]\n",
    "                n_cols = J_weights[i].shape[1]\n",
    "                vel_i = np.zeros((n_rows, n_cols))\n",
    "                self.velocity.append(vel_i)\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.velocity[i] = self.mu * self.velocity[i] - self.lr * J_weights[i]\n",
    "            self.weights[i] += self.velocity[i]\n",
    "      \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.validateHyperParams()\n",
    "        # convert to numpy arrays\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.to_numpy()\n",
    "            \n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.to_numpy()\n",
    "            \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_train)\n",
    "        ones = np.reshape(ones, (len(X_train), 1))\n",
    "        X_train = np.append(X_train, ones, axis=1)\n",
    "        \n",
    "        # save 10% for validation\n",
    "        val_rows = round(len(X_train) * .1)\n",
    "        X_val = X_train[:val_rows, :]\n",
    "        y_val = y_train[:val_rows]\n",
    "        \n",
    "        X_train = X_train[val_rows:, :]\n",
    "        y_train = y_train[val_rows:]\n",
    "        \n",
    "        # initalize weights on first iteration\n",
    "        M = X_train.shape[1] # M = number of features\n",
    "        self.weights = self.initialize_weights(M)\n",
    "        \n",
    "        best_v_loss = np.inf\n",
    "        n_epoch_no_change = 0 \n",
    "        while (self.epochs < self.max_epoch and n_epoch_no_change <= self.num_epochs_stop):\n",
    "            # ONE EPOCH \n",
    "            last_idx = 0\n",
    "            if self.shuffle == True: # shuffle data after every epoch, if specified \n",
    "                np.random.shuffle(X_train)\n",
    "            while (last_idx < len(X_train)):\n",
    "                first_idx = self.iters * self.batchSize\n",
    "                remaining_rows = len(X_train) - first_idx\n",
    "                last_idx = first_idx + min(self.batchSize, remaining_rows)\n",
    "                X_batch = X_train[first_idx: last_idx, :]\n",
    "                y_batch = y_train[first_idx: last_idx]\n",
    "\n",
    "                loss, y_pred = self.forward_pass(X_batch, y_batch)\n",
    "                self.backward_pass(y_pred, y_batch)\n",
    "                self.iters += 1\n",
    "            \n",
    "            # trainig and validation loss after one epoch\n",
    "            t_loss, y_pred = self.forward_pass(X_train, y_train)\n",
    "            v_loss, y_pred = self.forward_pass(X_val, y_val)\n",
    "            if self.verbose:\n",
    "                print(\"epoch:\", self.epochs)\n",
    "                print(\"training loss:\", t_loss)\n",
    "                print(\"validation loss:\", v_loss)\n",
    "            \n",
    "            self.iters = 0 # start over, next epoch\n",
    "            self.epochs += 1\n",
    "            \n",
    "            # decrease the learning rate by one of three methods, if specified\n",
    "            if self.lr_type == \"invscaling\":\n",
    "                self.lr = self.lr/pow(self.epochs, self.power_t)\n",
    "            elif self.lr_type == \"annealing\":\n",
    "                self.lr = self.lr * self.annealing_rate\n",
    "            elif self.lr_type == \"adaptive\":\n",
    "                if n_epoch_no_change >= 2: \n",
    "                    self.lr = self.lr/5\n",
    "                \n",
    "            # stops when validation loss doesn't improve for num_epochs_stop\n",
    "            if best_v_loss - v_loss < self.tol: \n",
    "                n_epoch_no_change += 1\n",
    "            else:\n",
    "                n_epoch_no_change = 0\n",
    "            # update best_v_loss\n",
    "            if v_loss < best_v_loss:\n",
    "                best_v_loss = v_loss\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # convert to numpy array\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.to_numpy()\n",
    "        \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_test)\n",
    "        ones = np.reshape(ones, (len(X_test), 1))\n",
    "        X_test = np.append(X_test, ones, axis=1)\n",
    "        \n",
    "        loss, y_pred = self.forward_pass(X_test, None)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 50\n",
      "Batch Size: 75\n",
      "Nodes: 50\n",
      "Batch Size: 150\n",
      "Nodes: 50\n",
      "Batch Size: 1000\n",
      "Nodes: 50\n",
      "Batch Size: 3000\n",
      "Nodes: 75\n",
      "Batch Size: 75\n",
      "Nodes: 75\n",
      "Batch Size: 150\n",
      "Nodes: 75\n",
      "Batch Size: 1000\n",
      "Nodes: 75\n",
      "Batch Size: 3000\n",
      "Nodes: 100\n",
      "Batch Size: 75\n",
      "Nodes: 100\n",
      "Batch Size: 150\n",
      "Nodes: 100\n",
      "Batch Size: 1000\n",
      "Nodes: 100\n",
      "Batch Size: 3000\n",
      "[0.334700830921914, 0.3511659086871845, 0.3849761388839855, 13.99775397973148, 0.32377908555174556, 0.3381455890073135, 0.38893143781257267, 14.173928165367805, 0.3187330118035012, 0.33780262716712867, 0.3952287889180863, 14.355393929100698]\n",
      "[0.26467429751417854, 0.2813412135947666, 0.3181679084474759, 202.6442366774898, 0.25653841061838634, 0.2691843667976965, 0.3235778786316311, 203.7303173572469, 0.2493696732280373, 0.2660380635251296, 0.32962905842067425, 210.4055204123752]\n"
     ]
    }
   ],
   "source": [
    "# finding the optimal number of nodes and batches\n",
    "\n",
    "nodes = [50, 75, 100] # use to specify a number of hidden nodes per layer\n",
    "batches = [75, 150, 1000, 3000]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for nod in nodes:\n",
    "    for batch in batches:\n",
    "        print(\"Nodes:\", nod)\n",
    "        print(\"Batch Size:\", batch)\n",
    "        nn = NeuralNetwork(layers=3, nnodes=nod, batchSize=batch, \n",
    "                   activationFn=\"tanh\", lr=0.00001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "        nn.fit(X_std, y_log)\n",
    "        \n",
    "        mae = get_mae(y_log, nn.predict(X_std))\n",
    "        means.append(mae)\n",
    "        mse = get_mse(y_log, nn.predict(X_std))\n",
    "        mses.append(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this, we determined that a neural network with 75 nodes in each layer and with a batch size of 75 is optimal. (the output accidentally got cleared and was too long to run again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1\n",
      "0.33586100446300227\n",
      "0.26972655313800314\n",
      "Number of layers: 2\n",
      "0.33118232169681355\n",
      "0.2657306799331579\n",
      "Number of layers: 3\n",
      "0.31751430615890786\n",
      "0.2498748700475913\n",
      "Number of layers: 4\n"
     ]
    }
   ],
   "source": [
    "# finding the optimal number of layers\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(\"Number of layers:\", i)\n",
    "    nn = NeuralNetwork(layers=i, nnodes=75, batchSize=75, \n",
    "               activationFn=\"tanh\", lr=0.00001, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of layers: 1\n",
    "- mae:0.32878571475904267\n",
    "- mse:0.2653659683032532\n",
    "\n",
    "Number of layers: 2\n",
    "- mae:0.33078621934338914\n",
    "- mse:0.2630391625102294\n",
    "\n",
    "Number of layers: 3\n",
    "- mae:0.32166828746298876\n",
    "- mse:0.25342771172128625\n",
    "\n",
    "Number of layers: 4\n",
    "- mae:0.3290997733116515\n",
    "- mse:0.259221440482325\n",
    "\n",
    "Number of layers: 5\n",
    "- mae:0.34146170976728923\n",
    "- mse:0.2754152768446973"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that a neural network with 3 hidden layers is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding the optimal activation function\n",
    "\n",
    "actives = [\"relu\", \"tanh\"]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for active in actives:\n",
    "    print(\"Activation Function:\", active)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "               activationFn=active, lr=0.000001, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function: relu\n",
    "- mae: 1.839555048410389\n",
    "- mse: 5.905572357965957\n",
    "\n",
    "Activation Function: tanh\n",
    "- mae:0.32166828746298876\n",
    "- mse:0.25342771172128625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a nerual network with tanh as the activation function is the most optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding the optimal learning rate\n",
    "\n",
    "lrs = [0.0001, 0.00001, 0.000001, 0.0000001]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for l in lrs:\n",
    "    print(\"Learning Rate:\", l)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "               activationFn=\"tanh\", lr=l, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(\"MAE:\", mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate: 0.0001\n",
    "- MAE: 0.28629193636873423\n",
    "- MSE: 0.22753831460589047\n",
    "\n",
    "Learning Rate: 1e-05\n",
    "- MAE: 0.3239843514618037\n",
    "- MSE: 0.2535032496097772\n",
    "\n",
    "Learning Rate: 1e-06\n",
    "- MAE: 0.3771263130700\n",
    "- MSE: 0.30886246555594116\n",
    "\n",
    "Learning Rate: 1e-07\n",
    "- MAE: 15.28382880833576\n",
    "- MSE: 235.08928618209882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that a neural network with a learning rate of 0.0001 is the most optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the optimal learning rate type\n",
    "\n",
    "lrstypes = [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]\n",
    "\n",
    "means = []\n",
    "\n",
    "for ltype in lrstypes: \n",
    "    print(\"Learning Type:\", ltype)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "               activationFn=\"tanh\", lr=0.0001, lr_type=ltype, \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y, np.exp(nn.predict(X_std)))\n",
    "    means.append(mae)\n",
    "\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non_log MAES = [1499680.4174009578, 7106315.024163467, 1596500.6056229444, 7104170.881633773]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the best learning rate type is the \"constant\" learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Now, we just need to the the final error with all the optimal tuning found above to determine the final training log MSE and log MAE___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "                   activationFn=\"tanh\", lr=0.0001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "nn.fit(X_train_std, y_train_log)\n",
    "mae = get_mae(y_log, nn.predict(X_std))\n",
    "mse = get_mse(y_log, nn.predict(X_std))\n",
    "print(\"log_mae:\", mae)\n",
    "print(\"log_mse:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation\n",
    "\n",
    "Implementing cross validation to determine the test log MSE and log MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler()]\n",
    "\n",
    "mses = []\n",
    "maes = []\n",
    "preds = []\n",
    "\n",
    "for i in range(len(scalers)):\n",
    "    copies = inds.copy()\n",
    "    test_ind = inds[i]\n",
    "    copies.remove(copies[i])\n",
    "    train_ind = []\n",
    "    for c in copies:\n",
    "        for num in c:\n",
    "            train_ind.append(num)\n",
    "    scaler = scalers[i]\n",
    "    \n",
    "    print(\"Iteration:\", i)\n",
    "    \n",
    "    X_train_std = scaler.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[train_ind])\n",
    "    X_test_std = scaler.transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[test_ind])\n",
    "    y_train = russia_df[\"price_doc\"].iloc[train_ind]\n",
    "    y_test = russia_df[\"price_doc\"].iloc[test_ind]\n",
    "    y_train_log = np.log(y_train)\n",
    "    y_test_log = np.log(y_test)\n",
    "    \n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "                       activationFn=\"tanh\", lr=0.0001, lr_type=\"constant\", \n",
    "                       max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_train_std, y_train_log)\n",
    "    \n",
    "    pred = nn.predict(X_test_std)\n",
    "    preds.append(pred)\n",
    "    mse = get_mse(y_test_log, pred)\n",
    "    mses.append(mse)\n",
    "    mae = get_mae(y_test_log, pred)\n",
    "    maes.append(mae)\n",
    "    print('Mean absolute error: $%0.2f'%(mae))\n",
    "    print('Root mean squared error: $%0.2f'%(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = russia_df[\"price_doc\"].iloc[inds[0]]\n",
    "y2 = russia_df[\"price_doc\"].iloc[inds[1]]\n",
    "y3 = russia_df[\"price_doc\"].iloc[inds[2]]\n",
    "y4 = russia_df[\"price_doc\"].iloc[inds[3]]\n",
    "y5 = russia_df[\"price_doc\"].iloc[inds[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [y1, y2, y3, y4, y5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_log_maes = []\n",
    "for i in range(len(preds)):\n",
    "    mae = get_mae(y[i], np.exp(preds[i]))\n",
    "    non_log_maes.append(mae)\n",
    "    print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(non_log_maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log Mean Squared Error: 0.02424\n",
    "- Log Mean Absolute Error: 0.3039\n",
    "- Mean absolute error: 4,491,396.67 rubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our final result when comparing these two models, we can clearly see that the neural network out-performs the linear regression model, as the log MSE is almost half that of the linear regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
