{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data to Test Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 9, 16]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sq = x ** 2\n",
    "x_sq\n",
    "np.sum(x_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers=None, nodes=None, nnodes=None, \n",
    "                 activations=[], activationFn=\"relu\", batchSize=50, \n",
    "                 lr=.001, lr_type=\"constant\", power_t=.5,\n",
    "                 annealing_rate=.999, max_epoch=200, momentum=.9, \n",
    "                 tol=0.0001, alpha=.0001, shuffle=False, \n",
    "                 early_stopping=False, num_epochs_stop=50):\n",
    "        \n",
    "        if layers != None:\n",
    "            self.layers = layers # total number of hidden layers\n",
    "        else:\n",
    "            self.layers = len(nodes)\n",
    "\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        self.nodes = nodes\n",
    "        if nodes != None:\n",
    "            self.nodes.insert(0, batchSize)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        self.nnodes = nnodes\n",
    "        if nnodes != None:\n",
    "            self.nodes = []\n",
    "            self.nodes.append(batchSize)\n",
    "            for i in range(layers):\n",
    "                self.nodes.append(nnodes)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        self.activations = activations\n",
    "        self.activationFn = activationFn\n",
    "        if activationFn != \"\":\n",
    "            self.activations = [activationFn] * self.layers\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.power_t = power_t\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.mu = momentum\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if early_stopping == False:\n",
    "            self.num_epochs_stop = max_epoch\n",
    "        else:\n",
    "            self.num_epochs_stop = num_epochs_stop\n",
    "    \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        self.epochs = 0\n",
    "                \n",
    "    def validateHyperParams(self):\n",
    "        \n",
    "        if self.layers != (len(self.nodes) - 2):\n",
    "            raise ValueError(\"layers must be equal to the number of hidden layers, got %s.\" % self.layers)\n",
    "        if self.nnodes != None and self.nnodes <= 0:\n",
    "            raise ValueError(\"nnodes must be > 0, got %s.\" % self.nnodes)\n",
    "        if self.lr <= 0 or self.lr > 1:\n",
    "            raise ValueError(\"lr must be in (0, 1], got %s.\" % self.lr)\n",
    "            \n",
    "        if self.lr_type not in [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]:\n",
    "            raise ValueError(\"lr_type is not valid\" % self.lr_type\n",
    "                            + \"\\nAvailable lr types: constant, invscaling, adaptive\")\n",
    "            \n",
    "        if self.max_epoch <= 0:\n",
    "            raise ValueError(\"max_iter must be > 0, got %s.\" % self.max_epoch)\n",
    "               \n",
    "        activation_functions = list(ACTIVATIONS.keys())\n",
    "        if self.activationFn != \"\":\n",
    "            if self.activationFn not in activation_functions:\n",
    "                raise ValueError(\"%s is not an activation function\" % self.activationFn\n",
    "                                + \"\\nAvailable activation functions: relu, leaky_relu, sigmoid, tanh\")\n",
    "    \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            \n",
    "            # Xavier (Glorot) Initialization\n",
    "            if self.activationFn == \"tanh\":\n",
    "                target_variance = 2 / (input_size + output_size)\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # He Initialization\n",
    "            elif self.activationFn == \"relu\":\n",
    "                target_variance = 2 / input_size\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # Random Uniform\n",
    "            else:\n",
    "                w_i = np.random.uniform(-1/np.sqrt(input_size), 1/np.sqrt(input_size))\n",
    "                #w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "    \n",
    "    # returns the weight term for L2 regularization\n",
    "    def get_weight_term(self):\n",
    "        weight_term = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_term = np.sum(self.weights[i] ** 2)\n",
    "        return weight_term\n",
    "        \n",
    "    def forward_pass(self, X_batch, y_batch):\n",
    "        \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(self.layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = 0\n",
    "        # only need predictions once we have fit the data\n",
    "        if isinstance(y_batch, np.ndarray): \n",
    "            l2_loss = squared_loss(y_pred, y_batch) # l2\n",
    "            weight_term = self.get_weight_term()\n",
    "            l2_loss += self.alpha * weight_term # l2 regularization\n",
    "            self.layer_values[self.layers + 1] = l2_loss\n",
    "        \n",
    "        return l2_loss, y_pred\n",
    "    \n",
    "    \n",
    "    def backward_pass(self, y_pred, y_batch):\n",
    "        \n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(y_pred, y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (self.layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        w_t = np.delete(w_t, w_t.shape[1] - 1, 1) # take out the bias\n",
    "        J = np.dot(J, w_t)\n",
    "        zeros = [0] * len(J)\n",
    "        zeros = np.reshape(zeros, (len(J), 1))\n",
    "        J = np.append(J, zeros, axis=1)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(self.layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "            \n",
    "            # jacobian w.r.t. inputs\n",
    "            w_t = self.weights[i - 1].T\n",
    "            w_t = np.delete(w_t, w_t.shape[1] - 1, 1)\n",
    "            J = np.dot(J, w_t)\n",
    "            zeros = [0] * len(J)\n",
    "            zeros = np.reshape(zeros, (len(J), 1))\n",
    "            J = np.append(J, zeros, axis=1)\n",
    "            \n",
    "            \n",
    "        # initialize velocity to 0\n",
    "        if self.epochs == 0 and self.iters == 0:\n",
    "            self.velocity = []\n",
    "            for i in range(len(J_weights)):\n",
    "                n_rows = J_weights[i].shape[0]\n",
    "                n_cols = J_weights[i].shape[1]\n",
    "                vel_i = np.zeros((n_rows, n_cols))\n",
    "                self.velocity.append(vel_i)\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.velocity[i] = self.mu * self.velocity[i] - self.lr * J_weights[i]\n",
    "            self.weights[i] += self.velocity[i]\n",
    "      \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.validateHyperParams()\n",
    "        # convert to numpy arrays\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.to_numpy()\n",
    "            \n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.to_numpy()\n",
    "            \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_train)\n",
    "        ones = np.reshape(ones, (len(X_train), 1))\n",
    "        X_train = np.append(X_train, ones, axis=1)\n",
    "        \n",
    "        # save 10% for validation\n",
    "        val_rows = round(len(X_train) * .1)\n",
    "        X_val = X_train[:val_rows, :]\n",
    "        y_val = y_train[:val_rows]\n",
    "        \n",
    "        X_train = X_train[val_rows:, :]\n",
    "        y_train = y_train[val_rows:]\n",
    "        \n",
    "        # initalize weights on first iteration\n",
    "        M = X_train.shape[1] # M = number of features\n",
    "        self.weights = self.initialize_weights(M)\n",
    "        \n",
    "        best_v_loss = np.inf\n",
    "        n_epoch_no_change = 0 \n",
    "        while (self.epochs < self.max_epoch and n_epoch_no_change <= self.num_epochs_stop):\n",
    "            # ONE EPOCH \n",
    "            last_idx = 0\n",
    "            if self.shuffle == True: # shuffle data after every epoch, if specified \n",
    "                np.random.shuffle(X_train)\n",
    "            while (last_idx < len(X_train)):\n",
    "                first_idx = self.iters * self.batchSize\n",
    "                remaining_rows = len(X_train) - first_idx\n",
    "                last_idx = first_idx + min(self.batchSize, remaining_rows)\n",
    "                X_batch = X_train[first_idx: last_idx, :]\n",
    "                y_batch = y_train[first_idx: last_idx]\n",
    "\n",
    "                loss, y_pred = self.forward_pass(X_batch, y_batch)\n",
    "                self.backward_pass(y_pred, y_batch)\n",
    "                self.iters += 1\n",
    "            \n",
    "            # trainig and validation loss after one epoch\n",
    "            t_loss, y_pred = self.forward_pass(X_train, y_train)\n",
    "            v_loss, y_pred = self.forward_pass(X_val, y_val)\n",
    "            print(\"epoch:\", self.epochs)\n",
    "            print(\"training loss:\", t_loss)\n",
    "            print(\"validation loss:\", v_loss)\n",
    "            \n",
    "            self.iters = 0 # start over, next epoch\n",
    "            self.epochs += 1\n",
    "            \n",
    "            # decrease the learning rate by one of three methods, if specified\n",
    "            if self.lr_type == \"invscaling\":\n",
    "                self.lr = self.lr/pow(self.epochs, self.power_t)\n",
    "            elif self.lr_type == \"annealing\":\n",
    "                self.lr = self.lr * self.annealing_rate\n",
    "            elif self.lr_type == \"adaptive\":\n",
    "                if n_epoch_no_change >= 2: \n",
    "                    self.lr = self.lr/5\n",
    "                \n",
    "            # stops when validation loss doesn't improve for num_epochs_stop\n",
    "            if best_v_loss - v_loss < self.tol: \n",
    "                n_epoch_no_change += 1\n",
    "            else:\n",
    "                n_epoch_no_change = 0\n",
    "            # update best_v_loss\n",
    "            if v_loss < best_v_loss:\n",
    "                best_v_loss = v_loss\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # convert to numpy array\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.to_numpy()\n",
    "        \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_test)\n",
    "        ones = np.reshape(ones, (len(X_test), 1))\n",
    "        X_test = np.append(X_test, ones, axis=1)\n",
    "        \n",
    "        loss, y_pred = self.forward_pass(X_test, None)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Nueral Network on Practice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-62598c1d20c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mactivationFn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    max_epoch=2000, momentum=0.9, early_stopping=False)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_std' is not defined"
     ]
    }
   ],
   "source": [
    "nodes = [100, 50, 100] # use to specify a number of hidden nodes per layer\n",
    "activations = [] # use if you want a diff activationFn per layer\n",
    "\n",
    "nn = NeuralNetwork(layers=3, nnodes=100, batchSize=50, \n",
    "                   activationFn=\"tanh\", lr=.001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=False)\n",
    "nn.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Neural Network on Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "russia_df = pd.read_csv(\"russian_housing.csv\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1))\n",
    "y = russia_df[\"price_doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 50\n",
      "Learning Rate: 1e-05\n",
      "epoch: 0\n",
      "training loss: 42277064794920.83\n",
      "validation loss: 53312912796533.36\n",
      "epoch: 1\n",
      "training loss: 39322610335220.64\n",
      "validation loss: 46051527583366.95\n",
      "epoch: 2\n",
      "training loss: 36349980359871.586\n",
      "validation loss: 38941246125926.1\n",
      "epoch: 3\n",
      "training loss: 35589172107292.664\n",
      "validation loss: 39123982245002.38\n",
      "epoch: 4\n",
      "training loss: 35070174052697.543\n",
      "validation loss: 39845954259681.09\n",
      "epoch: 5\n",
      "training loss: 34636499933404.15\n",
      "validation loss: 40460810732610.9\n",
      "epoch: 6\n",
      "training loss: 34274597007680.26\n",
      "validation loss: 40992627252081.43\n",
      "epoch: 7\n",
      "training loss: 33970600262322.84\n",
      "validation loss: 41449313544701.37\n",
      "epoch: 8\n",
      "training loss: 33713329260458.84\n",
      "validation loss: 41837627363459.67\n",
      "epoch: 9\n",
      "training loss: 33493920161713.44\n",
      "validation loss: 42164107651213.23\n",
      "epoch: 10\n",
      "training loss: 33305342365939.69\n",
      "validation loss: 42435019965959.766\n",
      "epoch: 11\n",
      "training loss: 33142006506115.28\n",
      "validation loss: 42656274568070.625\n",
      "epoch: 12\n",
      "training loss: 32999456822103.08\n",
      "validation loss: 42833376159807.53\n",
      "epoch: 13\n",
      "training loss: 32874130334087.895\n",
      "validation loss: 42971400207523.96\n",
      "epoch: 14\n",
      "training loss: 32763168232569.957\n",
      "validation loss: 43074988351862.625\n",
      "epoch: 15\n",
      "training loss: 32664268052321.48\n",
      "validation loss: 43148357019984.56\n",
      "epoch: 16\n",
      "training loss: 32575567710966.082\n",
      "validation loss: 43195314839996.34\n",
      "epoch: 17\n",
      "training loss: 32495554455129.633\n",
      "validation loss: 43219285609448.3\n",
      "epoch: 18\n",
      "training loss: 32422993286415.867\n",
      "validation loss: 43223334446173.87\n",
      "epoch: 19\n",
      "training loss: 32356870631300.47\n",
      "validation loss: 43210195413116.12\n",
      "epoch: 20\n",
      "training loss: 32296349948038.78\n",
      "validation loss: 43182299408535.53\n",
      "epoch: 21\n",
      "training loss: 32240736687956.703\n",
      "validation loss: 43141801487377.02\n",
      "epoch: 22\n",
      "training loss: 32189450593286.918\n",
      "validation loss: 43090607058138.34\n",
      "epoch: 23\n",
      "training loss: 32142003754252.387\n",
      "validation loss: 43030396605112.63\n",
      "epoch: 24\n",
      "training loss: 32097983191816.08\n",
      "validation loss: 42962648735922.234\n",
      "epoch: 25\n",
      "training loss: 32057037000773.527\n",
      "validation loss: 42888661462312.71\n",
      "epoch: 26\n",
      "training loss: 32018863297301.082\n",
      "validation loss: 42809571698597.164\n",
      "epoch: 27\n",
      "training loss: 31983201378646.746\n",
      "validation loss: 42726373014864.484\n",
      "epoch: 28\n",
      "training loss: 31949824630459.562\n",
      "validation loss: 42639931717160.79\n",
      "epoch: 29\n",
      "training loss: 31918534817163.082\n",
      "validation loss: 42551001348940.52\n",
      "epoch: 30\n",
      "training loss: 31889157468919.473\n",
      "validation loss: 42460235720680.27\n",
      "epoch: 31\n",
      "training loss: 31861538139881.375\n",
      "validation loss: 42368200580306.06\n",
      "epoch: 32\n",
      "training loss: 31835539360312.24\n",
      "validation loss: 42275384038012.914\n",
      "epoch: 33\n",
      "training loss: 31811038142678.203\n",
      "validation loss: 42182205856647.96\n",
      "epoch: 34\n",
      "training loss: 31787923931239.934\n",
      "validation loss: 42089025714204.695\n",
      "epoch: 35\n",
      "training loss: 31766096907768.28\n",
      "validation loss: 41996150538958.85\n",
      "epoch: 36\n",
      "training loss: 31745466584151.89\n",
      "validation loss: 41903841010963.555\n",
      "epoch: 37\n",
      "training loss: 31725950626934.92\n",
      "validation loss: 41812317316451.37\n",
      "epoch: 38\n",
      "training loss: 31707473870058.176\n",
      "validation loss: 41721764234461.23\n",
      "epoch: 39\n",
      "training loss: 31689967480934.703\n",
      "validation loss: 41632335627938.16\n",
      "epoch: 40\n",
      "training loss: 31673368251983.69\n",
      "validation loss: 41544158404776.91\n",
      "epoch: 41\n",
      "training loss: 31657617995275.656\n",
      "validation loss: 41457336007887.59\n",
      "epoch: 42\n",
      "training loss: 31642663022321.06\n",
      "validation loss: 41371951487403.805\n",
      "epoch: 43\n",
      "training loss: 31628453694509.37\n",
      "validation loss: 41288070202649.664\n",
      "epoch: 44\n",
      "training loss: 31614944032468.44\n",
      "validation loss: 41205742196440.51\n",
      "epoch: 45\n",
      "training loss: 31602091374815.535\n",
      "validation loss: 41125004279697.67\n",
      "epoch: 46\n",
      "training loss: 31589856078529.902\n",
      "validation loss: 41045881860195.3\n",
      "epoch: 47\n",
      "training loss: 31578201254584.67\n",
      "validation loss: 40968390545500.42\n",
      "epoch: 48\n",
      "training loss: 31567092533607.04\n",
      "validation loss: 40892537546789.836\n",
      "epoch: 49\n",
      "training loss: 31556497857246.047\n",
      "validation loss: 40818322907198.34\n",
      "epoch: 50\n",
      "training loss: 31546387291663.332\n",
      "validation loss: 40745740575645.086\n",
      "epoch: 51\n",
      "training loss: 31536732860158.81\n",
      "validation loss: 40674779344668.85\n",
      "epoch: 52\n",
      "training loss: 31527508392428.367\n",
      "validation loss: 40605423668651.06\n",
      "epoch: 53\n",
      "training loss: 31518689388347.375\n",
      "validation loss: 40537654376891.484\n",
      "Mean absolute error: $6492759066.52\n",
      "Nodes: 50\n",
      "Learning Rate: 1e-06\n",
      "epoch: 0\n",
      "training loss: 35025238446848.67\n",
      "validation loss: 30889837345874.05\n",
      "epoch: 1\n",
      "training loss: 33648145115359.65\n",
      "validation loss: 28613617959715.29\n",
      "epoch: 2\n",
      "training loss: 32696266838049.27\n",
      "validation loss: 27071897141279.15\n",
      "epoch: 3\n",
      "training loss: 32050637011385.707\n",
      "validation loss: 25663045911292.83\n",
      "epoch: 4\n",
      "training loss: 31634274953327.17\n",
      "validation loss: 24814504304504.168\n",
      "epoch: 5\n",
      "training loss: 31355443661124.508\n",
      "validation loss: 24199202975022.504\n",
      "epoch: 6\n",
      "training loss: 31165097022613.38\n",
      "validation loss: 23754992692987.62\n",
      "epoch: 7\n",
      "training loss: 31031880334916.887\n",
      "validation loss: 23437603601339.25\n",
      "epoch: 8\n",
      "training loss: 30935742867710.95\n",
      "validation loss: 23215077192603.336\n",
      "epoch: 9\n",
      "training loss: 30863855472406.07\n",
      "validation loss: 23064037883173.066\n",
      "epoch: 10\n",
      "training loss: 30808001812405.695\n",
      "validation loss: 22967172749644.188\n",
      "epoch: 11\n",
      "training loss: 30762911584358.215\n",
      "validation loss: 22911509678439.1\n",
      "epoch: 12\n",
      "training loss: 30725195817418.984\n",
      "validation loss: 22887227049238.36\n",
      "epoch: 13\n",
      "training loss: 30691671794738.31\n",
      "validation loss: 23082053178626.664\n",
      "epoch: 14\n",
      "training loss: 30663030044700.13\n",
      "validation loss: 23100738371178.33\n",
      "epoch: 15\n",
      "training loss: 30637227147221.67\n",
      "validation loss: 23132896993436.805\n",
      "epoch: 16\n",
      "training loss: 30613642251659.77\n",
      "validation loss: 23175063652353.85\n",
      "epoch: 17\n",
      "training loss: 30591857024325.59\n",
      "validation loss: 23224529208015.668\n",
      "epoch: 18\n",
      "training loss: 30571583126921.64\n",
      "validation loss: 23279170401779.7\n",
      "epoch: 19\n",
      "training loss: 30552615993764.453\n",
      "validation loss: 23337320925354.113\n",
      "epoch: 20\n",
      "training loss: 30534805378060.633\n",
      "validation loss: 23397672852063.832\n",
      "epoch: 21\n",
      "training loss: 30518036584116.594\n",
      "validation loss: 23459200604100.434\n",
      "epoch: 22\n",
      "training loss: 30502218504255.3\n",
      "validation loss: 23521101850855.77\n",
      "epoch: 23\n",
      "training loss: 30487275985409.94\n",
      "validation loss: 23582751268544.605\n",
      "epoch: 24\n",
      "training loss: 30473144948120.574\n",
      "validation loss: 23643664165944.94\n",
      "epoch: 25\n",
      "training loss: 30459769253601.133\n",
      "validation loss: 23703467743593.21\n",
      "epoch: 26\n",
      "training loss: 30447098679951.184\n",
      "validation loss: 23761878302347.43\n",
      "epoch: 27\n",
      "training loss: 30435087601515.836\n",
      "validation loss: 23818683117388.58\n",
      "epoch: 28\n",
      "training loss: 30423694113779.46\n",
      "validation loss: 23873725989536.105\n",
      "epoch: 29\n",
      "training loss: 30412879440514.867\n",
      "validation loss: 23926895707179.426\n",
      "epoch: 30\n",
      "training loss: 30402607520009.16\n",
      "validation loss: 23978116819827.41\n",
      "epoch: 31\n",
      "training loss: 30392844705163.75\n",
      "validation loss: 24027342252543.973\n",
      "epoch: 32\n",
      "training loss: 30383559536512.03\n",
      "validation loss: 24074547389609.414\n",
      "epoch: 33\n",
      "training loss: 30374722562327.316\n",
      "validation loss: 24119725332780.797\n",
      "epoch: 34\n",
      "training loss: 30366306189795.027\n",
      "validation loss: 24162883099904.918\n",
      "epoch: 35\n",
      "training loss: 30358284557042.074\n",
      "validation loss: 24204038577110.2\n",
      "epoch: 36\n",
      "training loss: 30350633419843.125\n",
      "validation loss: 24243218075452.414\n",
      "epoch: 37\n",
      "training loss: 30343330049028.58\n",
      "validation loss: 24280454372664.375\n",
      "epoch: 38\n",
      "training loss: 30336353136172.21\n",
      "validation loss: 24315785144440.207\n",
      "epoch: 39\n",
      "training loss: 30329682705937.312\n",
      "validation loss: 24349251708623.402\n",
      "epoch: 40\n",
      "training loss: 30323300034227.734\n",
      "validation loss: 24380898020840.363\n",
      "epoch: 41\n",
      "training loss: 30317187571311.64\n",
      "validation loss: 24410769872184.77\n",
      "epoch: 42\n",
      "training loss: 30311328869587.836\n",
      "validation loss: 24438914249359.36\n",
      "epoch: 43\n",
      "training loss: 30305708515601.496\n",
      "validation loss: 24465378825403.652\n",
      "epoch: 44\n",
      "training loss: 30300312066053.707\n",
      "validation loss: 24490211555422.258\n",
      "epoch: 45\n",
      "training loss: 30295125987569.844\n",
      "validation loss: 24513460356741.676\n",
      "epoch: 46\n",
      "training loss: 30290137600032.535\n",
      "validation loss: 24535172856959.0\n",
      "epoch: 47\n",
      "training loss: 30285335023292.3\n",
      "validation loss: 24555396196581.94\n",
      "epoch: 48\n",
      "training loss: 30280707127070.094\n",
      "validation loss: 24574176875562.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49\n",
      "training loss: 30276243483891.332\n",
      "validation loss: 24591560635123.016\n",
      "epoch: 50\n",
      "training loss: 30271934324900.453\n",
      "validation loss: 24607592367945.85\n",
      "epoch: 51\n",
      "training loss: 30267770498313.85\n",
      "validation loss: 24622316051156.543\n",
      "epoch: 52\n",
      "training loss: 30263743430547.707\n",
      "validation loss: 24635774697635.96\n",
      "epoch: 53\n",
      "training loss: 30259845089612.113\n",
      "validation loss: 24648010321999.473\n",
      "epoch: 54\n",
      "training loss: 30256067950902.75\n",
      "validation loss: 24659063918399.402\n",
      "epoch: 55\n",
      "training loss: 30252404965079.863\n",
      "validation loss: 24668975447739.41\n",
      "epoch: 56\n",
      "training loss: 30248849527885.055\n",
      "validation loss: 24677783832446.98\n",
      "epoch: 57\n",
      "training loss: 30245395451994.863\n",
      "validation loss: 24685526957299.863\n",
      "epoch: 58\n",
      "training loss: 30242036940502.86\n",
      "validation loss: 24692241675010.07\n",
      "epoch: 59\n",
      "training loss: 30238768562125.33\n",
      "validation loss: 24697963815645.316\n",
      "epoch: 60\n",
      "training loss: 30235585228045.47\n",
      "validation loss: 24702728199042.945\n",
      "epoch: 61\n",
      "training loss: 30232482170058.95\n",
      "validation loss: 24706568649545.17\n",
      "epoch: 62\n",
      "training loss: 30229454920282.477\n",
      "validation loss: 24709518012599.695\n",
      "epoch: 63\n",
      "training loss: 30226499291989.94\n",
      "validation loss: 24711608172714.336\n",
      "Mean absolute error: $6097805846.06\n",
      "Nodes: 50\n",
      "Learning Rate: 1e-07\n",
      "epoch: 0\n",
      "training loss: 36960745847027.3\n",
      "validation loss: 33346408215117.566\n",
      "epoch: 1\n",
      "training loss: 36824663916505.45\n",
      "validation loss: 32944669430814.457\n",
      "epoch: 2\n",
      "training loss: 36677782529321.08\n",
      "validation loss: 32743322864528.297\n",
      "epoch: 3\n",
      "training loss: 36464467075428.76\n",
      "validation loss: 32247065921213.344\n",
      "epoch: 4\n",
      "training loss: 36319506791693.19\n",
      "validation loss: 31941640944081.95\n",
      "epoch: 5\n",
      "training loss: 36120231611332.78\n",
      "validation loss: 31901132354007.86\n",
      "epoch: 6\n",
      "training loss: 36019374742766.44\n",
      "validation loss: 31799073980504.504\n",
      "epoch: 7\n",
      "training loss: 35819440804117.19\n",
      "validation loss: 31304957327241.05\n",
      "epoch: 8\n",
      "training loss: 35633545300904.27\n",
      "validation loss: 30845755932280.094\n",
      "epoch: 9\n",
      "training loss: 35505384787530.32\n",
      "validation loss: 30794409737291.816\n",
      "epoch: 10\n",
      "training loss: 35385182807152.83\n",
      "validation loss: 30780538547682.812\n",
      "epoch: 11\n",
      "training loss: 35163879018467.754\n",
      "validation loss: 30183744861094.84\n",
      "epoch: 12\n",
      "training loss: 35000811662291.453\n",
      "validation loss: 29926786811013.36\n",
      "epoch: 13\n",
      "training loss: 34872682697256.09\n",
      "validation loss: 29975414309569.285\n",
      "epoch: 14\n",
      "training loss: 34699393080457.75\n",
      "validation loss: 29033429190143.414\n",
      "epoch: 15\n",
      "training loss: 34570497958528.133\n",
      "validation loss: 28929135922267.94\n",
      "epoch: 16\n",
      "training loss: 34445846371248.84\n",
      "validation loss: 28634976249122.05\n",
      "epoch: 17\n",
      "training loss: 34302685051182.746\n",
      "validation loss: 28187153410852.395\n",
      "epoch: 18\n",
      "training loss: 34178255570608.98\n",
      "validation loss: 27925773014471.277\n",
      "epoch: 19\n",
      "training loss: 34062002632941.164\n",
      "validation loss: 27673180054874.234\n",
      "epoch: 20\n",
      "training loss: 33951004416942.418\n",
      "validation loss: 27429242920028.58\n",
      "epoch: 21\n",
      "training loss: 33845016017526.492\n",
      "validation loss: 27193635759752.23\n",
      "epoch: 22\n",
      "training loss: 33743803957697.215\n",
      "validation loss: 26966046106841.582\n",
      "epoch: 23\n",
      "training loss: 33647145655191.113\n",
      "validation loss: 26746174294694.6\n",
      "epoch: 24\n",
      "training loss: 33554828914103.14\n",
      "validation loss: 26533732901415.2\n",
      "epoch: 25\n",
      "training loss: 33466651440232.98\n",
      "validation loss: 26328446218894.04\n",
      "epoch: 26\n",
      "training loss: 33382420379040.406\n",
      "validation loss: 26130049745722.984\n",
      "epoch: 27\n",
      "training loss: 33301951874899.35\n",
      "validation loss: 25938289702853.574\n",
      "epoch: 28\n",
      "training loss: 33222644998209.02\n",
      "validation loss: 25753162523408.176\n",
      "epoch: 29\n",
      "training loss: 33149148403482.64\n",
      "validation loss: 25574242219084.426\n",
      "epoch: 30\n",
      "training loss: 33078908613610.062\n",
      "validation loss: 25401247725479.8\n",
      "epoch: 31\n",
      "training loss: 33011773859065.92\n",
      "validation loss: 25233964146288.59\n",
      "epoch: 32\n",
      "training loss: 32936954458299.977\n",
      "validation loss: 25054225046599.277\n",
      "epoch: 33\n",
      "training loss: 32874919939447.426\n",
      "validation loss: 24900360215587.1\n",
      "epoch: 34\n",
      "training loss: 32816006979369.68\n",
      "validation loss: 24759517508464.258\n",
      "epoch: 35\n",
      "training loss: 32758050132403.453\n",
      "validation loss: 24550905775442.65\n",
      "epoch: 36\n",
      "training loss: 32703804927136.49\n",
      "validation loss: 24406908618269.69\n",
      "epoch: 37\n",
      "training loss: 32651982352271.82\n",
      "validation loss: 24267610323255.457\n",
      "epoch: 38\n",
      "training loss: 32602468652449.31\n",
      "validation loss: 24132843115302.254\n",
      "epoch: 39\n",
      "training loss: 32555155423308.047\n",
      "validation loss: 24002445854541.406\n",
      "epoch: 40\n",
      "training loss: 32509939359786.1\n",
      "validation loss: 23876263753764.76\n",
      "epoch: 41\n",
      "training loss: 32466722016259.11\n",
      "validation loss: 23754148108453.43\n",
      "epoch: 42\n",
      "training loss: 32425409577960.746\n",
      "validation loss: 23635956038827.65\n",
      "epoch: 43\n",
      "training loss: 32385912643154.332\n",
      "validation loss: 23521550243368.07\n",
      "epoch: 44\n",
      "training loss: 32348146015549.992\n",
      "validation loss: 23410798763284.688\n",
      "epoch: 45\n",
      "training loss: 32312028506485.363\n",
      "validation loss: 23303574757433.695\n",
      "epoch: 46\n",
      "training loss: 32277482746409.484\n",
      "validation loss: 23199756287206.703\n",
      "epoch: 47\n",
      "training loss: 32244435005202.816\n",
      "validation loss: 23099226110945.004\n",
      "epoch: 48\n",
      "training loss: 32212833445880.1\n",
      "validation loss: 22998025185587.395\n",
      "epoch: 49\n",
      "training loss: 32177282014815.902\n",
      "validation loss: 22839789081636.574\n",
      "epoch: 50\n",
      "training loss: 32148297406469.29\n",
      "validation loss: 22747807527271.832\n",
      "epoch: 51\n",
      "training loss: 32120552028911.56\n",
      "validation loss: 22658714050312.2\n",
      "epoch: 52\n",
      "training loss: 32093987641923.844\n",
      "validation loss: 22572410981151.434\n",
      "epoch: 53\n",
      "training loss: 32068548752169.902\n",
      "validation loss: 22488804343476.355\n",
      "epoch: 54\n",
      "training loss: 32044182483639.41\n",
      "validation loss: 22407803701607.883\n",
      "epoch: 55\n",
      "training loss: 32020838454201.156\n",
      "validation loss: 22329322014534.504\n",
      "epoch: 56\n",
      "training loss: 31998468657978.176\n",
      "validation loss: 22253275496334.715\n",
      "epoch: 57\n",
      "training loss: 31977027353270.285\n",
      "validation loss: 22179583482698.5\n",
      "epoch: 58\n",
      "training loss: 31956470955762.27\n",
      "validation loss: 22108168303271.906\n",
      "epoch: 59\n",
      "training loss: 31936757936768.375\n",
      "validation loss: 22038955159561.008\n",
      "epoch: 60\n",
      "training loss: 31917848726276.008\n",
      "validation loss: 21971872008143.945\n",
      "epoch: 61\n",
      "training loss: 31899705620561.887\n",
      "validation loss: 21906849448951.418\n",
      "epoch: 62\n",
      "training loss: 31882292694165.27\n",
      "validation loss: 21843820618386.836\n",
      "epoch: 63\n",
      "training loss: 31865575716012.562\n",
      "validation loss: 21782721087068.004\n",
      "epoch: 64\n",
      "training loss: 31849522069497.805\n",
      "validation loss: 21723488761982.355\n",
      "epoch: 65\n",
      "training loss: 31834100676332.336\n",
      "validation loss: 21666063792857.06\n",
      "epoch: 66\n",
      "training loss: 31819281923985.73\n",
      "validation loss: 21610388482554.824\n",
      "epoch: 67\n",
      "training loss: 31805037596549.043\n",
      "validation loss: 21556407201314.51\n",
      "epoch: 68\n",
      "training loss: 31791340808858.44\n",
      "validation loss: 21504066304664.44\n",
      "epoch: 69\n",
      "training loss: 31778165943726.07\n",
      "validation loss: 21453314054843.715\n",
      "epoch: 70\n",
      "training loss: 31765488592130.95\n",
      "validation loss: 21404100545574.85\n",
      "epoch: 71\n",
      "training loss: 31753285496231.01\n",
      "validation loss: 21356377630037.918\n",
      "epoch: 72\n",
      "training loss: 31741534495062.67\n",
      "validation loss: 21310098851903.395\n",
      "epoch: 73\n",
      "training loss: 31730214472801.324\n",
      "validation loss: 21265219379287.45\n",
      "epoch: 74\n",
      "training loss: 31719305309462.273\n",
      "validation loss: 21221695941499.54\n",
      "epoch: 75\n",
      "training loss: 31708787833926.707\n",
      "validation loss: 21179486768458.402\n",
      "epoch: 76\n",
      "training loss: 31698643779183.113\n",
      "validation loss: 21138551532657.742\n",
      "epoch: 77\n",
      "training loss: 31688855739679.715\n",
      "validation loss: 21098851293568.82\n",
      "epoch: 78\n",
      "training loss: 31679407130688.434\n",
      "validation loss: 21060348444372.117\n",
      "epoch: 79\n",
      "training loss: 31670282149585.28\n",
      "validation loss: 21023006660914.84\n",
      "epoch: 80\n",
      "training loss: 31661465738957.15\n",
      "validation loss: 20986790852796.418\n",
      "epoch: 81\n",
      "training loss: 31652943551448.492\n",
      "validation loss: 20951667116487.91\n",
      "epoch: 82\n",
      "training loss: 31644701916266.1\n",
      "validation loss: 20917602690396.004\n",
      "epoch: 83\n",
      "training loss: 31636727807263.758\n",
      "validation loss: 20884565911786.105\n",
      "epoch: 84\n",
      "training loss: 31629008812531.98\n",
      "validation loss: 20852526175482.973\n",
      "epoch: 85\n",
      "training loss: 31621533105422.49\n",
      "validation loss: 20821453894271.023\n",
      "epoch: 86\n",
      "training loss: 31614289416938.887\n",
      "validation loss: 20791320460920.125\n",
      "epoch: 87\n",
      "training loss: 31607267009429.844\n",
      "validation loss: 20762098211765.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 88\n",
      "training loss: 31600455651522.992\n",
      "validation loss: 20733760391775.543\n",
      "epoch: 89\n",
      "training loss: 31593845594241.254\n",
      "validation loss: 20706281121038.85\n",
      "epoch: 90\n",
      "training loss: 31587427548245.438\n",
      "validation loss: 20679635362615.188\n",
      "epoch: 91\n",
      "training loss: 31581192662150.402\n",
      "validation loss: 20653798891685.01\n",
      "epoch: 92\n",
      "training loss: 31575132501864.098\n",
      "validation loss: 20628748265945.137\n",
      "epoch: 93\n",
      "training loss: 31569239030900.87\n",
      "validation loss: 20604460797195.336\n",
      "epoch: 94\n",
      "training loss: 31563504591623.55\n",
      "validation loss: 20580914524064.92\n",
      "epoch: 95\n",
      "training loss: 31557921887370.332\n",
      "validation loss: 20558088185830.094\n",
      "epoch: 96\n",
      "training loss: 31552483965424.55\n",
      "validation loss: 20535961197275.305\n",
      "epoch: 97\n",
      "training loss: 31547184200787.97\n",
      "validation loss: 20514513624553.844\n",
      "epoch: 98\n",
      "training loss: 31542016280719.22\n",
      "validation loss: 20493726162004.96\n",
      "epoch: 99\n",
      "training loss: 31536974190001.85\n",
      "validation loss: 20473580109886.66\n",
      "epoch: 100\n",
      "training loss: 31532052196906.992\n",
      "validation loss: 20454057352985.19\n",
      "epoch: 101\n",
      "training loss: 31527244839818.414\n",
      "validation loss: 20435140340063.965\n",
      "epoch: 102\n",
      "training loss: 31522546914488.273\n",
      "validation loss: 20416812064116.277\n",
      "epoch: 103\n",
      "training loss: 31517953461894.258\n",
      "validation loss: 20399056043387.9\n",
      "epoch: 104\n",
      "training loss: 31513459756669.234\n",
      "validation loss: 20381856303136.914\n",
      "epoch: 105\n",
      "training loss: 31509061296077.016\n",
      "validation loss: 20365197358099.88\n",
      "epoch: 106\n",
      "training loss: 31504753789507.844\n",
      "validation loss: 20349064195634.523\n",
      "epoch: 107\n",
      "training loss: 31500533148469.703\n",
      "validation loss: 20333442259510.67\n",
      "epoch: 108\n",
      "training loss: 31496395477051.457\n",
      "validation loss: 20318317434322.25\n",
      "epoch: 109\n",
      "training loss: 31492337062836.156\n",
      "validation loss: 20303676030494.492\n",
      "epoch: 110\n",
      "training loss: 31488354368242.895\n",
      "validation loss: 20289504769861.574\n",
      "epoch: 111\n",
      "training loss: 31484444022276.918\n",
      "validation loss: 20275790771790.945\n",
      "epoch: 112\n",
      "training loss: 31480602812669.324\n",
      "validation loss: 20262521539831.742\n",
      "epoch: 113\n",
      "training loss: 31476827678387.074\n",
      "validation loss: 20249684948865.715\n",
      "epoch: 114\n",
      "training loss: 31473115702496.83\n",
      "validation loss: 20237269232739.824\n",
      "epoch: 115\n",
      "training loss: 31469464105365.113\n",
      "validation loss: 20225262972360.844\n",
      "epoch: 116\n",
      "training loss: 31465870238179.363\n",
      "validation loss: 20213655084232.926\n",
      "epoch: 117\n",
      "training loss: 31462331576774.65\n",
      "validation loss: 20202434809420.184\n",
      "epoch: 118\n",
      "training loss: 31458845715751.73\n",
      "validation loss: 20191591702916.797\n",
      "epoch: 119\n",
      "training loss: 31455410362872.312\n",
      "validation loss: 20181115623408.203\n",
      "epoch: 120\n",
      "training loss: 31452023333719.06\n",
      "validation loss: 20170996723407.47\n",
      "epoch: 121\n",
      "training loss: 31448682546607.42\n",
      "validation loss: 20161225439751.75\n",
      "epoch: 122\n",
      "training loss: 31445386017737.348\n",
      "validation loss: 20151792484444.242\n",
      "epoch: 123\n",
      "training loss: 31442131856574.016\n",
      "validation loss: 20142688835827.836\n",
      "epoch: 124\n",
      "training loss: 31438918261446.367\n",
      "validation loss: 20133905730077.105\n",
      "epoch: 125\n",
      "training loss: 31435743515353.22\n",
      "validation loss: 20125434652996.05\n",
      "epoch: 126\n",
      "training loss: 31432605981967.438\n",
      "validation loss: 20117267332109.285\n",
      "epoch: 127\n",
      "training loss: 31429504101828.523\n",
      "validation loss: 20109395729035.113\n",
      "epoch: 128\n",
      "training loss: 31426436388714.88\n",
      "validation loss: 20101812032129.35\n",
      "epoch: 129\n",
      "training loss: 31423401426187.164\n",
      "validation loss: 20094508649389.17\n",
      "epoch: 130\n",
      "training loss: 31420397864294.887\n",
      "validation loss: 20087478201606.785\n",
      "epoch: 131\n",
      "training loss: 31417424416438.332\n",
      "validation loss: 20080713515763.22\n",
      "epoch: 132\n",
      "training loss: 31414479856378.52\n",
      "validation loss: 20074207618652.707\n",
      "epoch: 133\n",
      "training loss: 31411563015388.26\n",
      "validation loss: 20067953730728.855\n",
      "epoch: 134\n",
      "training loss: 31408672779537.707\n",
      "validation loss: 20061945260163.863\n",
      "epoch: 135\n",
      "training loss: 31405808087107.94\n",
      "validation loss: 20056175797112.703\n",
      "epoch: 136\n",
      "training loss: 31402967926126.645\n",
      "validation loss: 20050639108174.2\n",
      "epoch: 137\n",
      "training loss: 31400151332019.965\n",
      "validation loss: 20045329131041.62\n",
      "epoch: 138\n",
      "training loss: 31397357385375.305\n",
      "validation loss: 20040239969335.43\n",
      "epoch: 139\n",
      "training loss: 31394585209809.64\n",
      "validation loss: 20035365887611.3\n",
      "epoch: 140\n",
      "training loss: 31391833969938.43\n",
      "validation loss: 20030701306536.715\n",
      "epoch: 141\n",
      "training loss: 31389102869440.383\n",
      "validation loss: 20026240798229.82\n",
      "epoch: 142\n",
      "training loss: 31386391149213.516\n",
      "validation loss: 20021979081754.383\n",
      "epoch: 143\n",
      "training loss: 31383698085618.28\n",
      "validation loss: 20017911018764.95\n",
      "epoch: 144\n",
      "training loss: 31381022988803.637\n",
      "validation loss: 20014031609296.75\n",
      "epoch: 145\n",
      "training loss: 31378365201112.023\n",
      "validation loss: 20010335987694.715\n",
      "epoch: 146\n",
      "training loss: 31375724095559.746\n",
      "validation loss: 20006819418676.67\n",
      "epoch: 147\n",
      "training loss: 31373099074388.867\n",
      "validation loss: 20003477293525.58\n",
      "epoch: 148\n",
      "training loss: 31370489567687.74\n",
      "validation loss: 20000305126406.19\n",
      "epoch: 149\n",
      "training loss: 31367895032076.383\n",
      "validation loss: 19997298550801.473\n",
      "epoch: 150\n",
      "training loss: 31365314949454.11\n",
      "validation loss: 19994453316064.445\n",
      "epoch: 151\n",
      "training loss: 31362748825806.125\n",
      "validation loss: 19991765284081.242\n",
      "epoch: 152\n",
      "training loss: 31360196190066.473\n",
      "validation loss: 19989230426041.375\n",
      "epoch: 153\n",
      "training loss: 31357656593034.766\n",
      "validation loss: 19986844819311.27\n",
      "epoch: 154\n",
      "training loss: 31355129606343.93\n",
      "validation loss: 19984604644407.453\n",
      "epoch: 155\n",
      "training loss: 31352614821476.742\n",
      "validation loss: 19982506182065.684\n",
      "epoch: 156\n",
      "training loss: 31350111848828.754\n",
      "validation loss: 19980545810402.773\n",
      "epoch: 157\n",
      "training loss: 31347620316815.508\n",
      "validation loss: 19978720002167.645\n",
      "epoch: 158\n",
      "training loss: 31345139871021.785\n",
      "validation loss: 19977025322078.58\n",
      "epoch: 159\n",
      "training loss: 31342670173391.17\n",
      "validation loss: 19975458424243.54\n",
      "epoch: 160\n",
      "training loss: 31340210901453.777\n",
      "validation loss: 19974016049660.71\n",
      "epoch: 161\n",
      "training loss: 31337761747590.457\n",
      "validation loss: 19972695023796.426\n",
      "epoch: 162\n",
      "training loss: 31335322418331.758\n",
      "validation loss: 19971492254237.742\n",
      "epoch: 163\n",
      "training loss: 31332892633689.977\n",
      "validation loss: 19970404728417.203\n",
      "epoch: 164\n",
      "training loss: 31330472126522.746\n",
      "validation loss: 19969429511407.09\n",
      "epoch: 165\n",
      "training loss: 31328060641926.63\n",
      "validation loss: 19968563743781.02\n",
      "epoch: 166\n",
      "training loss: 31325657936659.47\n",
      "validation loss: 19967804639540.375\n",
      "epoch: 167\n",
      "training loss: 31323263778589.895\n",
      "validation loss: 19967149484103.46\n",
      "epoch: 168\n",
      "training loss: 31320877946172.836\n",
      "validation loss: 19966595632355.2\n",
      "epoch: 169\n",
      "training loss: 31318500227949.918\n",
      "validation loss: 19966140506755.445\n",
      "epoch: 170\n",
      "training loss: 31316130422073.27\n",
      "validation loss: 19965781595503.723\n",
      "epoch: 171\n",
      "training loss: 31313768335852.008\n",
      "validation loss: 19965516450758.7\n",
      "epoch: 172\n",
      "training loss: 31311413785319.965\n",
      "validation loss: 19965342686910.496\n",
      "epoch: 173\n",
      "training loss: 31309066594823.97\n",
      "validation loss: 19965257978904.004\n",
      "epoch: 174\n",
      "training loss: 31306726596631.504\n",
      "validation loss: 19965260060611.66\n",
      "epoch: 175\n",
      "training loss: 31304393630556.918\n",
      "validation loss: 19965346723253.895\n",
      "epoch: 176\n",
      "training loss: 31302067543605.28\n",
      "validation loss: 19965515813865.855\n",
      "epoch: 177\n",
      "training loss: 31299748189633.08\n",
      "validation loss: 19965765233808.72\n",
      "epoch: 178\n",
      "training loss: 31297435429024.883\n",
      "validation loss: 19966092937324.277\n",
      "epoch: 179\n",
      "training loss: 31295129128385.418\n",
      "validation loss: 19966496930131.31\n",
      "epoch: 180\n",
      "training loss: 31292829160245.965\n",
      "validation loss: 19966975268062.445\n",
      "epoch: 181\n",
      "training loss: 31290535402784.906\n",
      "validation loss: 19967526055740.168\n",
      "epoch: 182\n",
      "training loss: 31288247739561.23\n",
      "validation loss: 19968147445290.746\n",
      "epoch: 183\n",
      "training loss: 31285966059260.77\n",
      "validation loss: 19968837635094.88\n",
      "epoch: 184\n",
      "training loss: 31283690255454.375\n",
      "validation loss: 19969594868573.816\n",
      "epoch: 185\n",
      "training loss: 31281420226367.543\n",
      "validation loss: 19970417433009.977\n",
      "epoch: 186\n",
      "training loss: 31279155874660.88\n",
      "validation loss: 19971303658400.805\n",
      "epoch: 187\n",
      "training loss: 31276897107221.016\n",
      "validation loss: 19972251916345.004\n",
      "epoch: 188\n",
      "training loss: 31274643834961.344\n",
      "validation loss: 19973260618959.945\n",
      "epoch: 189\n",
      "training loss: 31272395972632.152\n",
      "validation loss: 19974328217829.44\n",
      "epoch: 190\n",
      "training loss: 31270153438639.82\n",
      "validation loss: 19975453202980.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 191\n",
      "training loss: 31267916154874.47\n",
      "validation loss: 19976634101890.62\n",
      "epoch: 192\n",
      "training loss: 31265684046545.83\n",
      "validation loss: 19977869478517.496\n",
      "epoch: 193\n",
      "training loss: 31263457042026.81\n",
      "validation loss: 19979157932362.31\n",
      "epoch: 194\n",
      "training loss: 31261235072704.49\n",
      "validation loss: 19980498097553.848\n",
      "epoch: 195\n",
      "training loss: 31259018072838.16\n",
      "validation loss: 19981888641959.72\n",
      "epoch: 196\n",
      "training loss: 31256805979424.03\n",
      "validation loss: 19983328266321.668\n",
      "epoch: 197\n",
      "training loss: 31254598732066.434\n",
      "validation loss: 19984815703414.51\n",
      "epoch: 198\n",
      "training loss: 31252396272854.98\n",
      "validation loss: 19986349717227.973\n",
      "epoch: 199\n",
      "training loss: 31250198546247.656\n",
      "validation loss: 19987929102170.766\n",
      "epoch: 200\n",
      "training loss: 31248005498959.344\n",
      "validation loss: 19989552682296.254\n",
      "epoch: 201\n",
      "training loss: 31245817079855.664\n",
      "validation loss: 19991219310549.06\n",
      "epoch: 202\n",
      "training loss: 31243633239851.875\n",
      "validation loss: 19992927868031.99\n",
      "epoch: 203\n",
      "training loss: 31241453931816.438\n",
      "validation loss: 19994677263292.582\n",
      "epoch: 204\n",
      "training loss: 31239279110479.29\n",
      "validation loss: 19996466431628.973\n",
      "epoch: 205\n",
      "training loss: 31237108732344.316\n",
      "validation loss: 19998294334414.156\n",
      "epoch: 206\n",
      "training loss: 31234942755606.08\n",
      "validation loss: 20000159958438.387\n",
      "epoch: 207\n",
      "training loss: 31232781140070.4\n",
      "validation loss: 20002062315269.04\n",
      "epoch: 208\n",
      "training loss: 31230623847078.73\n",
      "validation loss: 20004000440627.52\n",
      "epoch: 209\n",
      "training loss: 31228470839436.156\n",
      "validation loss: 20005973393782.637\n",
      "epoch: 210\n",
      "training loss: 31226322081342.668\n",
      "validation loss: 20007980256960.06\n",
      "epoch: 211\n",
      "training loss: 31224177538327.855\n",
      "validation loss: 20010020134767.29\n",
      "epoch: 212\n",
      "training loss: 31222037177188.547\n",
      "validation loss: 20012092153633.844\n",
      "epoch: 213\n",
      "training loss: 31219900965929.496\n",
      "validation loss: 20014195461266.074\n",
      "epoch: 214\n",
      "training loss: 31217768873706.81\n",
      "validation loss: 20016329226116.305\n",
      "epoch: 215\n",
      "training loss: 31215640870774.117\n",
      "validation loss: 20018492636865.83\n",
      "epoch: 216\n",
      "training loss: 31213516928431.24\n",
      "validation loss: 20020684901921.438\n",
      "epoch: 217\n",
      "training loss: 31211397018975.273\n",
      "validation loss: 20022905248924.996\n",
      "epoch: 218\n",
      "training loss: 31209281115654.082\n",
      "validation loss: 20025152924275.81\n",
      "epoch: 219\n",
      "training loss: 31207169192621.848\n",
      "validation loss: 20027427192665.355\n",
      "epoch: 220\n",
      "training loss: 31205061224896.887\n",
      "validation loss: 20029727336624.066\n",
      "epoch: 221\n",
      "training loss: 31202957188321.324\n",
      "validation loss: 20032052656079.84\n",
      "epoch: 222\n",
      "training loss: 31200857059522.746\n",
      "validation loss: 20034402467927.85\n",
      "epoch: 223\n",
      "training loss: 31198760815877.684\n",
      "validation loss: 20036776105611.562\n",
      "epoch: 224\n",
      "training loss: 31196668435476.76\n",
      "validation loss: 20039172918714.387\n",
      "Mean absolute error: $6198530949.58\n",
      "Nodes: 50\n",
      "Learning Rate: 1e-08\n",
      "epoch: 0\n",
      "training loss: 37111847959183.59\n",
      "validation loss: 33512983704848.598\n",
      "epoch: 1\n",
      "training loss: 37081455415671.58\n",
      "validation loss: 33527199411753.246\n",
      "epoch: 2\n",
      "training loss: 37060096673446.52\n",
      "validation loss: 33435792489380.59\n",
      "epoch: 3\n",
      "training loss: 37034592896285.875\n",
      "validation loss: 33400404346864.484\n",
      "epoch: 4\n",
      "training loss: 37021000107620.516\n",
      "validation loss: 33463313694176.383\n",
      "epoch: 5\n",
      "training loss: 36976485678014.72\n",
      "validation loss: 33308688903744.246\n",
      "epoch: 6\n",
      "training loss: 36966312094466.625\n",
      "validation loss: 33287182750688.996\n",
      "epoch: 7\n",
      "training loss: 36951563219531.13\n",
      "validation loss: 33321979565132.883\n",
      "epoch: 8\n",
      "training loss: 36932713494347.72\n",
      "validation loss: 33419698877182.19\n",
      "epoch: 9\n",
      "training loss: 36875882136689.62\n",
      "validation loss: 33139948777818.81\n",
      "epoch: 10\n",
      "training loss: 36850877226252.43\n",
      "validation loss: 33100037480259.176\n",
      "epoch: 11\n",
      "training loss: 36824997535070.016\n",
      "validation loss: 33147507410656.805\n",
      "epoch: 12\n",
      "training loss: 36786926433985.195\n",
      "validation loss: 33161199202443.97\n",
      "epoch: 13\n",
      "training loss: 36745516891317.695\n",
      "validation loss: 33047782329198.426\n",
      "epoch: 14\n",
      "training loss: 36727179377397.484\n",
      "validation loss: 33020670015861.05\n",
      "epoch: 15\n",
      "training loss: 36698268541324.32\n",
      "validation loss: 33000396464611.957\n",
      "epoch: 16\n",
      "training loss: 36659118300293.16\n",
      "validation loss: 33005139721909.24\n",
      "epoch: 17\n",
      "training loss: 36633396446073.18\n",
      "validation loss: 32996424115189.336\n",
      "epoch: 18\n",
      "training loss: 36612098920781.586\n",
      "validation loss: 33101912100194.24\n",
      "epoch: 19\n",
      "training loss: 36572921203465.766\n",
      "validation loss: 32890747799625.938\n",
      "epoch: 20\n",
      "training loss: 36542503811342.68\n",
      "validation loss: 32892468590106.156\n",
      "epoch: 21\n",
      "training loss: 36506888997315.7\n",
      "validation loss: 33032767145957.65\n",
      "epoch: 22\n",
      "training loss: 36483912082040.016\n",
      "validation loss: 33135286487874.797\n",
      "epoch: 23\n",
      "training loss: 36437941948491.586\n",
      "validation loss: 32933555559928.914\n",
      "epoch: 24\n",
      "training loss: 36432866085682.49\n",
      "validation loss: 33024403893186.62\n",
      "epoch: 25\n",
      "training loss: 36409998091516.11\n",
      "validation loss: 32790529725913.44\n",
      "epoch: 26\n",
      "training loss: 36369464070568.43\n",
      "validation loss: 32672223754820.496\n",
      "epoch: 27\n",
      "training loss: 36320358125806.15\n",
      "validation loss: 32674817192372.12\n",
      "epoch: 28\n",
      "training loss: 36273549663740.29\n",
      "validation loss: 33066897369679.504\n",
      "epoch: 29\n",
      "training loss: 36274446496541.11\n",
      "validation loss: 32801918303474.777\n",
      "epoch: 30\n",
      "training loss: 36237063358463.71\n",
      "validation loss: 32749363858772.03\n",
      "epoch: 31\n",
      "training loss: 36214226151345.164\n",
      "validation loss: 32557916461351.414\n",
      "epoch: 32\n",
      "training loss: 36196248010516.64\n",
      "validation loss: 32474412497329.49\n",
      "epoch: 33\n",
      "training loss: 36141409727724.18\n",
      "validation loss: 32417241313425.223\n",
      "epoch: 34\n",
      "training loss: 36142505222682.65\n",
      "validation loss: 32251999872440.094\n",
      "epoch: 35\n",
      "training loss: 36119925463059.72\n",
      "validation loss: 32217680525627.508\n",
      "epoch: 36\n",
      "training loss: 36093469070592.09\n",
      "validation loss: 32137842446858.945\n",
      "epoch: 37\n",
      "training loss: 36052911416871.89\n",
      "validation loss: 32193941097338.07\n",
      "epoch: 38\n",
      "training loss: 36021539319522.88\n",
      "validation loss: 32116932789173.75\n",
      "epoch: 39\n",
      "training loss: 36004208716345.33\n",
      "validation loss: 31923870477021.566\n",
      "epoch: 40\n",
      "training loss: 35976768934700.19\n",
      "validation loss: 32149009431389.957\n",
      "epoch: 41\n",
      "training loss: 35987458446507.1\n",
      "validation loss: 31899995700281.305\n",
      "epoch: 42\n",
      "training loss: 35958980095741.56\n",
      "validation loss: 31683942389053.086\n",
      "epoch: 43\n",
      "training loss: 35939420521927.04\n",
      "validation loss: 31586207653702.492\n",
      "epoch: 44\n",
      "training loss: 35913034593611.28\n",
      "validation loss: 31549379647527.92\n",
      "epoch: 45\n",
      "training loss: 35887545725658.266\n",
      "validation loss: 31489774107355.04\n",
      "epoch: 46\n",
      "training loss: 35878853471113.484\n",
      "validation loss: 31517241347909.062\n",
      "epoch: 47\n",
      "training loss: 35832289572432.0\n",
      "validation loss: 31442782793744.555\n",
      "epoch: 48\n",
      "training loss: 35806375427293.27\n",
      "validation loss: 31455459199749.86\n",
      "epoch: 49\n",
      "training loss: 35788183019582.93\n",
      "validation loss: 31339435282168.89\n",
      "epoch: 50\n",
      "training loss: 35764546943483.234\n",
      "validation loss: 31280086048773.36\n",
      "epoch: 51\n",
      "training loss: 35736149710903.39\n",
      "validation loss: 31238296741096.766\n",
      "epoch: 52\n",
      "training loss: 35717051421509.9\n",
      "validation loss: 31167735629280.402\n",
      "epoch: 53\n",
      "training loss: 35692589170308.34\n",
      "validation loss: 31127123941172.156\n",
      "epoch: 54\n",
      "training loss: 35668258823504.2\n",
      "validation loss: 31086673954892.14\n",
      "epoch: 55\n",
      "training loss: 35644054581771.586\n",
      "validation loss: 31046386350294.066\n",
      "epoch: 56\n",
      "training loss: 35616569856292.51\n",
      "validation loss: 31010452233378.418\n",
      "epoch: 57\n",
      "training loss: 35592713922688.07\n",
      "validation loss: 30968435142525.695\n",
      "epoch: 58\n",
      "training loss: 35569210515253.5\n",
      "validation loss: 30934911319332.977\n",
      "epoch: 59\n",
      "training loss: 35543804566074.586\n",
      "validation loss: 30955169779024.227\n",
      "epoch: 60\n",
      "training loss: 35520177926642.16\n",
      "validation loss: 30893668900843.25\n",
      "epoch: 61\n",
      "training loss: 35496500054474.27\n",
      "validation loss: 30821657679902.227\n",
      "epoch: 62\n",
      "training loss: 35473017685342.766\n",
      "validation loss: 30782615367132.0\n",
      "epoch: 63\n",
      "training loss: 35449630430497.24\n",
      "validation loss: 30743663440894.72\n",
      "epoch: 64\n",
      "training loss: 35425177675716.414\n",
      "validation loss: 30804630515131.312\n",
      "epoch: 65\n",
      "training loss: 35402088899688.66\n",
      "validation loss: 30700129152132.09\n",
      "epoch: 66\n",
      "training loss: 35378416708653.31\n",
      "validation loss: 30710478364860.652\n",
      "epoch: 67\n",
      "training loss: 35350592887504.47\n",
      "validation loss: 30672400458640.156\n",
      "epoch: 68\n",
      "training loss: 35320041071562.06\n",
      "validation loss: 30654467198483.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69\n",
      "training loss: 35318743083141.74\n",
      "validation loss: 30648319244244.43\n",
      "epoch: 70\n",
      "training loss: 35291284291883.125\n",
      "validation loss: 30608573867444.285\n",
      "epoch: 71\n",
      "training loss: 35268676590302.66\n",
      "validation loss: 31217141731480.613\n",
      "epoch: 72\n",
      "training loss: 35284789438764.6\n",
      "validation loss: 31362559647415.355\n",
      "epoch: 73\n",
      "training loss: 35292610651121.32\n",
      "validation loss: 31276219096747.266\n",
      "epoch: 74\n",
      "training loss: 35314780833932.04\n",
      "validation loss: 31555728757713.04\n",
      "epoch: 75\n",
      "training loss: 35249098778724.36\n",
      "validation loss: 31276914403886.805\n",
      "epoch: 76\n",
      "training loss: 35229307005607.76\n",
      "validation loss: 31380863187921.27\n",
      "epoch: 77\n",
      "training loss: 35183556563625.785\n",
      "validation loss: 31518516172573.977\n",
      "epoch: 78\n",
      "training loss: 35143475692730.0\n",
      "validation loss: 32122997473898.812\n",
      "epoch: 79\n",
      "training loss: 35079865653024.418\n",
      "validation loss: 31609567394745.492\n",
      "epoch: 80\n",
      "training loss: 35068832207979.1\n",
      "validation loss: 30839224796010.344\n",
      "epoch: 81\n",
      "training loss: 35034793170487.188\n",
      "validation loss: 31095575829231.434\n",
      "epoch: 82\n",
      "training loss: 35047062124847.676\n",
      "validation loss: 30878235762897.145\n",
      "epoch: 83\n",
      "training loss: 35036752490567.05\n",
      "validation loss: 30932814384392.293\n",
      "epoch: 84\n",
      "training loss: 35003970866219.707\n",
      "validation loss: 31036701882956.64\n",
      "epoch: 85\n",
      "training loss: 34946130151237.28\n",
      "validation loss: 30785921273600.254\n",
      "epoch: 86\n",
      "training loss: 34919983949934.727\n",
      "validation loss: 30562882558378.227\n",
      "epoch: 87\n",
      "training loss: 34894882817331.234\n",
      "validation loss: 30398103130420.695\n",
      "epoch: 88\n",
      "training loss: 34859996243718.08\n",
      "validation loss: 30339642648539.324\n",
      "epoch: 89\n",
      "training loss: 34845824249089.363\n",
      "validation loss: 30375112345484.67\n",
      "epoch: 90\n",
      "training loss: 34824724068121.03\n",
      "validation loss: 30356694683563.867\n",
      "epoch: 91\n",
      "training loss: 34811802939929.145\n",
      "validation loss: 30185592823665.27\n",
      "epoch: 92\n",
      "training loss: 34790515876122.844\n",
      "validation loss: 30155927257189.44\n",
      "epoch: 93\n",
      "training loss: 34769131477174.418\n",
      "validation loss: 30072558886019.496\n",
      "epoch: 94\n",
      "training loss: 34748573734754.613\n",
      "validation loss: 30036865992439.33\n",
      "epoch: 95\n",
      "training loss: 34728265406900.43\n",
      "validation loss: 29956803569009.785\n",
      "epoch: 96\n",
      "training loss: 34707328212510.676\n",
      "validation loss: 29943305597439.37\n",
      "epoch: 97\n",
      "training loss: 34686467134580.957\n",
      "validation loss: 29937107846825.44\n",
      "epoch: 98\n",
      "training loss: 34666334132472.348\n",
      "validation loss: 29862388838779.83\n",
      "epoch: 99\n",
      "training loss: 34646139988442.3\n",
      "validation loss: 29832364136345.203\n",
      "epoch: 100\n",
      "training loss: 34625808994974.29\n",
      "validation loss: 29816997922403.152\n",
      "epoch: 101\n",
      "training loss: 34605791440280.438\n",
      "validation loss: 29786707053872.066\n",
      "epoch: 102\n",
      "training loss: 34585893173802.273\n",
      "validation loss: 29756532558936.406\n",
      "epoch: 103\n",
      "training loss: 34566092052904.246\n",
      "validation loss: 29726474242762.53\n",
      "epoch: 104\n",
      "training loss: 34546387586250.285\n",
      "validation loss: 29696531613319.465\n",
      "epoch: 105\n",
      "training loss: 34526779240884.24\n",
      "validation loss: 29666704180970.875\n",
      "epoch: 106\n",
      "training loss: 34507025021624.195\n",
      "validation loss: 29637195957739.105\n",
      "epoch: 107\n",
      "training loss: 34487522332810.23\n",
      "validation loss: 29607748171700.24\n",
      "epoch: 108\n",
      "training loss: 34466793113300.227\n",
      "validation loss: 29595197106734.676\n",
      "epoch: 109\n",
      "training loss: 34438911363623.082\n",
      "validation loss: 29538407388255.945\n",
      "epoch: 110\n",
      "training loss: 34418108749649.465\n",
      "validation loss: 29507955954852.453\n",
      "epoch: 111\n",
      "training loss: 34401457629866.438\n",
      "validation loss: 29459007206354.23\n",
      "epoch: 112\n",
      "training loss: 34391762662255.79\n",
      "validation loss: 29415599926655.676\n",
      "epoch: 113\n",
      "training loss: 34368065404136.56\n",
      "validation loss: 29445448049673.168\n",
      "epoch: 114\n",
      "training loss: 34354086995757.504\n",
      "validation loss: 29366661708526.71\n",
      "epoch: 115\n",
      "training loss: 34335636635944.723\n",
      "validation loss: 29337833800256.395\n",
      "epoch: 116\n",
      "training loss: 34317280056782.613\n",
      "validation loss: 29309117277594.594\n",
      "epoch: 117\n",
      "training loss: 34299016779081.418\n",
      "validation loss: 29280511665511.0\n",
      "epoch: 118\n",
      "training loss: 34280846326043.395\n",
      "validation loss: 29252016491510.227\n",
      "epoch: 119\n",
      "training loss: 34262768220755.58\n",
      "validation loss: 29223631291937.33\n",
      "epoch: 120\n",
      "training loss: 34244740824759.098\n",
      "validation loss: 29200587167105.36\n",
      "epoch: 121\n",
      "training loss: 34232533781144.223\n",
      "validation loss: 29145666379609.94\n",
      "epoch: 122\n",
      "training loss: 34214799133509.184\n",
      "validation loss: 29117530599598.188\n",
      "epoch: 123\n",
      "training loss: 34197154524553.023\n",
      "validation loss: 29089502939003.895\n",
      "epoch: 124\n",
      "training loss: 34179599494475.56\n",
      "validation loss: 29061582939004.29\n",
      "epoch: 125\n",
      "training loss: 34162133585903.25\n",
      "validation loss: 29033770142892.152\n",
      "epoch: 126\n",
      "training loss: 34144756343799.85\n",
      "validation loss: 29006064096062.18\n",
      "epoch: 127\n",
      "training loss: 34127467315453.12\n",
      "validation loss: 28978464346000.086\n",
      "epoch: 128\n",
      "training loss: 34110266050462.574\n",
      "validation loss: 28950970442271.855\n",
      "epoch: 129\n",
      "training loss: 34093152100727.664\n",
      "validation loss: 28923581936513.21\n",
      "epoch: 130\n",
      "training loss: 34076125020435.85\n",
      "validation loss: 28896298382419.13\n",
      "epoch: 131\n",
      "training loss: 34059184366051.043\n",
      "validation loss: 28869119335733.523\n",
      "epoch: 132\n",
      "training loss: 34042329696301.863\n",
      "validation loss: 28842044354238.96\n",
      "epoch: 133\n",
      "training loss: 34025560572170.125\n",
      "validation loss: 28815072997746.58\n",
      "epoch: 134\n",
      "training loss: 34008876556879.3\n",
      "validation loss: 28788204828085.957\n",
      "epoch: 135\n",
      "training loss: 33992277215883.1\n",
      "validation loss: 28761439409095.188\n",
      "epoch: 136\n",
      "training loss: 33975762116854.05\n",
      "validation loss: 28734776306610.918\n",
      "epoch: 137\n",
      "training loss: 33959330829672.203\n",
      "validation loss: 28708215088458.6\n",
      "epoch: 138\n",
      "training loss: 33942982926413.812\n",
      "validation loss: 28681755324442.664\n",
      "epoch: 139\n",
      "training loss: 33926717981340.168\n",
      "validation loss: 28655396586336.832\n",
      "epoch: 140\n",
      "training loss: 33910535570886.402\n",
      "validation loss: 28629138447874.51\n",
      "epoch: 141\n",
      "training loss: 33894435273650.438\n",
      "validation loss: 28602980484739.24\n",
      "epoch: 142\n",
      "training loss: 33878416670381.875\n",
      "validation loss: 28576922274555.11\n",
      "epoch: 143\n",
      "training loss: 33862479343971.05\n",
      "validation loss: 28550963396877.406\n",
      "epoch: 144\n",
      "training loss: 33846622879438.133\n",
      "validation loss: 28525103433183.16\n",
      "epoch: 145\n",
      "training loss: 33830846863922.16\n",
      "validation loss: 28499341966861.816\n",
      "epoch: 146\n",
      "training loss: 33815150886670.312\n",
      "validation loss: 28473678583205.953\n",
      "epoch: 147\n",
      "training loss: 33799534539027.082\n",
      "validation loss: 28448112869402.05\n",
      "epoch: 148\n",
      "training loss: 33783997414423.6\n",
      "validation loss: 28422644414521.344\n",
      "epoch: 149\n",
      "training loss: 33768539108366.973\n",
      "validation loss: 28397272809510.64\n",
      "epoch: 150\n",
      "training loss: 33753159218429.652\n",
      "validation loss: 28371997647183.258\n",
      "epoch: 151\n",
      "training loss: 33737857344238.91\n",
      "validation loss: 28346818522210.027\n",
      "epoch: 152\n",
      "training loss: 33722633087466.36\n",
      "validation loss: 28321735031110.258\n",
      "epoch: 153\n",
      "training loss: 33707486051817.414\n",
      "validation loss: 28296746772242.875\n",
      "epoch: 154\n",
      "training loss: 33692415843020.953\n",
      "validation loss: 28271853345797.434\n",
      "epoch: 155\n",
      "training loss: 33677422068818.88\n",
      "validation loss: 28247054353785.4\n",
      "epoch: 156\n",
      "training loss: 33662504338955.633\n",
      "validation loss: 28222349400031.242\n",
      "epoch: 157\n",
      "training loss: 33647662265167.48\n",
      "validation loss: 28197738090163.758\n",
      "epoch: 158\n",
      "training loss: 33632895461170.656\n",
      "validation loss: 28173220031607.34\n",
      "epoch: 159\n",
      "training loss: 33618203542643.66\n",
      "validation loss: 28148794833573.258\n",
      "epoch: 160\n",
      "training loss: 33603586127156.812\n",
      "validation loss: 28124462107051.0\n",
      "epoch: 161\n",
      "training loss: 33589042831869.55\n",
      "validation loss: 28100221464798.973\n",
      "epoch: 162\n",
      "training loss: 33571698187832.297\n",
      "validation loss: 28078878364488.832\n",
      "epoch: 163\n",
      "training loss: 33557006296164.03\n",
      "validation loss: 28055706416771.5\n",
      "epoch: 164\n",
      "training loss: 33542658670817.77\n",
      "validation loss: 28031729119896.617\n",
      "epoch: 165\n",
      "training loss: 33528383844609.027\n",
      "validation loss: 28007842498523.2\n",
      "epoch: 166\n",
      "training loss: 33514181445619.074\n",
      "validation loss: 27984046173215.062\n",
      "epoch: 167\n",
      "training loss: 33500051103829.414\n",
      "validation loss: 27960339766261.062\n",
      "epoch: 168\n",
      "training loss: 33485992451112.004\n",
      "validation loss: 27936722901666.707\n",
      "epoch: 169\n",
      "training loss: 33472005121219.684\n",
      "validation loss: 27913195205145.957\n",
      "epoch: 170\n",
      "training loss: 33458088749776.504\n",
      "validation loss: 27889756304112.96\n",
      "epoch: 171\n",
      "training loss: 33444242974268.19\n",
      "validation loss: 27866405827673.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 172\n",
      "training loss: 33430467434032.62\n",
      "validation loss: 27843143406618.727\n",
      "epoch: 173\n",
      "training loss: 33416761770250.383\n",
      "validation loss: 27819968673413.324\n",
      "epoch: 174\n",
      "training loss: 33403125625935.312\n",
      "validation loss: 27796881262191.15\n",
      "epoch: 175\n",
      "training loss: 33389558645925.207\n",
      "validation loss: 27773880808745.38\n",
      "epoch: 176\n",
      "training loss: 33376060476872.418\n",
      "validation loss: 27750966950520.855\n",
      "epoch: 177\n",
      "training loss: 33362630767234.645\n",
      "validation loss: 27728139326606.184\n",
      "epoch: 178\n",
      "training loss: 33349269167265.68\n",
      "validation loss: 27705397577725.78\n",
      "epoch: 179\n",
      "training loss: 33335975329006.242\n",
      "validation loss: 27682741346232.03\n",
      "epoch: 180\n",
      "training loss: 33322748906274.848\n",
      "validation loss: 27660170276097.445\n",
      "epoch: 181\n",
      "training loss: 33309589554658.723\n",
      "validation loss: 27637684012906.867\n",
      "epoch: 182\n",
      "training loss: 33296496931504.766\n",
      "validation loss: 27615282203849.684\n",
      "epoch: 183\n",
      "training loss: 33283470695910.555\n",
      "validation loss: 27592964497712.156\n",
      "epoch: 184\n",
      "training loss: 33270510508715.42\n",
      "validation loss: 27570730544869.703\n",
      "epoch: 185\n",
      "training loss: 33257616032491.547\n",
      "validation loss: 27548579997279.246\n",
      "epoch: 186\n",
      "training loss: 33244786931535.074\n",
      "validation loss: 27526512508471.613\n",
      "epoch: 187\n",
      "training loss: 33232022871857.336\n",
      "validation loss: 27504527733543.953\n",
      "epoch: 188\n",
      "training loss: 33219323521176.098\n",
      "validation loss: 27482625329152.184\n",
      "epoch: 189\n",
      "training loss: 33206688548906.812\n",
      "validation loss: 27460804953503.492\n",
      "epoch: 190\n",
      "training loss: 33194117626153.94\n",
      "validation loss: 27439066266348.87\n",
      "epoch: 191\n",
      "training loss: 33181610425702.38\n",
      "validation loss: 27417408928975.65\n",
      "epoch: 192\n",
      "training loss: 33169166622008.793\n",
      "validation loss: 27395832604200.098\n",
      "epoch: 193\n",
      "training loss: 33156785891193.152\n",
      "validation loss: 27374336956360.09\n",
      "epoch: 194\n",
      "training loss: 33144467911030.17\n",
      "validation loss: 27352921651307.7\n",
      "epoch: 195\n",
      "training loss: 33132212360940.89\n",
      "validation loss: 27331586356401.918\n",
      "epoch: 196\n",
      "training loss: 33120018921984.254\n",
      "validation loss: 27310330740501.4\n",
      "epoch: 197\n",
      "training loss: 33107887276848.75\n",
      "validation loss: 27289154473957.17\n",
      "epoch: 198\n",
      "training loss: 33095817109844.066\n",
      "validation loss: 27268057228605.45\n",
      "epoch: 199\n",
      "training loss: 33083808106892.83\n",
      "validation loss: 27247038677760.414\n",
      "epoch: 200\n",
      "training loss: 33071859955522.348\n",
      "validation loss: 27226098496207.117\n",
      "epoch: 201\n",
      "training loss: 33059972344856.434\n",
      "validation loss: 27205236360194.273\n",
      "epoch: 202\n",
      "training loss: 33048144965607.19\n",
      "validation loss: 27184451947427.19\n",
      "epoch: 203\n",
      "training loss: 33036377510067.0\n",
      "validation loss: 27163744937060.723\n",
      "epoch: 204\n",
      "training loss: 33024669672100.336\n",
      "validation loss: 27143115009692.15\n",
      "epoch: 205\n",
      "training loss: 33013021147135.816\n",
      "validation loss: 27122561847354.22\n",
      "epoch: 206\n",
      "training loss: 33001431632158.15\n",
      "validation loss: 27102085133508.11\n",
      "epoch: 207\n",
      "training loss: 32989900825700.246\n",
      "validation loss: 27081684553036.426\n",
      "epoch: 208\n",
      "training loss: 32978428427835.254\n",
      "validation loss: 27061359792236.3\n",
      "epoch: 209\n",
      "training loss: 32967014140168.734\n",
      "validation loss: 27041110538812.38\n",
      "epoch: 210\n",
      "training loss: 32955657665830.777\n",
      "validation loss: 27020936481869.95\n",
      "epoch: 211\n",
      "training loss: 32944358709468.277\n",
      "validation loss: 27000837311908.008\n",
      "epoch: 212\n",
      "training loss: 32933116977237.133\n",
      "validation loss: 26980812720812.348\n",
      "epoch: 213\n",
      "training loss: 32921932176794.56\n",
      "validation loss: 26960862401848.723\n",
      "epoch: 214\n",
      "training loss: 32910804017291.418\n",
      "validation loss: 26940986049655.902\n",
      "epoch: 215\n",
      "training loss: 32899732209364.58\n",
      "validation loss: 26921183360238.855\n",
      "epoch: 216\n",
      "training loss: 32888716465129.34\n",
      "validation loss: 26901454030961.844\n",
      "epoch: 217\n",
      "training loss: 32877756498171.848\n",
      "validation loss: 26881797760541.562\n",
      "epoch: 218\n",
      "training loss: 32866852023541.617\n",
      "validation loss: 26862214249040.254\n",
      "epoch: 219\n",
      "training loss: 32856002757744.035\n",
      "validation loss: 26842703197858.805\n",
      "epoch: 220\n",
      "training loss: 32845208418732.902\n",
      "validation loss: 26823264309729.84\n",
      "epoch: 221\n",
      "training loss: 32834468725903.086\n",
      "validation loss: 26803897288710.797\n",
      "epoch: 222\n",
      "training loss: 32823783400083.12\n",
      "validation loss: 26784601840176.91\n",
      "epoch: 223\n",
      "training loss: 32813152163527.867\n",
      "validation loss: 26765377670814.23\n",
      "epoch: 224\n",
      "training loss: 32802574739911.273\n",
      "validation loss: 26746224488612.535\n",
      "epoch: 225\n",
      "training loss: 32792050854319.082\n",
      "validation loss: 26727142002858.195\n",
      "epoch: 226\n",
      "training loss: 32781580233241.64\n",
      "validation loss: 26708129924126.934\n",
      "epoch: 227\n",
      "training loss: 32771162604566.742\n",
      "validation loss: 26689187964276.504\n",
      "epoch: 228\n",
      "training loss: 32760797697572.395\n",
      "validation loss: 26670315836439.215\n",
      "epoch: 229\n",
      "training loss: 32750485242919.86\n",
      "validation loss: 26651513255014.37\n",
      "epoch: 230\n",
      "training loss: 32740224972646.46\n",
      "validation loss: 26632779935660.43\n",
      "epoch: 231\n",
      "training loss: 32730016620158.63\n",
      "validation loss: 26614115595286.965\n",
      "epoch: 232\n",
      "training loss: 32719859920224.883\n",
      "validation loss: 26595519952046.453\n",
      "epoch: 233\n",
      "training loss: 32709754608968.836\n",
      "validation loss: 26576992725325.555\n",
      "epoch: 234\n",
      "training loss: 32699700423862.35\n",
      "validation loss: 26558533635736.117\n",
      "epoch: 235\n",
      "training loss: 32689697103718.562\n",
      "validation loss: 26540142405105.633\n",
      "epoch: 236\n",
      "training loss: 32679744388685.098\n",
      "validation loss: 26521818756467.062\n",
      "epoch: 237\n",
      "training loss: 32669842020237.223\n",
      "validation loss: 26503562414047.94\n",
      "epoch: 238\n",
      "training loss: 32659989741171.05\n",
      "validation loss: 26485373103258.465\n",
      "epoch: 239\n",
      "training loss: 32650187295596.84\n",
      "validation loss: 26467250550678.402\n",
      "epoch: 240\n",
      "training loss: 32640434428932.195\n",
      "validation loss: 26449194484042.46\n",
      "epoch: 241\n",
      "training loss: 32630730887895.49\n",
      "validation loss: 26431204632223.746\n",
      "epoch: 242\n",
      "training loss: 32621076420499.15\n",
      "validation loss: 26413280725214.637\n",
      "epoch: 243\n",
      "training loss: 32611470776043.043\n",
      "validation loss: 26395422494104.33\n",
      "epoch: 244\n",
      "training loss: 32601913705107.953\n",
      "validation loss: 26377629671052.137\n",
      "epoch: 245\n",
      "training loss: 32592404959548.98\n",
      "validation loss: 26359901989254.727\n",
      "epoch: 246\n",
      "training loss: 32582944292489.062\n",
      "validation loss: 26342239182905.48\n",
      "epoch: 247\n",
      "training loss: 32573531458312.47\n",
      "validation loss: 26324640987142.66\n",
      "epoch: 248\n",
      "training loss: 32564166212658.39\n",
      "validation loss: 26307107137981.84\n",
      "epoch: 249\n",
      "training loss: 32554848312414.477\n",
      "validation loss: 26289637372225.508\n",
      "epoch: 250\n",
      "training loss: 32545577515710.504\n",
      "validation loss: 26272231427338.93\n",
      "epoch: 251\n",
      "training loss: 32536353581911.977\n",
      "validation loss: 26254889041274.355\n",
      "epoch: 252\n",
      "training loss: 32527176271613.79\n",
      "validation loss: 26237609952213.977\n",
      "epoch: 253\n",
      "training loss: 32518045346633.96\n",
      "validation loss: 26220393898180.12\n",
      "epoch: 254\n",
      "training loss: 32508960570007.242\n",
      "validation loss: 26203240616419.4\n",
      "epoch: 255\n",
      "training loss: 32499921705978.727\n",
      "validation loss: 26186149842384.86\n",
      "epoch: 256\n",
      "training loss: 32490928519997.16\n",
      "validation loss: 26169121307972.223\n",
      "epoch: 257\n",
      "training loss: 32481980778706.91\n",
      "validation loss: 26152154738349.176\n",
      "epoch: 258\n",
      "training loss: 32473078249933.645\n",
      "validation loss: 26135249846482.523\n",
      "epoch: 259\n",
      "training loss: 32464220702609.805\n",
      "validation loss: 26118406329642.36\n",
      "epoch: 260\n",
      "training loss: 32455407903751.832\n",
      "validation loss: 26101623973075.023\n",
      "epoch: 261\n",
      "training loss: 32446617624785.95\n",
      "validation loss: 26084649284199.93\n",
      "epoch: 262\n",
      "training loss: 32437893554565.67\n",
      "validation loss: 26067989417819.68\n",
      "epoch: 263\n",
      "training loss: 32429213554375.89\n",
      "validation loss: 26051390123665.72\n",
      "epoch: 264\n",
      "training loss: 32420577399079.914\n",
      "validation loss: 26034851157514.688\n",
      "epoch: 265\n",
      "training loss: 32411984864691.47\n",
      "validation loss: 26018372276246.1\n",
      "epoch: 266\n",
      "training loss: 32403435728368.77\n",
      "validation loss: 26001953237825.195\n",
      "epoch: 267\n",
      "training loss: 32394929768408.523\n",
      "validation loss: 25985593801292.8\n",
      "epoch: 268\n",
      "training loss: 32386466764240.12\n",
      "validation loss: 25969293726758.117\n",
      "epoch: 269\n",
      "training loss: 32378046496419.664\n",
      "validation loss: 25953052775392.574\n",
      "epoch: 270\n",
      "training loss: 32369668746623.88\n",
      "validation loss: 25936870709423.906\n",
      "epoch: 271\n",
      "training loss: 32361333297643.508\n",
      "validation loss: 25920747292129.473\n",
      "epoch: 272\n",
      "training loss: 32353039933374.01\n",
      "validation loss: 25904682287825.387\n",
      "epoch: 273\n",
      "training loss: 32344788438789.297\n",
      "validation loss: 25888675461836.164\n",
      "epoch: 274\n",
      "training loss: 32336578599648.496\n",
      "validation loss: 25872726580358.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 275\n",
      "training loss: 32328409813650.152\n",
      "validation loss: 25856835409896.28\n",
      "epoch: 276\n",
      "training loss: 32319441673048.89\n",
      "validation loss: 25858232282723.78\n",
      "epoch: 277\n",
      "training loss: 32311030006554.344\n",
      "validation loss: 25860482990106.9\n",
      "epoch: 278\n",
      "training loss: 32302977484896.555\n",
      "validation loss: 25844833619203.05\n",
      "epoch: 279\n",
      "training loss: 32294965612338.965\n",
      "validation loss: 25829240761804.832\n",
      "epoch: 280\n",
      "training loss: 32286994165512.746\n",
      "validation loss: 25813704248956.082\n",
      "epoch: 281\n",
      "training loss: 32279062937941.13\n",
      "validation loss: 25798223854959.53\n",
      "epoch: 282\n",
      "training loss: 32271171724202.074\n",
      "validation loss: 25782799355109.676\n",
      "epoch: 283\n",
      "training loss: 32263320319922.9\n",
      "validation loss: 25767430525687.934\n",
      "epoch: 284\n",
      "training loss: 32255508521774.92\n",
      "validation loss: 25752117143957.902\n",
      "epoch: 285\n",
      "training loss: 32247736127468.098\n",
      "validation loss: 25736858988160.56\n",
      "epoch: 286\n",
      "training loss: 32240002935745.766\n",
      "validation loss: 25721655837509.49\n",
      "epoch: 287\n",
      "training loss: 32232308746379.3\n",
      "validation loss: 25706507472186.12\n",
      "epoch: 288\n",
      "training loss: 32224653360162.934\n",
      "validation loss: 25691413673335.03\n",
      "epoch: 289\n",
      "training loss: 32217036578908.496\n",
      "validation loss: 25676374223059.176\n",
      "epoch: 290\n",
      "training loss: 32209458205440.188\n",
      "validation loss: 25661388904415.17\n",
      "epoch: 291\n",
      "training loss: 32201918043589.457\n",
      "validation loss: 25646457501408.586\n",
      "epoch: 292\n",
      "training loss: 32194415898189.836\n",
      "validation loss: 25631579798989.24\n",
      "epoch: 293\n",
      "training loss: 32186951575071.816\n",
      "validation loss: 25616755583046.477\n",
      "epoch: 294\n",
      "training loss: 32179524881057.77\n",
      "validation loss: 25601984640404.496\n",
      "epoch: 295\n",
      "training loss: 32172135623956.855\n",
      "validation loss: 25587266758817.613\n",
      "epoch: 296\n",
      "training loss: 32164783612560.004\n",
      "validation loss: 25572601726965.58\n",
      "epoch: 297\n",
      "training loss: 32157468656634.9\n",
      "validation loss: 25557989334448.883\n",
      "epoch: 298\n",
      "training loss: 32150190566920.965\n",
      "validation loss: 25543429371784.016\n",
      "epoch: 299\n",
      "training loss: 32142949155124.418\n",
      "validation loss: 25528921630398.74\n",
      "epoch: 300\n",
      "training loss: 32135744233913.34\n",
      "validation loss: 25514465902627.387\n",
      "epoch: 301\n",
      "training loss: 32128575616912.72\n",
      "validation loss: 25500061981706.1\n",
      "epoch: 302\n",
      "training loss: 32121443118699.582\n",
      "validation loss: 25485709661767.977\n",
      "epoch: 303\n",
      "training loss: 32114346554798.117\n",
      "validation loss: 25471408737838.375\n",
      "epoch: 304\n",
      "training loss: 32107285741674.766\n",
      "validation loss: 25457159005829.99\n",
      "epoch: 305\n",
      "training loss: 32100260496733.37\n",
      "validation loss: 25442960262537.973\n",
      "epoch: 306\n",
      "training loss: 32093270638310.23\n",
      "validation loss: 25428812305634.95\n",
      "epoch: 307\n",
      "training loss: 32086315985668.88\n",
      "validation loss: 25414714933665.945\n",
      "epoch: 308\n",
      "training loss: 32079396358993.973\n",
      "validation loss: 25400667946043.01\n",
      "epoch: 309\n",
      "training loss: 32072511579380.76\n",
      "validation loss: 25386671143039.367\n",
      "epoch: 310\n",
      "training loss: 32065661468789.445\n",
      "validation loss: 25372724325781.95\n",
      "epoch: 311\n",
      "training loss: 32058845849040.023\n",
      "validation loss: 25358827296240.14\n",
      "epoch: 312\n",
      "training loss: 32052061640524.434\n",
      "validation loss: 25344979857229.383\n",
      "epoch: 313\n",
      "training loss: 32045314441091.957\n",
      "validation loss: 25331181787567.195\n",
      "epoch: 314\n",
      "training loss: 32038601208606.258\n",
      "validation loss: 25317432916789.61\n",
      "epoch: 315\n",
      "training loss: 32031921768953.29\n",
      "validation loss: 25303733050242.402\n",
      "epoch: 316\n",
      "training loss: 32025275949539.52\n",
      "validation loss: 25290081994089.676\n",
      "epoch: 317\n",
      "training loss: 32018663578653.082\n",
      "validation loss: 25276479555313.383\n",
      "epoch: 318\n",
      "training loss: 32012084485459.246\n",
      "validation loss: 25262925541706.92\n",
      "epoch: 319\n",
      "training loss: 32005538499995.95\n",
      "validation loss: 25249419761868.418\n",
      "epoch: 320\n",
      "training loss: 31999025453169.36\n",
      "validation loss: 25235962025193.64\n",
      "epoch: 321\n",
      "training loss: 31992545176749.41\n",
      "validation loss: 25222552141868.508\n",
      "epoch: 322\n",
      "training loss: 31986097503365.402\n",
      "validation loss: 25209189922861.168\n",
      "epoch: 323\n",
      "training loss: 31979682266501.62\n",
      "validation loss: 25195875179913.51\n",
      "epoch: 324\n",
      "training loss: 31973299300492.953\n",
      "validation loss: 25182607725531.965\n",
      "epoch: 325\n",
      "training loss: 31966948440520.547\n",
      "validation loss: 25169387372977.63\n",
      "epoch: 326\n",
      "training loss: 31960629522607.51\n",
      "validation loss: 25156213936255.395\n",
      "epoch: 327\n",
      "training loss: 31954342383614.55\n",
      "validation loss: 25143087230102.008\n",
      "epoch: 328\n",
      "training loss: 31948086861235.78\n",
      "validation loss: 25130007069972.8\n",
      "epoch: 329\n",
      "training loss: 31941862793994.4\n",
      "validation loss: 25116973272026.89\n",
      "epoch: 330\n",
      "training loss: 31935670021238.473\n",
      "validation loss: 25103985653110.34\n",
      "epoch: 331\n",
      "training loss: 31929508383136.727\n",
      "validation loss: 25091044030737.086\n",
      "epoch: 332\n",
      "training loss: 31923377720674.332\n",
      "validation loss: 25078148223066.93\n",
      "epoch: 333\n",
      "training loss: 31917277875648.8\n",
      "validation loss: 25065298048879.977\n",
      "epoch: 334\n",
      "training loss: 31911208690665.723\n",
      "validation loss: 25052493327546.613\n",
      "epoch: 335\n",
      "training loss: 31905170009134.766\n",
      "validation loss: 25039733878992.016\n",
      "epoch: 336\n",
      "training loss: 31899161675265.457\n",
      "validation loss: 25027019523653.48\n",
      "epoch: 337\n",
      "training loss: 31893183534063.17\n",
      "validation loss: 25014350082428.863\n",
      "epoch: 338\n",
      "training loss: 31887235431325.035\n",
      "validation loss: 25001725376613.344\n",
      "epoch: 339\n",
      "training loss: 31881317213635.88\n",
      "validation loss: 24989145227821.035\n",
      "epoch: 340\n",
      "training loss: 31875428728364.246\n",
      "validation loss: 24976609457886.527\n",
      "epoch: 341\n",
      "training loss: 31869569823658.33\n",
      "validation loss: 24964117888739.645\n",
      "epoch: 342\n",
      "training loss: 31863740348442.05\n",
      "validation loss: 24951670342243.61\n",
      "epoch: 343\n",
      "training loss: 31857940152411.047\n",
      "validation loss: 24939266639982.98\n",
      "epoch: 344\n",
      "training loss: 31852169086028.773\n",
      "validation loss: 24926906602980.586\n",
      "epoch: 345\n",
      "training loss: 31846427000522.562\n",
      "validation loss: 24914590051312.965\n",
      "epoch: 346\n",
      "training loss: 31840713747879.684\n",
      "validation loss: 24902316803576.527\n",
      "epoch: 347\n",
      "training loss: 31835029180843.523\n",
      "validation loss: 24890086676129.207\n",
      "epoch: 348\n",
      "training loss: 31829373152909.67\n",
      "validation loss: 24877899481983.965\n",
      "epoch: 349\n",
      "training loss: 31823745518322.094\n",
      "validation loss: 24865755029144.035\n",
      "epoch: 350\n",
      "training loss: 31818146132069.3\n",
      "validation loss: 24853653118007.223\n",
      "epoch: 351\n",
      "training loss: 31812574849880.51\n",
      "validation loss: 24841593537144.188\n",
      "epoch: 352\n",
      "training loss: 31807031528221.887\n",
      "validation loss: 24829576056073.8\n",
      "epoch: 353\n",
      "training loss: 31801516024292.625\n",
      "validation loss: 24817600412103.848\n",
      "epoch: 354\n",
      "training loss: 31796028196021.14\n",
      "validation loss: 24805666284402.066\n",
      "epoch: 355\n",
      "training loss: 31790567902060.97\n",
      "validation loss: 24793773237406.453\n",
      "epoch: 356\n",
      "training loss: 31785135001785.887\n",
      "validation loss: 24781920578913.527\n",
      "epoch: 357\n",
      "training loss: 31779729355281.8\n",
      "validation loss: 24770106924947.56\n",
      "epoch: 358\n",
      "training loss: 31774350823314.55\n",
      "validation loss: 24758328358375.637\n",
      "epoch: 359\n",
      "training loss: 31768999266819.887\n",
      "validation loss: 24746563821635.395\n",
      "epoch: 360\n",
      "training loss: 31763666005540.273\n",
      "validation loss: 24734277634134.516\n",
      "epoch: 361\n",
      "training loss: 31846634987535.375\n",
      "validation loss: 24882139356760.375\n",
      "epoch: 362\n",
      "training loss: 31795072394647.066\n",
      "validation loss: 24710361708836.023\n",
      "epoch: 363\n",
      "training loss: 31758142625337.133\n",
      "validation loss: 24709999640530.25\n",
      "epoch: 364\n",
      "training loss: 31752937495844.78\n",
      "validation loss: 24692622249829.133\n",
      "epoch: 365\n",
      "training loss: 31745301947948.348\n",
      "validation loss: 24684430156457.86\n",
      "epoch: 366\n",
      "training loss: 31731354293073.996\n",
      "validation loss: 24699885089862.33\n",
      "epoch: 367\n",
      "training loss: 31724320928617.49\n",
      "validation loss: 24820430468638.477\n",
      "epoch: 368\n",
      "training loss: 31719184735913.27\n",
      "validation loss: 24809251925288.062\n",
      "epoch: 369\n",
      "training loss: 31714074251012.863\n",
      "validation loss: 24798112391096.395\n",
      "epoch: 370\n",
      "training loss: 31708989342066.28\n",
      "validation loss: 24787011715822.17\n",
      "epoch: 371\n",
      "training loss: 31704191903193.76\n",
      "validation loss: 24780397556508.89\n",
      "epoch: 372\n",
      "training loss: 31699096713973.727\n",
      "validation loss: 24768325488827.38\n",
      "epoch: 373\n",
      "training loss: 31694087873215.07\n",
      "validation loss: 24757342746935.74\n",
      "epoch: 374\n",
      "training loss: 31689104089342.125\n",
      "validation loss: 24746398252606.234\n",
      "epoch: 375\n",
      "training loss: 31684145234588.926\n",
      "validation loss: 24735491858877.7\n",
      "epoch: 376\n",
      "training loss: 31679211181841.1\n",
      "validation loss: 24724623419416.07\n",
      "epoch: 377\n",
      "training loss: 31674301804632.62\n",
      "validation loss: 24713792788511.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 378\n",
      "training loss: 31669416977142.457\n",
      "validation loss: 24702999821075.824\n",
      "epoch: 379\n",
      "training loss: 31664556574191.273\n",
      "validation loss: 24692244372638.957\n",
      "epoch: 380\n",
      "training loss: 31659720471238.203\n",
      "validation loss: 24681526299346.8\n",
      "epoch: 381\n",
      "training loss: 31654908544377.57\n",
      "validation loss: 24670845457958.03\n",
      "epoch: 382\n",
      "training loss: 31650120670335.617\n",
      "validation loss: 24660201705841.363\n",
      "epoch: 383\n",
      "training loss: 31645356726467.324\n",
      "validation loss: 24649594900972.74\n",
      "epoch: 384\n",
      "training loss: 31640616590753.19\n",
      "validation loss: 24639024901932.54\n",
      "epoch: 385\n",
      "training loss: 31635900141796.05\n",
      "validation loss: 24628491567902.8\n",
      "epoch: 386\n",
      "training loss: 31631207258817.875\n",
      "validation loss: 24617994758664.473\n",
      "epoch: 387\n",
      "training loss: 31626537821656.637\n",
      "validation loss: 24607534334594.613\n",
      "epoch: 388\n",
      "training loss: 31621891710763.17\n",
      "validation loss: 24597110156663.74\n",
      "epoch: 389\n",
      "training loss: 31617268807198.035\n",
      "validation loss: 24586722086432.99\n",
      "epoch: 390\n",
      "training loss: 31612668992628.402\n",
      "validation loss: 24576369986051.527\n",
      "epoch: 391\n",
      "training loss: 31608092149324.96\n",
      "validation loss: 24566053718253.758\n",
      "epoch: 392\n",
      "training loss: 31603538160158.867\n",
      "validation loss: 24555773146356.668\n",
      "epoch: 393\n",
      "training loss: 31599006908598.664\n",
      "validation loss: 24545528134257.184\n",
      "epoch: 394\n",
      "training loss: 31594498278707.19\n",
      "validation loss: 24535318546429.465\n",
      "epoch: 395\n",
      "training loss: 31590012155138.645\n",
      "validation loss: 24525144247922.285\n",
      "epoch: 396\n",
      "training loss: 31585548423135.49\n",
      "validation loss: 24515005104356.38\n",
      "epoch: 397\n",
      "training loss: 31581106968525.47\n",
      "validation loss: 24504900981921.86\n",
      "epoch: 398\n",
      "training loss: 31576687677718.652\n",
      "validation loss: 24494831747375.547\n",
      "epoch: 399\n",
      "training loss: 31572290437704.426\n",
      "validation loss: 24484797268038.438\n",
      "epoch: 400\n",
      "training loss: 31567915136048.586\n",
      "validation loss: 24474797411793.086\n",
      "epoch: 401\n",
      "training loss: 31563561660890.332\n",
      "validation loss: 24464832047081.023\n",
      "epoch: 402\n",
      "training loss: 31559229900939.414\n",
      "validation loss: 24454901042900.215\n",
      "epoch: 403\n",
      "training loss: 31554919745473.15\n",
      "validation loss: 24445004268802.527\n",
      "epoch: 404\n",
      "training loss: 31550631084333.617\n",
      "validation loss: 24435141594891.164\n",
      "epoch: 405\n",
      "training loss: 31546363807924.69\n",
      "validation loss: 24425312891818.152\n",
      "epoch: 406\n",
      "training loss: 31542117807209.227\n",
      "validation loss: 24415518030781.867\n",
      "epoch: 407\n",
      "training loss: 31537892973706.184\n",
      "validation loss: 24405756883524.465\n",
      "epoch: 408\n",
      "training loss: 31533689199487.816\n",
      "validation loss: 24396029322329.457\n",
      "epoch: 409\n",
      "training loss: 31529506377176.832\n",
      "validation loss: 24386335220019.227\n",
      "epoch: 410\n",
      "training loss: 31525344399943.57\n",
      "validation loss: 24376674449952.527\n",
      "epoch: 411\n",
      "training loss: 31521203161503.26\n",
      "validation loss: 24367046886022.094\n",
      "epoch: 412\n",
      "training loss: 31517082556113.184\n",
      "validation loss: 24357452402652.12\n",
      "epoch: 413\n",
      "training loss: 31512982478569.945\n",
      "validation loss: 24347890874795.95\n",
      "epoch: 414\n",
      "training loss: 31508902824206.703\n",
      "validation loss: 24338362177933.523\n",
      "epoch: 415\n",
      "training loss: 31504843488890.46\n",
      "validation loss: 24328866188069.105\n",
      "epoch: 416\n",
      "training loss: 31500804369019.316\n",
      "validation loss: 24319402781728.79\n",
      "epoch: 417\n",
      "training loss: 31496785361519.758\n",
      "validation loss: 24309971835958.184\n",
      "epoch: 418\n",
      "training loss: 31492786363843.977\n",
      "validation loss: 24300573228320.01\n",
      "epoch: 419\n",
      "training loss: 31488807273967.19\n",
      "validation loss: 24291206836891.758\n",
      "epoch: 420\n",
      "training loss: 31484847990384.95\n",
      "validation loss: 24281872540263.316\n",
      "epoch: 421\n",
      "training loss: 31480908412110.535\n",
      "validation loss: 24272570217534.684\n",
      "epoch: 422\n",
      "training loss: 31476988438672.258\n",
      "validation loss: 24263299748313.6\n",
      "epoch: 423\n",
      "training loss: 31473087970110.855\n",
      "validation loss: 24254061012713.254\n",
      "epoch: 424\n",
      "training loss: 31469206906976.92\n",
      "validation loss: 24244853891349.984\n",
      "epoch: 425\n",
      "training loss: 31465345150328.203\n",
      "validation loss: 24235678265340.957\n",
      "epoch: 426\n",
      "training loss: 31461502601727.156\n",
      "validation loss: 24226534016301.953\n",
      "epoch: 427\n",
      "training loss: 31457679163238.22\n",
      "validation loss: 24217421026345.016\n",
      "epoch: 428\n",
      "training loss: 31453874737425.375\n",
      "validation loss: 24208339178076.26\n",
      "epoch: 429\n",
      "training loss: 31450089227349.543\n",
      "validation loss: 24199288354593.6\n",
      "epoch: 430\n",
      "training loss: 31446322536566.04\n",
      "validation loss: 24190268439484.5\n",
      "epoch: 431\n",
      "training loss: 31442574569122.09\n",
      "validation loss: 24181279316823.758\n",
      "epoch: 432\n",
      "training loss: 31438845229554.31\n",
      "validation loss: 24172320871171.32\n",
      "epoch: 433\n",
      "training loss: 31435134422886.18\n",
      "validation loss: 24163392987570.016\n",
      "epoch: 434\n",
      "training loss: 31431442054625.61\n",
      "validation loss: 24154495551543.434\n",
      "epoch: 435\n",
      "training loss: 31427768030762.434\n",
      "validation loss: 24145628449093.676\n",
      "epoch: 436\n",
      "training loss: 31424112257765.938\n",
      "validation loss: 24136791566699.215\n",
      "epoch: 437\n",
      "training loss: 31420474642582.49\n",
      "validation loss: 24127984791312.742\n",
      "epoch: 438\n",
      "training loss: 31416855092633.004\n",
      "validation loss: 24119208010358.984\n",
      "epoch: 439\n",
      "training loss: 31413253515810.613\n",
      "validation loss: 24110461111732.617\n",
      "epoch: 440\n",
      "training loss: 31409669820478.19\n",
      "validation loss: 24101743983796.027\n",
      "epoch: 441\n",
      "training loss: 31406103915465.992\n",
      "validation loss: 24093056515377.336\n",
      "epoch: 442\n",
      "training loss: 31402555710069.273\n",
      "validation loss: 24084398595768.164\n",
      "epoch: 443\n",
      "training loss: 31399025114045.895\n",
      "validation loss: 24075770114721.582\n",
      "epoch: 444\n",
      "training loss: 31395512037613.973\n",
      "validation loss: 24067170962450.047\n",
      "epoch: 445\n",
      "training loss: 31392016391449.55\n",
      "validation loss: 24058601029623.254\n",
      "epoch: 446\n",
      "training loss: 31388538086684.24\n",
      "validation loss: 24050060207366.113\n",
      "epoch: 447\n",
      "training loss: 31385077034902.9\n",
      "validation loss: 24041548387256.69\n",
      "epoch: 448\n",
      "training loss: 31381633148141.332\n",
      "validation loss: 24033065461324.1\n",
      "epoch: 449\n",
      "training loss: 31378206338883.984\n",
      "validation loss: 24024611322046.57\n",
      "epoch: 450\n",
      "training loss: 31374796520061.652\n",
      "validation loss: 24016185862349.293\n",
      "epoch: 451\n",
      "training loss: 31371403605049.22\n",
      "validation loss: 24007788975602.473\n",
      "epoch: 452\n",
      "training loss: 31368027507663.355\n",
      "validation loss: 23999420555619.29\n",
      "epoch: 453\n",
      "training loss: 31364668142160.32\n",
      "validation loss: 23991080496653.914\n",
      "epoch: 454\n",
      "training loss: 31361325423233.664\n",
      "validation loss: 23982768693399.5\n",
      "epoch: 455\n",
      "training loss: 31357999266012.03\n",
      "validation loss: 23974485040986.203\n",
      "epoch: 456\n",
      "training loss: 31354689586056.94\n",
      "validation loss: 23966229434979.2\n",
      "epoch: 457\n",
      "training loss: 31351396299360.562\n",
      "validation loss: 23958001771376.74\n",
      "epoch: 458\n",
      "training loss: 31348119322343.523\n",
      "validation loss: 23949801946608.168\n",
      "epoch: 459\n",
      "training loss: 31344858571852.746\n",
      "validation loss: 23941629857532.02\n",
      "epoch: 460\n",
      "training loss: 31341613965159.25\n",
      "validation loss: 23933485401434.016\n",
      "epoch: 461\n",
      "training loss: 31338385419955.984\n",
      "validation loss: 23925368476025.188\n",
      "epoch: 462\n",
      "training loss: 31335172854355.7\n",
      "validation loss: 23917278979439.973\n",
      "epoch: 463\n",
      "training loss: 31331976186888.777\n",
      "validation loss: 23909216810234.246\n",
      "epoch: 464\n",
      "training loss: 31328795336501.14\n",
      "validation loss: 23901181867383.453\n",
      "epoch: 465\n",
      "training loss: 31325630222552.1\n",
      "validation loss: 23893174050280.74\n",
      "epoch: 466\n",
      "training loss: 31322480764812.242\n",
      "validation loss: 23885193258735.016\n",
      "epoch: 467\n",
      "training loss: 31319346883461.37\n",
      "validation loss: 23877239392969.152\n",
      "epoch: 468\n",
      "training loss: 31316228499086.387\n",
      "validation loss: 23869312353618.055\n",
      "epoch: 469\n",
      "training loss: 31313125532679.195\n",
      "validation loss: 23861412041726.816\n",
      "epoch: 470\n",
      "training loss: 31310037905634.715\n",
      "validation loss: 23853538358748.934\n",
      "epoch: 471\n",
      "training loss: 31306965539748.71\n",
      "validation loss: 23845691206544.37\n",
      "epoch: 472\n",
      "training loss: 31303908357215.87\n",
      "validation loss: 23837870487377.824\n",
      "epoch: 473\n",
      "training loss: 31300866280627.668\n",
      "validation loss: 23830076103916.83\n",
      "epoch: 474\n",
      "training loss: 31297839232970.41\n",
      "validation loss: 23822307959230.004\n",
      "epoch: 475\n",
      "training loss: 31294827137623.188\n",
      "validation loss: 23814565956785.2\n",
      "epoch: 476\n",
      "training loss: 31291829918355.86\n",
      "validation loss: 23806850000447.742\n",
      "epoch: 477\n",
      "training loss: 31288847499327.12\n",
      "validation loss: 23799159994478.625\n",
      "epoch: 478\n",
      "training loss: 31285879805082.434\n",
      "validation loss: 23791495843532.746\n",
      "epoch: 479\n",
      "training loss: 31282926760552.14\n",
      "validation loss: 23783857452657.12\n",
      "epoch: 480\n",
      "training loss: 31279988291049.43\n",
      "validation loss: 23776244727289.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 481\n",
      "training loss: 31277064322268.434\n",
      "validation loss: 23768657573254.84\n",
      "epoch: 482\n",
      "training loss: 31274154780282.258\n",
      "validation loss: 23761095896767.066\n",
      "epoch: 483\n",
      "training loss: 31271259591541.062\n",
      "validation loss: 23753559604423.84\n",
      "epoch: 484\n",
      "training loss: 31268378682870.12\n",
      "validation loss: 23746048603206.594\n",
      "epoch: 485\n",
      "training loss: 31265511981467.926\n",
      "validation loss: 23738562800478.44\n",
      "epoch: 486\n",
      "training loss: 31262659414904.285\n",
      "validation loss: 23731102103982.477\n",
      "epoch: 487\n",
      "training loss: 31259820911118.42\n",
      "validation loss: 23723666421840.062\n",
      "epoch: 488\n",
      "training loss: 31256996398417.082\n",
      "validation loss: 23716255662549.176\n",
      "epoch: 489\n",
      "training loss: 31254185805472.66\n",
      "validation loss: 23708869734982.645\n",
      "epoch: 490\n",
      "training loss: 31251389061321.367\n",
      "validation loss: 23701508548386.543\n",
      "epoch: 491\n",
      "training loss: 31248606095361.336\n",
      "validation loss: 23694172012378.484\n",
      "epoch: 492\n",
      "training loss: 31245836837350.785\n",
      "validation loss: 23686860036945.96\n",
      "epoch: 493\n",
      "training loss: 31243081217406.195\n",
      "validation loss: 23679572532444.69\n",
      "epoch: 494\n",
      "training loss: 31240339166000.48\n",
      "validation loss: 23672309409596.996\n",
      "epoch: 495\n",
      "training loss: 31237610613961.14\n",
      "validation loss: 23665070579490.105\n",
      "epoch: 496\n",
      "training loss: 31234895492468.504\n",
      "validation loss: 23657855953574.594\n",
      "epoch: 497\n",
      "training loss: 31232193733053.88\n",
      "validation loss: 23650665443662.688\n",
      "epoch: 498\n",
      "training loss: 31229505267597.805\n",
      "validation loss: 23643498961926.723\n",
      "epoch: 499\n",
      "training loss: 31226830028328.215\n",
      "validation loss: 23636356420897.473\n",
      "epoch: 500\n",
      "training loss: 31224167947818.75\n",
      "validation loss: 23629237733462.586\n",
      "epoch: 501\n",
      "training loss: 31221518958986.895\n",
      "validation loss: 23622142812864.965\n",
      "epoch: 502\n",
      "training loss: 31218882995092.324\n",
      "validation loss: 23615071572701.207\n",
      "epoch: 503\n",
      "training loss: 31216259989735.062\n",
      "validation loss: 23608023926920.027\n",
      "epoch: 504\n",
      "training loss: 31213649876853.848\n",
      "validation loss: 23600999789820.65\n",
      "epoch: 505\n",
      "training loss: 31211052590724.285\n",
      "validation loss: 23593999076051.28\n",
      "epoch: 506\n",
      "training loss: 31208468065957.254\n",
      "validation loss: 23587021700607.555\n",
      "epoch: 507\n",
      "training loss: 31205896237497.1\n",
      "validation loss: 23580067578830.96\n",
      "epoch: 508\n",
      "training loss: 31203337040620.0\n",
      "validation loss: 23573136626407.316\n",
      "epoch: 509\n",
      "training loss: 31200790410932.215\n",
      "validation loss: 23566228759365.23\n",
      "epoch: 510\n",
      "training loss: 31198256284368.46\n",
      "validation loss: 23559343894074.594\n",
      "epoch: 511\n",
      "training loss: 31195734597190.176\n",
      "validation loss: 23552481947245.016\n",
      "epoch: 512\n",
      "training loss: 31193225285983.914\n",
      "validation loss: 23545642835924.38\n",
      "epoch: 513\n",
      "training loss: 31190728287659.652\n",
      "validation loss: 23538826477497.273\n",
      "epoch: 514\n",
      "training loss: 31188243539449.133\n",
      "validation loss: 23532032789683.51\n",
      "epoch: 515\n",
      "training loss: 31185770978904.234\n",
      "validation loss: 23525261690536.67\n",
      "epoch: 516\n",
      "training loss: 31183310543895.36\n",
      "validation loss: 23518513098442.566\n",
      "epoch: 517\n",
      "training loss: 31180862172609.78\n",
      "validation loss: 23511786932117.812\n",
      "epoch: 518\n",
      "training loss: 31178425803550.04\n",
      "validation loss: 23505083110608.332\n",
      "epoch: 519\n",
      "training loss: 31176001375532.32\n",
      "validation loss: 23498401553287.89\n",
      "epoch: 520\n",
      "training loss: 31173588827684.883\n",
      "validation loss: 23491742179856.645\n",
      "epoch: 521\n",
      "training loss: 31171188099446.44\n",
      "validation loss: 23485104910339.695\n",
      "epoch: 522\n",
      "training loss: 31168799130564.586\n",
      "validation loss: 23478489665085.664\n",
      "epoch: 523\n",
      "training loss: 31166421861094.24\n",
      "validation loss: 23471896364765.21\n",
      "epoch: 524\n",
      "training loss: 31164056231396.023\n",
      "validation loss: 23465324930369.664\n",
      "epoch: 525\n",
      "training loss: 31161702182134.78\n",
      "validation loss: 23458775283209.54\n",
      "epoch: 526\n",
      "training loss: 31159359654277.953\n",
      "validation loss: 23452247344913.17\n",
      "epoch: 527\n",
      "training loss: 31157028589094.082\n",
      "validation loss: 23445741037425.285\n",
      "epoch: 528\n",
      "training loss: 31154708928151.254\n",
      "validation loss: 23439256283005.605\n",
      "epoch: 529\n",
      "training loss: 31152400613315.57\n",
      "validation loss: 23432793004227.414\n",
      "epoch: 530\n",
      "training loss: 31150103586749.66\n",
      "validation loss: 23426351123976.26\n",
      "epoch: 531\n",
      "training loss: 31147817790911.105\n",
      "validation loss: 23419930565448.473\n",
      "epoch: 532\n",
      "training loss: 31145543168551.004\n",
      "validation loss: 23413531252149.85\n",
      "epoch: 533\n",
      "training loss: 31143279662712.42\n",
      "validation loss: 23407153107894.266\n",
      "epoch: 534\n",
      "training loss: 31141027216728.934\n",
      "validation loss: 23400796056802.312\n",
      "epoch: 535\n",
      "training loss: 31138785774223.125\n",
      "validation loss: 23394460023299.94\n",
      "epoch: 536\n",
      "training loss: 31136555279105.14\n",
      "validation loss: 23388144932117.13\n",
      "epoch: 537\n",
      "training loss: 31134335675571.18\n",
      "validation loss: 23381850708286.51\n",
      "epoch: 538\n",
      "training loss: 31132126908102.08\n",
      "validation loss: 23375577277142.047\n",
      "epoch: 539\n",
      "training loss: 31129928921461.82\n",
      "validation loss: 23369324564317.71\n",
      "epoch: 540\n",
      "training loss: 31127741660696.13\n",
      "validation loss: 23363092495746.156\n",
      "epoch: 541\n",
      "training loss: 31125565071131.02\n",
      "validation loss: 23356880997657.375\n",
      "epoch: 542\n",
      "training loss: 31123399098371.32\n",
      "validation loss: 23350689996577.426\n",
      "epoch: 543\n",
      "training loss: 31121243688299.34\n",
      "validation loss: 23344519419327.09\n",
      "epoch: 544\n",
      "training loss: 31119098787073.39\n",
      "validation loss: 23338369193020.61\n",
      "epoch: 545\n",
      "training loss: 31116964341126.383\n",
      "validation loss: 23332239245064.35\n",
      "epoch: 546\n",
      "training loss: 31114840297164.457\n",
      "validation loss: 23326129503155.562\n",
      "epoch: 547\n",
      "training loss: 31112726602165.55\n",
      "validation loss: 23320039895281.05\n",
      "epoch: 548\n",
      "training loss: 31110623203378.05\n",
      "validation loss: 23313970349715.93\n",
      "epoch: 549\n",
      "training loss: 31108530048319.38\n",
      "validation loss: 23307920795022.355\n",
      "epoch: 550\n",
      "training loss: 31106447084774.65\n",
      "validation loss: 23301891160048.246\n",
      "epoch: 551\n",
      "training loss: 31104374260795.28\n",
      "validation loss: 23295881373926.016\n",
      "epoch: 552\n",
      "training loss: 31102311524697.645\n",
      "validation loss: 23289891366071.367\n",
      "epoch: 553\n",
      "training loss: 31100258825061.723\n",
      "validation loss: 23283921066181.984\n",
      "epoch: 554\n",
      "training loss: 31098216110729.777\n",
      "validation loss: 23277970404236.367\n",
      "epoch: 555\n",
      "training loss: 31096183330804.945\n",
      "validation loss: 23272039310492.508\n",
      "epoch: 556\n",
      "training loss: 31094160434650.004\n",
      "validation loss: 23266127715486.754\n",
      "epoch: 557\n",
      "training loss: 31092147371885.953\n",
      "validation loss: 23260235550032.504\n",
      "epoch: 558\n",
      "training loss: 31090144092390.79\n",
      "validation loss: 23254362745219.05\n",
      "epoch: 559\n",
      "training loss: 31088150546298.11\n",
      "validation loss: 23248509232410.336\n",
      "epoch: 560\n",
      "training loss: 31086166683995.867\n",
      "validation loss: 23242674943243.76\n",
      "epoch: 561\n",
      "training loss: 31084192456125.05\n",
      "validation loss: 23236859809628.992\n",
      "epoch: 562\n",
      "training loss: 31082227813578.395\n",
      "validation loss: 23231063763746.734\n",
      "epoch: 563\n",
      "training loss: 31080272707499.098\n",
      "validation loss: 23225286738047.582\n",
      "epoch: 564\n",
      "training loss: 31078327089279.555\n",
      "validation loss: 23219528665250.82\n",
      "epoch: 565\n",
      "training loss: 31076390910560.062\n",
      "validation loss: 23213789478343.227\n",
      "epoch: 566\n",
      "training loss: 31074464123227.574\n",
      "validation loss: 23208069110577.938\n",
      "epoch: 567\n",
      "training loss: 31072546679414.445\n",
      "validation loss: 23202367495473.258\n",
      "epoch: 568\n",
      "training loss: 31070638531497.152\n",
      "validation loss: 23196684566811.496\n",
      "epoch: 569\n",
      "training loss: 31068739632095.1\n",
      "validation loss: 23191020258637.84\n",
      "epoch: 570\n",
      "training loss: 31066849934069.316\n",
      "validation loss: 23185374505259.15\n",
      "epoch: 571\n",
      "training loss: 31064969390521.285\n",
      "validation loss: 23179747241242.867\n",
      "epoch: 572\n",
      "training loss: 31063097954791.67\n",
      "validation loss: 23174138401415.86\n",
      "epoch: 573\n",
      "training loss: 31061235580459.125\n",
      "validation loss: 23168547920863.27\n",
      "epoch: 574\n",
      "training loss: 31059382221339.05\n",
      "validation loss: 23162975734927.406\n",
      "epoch: 575\n",
      "training loss: 31057537831482.434\n",
      "validation loss: 23157421779206.617\n",
      "epoch: 576\n",
      "training loss: 31055702365174.6\n",
      "validation loss: 23151885989554.16\n",
      "epoch: 577\n",
      "training loss: 31053875776934.047\n",
      "validation loss: 23146368302077.09\n",
      "epoch: 578\n",
      "training loss: 31052058021511.215\n",
      "validation loss: 23140868653135.184\n",
      "epoch: 579\n",
      "training loss: 31050249053887.375\n",
      "validation loss: 23135386979339.78\n",
      "epoch: 580\n",
      "training loss: 31048448829273.37\n",
      "validation loss: 23129923217552.73\n",
      "epoch: 581\n",
      "training loss: 31046657303108.523\n",
      "validation loss: 23124477304885.3\n",
      "epoch: 582\n",
      "training loss: 31044874431059.395\n",
      "validation loss: 23119049178697.047\n",
      "epoch: 583\n",
      "training loss: 31043100169018.69\n",
      "validation loss: 23113638776594.773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 584\n",
      "training loss: 31041334473104.047\n",
      "validation loss: 23108246036431.434\n",
      "epoch: 585\n",
      "training loss: 31039577299656.945\n",
      "validation loss: 23102870896305.086\n",
      "epoch: 586\n",
      "training loss: 31037828605241.523\n",
      "validation loss: 23097513294557.777\n",
      "epoch: 587\n",
      "training loss: 31036088346643.46\n",
      "validation loss: 23092173169774.53\n",
      "epoch: 588\n",
      "training loss: 31034356480868.82\n",
      "validation loss: 23086850460782.26\n",
      "epoch: 589\n",
      "training loss: 31032632965142.977\n",
      "validation loss: 23081545106648.723\n",
      "epoch: 590\n",
      "training loss: 31030917756909.414\n",
      "validation loss: 23076257046681.48\n",
      "epoch: 591\n",
      "training loss: 31029210813828.695\n",
      "validation loss: 23070986220426.836\n",
      "epoch: 592\n",
      "training loss: 31027512093777.29\n",
      "validation loss: 23065732567668.832\n",
      "epoch: 593\n",
      "training loss: 31025821554846.445\n",
      "validation loss: 23060496028428.17\n",
      "epoch: 594\n",
      "training loss: 31024139155341.13\n",
      "validation loss: 23055276542961.24\n",
      "epoch: 595\n",
      "training loss: 31022464853778.824\n",
      "validation loss: 23050074051759.03\n",
      "epoch: 596\n",
      "training loss: 31020798608888.348\n",
      "validation loss: 23044888495546.176\n",
      "epoch: 597\n",
      "training loss: 31019140379608.574\n",
      "validation loss: 23039719815279.89\n",
      "epoch: 598\n",
      "training loss: 31017490125086.676\n",
      "validation loss: 23034567952148.992\n",
      "epoch: 599\n",
      "training loss: 31015847804675.28\n",
      "validation loss: 23029432847572.875\n",
      "epoch: 600\n",
      "training loss: 31014213377924.047\n",
      "validation loss: 23024314443200.52\n",
      "epoch: 601\n",
      "training loss: 31012586804532.72\n",
      "validation loss: 23019212680909.484\n",
      "epoch: 602\n",
      "training loss: 31010968043445.895\n",
      "validation loss: 23014127502804.81\n",
      "epoch: 603\n",
      "training loss: 31009342772032.273\n",
      "validation loss: 23009058851535.34\n",
      "epoch: 604\n",
      "training loss: 31007118224113.176\n",
      "validation loss: 23005374915418.605\n",
      "epoch: 605\n",
      "training loss: 31005522060752.75\n",
      "validation loss: 23000338425062.164\n",
      "epoch: 606\n",
      "training loss: 31003899484160.855\n",
      "validation loss: 22997014848684.824\n",
      "epoch: 607\n",
      "training loss: 31002318595321.285\n",
      "validation loss: 22992059867400.312\n",
      "epoch: 608\n",
      "training loss: 30968224918168.164\n",
      "validation loss: 23467244628477.973\n",
      "epoch: 609\n",
      "training loss: 30960333278233.105\n",
      "validation loss: 23464983982682.008\n",
      "epoch: 610\n",
      "training loss: 30958343689396.11\n",
      "validation loss: 23459746123620.258\n",
      "epoch: 611\n",
      "training loss: 30956362731306.93\n",
      "validation loss: 23454526041244.18\n",
      "epoch: 612\n",
      "training loss: 30954390361625.297\n",
      "validation loss: 23449323668084.79\n",
      "epoch: 613\n",
      "training loss: 30952426534531.35\n",
      "validation loss: 23444138943008.57\n",
      "epoch: 614\n",
      "training loss: 30934352979287.906\n",
      "validation loss: 23440189560165.8\n",
      "epoch: 615\n",
      "training loss: 30960338548552.41\n",
      "validation loss: 23430215690726.418\n",
      "epoch: 616\n",
      "training loss: 30993034089714.75\n",
      "validation loss: 23146540849956.58\n",
      "epoch: 617\n",
      "training loss: 30991525025753.07\n",
      "validation loss: 23141755382064.844\n",
      "epoch: 618\n",
      "training loss: 30989865671668.055\n",
      "validation loss: 23136985325088.83\n",
      "epoch: 619\n",
      "training loss: 30956520648883.293\n",
      "validation loss: 23617072763194.09\n",
      "epoch: 620\n",
      "training loss: 30982105579206.39\n",
      "validation loss: 23152427112892.78\n",
      "epoch: 621\n",
      "training loss: 30980624276359.254\n",
      "validation loss: 23147713297240.96\n",
      "epoch: 622\n",
      "training loss: 30979150042253.19\n",
      "validation loss: 23143014658853.58\n",
      "epoch: 623\n",
      "training loss: 30977682840966.082\n",
      "validation loss: 23138331145515.38\n",
      "epoch: 624\n",
      "training loss: 30976222636749.383\n",
      "validation loss: 23133662705214.23\n",
      "epoch: 625\n",
      "training loss: 30974769394031.176\n",
      "validation loss: 23129009286139.832\n",
      "epoch: 626\n",
      "training loss: 30973323077417.36\n",
      "validation loss: 23124370836682.63\n",
      "epoch: 627\n",
      "training loss: 30971883651691.86\n",
      "validation loss: 23119747305432.816\n",
      "epoch: 628\n",
      "training loss: 30970451081816.406\n",
      "validation loss: 23115138641179.41\n",
      "epoch: 629\n",
      "training loss: 30969025332930.062\n",
      "validation loss: 23110544792909.35\n",
      "epoch: 630\n",
      "training loss: 30967606370348.574\n",
      "validation loss: 23105965709806.633\n",
      "epoch: 631\n",
      "training loss: 30966194159563.707\n",
      "validation loss: 23101401341251.41\n",
      "epoch: 632\n",
      "training loss: 30964788666242.418\n",
      "validation loss: 23096851636819.18\n",
      "epoch: 633\n",
      "training loss: 30963389856226.08\n",
      "validation loss: 23092316546279.91\n",
      "epoch: 634\n",
      "training loss: 30961997695529.7\n",
      "validation loss: 23087796019597.2\n",
      "epoch: 635\n",
      "training loss: 30960612150340.99\n",
      "validation loss: 23083290006927.453\n",
      "epoch: 636\n",
      "training loss: 30959233187019.598\n",
      "validation loss: 23078798458619.062\n",
      "epoch: 637\n",
      "training loss: 30957860772096.215\n",
      "validation loss: 23074321325211.543\n",
      "epoch: 638\n",
      "training loss: 30956494872271.656\n",
      "validation loss: 23069858557434.773\n",
      "epoch: 639\n",
      "training loss: 30955135454415.93\n",
      "validation loss: 23065410106208.12\n",
      "epoch: 640\n",
      "training loss: 30953782485567.176\n",
      "validation loss: 23060975922639.688\n",
      "epoch: 641\n",
      "training loss: 30952435932930.125\n",
      "validation loss: 23056555958025.43\n",
      "epoch: 642\n",
      "training loss: 30951095763872.89\n",
      "validation loss: 23052150163848.44\n",
      "epoch: 643\n",
      "training loss: 30949761945914.35\n",
      "validation loss: 23047758491778.09\n",
      "epoch: 644\n",
      "training loss: 30948434446605.863\n",
      "validation loss: 23043380893669.215\n",
      "epoch: 645\n",
      "training loss: 30947113221296.07\n",
      "validation loss: 23039017321561.277\n",
      "epoch: 646\n",
      "training loss: 30945779056214.05\n",
      "validation loss: 23034667727667.727\n",
      "epoch: 647\n",
      "training loss: 30944470316711.723\n",
      "validation loss: 23030332049997.75\n",
      "epoch: 648\n",
      "training loss: 30943167768568.53\n",
      "validation loss: 23026010255652.984\n",
      "epoch: 649\n",
      "training loss: 30941871380302.195\n",
      "validation loss: 23021702297403.52\n",
      "epoch: 650\n",
      "training loss: 30940581120590.63\n",
      "validation loss: 23017408128200.242\n",
      "epoch: 651\n",
      "training loss: 30939296958271.17\n",
      "validation loss: 23013127701174.09\n",
      "epoch: 652\n",
      "training loss: 30938018862339.754\n",
      "validation loss: 23008860969635.28\n",
      "epoch: 653\n",
      "training loss: 30936746801950.094\n",
      "validation loss: 23004607887072.55\n",
      "epoch: 654\n",
      "training loss: 30935480746412.91\n",
      "validation loss: 23000368407152.38\n",
      "epoch: 655\n",
      "training loss: 30934220665195.11\n",
      "validation loss: 22996142483718.277\n",
      "epoch: 656\n",
      "training loss: 30932966527918.973\n",
      "validation loss: 22991930070789.97\n",
      "epoch: 657\n",
      "training loss: 30931718304361.41\n",
      "validation loss: 22987731122562.715\n",
      "epoch: 658\n",
      "training loss: 30930475964453.164\n",
      "validation loss: 22983545593406.504\n",
      "epoch: 659\n",
      "training loss: 30929239478277.98\n",
      "validation loss: 22979373437865.363\n",
      "epoch: 660\n",
      "training loss: 30928008816071.902\n",
      "validation loss: 22975214610656.59\n",
      "epoch: 661\n",
      "training loss: 30926783948222.43\n",
      "validation loss: 22971069066670.04\n",
      "epoch: 662\n",
      "training loss: 30925564845267.8\n",
      "validation loss: 22966936760967.34\n",
      "epoch: 663\n",
      "training loss: 30924351477896.184\n",
      "validation loss: 22962817648781.266\n",
      "epoch: 664\n",
      "training loss: 30923143816944.945\n",
      "validation loss: 22958711685514.902\n",
      "epoch: 665\n",
      "training loss: 30921941833399.875\n",
      "validation loss: 22954618826740.992\n",
      "epoch: 666\n",
      "training loss: 30920745498394.43\n",
      "validation loss: 22950539028201.207\n",
      "epoch: 667\n",
      "training loss: 30919554783209.004\n",
      "validation loss: 22946472245805.42\n",
      "epoch: 668\n",
      "training loss: 30918369659270.15\n",
      "validation loss: 22942418435631.0\n",
      "epoch: 669\n",
      "training loss: 30917190098149.844\n",
      "validation loss: 22938377553922.098\n",
      "epoch: 670\n",
      "training loss: 30916016071564.766\n",
      "validation loss: 22934349557088.957\n",
      "epoch: 671\n",
      "training loss: 30914847551375.55\n",
      "validation loss: 22930334401707.22\n",
      "epoch: 672\n",
      "training loss: 30913684509586.043\n",
      "validation loss: 22926332044517.168\n",
      "epoch: 673\n",
      "training loss: 30912526918342.58\n",
      "validation loss: 22922342442423.117\n",
      "epoch: 674\n",
      "training loss: 30911374749933.273\n",
      "validation loss: 22918365552492.652\n",
      "epoch: 675\n",
      "training loss: 30910227976787.273\n",
      "validation loss: 22914401331955.98\n",
      "epoch: 676\n",
      "training loss: 30909086571474.06\n",
      "validation loss: 22910449738205.22\n",
      "epoch: 677\n",
      "training loss: 30907950506702.734\n",
      "validation loss: 22906510728793.74\n",
      "epoch: 678\n",
      "training loss: 30906819755321.285\n",
      "validation loss: 22902584261435.46\n",
      "epoch: 679\n",
      "training loss: 30905694290315.9\n",
      "validation loss: 22898670294004.2\n",
      "epoch: 680\n",
      "training loss: 30904574084810.285\n",
      "validation loss: 22894768784532.977\n",
      "epoch: 681\n",
      "training loss: 30903459112064.887\n",
      "validation loss: 22890879691213.348\n",
      "epoch: 682\n",
      "training loss: 30902349345476.305\n",
      "validation loss: 22887002972394.746\n",
      "epoch: 683\n",
      "training loss: 30901244758576.51\n",
      "validation loss: 22883138586583.832\n",
      "epoch: 684\n",
      "training loss: 30900145325032.184\n",
      "validation loss: 22879286492443.793\n",
      "epoch: 685\n",
      "training loss: 30899051018644.047\n",
      "validation loss: 22875446648793.71\n",
      "epoch: 686\n",
      "training loss: 30897961813346.164\n",
      "validation loss: 22871619014607.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 687\n",
      "training loss: 30896877683205.254\n",
      "validation loss: 22867803549015.297\n",
      "epoch: 688\n",
      "training loss: 30895798602420.02\n",
      "validation loss: 22864000211298.71\n",
      "epoch: 689\n",
      "training loss: 30894724545320.484\n",
      "validation loss: 22860208960894.29\n",
      "epoch: 690\n",
      "training loss: 30893655486367.293\n",
      "validation loss: 22856429757390.797\n",
      "epoch: 691\n",
      "training loss: 30892591400151.04\n",
      "validation loss: 22852662560529.023\n",
      "epoch: 692\n",
      "training loss: 30891532261391.586\n",
      "validation loss: 22848907330201.113\n",
      "epoch: 693\n",
      "training loss: 30890478044937.34\n",
      "validation loss: 22845164026449.95\n",
      "epoch: 694\n",
      "training loss: 30889428725764.46\n",
      "validation loss: 22841432609468.523\n",
      "epoch: 695\n",
      "training loss: 30888384278975.88\n",
      "validation loss: 22837713039599.277\n",
      "epoch: 696\n",
      "training loss: 30887344679799.64\n",
      "validation loss: 22834005277333.53\n",
      "epoch: 697\n",
      "training loss: 30886309903584.254\n",
      "validation loss: 22830309283310.81\n",
      "epoch: 698\n",
      "training loss: 30885279925773.98\n",
      "validation loss: 22826625018318.242\n",
      "epoch: 699\n",
      "training loss: 30884254721532.22\n",
      "validation loss: 22822952443289.92\n",
      "epoch: 700\n",
      "training loss: 30883232752997.438\n",
      "validation loss: 22819291519306.08\n",
      "epoch: 701\n",
      "training loss: 30880221685218.12\n",
      "validation loss: 22839386105686.957\n",
      "epoch: 702\n",
      "training loss: 30879209420316.14\n",
      "validation loss: 22835750199063.72\n",
      "epoch: 703\n",
      "training loss: 30878201843458.93\n",
      "validation loss: 22832125817986.215\n",
      "epoch: 704\n",
      "training loss: 30877198930813.71\n",
      "validation loss: 22828512924142.84\n",
      "epoch: 705\n",
      "training loss: 30876200658669.06\n",
      "validation loss: 22824911479365.27\n",
      "epoch: 706\n",
      "training loss: 30875207003434.207\n",
      "validation loss: 22821321445627.78\n",
      "epoch: 707\n",
      "training loss: 30874217941638.484\n",
      "validation loss: 22817742785046.715\n",
      "epoch: 708\n",
      "training loss: 30873233449930.68\n",
      "validation loss: 22814175459879.855\n",
      "epoch: 709\n",
      "training loss: 30872253505078.453\n",
      "validation loss: 22810619432525.863\n",
      "epoch: 710\n",
      "training loss: 30871278083967.707\n",
      "validation loss: 22807074665523.656\n",
      "epoch: 711\n",
      "training loss: 30870307163602.023\n",
      "validation loss: 22803541121551.848\n",
      "epoch: 712\n",
      "training loss: 30869340721102.008\n",
      "validation loss: 22800018763428.176\n",
      "epoch: 713\n",
      "training loss: 30868378733704.74\n",
      "validation loss: 22796507554108.902\n",
      "epoch: 714\n",
      "training loss: 30867421178763.17\n",
      "validation loss: 22793007456688.242\n",
      "epoch: 715\n",
      "training loss: 30866468033745.51\n",
      "validation loss: 22789518434397.79\n",
      "epoch: 716\n",
      "training loss: 30865519276234.69\n",
      "validation loss: 22786040450605.97\n",
      "epoch: 717\n",
      "training loss: 30864574883927.707\n",
      "validation loss: 22782573468817.418\n",
      "epoch: 718\n",
      "training loss: 30863634834635.113\n",
      "validation loss: 22779117452672.47\n",
      "epoch: 719\n",
      "training loss: 30862699106280.406\n",
      "validation loss: 22775672365946.57\n",
      "epoch: 720\n",
      "training loss: 30861767676899.434\n",
      "validation loss: 22772238172549.707\n",
      "epoch: 721\n",
      "training loss: 30860840524639.875\n",
      "validation loss: 22768814836525.875\n",
      "epoch: 722\n",
      "training loss: 30859917627760.62\n",
      "validation loss: 22765402322052.496\n",
      "epoch: 723\n",
      "training loss: 30858998964631.23\n",
      "validation loss: 22762000593439.875\n",
      "epoch: 724\n",
      "training loss: 30858084513731.363\n",
      "validation loss: 22758609615130.668\n",
      "epoch: 725\n",
      "training loss: 30857174253650.24\n",
      "validation loss: 22755229351699.31\n",
      "epoch: 726\n",
      "training loss: 30856268163086.023\n",
      "validation loss: 22751859767851.47\n",
      "epoch: 727\n",
      "training loss: 30855366220845.363\n",
      "validation loss: 22748500828423.52\n",
      "epoch: 728\n",
      "training loss: 30854468405842.74\n",
      "validation loss: 22745152498381.98\n",
      "epoch: 729\n",
      "training loss: 30853574697099.973\n",
      "validation loss: 22741814742823.008\n",
      "epoch: 730\n",
      "training loss: 30852685073745.68\n",
      "validation loss: 22738487526971.816\n",
      "epoch: 731\n",
      "training loss: 30851799515014.71\n",
      "validation loss: 22735170816182.18\n",
      "epoch: 732\n",
      "training loss: 30850918000247.6\n",
      "validation loss: 22731864575935.9\n",
      "epoch: 733\n",
      "training loss: 30850040508890.055\n",
      "validation loss: 22728568771842.254\n",
      "epoch: 734\n",
      "training loss: 30849167020492.406\n",
      "validation loss: 22725283369637.48\n",
      "epoch: 735\n",
      "training loss: 30848297514709.066\n",
      "validation loss: 22722008335184.25\n",
      "epoch: 736\n",
      "training loss: 30847431971298.035\n",
      "validation loss: 22718743634471.17\n",
      "epoch: 737\n",
      "training loss: 30846570370120.316\n",
      "validation loss: 22715489233612.234\n",
      "epoch: 738\n",
      "training loss: 30845712691139.45\n",
      "validation loss: 22712245098846.31\n",
      "epoch: 739\n",
      "training loss: 30844858914420.945\n",
      "validation loss: 22709011196536.63\n",
      "epoch: 740\n",
      "training loss: 30844009020131.8\n",
      "validation loss: 22705787493170.29\n",
      "epoch: 741\n",
      "training loss: 30843162988539.95\n",
      "validation loss: 22702573955357.72\n",
      "epoch: 742\n",
      "training loss: 30842320800013.78\n",
      "validation loss: 22699370549832.184\n",
      "epoch: 743\n",
      "training loss: 30841482435021.598\n",
      "validation loss: 22696177243449.285\n",
      "epoch: 744\n",
      "training loss: 30840647874131.137\n",
      "validation loss: 22692994003186.45\n",
      "epoch: 745\n",
      "training loss: 30839817098009.04\n",
      "validation loss: 22689820796142.41\n",
      "epoch: 746\n",
      "training loss: 30838990087420.355\n",
      "validation loss: 22686657589536.754\n",
      "epoch: 747\n",
      "training loss: 30838166823228.047\n",
      "validation loss: 22683504350709.375\n",
      "epoch: 748\n",
      "training loss: 30837347286392.492\n",
      "validation loss: 22680361047120.027\n",
      "epoch: 749\n",
      "training loss: 30836531457970.99\n",
      "validation loss: 22677227646347.777\n",
      "epoch: 750\n",
      "training loss: 30835719319117.227\n",
      "validation loss: 22674104116090.574\n",
      "epoch: 751\n",
      "training loss: 30834910851080.863\n",
      "validation loss: 22670990424164.71\n",
      "epoch: 752\n",
      "training loss: 30834106035206.984\n",
      "validation loss: 22667886538504.38\n",
      "epoch: 753\n",
      "training loss: 30833304852935.62\n",
      "validation loss: 22664792427161.152\n",
      "epoch: 754\n",
      "training loss: 30832507285801.305\n",
      "validation loss: 22661708058303.535\n",
      "epoch: 755\n",
      "training loss: 30831713315432.523\n",
      "validation loss: 22658633400216.46\n",
      "epoch: 756\n",
      "training loss: 30830922923551.32\n",
      "validation loss: 22655568421300.82\n",
      "epoch: 757\n",
      "training loss: 30830136091972.74\n",
      "validation loss: 22652513090072.996\n",
      "epoch: 758\n",
      "training loss: 30829352802604.42\n",
      "validation loss: 22649467375164.39\n",
      "epoch: 759\n",
      "training loss: 30828573037446.074\n",
      "validation loss: 22646431245320.926\n",
      "epoch: 760\n",
      "training loss: 30827796778589.047\n",
      "validation loss: 22643404669402.617\n",
      "epoch: 761\n",
      "training loss: 30827024008215.848\n",
      "validation loss: 22640387616383.082\n",
      "epoch: 762\n",
      "training loss: 30826254708599.664\n",
      "validation loss: 22637380055349.086\n",
      "epoch: 763\n",
      "training loss: 30825488862103.938\n",
      "validation loss: 22634381955500.066\n",
      "epoch: 764\n",
      "training loss: 30824726451181.867\n",
      "validation loss: 22631393286147.703\n",
      "epoch: 765\n",
      "training loss: 30823967458375.973\n",
      "validation loss: 22628414016715.414\n",
      "epoch: 766\n",
      "training loss: 30823211866317.637\n",
      "validation loss: 22625444116737.926\n",
      "epoch: 767\n",
      "training loss: 30822459657726.66\n",
      "validation loss: 22622483555860.85\n",
      "epoch: 768\n",
      "training loss: 30821710815410.79\n",
      "validation loss: 22619532303840.17\n",
      "epoch: 769\n",
      "training loss: 30820965322265.285\n",
      "validation loss: 22616590330541.836\n",
      "epoch: 770\n",
      "training loss: 30820223161272.484\n",
      "validation loss: 22613657605941.28\n",
      "epoch: 771\n",
      "training loss: 30819484315501.336\n",
      "validation loss: 22610734100123.027\n",
      "epoch: 772\n",
      "training loss: 30818748768106.992\n",
      "validation loss: 22607819783280.188\n",
      "epoch: 773\n",
      "training loss: 30818016502330.312\n",
      "validation loss: 22604914625714.062\n",
      "epoch: 774\n",
      "training loss: 30817287501497.51\n",
      "validation loss: 22602018597833.668\n",
      "epoch: 775\n",
      "training loss: 30816561749019.637\n",
      "validation loss: 22599131670155.336\n",
      "epoch: 776\n",
      "training loss: 30815839228392.18\n",
      "validation loss: 22596253813302.24\n",
      "epoch: 777\n",
      "training loss: 30815119923194.68\n",
      "validation loss: 22593384998003.984\n",
      "epoch: 778\n",
      "training loss: 30814403817090.223\n",
      "validation loss: 22590525195096.16\n",
      "epoch: 779\n",
      "training loss: 30813690893825.066\n",
      "validation loss: 22587674375519.934\n",
      "epoch: 780\n",
      "training loss: 30812981137228.19\n",
      "validation loss: 22584832510321.586\n",
      "epoch: 781\n",
      "training loss: 30812274531210.926\n",
      "validation loss: 22581999570652.12\n",
      "epoch: 782\n",
      "training loss: 30811571059766.45\n",
      "validation loss: 22579175527766.816\n",
      "epoch: 783\n",
      "training loss: 30810870706969.465\n",
      "validation loss: 22576360353024.816\n",
      "epoch: 784\n",
      "training loss: 30810173456975.688\n",
      "validation loss: 22573554017888.7\n",
      "epoch: 785\n",
      "training loss: 30809479294021.535\n",
      "validation loss: 22570756493924.062\n",
      "epoch: 786\n",
      "training loss: 30808788202423.625\n",
      "validation loss: 22567967752799.11\n",
      "epoch: 787\n",
      "training loss: 30808100166578.42\n",
      "validation loss: 22565187766284.234\n",
      "epoch: 788\n",
      "training loss: 30807415170961.836\n",
      "validation loss: 22562416506251.6\n",
      "epoch: 789\n",
      "training loss: 30806733200128.76\n",
      "validation loss: 22559653944674.734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 790\n",
      "training loss: 30806054238712.73\n",
      "validation loss: 22556900053628.12\n",
      "epoch: 791\n",
      "training loss: 30805378271425.496\n",
      "validation loss: 22554154805286.785\n",
      "epoch: 792\n",
      "training loss: 30804705283056.633\n",
      "validation loss: 22551418171925.895\n",
      "epoch: 793\n",
      "training loss: 30804035258473.133\n",
      "validation loss: 22548690125920.36\n",
      "epoch: 794\n",
      "training loss: 30803368182619.03\n",
      "validation loss: 22545970639744.406\n",
      "epoch: 795\n",
      "training loss: 30802704040514.97\n",
      "validation loss: 22543259685971.19\n",
      "epoch: 796\n",
      "training loss: 30802042817257.883\n",
      "validation loss: 22540557237272.43\n",
      "epoch: 797\n",
      "training loss: 30801384498020.535\n",
      "validation loss: 22537863266417.957\n",
      "epoch: 798\n",
      "training loss: 30800729068051.184\n",
      "validation loss: 22535177746275.34\n",
      "epoch: 799\n",
      "training loss: 30800076512673.145\n",
      "validation loss: 22532500649809.504\n",
      "epoch: 800\n",
      "training loss: 30799426817284.484\n",
      "validation loss: 22529831950082.332\n",
      "epoch: 801\n",
      "training loss: 30798779967357.562\n",
      "validation loss: 22527171620252.26\n",
      "epoch: 802\n",
      "training loss: 30798135948438.703\n",
      "validation loss: 22524519633573.902\n",
      "epoch: 803\n",
      "training loss: 30797494746147.8\n",
      "validation loss: 22521875963397.67\n",
      "epoch: 804\n",
      "training loss: 30796856346177.957\n",
      "validation loss: 22519240583169.375\n",
      "epoch: 805\n",
      "training loss: 30796220734295.08\n",
      "validation loss: 22516613466429.855\n",
      "epoch: 806\n",
      "training loss: 30795587896337.54\n",
      "validation loss: 22513994586814.566\n",
      "epoch: 807\n",
      "training loss: 30794957818215.805\n",
      "validation loss: 22511383918053.25\n",
      "epoch: 808\n",
      "training loss: 30794330485912.043\n",
      "validation loss: 22508781433969.52\n",
      "epoch: 809\n",
      "training loss: 30793705885479.805\n",
      "validation loss: 22506187108480.49\n",
      "epoch: 810\n",
      "training loss: 30793084003043.6\n",
      "validation loss: 22503600915596.39\n",
      "epoch: 811\n",
      "training loss: 30792464824798.594\n",
      "validation loss: 22501022829420.246\n",
      "epoch: 812\n",
      "training loss: 30791848337010.19\n",
      "validation loss: 22498452824147.426\n",
      "epoch: 813\n",
      "training loss: 30791234526013.742\n",
      "validation loss: 22495890874065.348\n",
      "epoch: 814\n",
      "training loss: 30790623378214.137\n",
      "validation loss: 22493336953553.04\n",
      "epoch: 815\n",
      "training loss: 30790014880085.473\n",
      "validation loss: 22490791037080.824\n",
      "epoch: 816\n",
      "training loss: 30789409018170.695\n",
      "validation loss: 22488253099209.96\n",
      "epoch: 817\n",
      "training loss: 30788805779081.246\n",
      "validation loss: 22485723114592.23\n",
      "epoch: 818\n",
      "training loss: 30788205149496.734\n",
      "validation loss: 22483201057969.605\n",
      "epoch: 819\n",
      "training loss: 30787607116164.547\n",
      "validation loss: 22480686904173.9\n",
      "epoch: 820\n",
      "training loss: 30787011665899.55\n",
      "validation loss: 22478180628126.375\n",
      "epoch: 821\n",
      "training loss: 30786418785583.723\n",
      "validation loss: 22475682204837.438\n",
      "epoch: 822\n",
      "training loss: 30785828462165.816\n",
      "validation loss: 22473191609406.215\n",
      "epoch: 823\n",
      "training loss: 30785240682661.004\n",
      "validation loss: 22470708817020.254\n",
      "epoch: 824\n",
      "training loss: 30784655434150.59\n",
      "validation loss: 22468233802955.164\n",
      "epoch: 825\n",
      "training loss: 30784072703781.586\n",
      "validation loss: 22465766542574.22\n",
      "epoch: 826\n",
      "training loss: 30783492478766.48\n",
      "validation loss: 22463307011328.082\n",
      "epoch: 827\n",
      "training loss: 30782914746382.83\n",
      "validation loss: 22460855184754.39\n",
      "epoch: 828\n",
      "training loss: 30782339493972.94\n",
      "validation loss: 22458411038477.434\n",
      "epoch: 829\n",
      "training loss: 30781766708943.58\n",
      "validation loss: 22455974548207.836\n",
      "epoch: 830\n",
      "training loss: 30781196378765.586\n",
      "validation loss: 22453545689742.156\n",
      "epoch: 831\n",
      "training loss: 30780628490973.59\n",
      "validation loss: 22451124438962.613\n",
      "epoch: 832\n",
      "training loss: 30780063033165.68\n",
      "validation loss: 22448710771836.684\n",
      "epoch: 833\n",
      "training loss: 30779499993003.066\n",
      "validation loss: 22446304664416.805\n",
      "epoch: 834\n",
      "training loss: 30778939358209.773\n",
      "validation loss: 22443906092840.016\n",
      "epoch: 835\n",
      "training loss: 30778381116572.29\n",
      "validation loss: 22441515033327.613\n",
      "epoch: 836\n",
      "training loss: 30777825255939.31\n",
      "validation loss: 22439131462184.863\n",
      "epoch: 837\n",
      "training loss: 30777271764221.355\n",
      "validation loss: 22436755355800.61\n",
      "epoch: 838\n",
      "training loss: 30776720629390.5\n",
      "validation loss: 22434386690646.99\n",
      "epoch: 839\n",
      "training loss: 30776171839480.047\n",
      "validation loss: 22432025443279.066\n",
      "epoch: 840\n",
      "training loss: 30775625382584.19\n",
      "validation loss: 22429671590334.53\n",
      "epoch: 841\n",
      "training loss: 30775081246857.76\n",
      "validation loss: 22427325108533.363\n",
      "epoch: 842\n",
      "training loss: 30774539420515.85\n",
      "validation loss: 22424985974677.48\n",
      "epoch: 843\n",
      "training loss: 30773999891833.566\n",
      "validation loss: 22422654165650.47\n",
      "epoch: 844\n",
      "training loss: 30773462649145.684\n",
      "validation loss: 22420329658417.207\n",
      "epoch: 845\n",
      "training loss: 30772927680846.363\n",
      "validation loss: 22418012430023.566\n",
      "epoch: 846\n",
      "training loss: 30772394975388.855\n",
      "validation loss: 22415702457596.12\n",
      "epoch: 847\n",
      "training loss: 30771864521285.156\n",
      "validation loss: 22413399718341.754\n",
      "epoch: 848\n",
      "training loss: 30771336307105.76\n",
      "validation loss: 22411104189547.426\n",
      "epoch: 849\n",
      "training loss: 30770810321479.348\n",
      "validation loss: 22408815848579.785\n",
      "epoch: 850\n",
      "training loss: 30770286553092.457\n",
      "validation loss: 22406534672884.906\n",
      "epoch: 851\n",
      "training loss: 30769764990689.24\n",
      "validation loss: 22404260639987.95\n",
      "epoch: 852\n",
      "training loss: 30769245623071.133\n",
      "validation loss: 22401993727492.867\n",
      "epoch: 853\n",
      "training loss: 30768728439096.58\n",
      "validation loss: 22399733913082.06\n",
      "epoch: 854\n",
      "training loss: 30768213427680.727\n",
      "validation loss: 22397481174516.105\n",
      "epoch: 855\n",
      "training loss: 30767700577795.17\n",
      "validation loss: 22395235489633.44\n",
      "epoch: 856\n",
      "training loss: 30767189878467.63\n",
      "validation loss: 22392996836350.027\n",
      "epoch: 857\n",
      "training loss: 30766681318781.664\n",
      "validation loss: 22390765192659.094\n",
      "epoch: 858\n",
      "training loss: 30766174887876.42\n",
      "validation loss: 22388540536630.785\n",
      "epoch: 859\n",
      "training loss: 30765670574946.32\n",
      "validation loss: 22386322846411.895\n",
      "epoch: 860\n",
      "training loss: 30765168369240.793\n",
      "validation loss: 22384112100225.56\n",
      "epoch: 861\n",
      "training loss: 30764668260064.0\n",
      "validation loss: 22381908276370.92\n",
      "epoch: 862\n",
      "training loss: 30764170236774.52\n",
      "validation loss: 22379711353222.887\n",
      "epoch: 863\n",
      "training loss: 30763674288785.137\n",
      "validation loss: 22377521309231.797\n",
      "epoch: 864\n",
      "training loss: 30763180405562.5\n",
      "validation loss: 22375338122923.113\n",
      "epoch: 865\n",
      "training loss: 30762688576626.883\n",
      "validation loss: 22373161772897.176\n",
      "epoch: 866\n",
      "training loss: 30762198791551.918\n",
      "validation loss: 22370992237828.855\n",
      "epoch: 867\n",
      "training loss: 30761711039964.28\n",
      "validation loss: 22368829496467.3\n",
      "epoch: 868\n",
      "training loss: 30761225311543.49\n",
      "validation loss: 22366673527635.625\n",
      "epoch: 869\n",
      "training loss: 30760741596021.566\n",
      "validation loss: 22364524310230.625\n",
      "epoch: 870\n",
      "training loss: 30760259883182.812\n",
      "validation loss: 22362381823222.48\n",
      "epoch: 871\n",
      "training loss: 30759780162863.52\n",
      "validation loss: 22360246045654.508\n",
      "epoch: 872\n",
      "training loss: 30759302424951.75\n",
      "validation loss: 22358116956642.812\n",
      "epoch: 873\n",
      "training loss: 30758826659386.99\n",
      "validation loss: 22355994535376.043\n",
      "epoch: 874\n",
      "training loss: 30758352856159.97\n",
      "validation loss: 22353878761115.117\n",
      "epoch: 875\n",
      "training loss: 30757881005312.37\n",
      "validation loss: 22351769613192.902\n",
      "epoch: 876\n",
      "training loss: 30757411096936.543\n",
      "validation loss: 22349667071013.965\n",
      "epoch: 877\n",
      "training loss: 30756943121175.29\n",
      "validation loss: 22347571114054.28\n",
      "epoch: 878\n",
      "training loss: 30756477068221.586\n",
      "validation loss: 22345481721860.95\n",
      "epoch: 879\n",
      "training loss: 30756012928318.33\n",
      "validation loss: 22343398874051.953\n",
      "epoch: 880\n",
      "training loss: 30755550691758.07\n",
      "validation loss: 22341322550315.81\n",
      "epoch: 881\n",
      "training loss: 30755090348882.78\n",
      "validation loss: 22339252730411.363\n",
      "epoch: 882\n",
      "training loss: 30754631890083.613\n",
      "validation loss: 22337189394167.473\n",
      "epoch: 883\n",
      "training loss: 30754175305800.61\n",
      "validation loss: 22335132521482.777\n",
      "epoch: 884\n",
      "training loss: 30753720586522.484\n",
      "validation loss: 22333082092325.387\n",
      "epoch: 885\n",
      "training loss: 30753267722786.375\n",
      "validation loss: 22331038086732.617\n",
      "epoch: 886\n",
      "training loss: 30752816705177.562\n",
      "validation loss: 22329000484810.742\n",
      "epoch: 887\n",
      "training loss: 30752367524329.285\n",
      "validation loss: 22326969266734.71\n",
      "epoch: 888\n",
      "training loss: 30751920170922.434\n",
      "validation loss: 22324944412747.875\n",
      "epoch: 889\n",
      "training loss: 30751474635685.36\n",
      "validation loss: 22322925903161.746\n",
      "epoch: 890\n",
      "training loss: 30751030909393.586\n",
      "validation loss: 22320913718355.7\n",
      "epoch: 891\n",
      "training loss: 30750588982869.62\n",
      "validation loss: 22318907838776.746\n",
      "epoch: 892\n",
      "training loss: 30750148846982.652\n",
      "validation loss: 22316908244939.227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 893\n",
      "training loss: 30749710492648.38\n",
      "validation loss: 22314914917424.613\n",
      "epoch: 894\n",
      "training loss: 30749273910828.73\n",
      "validation loss: 22312927836881.17\n",
      "epoch: 895\n",
      "training loss: 30748839092531.64\n",
      "validation loss: 22310946984023.766\n",
      "epoch: 896\n",
      "training loss: 30748406028810.824\n",
      "validation loss: 22308972339633.574\n",
      "epoch: 897\n",
      "training loss: 30747974710765.523\n",
      "validation loss: 22307003884557.83\n",
      "epoch: 898\n",
      "training loss: 30747545129540.3\n",
      "validation loss: 22305041599709.574\n",
      "epoch: 899\n",
      "training loss: 30747117276324.777\n",
      "validation loss: 22303085466067.406\n",
      "epoch: 900\n",
      "training loss: 30746691142353.457\n",
      "validation loss: 22301135464675.21\n",
      "epoch: 901\n",
      "training loss: 30746266718905.414\n",
      "validation loss: 22299191576641.93\n",
      "epoch: 902\n",
      "training loss: 30745843997304.17\n",
      "validation loss: 22297253783141.285\n",
      "epoch: 903\n",
      "training loss: 30745422968917.367\n",
      "validation loss: 22295322065411.555\n",
      "epoch: 904\n",
      "training loss: 30745003625156.625\n",
      "validation loss: 22293396404755.305\n",
      "epoch: 905\n",
      "training loss: 30744585957477.242\n",
      "validation loss: 22291476782539.15\n",
      "epoch: 906\n",
      "training loss: 30744169957378.047\n",
      "validation loss: 22289563180193.504\n",
      "epoch: 907\n",
      "training loss: 30743755616401.13\n",
      "validation loss: 22287655579212.34\n",
      "epoch: 908\n",
      "training loss: 30743342926131.625\n",
      "validation loss: 22285753961152.93\n",
      "epoch: 909\n",
      "training loss: 30742931878197.527\n",
      "validation loss: 22283858307635.62\n",
      "epoch: 910\n",
      "training loss: 30742522464269.418\n",
      "validation loss: 22281968600343.582\n",
      "epoch: 911\n",
      "training loss: 30742114676060.305\n",
      "validation loss: 22280084821022.547\n",
      "epoch: 912\n",
      "training loss: 30741708505325.35\n",
      "validation loss: 22278206951480.61\n",
      "epoch: 913\n",
      "training loss: 30741303943861.72\n",
      "validation loss: 22276334973587.965\n",
      "epoch: 914\n",
      "training loss: 30740900983508.305\n",
      "validation loss: 22274468869276.656\n",
      "epoch: 915\n",
      "training loss: 30740499616145.55\n",
      "validation loss: 22272608620540.35\n",
      "epoch: 916\n",
      "training loss: 30740099833695.254\n",
      "validation loss: 22270754209434.13\n",
      "epoch: 917\n",
      "training loss: 30739701628120.29\n",
      "validation loss: 22268905618074.19\n",
      "epoch: 918\n",
      "training loss: 30739304991424.484\n",
      "validation loss: 22267062828637.684\n",
      "epoch: 919\n",
      "training loss: 30738909915652.344\n",
      "validation loss: 22265225823362.414\n",
      "epoch: 920\n",
      "training loss: 30738516392888.895\n",
      "validation loss: 22263394584546.65\n",
      "epoch: 921\n",
      "training loss: 30738124415259.414\n",
      "validation loss: 22261569094548.883\n",
      "epoch: 922\n",
      "training loss: 30737733974929.305\n",
      "validation loss: 22259749335787.594\n",
      "epoch: 923\n",
      "training loss: 30737345064103.836\n",
      "validation loss: 22257935290741.02\n",
      "epoch: 924\n",
      "training loss: 30736957675027.95\n",
      "validation loss: 22256126941946.93\n",
      "epoch: 925\n",
      "training loss: 30736571799986.07\n",
      "validation loss: 22254324272002.402\n",
      "epoch: 926\n",
      "training loss: 30736187431301.887\n",
      "validation loss: 22252527263563.582\n",
      "epoch: 927\n",
      "training loss: 30735804561338.18\n",
      "validation loss: 22250735899345.47\n",
      "epoch: 928\n",
      "training loss: 30735423182496.605\n",
      "validation loss: 22248950162121.703\n",
      "epoch: 929\n",
      "training loss: 30735043287217.477\n",
      "validation loss: 22247170034724.297\n",
      "epoch: 930\n",
      "training loss: 30734664867979.605\n",
      "validation loss: 22245395500043.477\n",
      "epoch: 931\n",
      "training loss: 30734287917300.094\n",
      "validation loss: 22243626541027.395\n",
      "epoch: 932\n",
      "training loss: 30733912427734.125\n",
      "validation loss: 22241863140681.97\n",
      "epoch: 933\n",
      "training loss: 30733538391874.785\n",
      "validation loss: 22240105282070.62\n",
      "epoch: 934\n",
      "training loss: 30733165802352.855\n",
      "validation loss: 22238352948314.066\n",
      "epoch: 935\n",
      "training loss: 30732794651836.63\n",
      "validation loss: 22236606122590.105\n",
      "epoch: 936\n",
      "training loss: 30732424933031.75\n",
      "validation loss: 22234864788133.4\n",
      "epoch: 937\n",
      "training loss: 30732056638680.953\n",
      "validation loss: 22233128928235.25\n",
      "epoch: 938\n",
      "training loss: 30731689761563.934\n",
      "validation loss: 22231398526243.4\n",
      "epoch: 939\n",
      "training loss: 30731324294497.14\n",
      "validation loss: 22229673565561.8\n",
      "epoch: 940\n",
      "training loss: 30730960230333.594\n",
      "validation loss: 22227954029650.41\n",
      "epoch: 941\n",
      "training loss: 30730597561962.688\n",
      "validation loss: 22226239902024.973\n",
      "epoch: 942\n",
      "training loss: 30730236282310.008\n",
      "validation loss: 22224531166256.81\n",
      "epoch: 943\n",
      "training loss: 30729876384337.168\n",
      "validation loss: 22222827805972.61\n",
      "epoch: 944\n",
      "training loss: 30729517861041.594\n",
      "validation loss: 22221129804854.223\n",
      "epoch: 945\n",
      "training loss: 30729160705456.363\n",
      "validation loss: 22219437146638.45\n",
      "epoch: 946\n",
      "training loss: 30728804910650.03\n",
      "validation loss: 22217749815116.81\n",
      "epoch: 947\n",
      "training loss: 30728450469726.414\n",
      "validation loss: 22216067794135.383\n",
      "epoch: 948\n",
      "training loss: 30728097375824.438\n",
      "validation loss: 22214391067594.547\n",
      "epoch: 949\n",
      "training loss: 30727745622117.96\n",
      "validation loss: 22212719619448.816\n",
      "epoch: 950\n",
      "training loss: 30727395201815.6\n",
      "validation loss: 22211053433706.625\n",
      "epoch: 951\n",
      "training loss: 30727046108160.516\n",
      "validation loss: 22209392494430.105\n",
      "epoch: 952\n",
      "training loss: 30726698334430.293\n",
      "validation loss: 22207736785734.91\n",
      "epoch: 953\n",
      "training loss: 30726351873936.7\n",
      "validation loss: 22206086291789.98\n",
      "epoch: 954\n",
      "training loss: 30726006720025.598\n",
      "validation loss: 22204440996817.38\n",
      "epoch: 955\n",
      "training loss: 30725662866076.68\n",
      "validation loss: 22202800885092.08\n",
      "epoch: 956\n",
      "training loss: 30725320305503.36\n",
      "validation loss: 22201165940941.727\n",
      "epoch: 957\n",
      "training loss: 30724979031752.57\n",
      "validation loss: 22199536148746.504\n",
      "epoch: 958\n",
      "training loss: 30724639038304.61\n",
      "validation loss: 22197911492938.88\n",
      "epoch: 959\n",
      "training loss: 30724300318672.96\n",
      "validation loss: 22196291958003.465\n",
      "epoch: 960\n",
      "training loss: 30723962866404.125\n",
      "validation loss: 22194677528476.746\n",
      "epoch: 961\n",
      "training loss: 30723626675077.45\n",
      "validation loss: 22193068188946.95\n",
      "epoch: 962\n",
      "training loss: 30723291738304.98\n",
      "validation loss: 22191463924053.816\n",
      "epoch: 963\n",
      "training loss: 30722958049731.273\n",
      "validation loss: 22189864718488.44\n",
      "epoch: 964\n",
      "training loss: 30722625603033.227\n",
      "validation loss: 22188270556993.027\n",
      "epoch: 965\n",
      "training loss: 30722294391919.938\n",
      "validation loss: 22186681424360.734\n",
      "epoch: 966\n",
      "training loss: 30721964410132.52\n",
      "validation loss: 22185097305435.48\n",
      "epoch: 967\n",
      "training loss: 30721635651443.945\n",
      "validation loss: 22183518185111.734\n",
      "epoch: 968\n",
      "training loss: 30721308109658.902\n",
      "validation loss: 22181944048334.36\n",
      "epoch: 969\n",
      "training loss: 30720981778613.6\n",
      "validation loss: 22180374880098.375\n",
      "epoch: 970\n",
      "training loss: 30720656652175.613\n",
      "validation loss: 22178810665448.816\n",
      "epoch: 971\n",
      "training loss: 30720332724243.766\n",
      "validation loss: 22177251389480.527\n",
      "epoch: 972\n",
      "training loss: 30720009988747.914\n",
      "validation loss: 22175697037337.96\n",
      "epoch: 973\n",
      "training loss: 30719688439648.816\n",
      "validation loss: 22174147594215.01\n",
      "epoch: 974\n",
      "training loss: 30719368070937.977\n",
      "validation loss: 22172603045354.84\n",
      "epoch: 975\n",
      "training loss: 30719048876637.49\n",
      "validation loss: 22171063376049.645\n",
      "epoch: 976\n",
      "training loss: 30718730850799.875\n",
      "validation loss: 22169528571640.516\n",
      "epoch: 977\n",
      "training loss: 30718413987507.938\n",
      "validation loss: 22167998617517.27\n",
      "epoch: 978\n",
      "training loss: 30718098280874.574\n",
      "validation loss: 22166473499118.21\n",
      "epoch: 979\n",
      "training loss: 30717783725042.676\n",
      "validation loss: 22164953201929.99\n",
      "epoch: 980\n",
      "training loss: 30717470314184.957\n",
      "validation loss: 22163437711487.41\n",
      "epoch: 981\n",
      "training loss: 30717158042503.785\n",
      "validation loss: 22161927013373.258\n",
      "epoch: 982\n",
      "training loss: 30716846904231.008\n",
      "validation loss: 22160421093218.117\n",
      "epoch: 983\n",
      "training loss: 30716536893627.887\n",
      "validation loss: 22158919936700.188\n",
      "epoch: 984\n",
      "training loss: 30716228004984.87\n",
      "validation loss: 22157423529545.113\n",
      "epoch: 985\n",
      "training loss: 30715920232621.46\n",
      "validation loss: 22155931857525.797\n",
      "epoch: 986\n",
      "training loss: 30715613570886.11\n",
      "validation loss: 22154444906462.234\n",
      "epoch: 987\n",
      "training loss: 30715308014155.996\n",
      "validation loss: 22152962662221.332\n",
      "epoch: 988\n",
      "training loss: 30715003556836.97\n",
      "validation loss: 22151485110716.746\n",
      "epoch: 989\n",
      "training loss: 30714700193363.3\n",
      "validation loss: 22150012237908.68\n",
      "epoch: 990\n",
      "training loss: 30714397918197.65\n",
      "validation loss: 22148544029803.746\n",
      "epoch: 991\n",
      "training loss: 30714096725830.812\n",
      "validation loss: 22147080472454.766\n",
      "epoch: 992\n",
      "training loss: 30713796610781.68\n",
      "validation loss: 22145621551960.61\n",
      "epoch: 993\n",
      "training loss: 30713497567596.98\n",
      "validation loss: 22144167254466.023\n",
      "epoch: 994\n",
      "training loss: 30713199590851.273\n",
      "validation loss: 22142717566161.47\n",
      "epoch: 995\n",
      "training loss: 30712902675146.684\n",
      "validation loss: 22141272473282.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 996\n",
      "training loss: 30712606815112.84\n",
      "validation loss: 22139831962111.78\n",
      "epoch: 997\n",
      "training loss: 30712312005406.695\n",
      "validation loss: 22138396018974.562\n",
      "epoch: 998\n",
      "training loss: 30712018240712.406\n",
      "validation loss: 22136964630242.875\n",
      "epoch: 999\n",
      "training loss: 30711725515741.2\n",
      "validation loss: 22135537782333.17\n",
      "epoch: 1000\n",
      "training loss: 30711433825231.2\n",
      "validation loss: 22134115461706.605\n",
      "epoch: 1001\n",
      "training loss: 30711143163947.34\n",
      "validation loss: 22132697654868.88\n",
      "epoch: 1002\n",
      "training loss: 30710853526681.188\n",
      "validation loss: 22131284348370.047\n",
      "epoch: 1003\n",
      "training loss: 30710564908250.836\n",
      "validation loss: 22129875528804.375\n",
      "epoch: 1004\n",
      "training loss: 30710277303500.75\n",
      "validation loss: 22128471182810.156\n",
      "epoch: 1005\n",
      "training loss: 30709990707301.633\n",
      "validation loss: 22127071297069.586\n",
      "epoch: 1006\n",
      "training loss: 30709705114550.3\n",
      "validation loss: 22125675858308.562\n",
      "epoch: 1007\n",
      "training loss: 30709420520169.566\n",
      "validation loss: 22124284853296.55\n",
      "epoch: 1008\n",
      "training loss: 30709136919108.06\n",
      "validation loss: 22122898268846.395\n",
      "epoch: 1009\n",
      "training loss: 30708854306340.133\n",
      "validation loss: 22121516091814.188\n",
      "epoch: 1010\n",
      "training loss: 30708572676865.746\n",
      "validation loss: 22120138309099.094\n",
      "epoch: 1011\n",
      "training loss: 30708292025710.27\n",
      "validation loss: 22118764907643.19\n",
      "epoch: 1012\n",
      "training loss: 30708012347924.414\n",
      "validation loss: 22117395874431.316\n",
      "epoch: 1013\n",
      "training loss: 30707733638584.11\n",
      "validation loss: 22116031196490.918\n",
      "epoch: 1014\n",
      "training loss: 30707455892790.32\n",
      "validation loss: 22114670860891.867\n",
      "epoch: 1015\n",
      "training loss: 30707179105668.957\n",
      "validation loss: 22113314854746.35\n",
      "epoch: 1016\n",
      "training loss: 30706903272370.754\n",
      "validation loss: 22111963165208.664\n",
      "epoch: 1017\n",
      "training loss: 30706628388071.105\n",
      "validation loss: 22110615779475.1\n",
      "epoch: 1018\n",
      "training loss: 30706354447969.992\n",
      "validation loss: 22109272684783.766\n",
      "epoch: 1019\n",
      "training loss: 30706081447291.81\n",
      "validation loss: 22107933868414.434\n",
      "epoch: 1020\n",
      "training loss: 30705809381285.28\n",
      "validation loss: 22106599317688.414\n",
      "epoch: 1021\n",
      "training loss: 30705538245223.305\n",
      "validation loss: 22105269019968.35\n",
      "epoch: 1022\n",
      "training loss: 30705268034402.824\n",
      "validation loss: 22103942962658.137\n",
      "epoch: 1023\n",
      "training loss: 30704998744144.773\n",
      "validation loss: 22102621133202.69\n",
      "epoch: 1024\n",
      "training loss: 30704730369793.863\n",
      "validation loss: 22101303519087.883\n",
      "epoch: 1025\n",
      "training loss: 30704462906718.5\n",
      "validation loss: 22099990107840.312\n",
      "epoch: 1026\n",
      "training loss: 30704196350310.703\n",
      "validation loss: 22098680887027.21\n",
      "epoch: 1027\n",
      "training loss: 30703930695985.902\n",
      "validation loss: 22097375844256.258\n",
      "epoch: 1028\n",
      "training loss: 30703665939182.914\n",
      "validation loss: 22096074967175.473\n",
      "epoch: 1029\n",
      "training loss: 30703402075363.727\n",
      "validation loss: 22094778243473.03\n",
      "epoch: 1030\n",
      "training loss: 30703139100013.46\n",
      "validation loss: 22093485660877.113\n",
      "epoch: 1031\n",
      "training loss: 30702877008640.223\n",
      "validation loss: 22092197207155.812\n",
      "epoch: 1032\n",
      "training loss: 30702615796774.953\n",
      "validation loss: 22090912870116.914\n",
      "epoch: 1033\n",
      "training loss: 30702355459971.37\n",
      "validation loss: 22089632637607.824\n",
      "epoch: 1034\n",
      "training loss: 30702095993805.812\n",
      "validation loss: 22088356497515.363\n",
      "epoch: 1035\n",
      "training loss: 30701837393877.14\n",
      "validation loss: 22087084437765.656\n",
      "epoch: 1036\n",
      "training loss: 30701579655806.633\n",
      "validation loss: 22085816446324.0\n",
      "epoch: 1037\n",
      "training loss: 30701322775237.836\n",
      "validation loss: 22084552511194.68\n",
      "epoch: 1038\n",
      "training loss: 30701066747836.477\n",
      "validation loss: 22083292620420.85\n",
      "epoch: 1039\n",
      "training loss: 30700811569290.36\n",
      "validation loss: 22082036762084.418\n",
      "epoch: 1040\n",
      "training loss: 30700557235309.223\n",
      "validation loss: 22080784924305.844\n",
      "epoch: 1041\n",
      "training loss: 30700303741624.656\n",
      "validation loss: 22079537095244.074\n",
      "epoch: 1042\n",
      "training loss: 30700051083989.98\n",
      "validation loss: 22078293263096.32\n",
      "epoch: 1043\n",
      "training loss: 30699799258180.12\n",
      "validation loss: 22077053416097.996\n",
      "epoch: 1044\n",
      "training loss: 30699548259991.52\n",
      "validation loss: 22075817542522.527\n",
      "epoch: 1045\n",
      "training loss: 30699298085242.01\n",
      "validation loss: 22074585630681.23\n",
      "epoch: 1046\n",
      "training loss: 30699048729770.734\n",
      "validation loss: 22073357668923.19\n",
      "epoch: 1047\n",
      "training loss: 30698800189437.977\n",
      "validation loss: 22072133645635.074\n",
      "epoch: 1048\n",
      "training loss: 30698552460125.133\n",
      "validation loss: 22070913549241.086\n",
      "epoch: 1049\n",
      "training loss: 30698305537734.56\n",
      "validation loss: 22069697368202.71\n",
      "epoch: 1050\n",
      "training loss: 30698059418189.44\n",
      "validation loss: 22068485091018.7\n",
      "epoch: 1051\n",
      "training loss: 30697814097433.754\n",
      "validation loss: 22067276706224.84\n",
      "epoch: 1052\n",
      "training loss: 30697569571432.098\n",
      "validation loss: 22066072202393.87\n",
      "epoch: 1053\n",
      "training loss: 30697325836169.62\n",
      "validation loss: 22064871568135.348\n",
      "epoch: 1054\n",
      "training loss: 30697082887651.906\n",
      "validation loss: 22063674792095.484\n",
      "epoch: 1055\n",
      "training loss: 30696840721904.875\n",
      "validation loss: 22062481862957.047\n",
      "epoch: 1056\n",
      "training loss: 30696599334974.684\n",
      "validation loss: 22061292769439.21\n",
      "epoch: 1057\n",
      "training loss: 30696358722927.61\n",
      "validation loss: 22060107500297.418\n",
      "epoch: 1058\n",
      "training loss: 30696118881849.95\n",
      "validation loss: 22058926044323.27\n",
      "epoch: 1059\n",
      "training loss: 30695879807847.914\n",
      "validation loss: 22057748390344.375\n",
      "epoch: 1060\n",
      "training loss: 30695641497047.566\n",
      "validation loss: 22056574527224.246\n",
      "epoch: 1061\n",
      "training loss: 30695403945594.676\n",
      "validation loss: 22055404443862.117\n",
      "epoch: 1062\n",
      "training loss: 30695167149654.61\n",
      "validation loss: 22054238129192.9\n",
      "epoch: 1063\n",
      "training loss: 30694931105412.285\n",
      "validation loss: 22053075572186.96\n",
      "epoch: 1064\n",
      "training loss: 30694695809072.023\n",
      "validation loss: 22051916761850.055\n",
      "epoch: 1065\n",
      "training loss: 30694461256857.49\n",
      "validation loss: 22050761687223.203\n",
      "epoch: 1066\n",
      "training loss: 30694227445011.527\n",
      "validation loss: 22049610337382.51\n",
      "epoch: 1067\n",
      "training loss: 30693994369796.156\n",
      "validation loss: 22048462701439.09\n",
      "epoch: 1068\n",
      "training loss: 30693762027492.395\n",
      "validation loss: 22047318768538.934\n",
      "epoch: 1069\n",
      "training loss: 30693530414400.2\n",
      "validation loss: 22046178527862.75\n",
      "epoch: 1070\n",
      "training loss: 30693299526838.367\n",
      "validation loss: 22045041968625.883\n",
      "epoch: 1071\n",
      "training loss: 30693069361144.426\n",
      "validation loss: 22043909080078.16\n",
      "epoch: 1072\n",
      "training loss: 30692839913674.547\n",
      "validation loss: 22042779851503.78\n",
      "epoch: 1073\n",
      "training loss: 30692611180803.465\n",
      "validation loss: 22041654272221.18\n",
      "epoch: 1074\n",
      "training loss: 30692383158924.35\n",
      "validation loss: 22040532331582.92\n",
      "epoch: 1075\n",
      "training loss: 30692155844448.754\n",
      "validation loss: 22039414018975.582\n",
      "epoch: 1076\n",
      "training loss: 30691929233806.465\n",
      "validation loss: 22038299323819.598\n",
      "epoch: 1077\n",
      "training loss: 30691703323445.477\n",
      "validation loss: 22037188235569.17\n",
      "epoch: 1078\n",
      "training loss: 30691478109831.848\n",
      "validation loss: 22036080743712.137\n",
      "epoch: 1079\n",
      "training loss: 30691253589449.63\n",
      "validation loss: 22034976837769.855\n",
      "epoch: 1080\n",
      "training loss: 30691029758800.77\n",
      "validation loss: 22033876507297.07\n",
      "epoch: 1081\n",
      "training loss: 30690806614405.03\n",
      "validation loss: 22032779741881.81\n",
      "epoch: 1082\n",
      "training loss: 30690584152799.887\n",
      "validation loss: 22031686531145.258\n",
      "epoch: 1083\n",
      "training loss: 30690362370540.426\n",
      "validation loss: 22030596864741.633\n",
      "epoch: 1084\n",
      "training loss: 30690141264199.29\n",
      "validation loss: 22029510732358.1\n",
      "epoch: 1085\n",
      "training loss: 30689920830366.562\n",
      "validation loss: 22028428123714.594\n",
      "epoch: 1086\n",
      "training loss: 30689701065649.68\n",
      "validation loss: 22027349028563.76\n",
      "epoch: 1087\n",
      "training loss: 30689481966673.355\n",
      "validation loss: 22026273436690.82\n",
      "epoch: 1088\n",
      "training loss: 30689263530079.477\n",
      "validation loss: 22025201337913.426\n",
      "epoch: 1089\n",
      "training loss: 30689045752527.043\n",
      "validation loss: 22024132722081.582\n",
      "epoch: 1090\n",
      "training loss: 30688828630692.035\n",
      "validation loss: 22023067579077.523\n",
      "epoch: 1091\n",
      "training loss: 30688612161267.367\n",
      "validation loss: 22022005898815.598\n",
      "epoch: 1092\n",
      "training loss: 30688396340962.8\n",
      "validation loss: 22020947671242.12\n",
      "epoch: 1093\n",
      "training loss: 30688181166504.82\n",
      "validation loss: 22019892886335.332\n",
      "epoch: 1094\n",
      "training loss: 30687966634636.605\n",
      "validation loss: 22018841534105.21\n",
      "epoch: 1095\n",
      "training loss: 30687752742117.887\n",
      "validation loss: 22017793604593.4\n",
      "epoch: 1096\n",
      "training loss: 30687539485724.9\n",
      "validation loss: 22016749087873.086\n",
      "epoch: 1097\n",
      "training loss: 30687326862250.297\n",
      "validation loss: 22015707974048.887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1098\n",
      "training loss: 30687114868503.05\n",
      "validation loss: 22014670253256.742\n",
      "epoch: 1099\n",
      "training loss: 30686903501308.38\n",
      "validation loss: 22013635915663.805\n",
      "epoch: 1100\n",
      "training loss: 30686692757507.66\n",
      "validation loss: 22012604951468.32\n",
      "epoch: 1101\n",
      "training loss: 30686482633958.344\n",
      "validation loss: 22011577350899.527\n",
      "epoch: 1102\n",
      "training loss: 30686273127533.906\n",
      "validation loss: 22010553104217.543\n",
      "epoch: 1103\n",
      "training loss: 30686064235123.695\n",
      "validation loss: 22009532201713.242\n",
      "epoch: 1104\n",
      "training loss: 30685855953632.93\n",
      "validation loss: 22008514633708.195\n",
      "epoch: 1105\n",
      "training loss: 30685648279982.574\n",
      "validation loss: 22007500390554.492\n",
      "epoch: 1106\n",
      "training loss: 30685441211109.25\n",
      "validation loss: 22006489462634.676\n",
      "epoch: 1107\n",
      "training loss: 30685234743965.188\n",
      "validation loss: 22005481840361.645\n",
      "epoch: 1108\n",
      "training loss: 30685028875518.14\n",
      "validation loss: 22004477514178.492\n",
      "epoch: 1109\n",
      "training loss: 30684823602751.27\n",
      "validation loss: 22003476474558.477\n",
      "epoch: 1110\n",
      "training loss: 30684618922663.117\n",
      "validation loss: 22002478712004.844\n",
      "epoch: 1111\n",
      "training loss: 30684414832267.51\n",
      "validation loss: 22001484217050.77\n",
      "epoch: 1112\n",
      "training loss: 30684211328593.45\n",
      "validation loss: 22000492980259.23\n",
      "epoch: 1113\n",
      "training loss: 30684008408685.086\n",
      "validation loss: 21999504992222.902\n",
      "epoch: 1114\n",
      "training loss: 30683806069601.594\n",
      "validation loss: 21998520243564.07\n",
      "epoch: 1115\n",
      "training loss: 30683604308417.137\n",
      "validation loss: 21997538724934.492\n",
      "epoch: 1116\n",
      "training loss: 30683403122220.76\n",
      "validation loss: 21996560427015.348\n",
      "epoch: 1117\n",
      "training loss: 30683202508116.34\n",
      "validation loss: 21995585340517.07\n",
      "epoch: 1118\n",
      "training loss: 30683002463222.484\n",
      "validation loss: 21994613456179.31\n",
      "epoch: 1119\n",
      "training loss: 30682802984672.484\n",
      "validation loss: 21993644764770.773\n",
      "epoch: 1120\n",
      "training loss: 30682604069614.19\n",
      "validation loss: 21992679257089.16\n",
      "epoch: 1121\n",
      "training loss: 30682405715210.008\n",
      "validation loss: 21991716923961.055\n",
      "epoch: 1122\n",
      "training loss: 30682207918636.77\n",
      "validation loss: 21990757756241.812\n",
      "epoch: 1123\n",
      "training loss: 30682010677085.688\n",
      "validation loss: 21989801744815.473\n",
      "epoch: 1124\n",
      "training loss: 30681813987762.26\n",
      "validation loss: 21988848880594.65\n",
      "epoch: 1125\n",
      "training loss: 30681617847886.227\n",
      "validation loss: 21987899154520.445\n",
      "epoch: 1126\n",
      "training loss: 30681422254691.46\n",
      "validation loss: 21986952557562.332\n",
      "epoch: 1127\n",
      "training loss: 30681227205425.938\n",
      "validation loss: 21986009080718.074\n",
      "epoch: 1128\n",
      "training loss: 30681032697351.633\n",
      "validation loss: 21985068715013.625\n",
      "epoch: 1129\n",
      "training loss: 30680838727744.44\n",
      "validation loss: 21984131451503.01\n",
      "epoch: 1130\n",
      "training loss: 30680645293894.164\n",
      "validation loss: 21983197281268.27\n",
      "epoch: 1131\n",
      "training loss: 30680452393104.355\n",
      "validation loss: 21982266195419.332\n",
      "epoch: 1132\n",
      "training loss: 30680260022692.336\n",
      "validation loss: 21981338185093.91\n",
      "epoch: 1133\n",
      "training loss: 30680068179989.06\n",
      "validation loss: 21980413241457.465\n",
      "epoch: 1134\n",
      "training loss: 30679876862339.06\n",
      "validation loss: 21979491355703.03\n",
      "epoch: 1135\n",
      "training loss: 30679686067100.406\n",
      "validation loss: 21978572519051.2\n",
      "epoch: 1136\n",
      "training loss: 30679495791644.613\n",
      "validation loss: 21977656722749.98\n",
      "epoch: 1137\n",
      "training loss: 30679306033356.57\n",
      "validation loss: 21976743958074.742\n",
      "epoch: 1138\n",
      "training loss: 30679116789634.473\n",
      "validation loss: 21975834216328.086\n",
      "epoch: 1139\n",
      "training loss: 30678928057889.78\n",
      "validation loss: 21974927488839.832\n",
      "epoch: 1140\n",
      "training loss: 30678739835547.113\n",
      "validation loss: 21974023766966.85\n",
      "epoch: 1141\n",
      "training loss: 30678552120044.2\n",
      "validation loss: 21973123042093.05\n",
      "epoch: 1142\n",
      "training loss: 30678364908831.812\n",
      "validation loss: 21972225305629.29\n",
      "epoch: 1143\n",
      "training loss: 30678178199373.723\n",
      "validation loss: 21971330549013.285\n",
      "epoch: 1144\n",
      "training loss: 30677991989146.586\n",
      "validation loss: 21970438763709.574\n",
      "epoch: 1145\n",
      "training loss: 30677806275639.9\n",
      "validation loss: 21969549941209.473\n",
      "epoch: 1146\n",
      "training loss: 30677621056355.953\n",
      "validation loss: 21968664073031.027\n",
      "epoch: 1147\n",
      "training loss: 30677436328809.76\n",
      "validation loss: 21967781150719.023\n",
      "epoch: 1148\n",
      "training loss: 30677252090528.965\n",
      "validation loss: 21966901165844.96\n",
      "epoch: 1149\n",
      "training loss: 30677068339053.79\n",
      "validation loss: 21966024110007.152\n",
      "epoch: 1150\n",
      "training loss: 30676885071936.992\n",
      "validation loss: 21965149974830.836\n",
      "epoch: 1151\n",
      "training loss: 30676702286743.78\n",
      "validation loss: 21964278751968.387\n",
      "epoch: 1152\n",
      "training loss: 30676519981051.746\n",
      "validation loss: 21963410433099.652\n",
      "epoch: 1153\n",
      "training loss: 30676338152450.805\n",
      "validation loss: 21962545009932.574\n",
      "epoch: 1154\n",
      "training loss: 30676156798543.13\n",
      "validation loss: 21961682474204.188\n",
      "epoch: 1155\n",
      "training loss: 30675975916943.094\n",
      "validation loss: 21960822817682.242\n",
      "epoch: 1156\n",
      "training loss: 30675795505277.18\n",
      "validation loss: 21959966032168.113\n",
      "epoch: 1157\n",
      "training loss: 30675615561183.89\n",
      "validation loss: 21959112109502.066\n",
      "epoch: 1158\n",
      "training loss: 30675436082313.688\n",
      "validation loss: 21958261041573.46\n",
      "epoch: 1159\n",
      "training loss: 30675257066328.85\n",
      "validation loss: 21957412820342.047\n",
      "epoch: 1160\n",
      "training loss: 30675078510903.168\n",
      "validation loss: 21956567437887.59\n",
      "epoch: 1161\n",
      "training loss: 30674900413721.438\n",
      "validation loss: 21955724886543.152\n",
      "epoch: 1162\n",
      "training loss: 30674722772477.676\n",
      "validation loss: 21954885159338.145\n",
      "epoch: 1163\n",
      "training loss: 30674545584866.312\n",
      "validation loss: 21954048252079.15\n",
      "epoch: 1164\n",
      "training loss: 30674368848489.45\n",
      "validation loss: 21953214183379.016\n",
      "epoch: 1165\n",
      "training loss: 30674191151765.195\n",
      "validation loss: 21952485014698.277\n",
      "epoch: 1166\n",
      "training loss: 30514214360692.863\n",
      "validation loss: 22503761056165.984\n",
      "epoch: 1167\n",
      "training loss: 30520530863904.23\n",
      "validation loss: 22502967208884.65\n",
      "epoch: 1168\n",
      "training loss: 30519968725660.316\n",
      "validation loss: 22502179361973.203\n",
      "epoch: 1169\n",
      "training loss: 30519407473423.87\n",
      "validation loss: 22501395472582.66\n",
      "epoch: 1170\n",
      "training loss: 30518847102993.434\n",
      "validation loss: 22500615529297.4\n",
      "epoch: 1171\n",
      "training loss: 30518287610188.5\n",
      "validation loss: 22499839520737.906\n",
      "epoch: 1172\n",
      "training loss: 30517728990849.402\n",
      "validation loss: 22499067435560.7\n",
      "epoch: 1173\n",
      "training loss: 30517171240837.21\n",
      "validation loss: 22498299262458.18\n",
      "epoch: 1174\n",
      "training loss: 30516614356033.64\n",
      "validation loss: 22497534990158.496\n",
      "epoch: 1175\n",
      "training loss: 30516058332340.92\n",
      "validation loss: 22496774607425.434\n",
      "epoch: 1176\n",
      "training loss: 30515503165681.73\n",
      "validation loss: 22496018103058.27\n",
      "epoch: 1177\n",
      "training loss: 30514948851999.047\n",
      "validation loss: 22495265465891.66\n",
      "epoch: 1178\n",
      "training loss: 30514395387256.105\n",
      "validation loss: 22494516684795.492\n",
      "epoch: 1179\n",
      "training loss: 30513842767436.227\n",
      "validation loss: 22493771748674.79\n",
      "epoch: 1180\n",
      "training loss: 30513290988542.76\n",
      "validation loss: 22493030646469.57\n",
      "epoch: 1181\n",
      "training loss: 30512740046598.957\n",
      "validation loss: 22492293367154.71\n",
      "epoch: 1182\n",
      "training loss: 30512189937647.883\n",
      "validation loss: 22491559899739.84\n",
      "epoch: 1183\n",
      "training loss: 30511640657752.24\n",
      "validation loss: 22490830233269.21\n",
      "epoch: 1184\n",
      "training loss: 30511092202994.305\n",
      "validation loss: 22490104356821.56\n",
      "epoch: 1185\n",
      "training loss: 30510544569475.652\n",
      "validation loss: 22489382259510.004\n",
      "epoch: 1186\n",
      "training loss: 30509997753316.902\n",
      "validation loss: 22488663930481.906\n",
      "epoch: 1187\n",
      "training loss: 30509451750656.91\n",
      "validation loss: 22487949358918.785\n",
      "epoch: 1188\n",
      "training loss: 30508906557650.438\n",
      "validation loss: 22487238534036.137\n",
      "epoch: 1189\n",
      "training loss: 30508362170456.71\n",
      "validation loss: 22486531445083.38\n",
      "epoch: 1190\n",
      "training loss: 30507818585130.03\n",
      "validation loss: 22485828081343.77\n",
      "epoch: 1191\n",
      "training loss: 30507275787665.64\n",
      "validation loss: 22485128432134.78\n",
      "epoch: 1192\n",
      "training loss: 30506508274024.445\n",
      "validation loss: 22484432669677.48\n",
      "epoch: 1193\n",
      "training loss: 30493037703759.08\n",
      "validation loss: 22485818665905.7\n",
      "epoch: 1194\n",
      "training loss: 30490848844172.754\n",
      "validation loss: 22485318521655.426\n",
      "epoch: 1195\n",
      "training loss: 30490291853247.918\n",
      "validation loss: 22484839940973.15\n",
      "epoch: 1196\n",
      "training loss: 30489735600537.09\n",
      "validation loss: 22484364781679.574\n",
      "epoch: 1197\n",
      "training loss: 30489180082618.13\n",
      "validation loss: 22483893034060.086\n",
      "epoch: 1198\n",
      "training loss: 30488625296085.906\n",
      "validation loss: 22483424688430.19\n",
      "epoch: 1199\n",
      "training loss: 30488071237552.242\n",
      "validation loss: 22482959735135.457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1200\n",
      "training loss: 30487517903645.785\n",
      "validation loss: 22482498164551.38\n",
      "epoch: 1201\n",
      "training loss: 30486965291011.934\n",
      "validation loss: 22482039967083.316\n",
      "epoch: 1202\n",
      "training loss: 30486413396312.76\n",
      "validation loss: 22481585133166.42\n",
      "epoch: 1203\n",
      "training loss: 30485862216226.867\n",
      "validation loss: 22481133653265.574\n",
      "epoch: 1204\n",
      "training loss: 30485311747449.27\n",
      "validation loss: 22480685517875.49\n",
      "epoch: 1205\n",
      "training loss: 30484761986691.2\n",
      "validation loss: 22480240717520.895\n",
      "epoch: 1206\n",
      "training loss: 30484212930679.793\n",
      "validation loss: 22479799242757.184\n",
      "epoch: 1207\n",
      "training loss: 30483664576157.156\n",
      "validation loss: 22479361084172.066\n",
      "epoch: 1208\n",
      "training loss: 30483116919877.48\n",
      "validation loss: 22478926232390.42\n",
      "epoch: 1209\n",
      "training loss: 30482569958591.49\n",
      "validation loss: 22478494678089.68\n",
      "epoch: 1210\n",
      "training loss: 30482023688867.59\n",
      "validation loss: 22478066412058.71\n",
      "epoch: 1211\n",
      "training loss: 30481478057590.73\n",
      "validation loss: 22477641425369.914\n",
      "epoch: 1212\n",
      "training loss: 30481025246290.355\n",
      "validation loss: 22472678574377.906\n",
      "epoch: 1213\n",
      "training loss: 30480481038604.285\n",
      "validation loss: 22472260361963.5\n",
      "epoch: 1214\n",
      "training loss: 30479937509853.7\n",
      "validation loss: 22471845401210.93\n",
      "epoch: 1215\n",
      "training loss: 30479394656828.926\n",
      "validation loss: 22471433682931.95\n",
      "epoch: 1216\n",
      "training loss: 30478852476416.156\n",
      "validation loss: 22471025197966.645\n",
      "Mean absolute error: $6158860910.59\n",
      "Nodes: 75\n",
      "Learning Rate: 1e-05\n",
      "epoch: 0\n",
      "training loss: 48944858400432.6\n",
      "validation loss: 71218307606160.02\n",
      "epoch: 1\n",
      "training loss: 47770542849474.87\n",
      "validation loss: 68127388416425.43\n",
      "epoch: 2\n",
      "training loss: 43764669165856.22\n",
      "validation loss: 53515374271212.266\n",
      "epoch: 3\n",
      "training loss: 40713756887575.78\n",
      "validation loss: 46070734129940.94\n",
      "epoch: 4\n",
      "training loss: 39755975170062.06\n",
      "validation loss: 44174996269641.45\n",
      "epoch: 5\n",
      "training loss: 39629153730703.53\n",
      "validation loss: 43863507532214.49\n",
      "epoch: 6\n",
      "training loss: 39351665186233.234\n",
      "validation loss: 43698889594333.16\n",
      "epoch: 7\n",
      "training loss: 39152305306588.49\n",
      "validation loss: 43729491959703.88\n",
      "epoch: 8\n",
      "training loss: 38972466362770.55\n",
      "validation loss: 43780103298416.8\n",
      "epoch: 9\n",
      "training loss: 38807523245632.555\n",
      "validation loss: 43842015880306.22\n",
      "epoch: 10\n",
      "training loss: 38679867514857.03\n",
      "validation loss: 43908177044836.0\n",
      "epoch: 11\n",
      "training loss: 38537684623615.586\n",
      "validation loss: 43982335521791.695\n",
      "epoch: 12\n",
      "training loss: 38406260729587.734\n",
      "validation loss: 44064609559302.56\n",
      "epoch: 13\n",
      "training loss: 38284223643902.48\n",
      "validation loss: 44150993752618.34\n",
      "epoch: 14\n",
      "training loss: 38170651188196.72\n",
      "validation loss: 44240429767728.78\n",
      "epoch: 15\n",
      "training loss: 38064780069664.164\n",
      "validation loss: 44332176381807.09\n",
      "epoch: 16\n",
      "training loss: 37965930319804.766\n",
      "validation loss: 44425617816247.625\n",
      "epoch: 17\n",
      "training loss: 37873491488912.04\n",
      "validation loss: 44520233696019.32\n",
      "epoch: 18\n",
      "training loss: 37786913738123.42\n",
      "validation loss: 44615580887599.1\n",
      "epoch: 19\n",
      "training loss: 37705700876872.2\n",
      "validation loss: 44711280349261.42\n",
      "epoch: 20\n",
      "training loss: 37629404570199.84\n",
      "validation loss: 44807007170205.21\n",
      "epoch: 21\n",
      "training loss: 37557619342529.63\n",
      "validation loss: 44902482757046.18\n",
      "epoch: 22\n",
      "training loss: 37489978181115.2\n",
      "validation loss: 44997468522037.27\n",
      "epoch: 23\n",
      "training loss: 37426148627045.79\n",
      "validation loss: 45091760660759.68\n",
      "epoch: 24\n",
      "training loss: 37365829282674.0\n",
      "validation loss: 45185185748595.164\n",
      "epoch: 25\n",
      "training loss: 37308746684588.64\n",
      "validation loss: 45277596973029.94\n",
      "epoch: 26\n",
      "training loss: 37254652501815.89\n",
      "validation loss: 45368870874359.31\n",
      "epoch: 27\n",
      "training loss: 37203321025023.52\n",
      "validation loss: 45458904503224.766\n",
      "epoch: 28\n",
      "training loss: 37154546916558.766\n",
      "validation loss: 45547612927074.34\n",
      "epoch: 29\n",
      "training loss: 37108143194268.74\n",
      "validation loss: 45634927033598.38\n",
      "epoch: 30\n",
      "training loss: 37063939424708.92\n",
      "validation loss: 45720791590221.61\n",
      "epoch: 31\n",
      "training loss: 37021780103733.93\n",
      "validation loss: 45805163526555.97\n",
      "epoch: 32\n",
      "training loss: 36981523204657.02\n",
      "validation loss: 45888010412422.98\n",
      "epoch: 33\n",
      "training loss: 36943038876182.195\n",
      "validation loss: 45969309108341.1\n",
      "epoch: 34\n",
      "training loss: 36906208274163.23\n",
      "validation loss: 46049044568683.35\n",
      "epoch: 35\n",
      "training loss: 36870922512930.23\n",
      "validation loss: 46127208780345.28\n",
      "epoch: 36\n",
      "training loss: 36837081723451.71\n",
      "validation loss: 46203799821908.64\n",
      "epoch: 37\n",
      "training loss: 36804594206975.87\n",
      "validation loss: 46278821030075.24\n",
      "epoch: 38\n",
      "training loss: 36773375674029.24\n",
      "validation loss: 46352280261663.32\n",
      "epoch: 39\n",
      "training loss: 36743348559754.12\n",
      "validation loss: 46424189240765.97\n",
      "epoch: 40\n",
      "training loss: 36714441407549.67\n",
      "validation loss: 46494562981809.64\n",
      "epoch: 41\n",
      "training loss: 36686588313858.22\n",
      "validation loss: 46563419280251.375\n",
      "epoch: 42\n",
      "training loss: 36659728427715.97\n",
      "validation loss: 46630778263536.875\n",
      "epoch: 43\n",
      "training loss: 36633805499380.414\n",
      "validation loss: 46696661995727.57\n",
      "epoch: 44\n",
      "training loss: 36608767472960.63\n",
      "validation loss: 46761094129903.55\n",
      "epoch: 45\n",
      "training loss: 36584566118523.05\n",
      "validation loss: 46824099603074.86\n",
      "epoch: 46\n",
      "training loss: 36561156699630.5\n",
      "validation loss: 46885704368891.49\n",
      "epoch: 47\n",
      "training loss: 36538497672703.32\n",
      "validation loss: 46945935163942.79\n",
      "epoch: 48\n",
      "training loss: 36516550414975.0\n",
      "validation loss: 47004819303884.82\n",
      "epoch: 49\n",
      "training loss: 36495278978155.8\n",
      "validation loss: 47062384506035.59\n",
      "epoch: 50\n",
      "training loss: 36474649865221.516\n",
      "validation loss: 47118658735437.14\n",
      "epoch: 51\n",
      "training loss: 36454631828015.31\n",
      "validation loss: 47173670071706.26\n",
      "epoch: 52\n",
      "training loss: 36435195683591.09\n",
      "validation loss: 47227446594283.5\n",
      "epoch: 53\n",
      "training loss: 36416314147442.74\n",
      "validation loss: 47280016283949.086\n",
      "epoch: 54\n",
      "training loss: 36397961681954.72\n",
      "validation loss: 47331406938705.86\n",
      "epoch: 55\n",
      "training loss: 36380114358581.125\n",
      "validation loss: 47381646102335.945\n",
      "epoch: 56\n",
      "training loss: 36362749732413.79\n",
      "validation loss: 47430761004123.97\n",
      "epoch: 57\n",
      "training loss: 36345846727935.94\n",
      "validation loss: 47478778508405.18\n",
      "Mean absolute error: $7111780459.29\n",
      "Nodes: 75\n",
      "Learning Rate: 1e-06\n",
      "epoch: 0\n",
      "training loss: 36156960266130.52\n",
      "validation loss: 34268978278107.812\n",
      "epoch: 1\n",
      "training loss: 35786190163635.016\n",
      "validation loss: 34901411382747.246\n",
      "epoch: 2\n",
      "training loss: 35624658782303.125\n",
      "validation loss: 35348583719051.65\n",
      "epoch: 3\n",
      "training loss: 35538646591871.64\n",
      "validation loss: 35740238690015.01\n",
      "epoch: 4\n",
      "training loss: 35500904260923.1\n",
      "validation loss: 36017565738275.85\n",
      "epoch: 5\n",
      "training loss: 35485421358207.77\n",
      "validation loss: 36204054894065.72\n",
      "epoch: 6\n",
      "training loss: 35479808909045.2\n",
      "validation loss: 36322020029720.7\n",
      "epoch: 7\n",
      "training loss: 35478284704624.42\n",
      "validation loss: 36390003452661.73\n",
      "epoch: 8\n",
      "training loss: 35478236316207.88\n",
      "validation loss: 36422433360353.555\n",
      "epoch: 9\n",
      "training loss: 35478547171575.15\n",
      "validation loss: 36430119506158.08\n",
      "epoch: 10\n",
      "training loss: 35478788503534.016\n",
      "validation loss: 36420967731701.18\n",
      "epoch: 11\n",
      "training loss: 35478834455504.305\n",
      "validation loss: 36400664314027.695\n",
      "epoch: 12\n",
      "training loss: 35478682492608.72\n",
      "validation loss: 36373247611055.14\n",
      "epoch: 13\n",
      "training loss: 35478372353246.03\n",
      "validation loss: 36341554987997.65\n",
      "epoch: 14\n",
      "training loss: 35477951525822.15\n",
      "validation loss: 36307559432145.62\n",
      "epoch: 15\n",
      "training loss: 35477462129764.7\n",
      "validation loss: 36272617346795.195\n",
      "epoch: 16\n",
      "training loss: 35476937227309.69\n",
      "validation loss: 36237648284153.38\n",
      "epoch: 17\n",
      "training loss: 35476400974095.49\n",
      "validation loss: 36203264024947.88\n",
      "epoch: 18\n",
      "training loss: 35475870081463.07\n",
      "validation loss: 36169860644699.33\n",
      "epoch: 19\n",
      "training loss: 35475355511753.664\n",
      "validation loss: 36137683855688.14\n",
      "epoch: 20\n",
      "training loss: 35474863995185.81\n",
      "validation loss: 36106875208218.76\n",
      "epoch: 21\n",
      "training loss: 35474399251580.12\n",
      "validation loss: 36077504658424.125\n",
      "epoch: 22\n",
      "training loss: 35473962920654.016\n",
      "validation loss: 36049593462970.78\n",
      "epoch: 23\n",
      "training loss: 35473555246166.73\n",
      "validation loss: 36023130229840.61\n",
      "epoch: 24\n",
      "training loss: 35473175566956.58\n",
      "validation loss: 35998082137117.945\n",
      "epoch: 25\n",
      "training loss: 35472822662691.945\n",
      "validation loss: 35974402746024.516\n",
      "epoch: 26\n",
      "training loss: 35472494993169.27\n",
      "validation loss: 35952037417001.17\n",
      "epoch: 27\n",
      "training loss: 35472190861024.23\n",
      "validation loss: 35930927041224.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28\n",
      "training loss: 35471908520098.34\n",
      "validation loss: 35911010590026.78\n",
      "epoch: 29\n",
      "training loss: 35471646245686.22\n",
      "validation loss: 35892226836278.75\n",
      "epoch: 30\n",
      "training loss: 35471402378332.97\n",
      "validation loss: 35874515497042.055\n",
      "epoch: 31\n",
      "training loss: 35471175349486.945\n",
      "validation loss: 35857817972872.09\n",
      "epoch: 32\n",
      "training loss: 35470963694870.1\n",
      "validation loss: 35842077807079.33\n",
      "epoch: 33\n",
      "training loss: 35470766059673.36\n",
      "validation loss: 35827240951548.45\n",
      "epoch: 34\n",
      "training loss: 35470581198434.77\n",
      "validation loss: 35813255899874.55\n",
      "epoch: 35\n",
      "training loss: 35470407971573.555\n",
      "validation loss: 35800073730384.13\n",
      "epoch: 36\n",
      "training loss: 35470245339930.09\n",
      "validation loss: 35787648088809.78\n",
      "epoch: 37\n",
      "training loss: 35470092358225.55\n",
      "validation loss: 35775935131385.414\n",
      "epoch: 38\n",
      "training loss: 35469948168050.04\n",
      "validation loss: 35764893442802.9\n",
      "epoch: 39\n",
      "training loss: 35469811990777.3\n",
      "validation loss: 35754483939026.48\n",
      "epoch: 40\n",
      "training loss: 35469683120658.04\n",
      "validation loss: 35744669761843.805\n",
      "epoch: 41\n",
      "training loss: 35469560918244.414\n",
      "validation loss: 35735416169846.52\n",
      "epoch: 42\n",
      "training loss: 35469444804230.734\n",
      "validation loss: 35726690429004.4\n",
      "epoch: 43\n",
      "training loss: 35469334253750.17\n",
      "validation loss: 35718461704929.24\n",
      "epoch: 44\n",
      "training loss: 35469228791138.06\n",
      "validation loss: 35710700958181.75\n",
      "epoch: 45\n",
      "training loss: 35469127985152.83\n",
      "validation loss: 35703380843458.664\n",
      "epoch: 46\n",
      "training loss: 35469031444634.8\n",
      "validation loss: 35696475613142.61\n",
      "epoch: 47\n",
      "training loss: 35468938814575.86\n",
      "validation loss: 35689961025453.14\n",
      "epoch: 48\n",
      "training loss: 35468849772570.19\n",
      "validation loss: 35683814257272.93\n",
      "epoch: 49\n",
      "training loss: 35468764025614.98\n",
      "validation loss: 35678013821612.99\n",
      "epoch: 50\n",
      "training loss: 35468681307230.36\n",
      "validation loss: 35672539489608.63\n",
      "epoch: 51\n",
      "training loss: 35468601374869.22\n",
      "validation loss: 35667372216893.016\n",
      "Mean absolute error: $6958060476.29\n",
      "Nodes: 75\n",
      "Learning Rate: 1e-07\n",
      "epoch: 0\n",
      "training loss: 36936967439156.25\n",
      "validation loss: 33189888242517.18\n",
      "epoch: 1\n",
      "training loss: 36687534425180.3\n",
      "validation loss: 32882550261089.76\n",
      "epoch: 2\n",
      "training loss: 36420163869346.9\n",
      "validation loss: 32558646766519.066\n",
      "epoch: 3\n",
      "training loss: 36231088511833.99\n",
      "validation loss: 32324472312489.5\n",
      "epoch: 4\n",
      "training loss: 36077479830813.95\n",
      "validation loss: 32190193781621.53\n",
      "epoch: 5\n",
      "training loss: 35952883573914.05\n",
      "validation loss: 32110256427048.402\n",
      "epoch: 6\n",
      "training loss: 35792639731018.164\n",
      "validation loss: 32006539501769.344\n",
      "epoch: 7\n",
      "training loss: 35681862669517.06\n",
      "validation loss: 31934424902700.105\n",
      "epoch: 8\n",
      "training loss: 35636921506795.52\n",
      "validation loss: 31696716122522.35\n",
      "epoch: 9\n",
      "training loss: 35628368805453.62\n",
      "validation loss: 31700939836968.477\n",
      "epoch: 10\n",
      "training loss: 35478501634429.33\n",
      "validation loss: 31479948907544.863\n",
      "epoch: 11\n",
      "training loss: 35304930381395.89\n",
      "validation loss: 31263096288340.785\n",
      "epoch: 12\n",
      "training loss: 35147086132468.082\n",
      "validation loss: 31066928790558.934\n",
      "epoch: 13\n",
      "training loss: 35000366863224.28\n",
      "validation loss: 30882174504693.816\n",
      "epoch: 14\n",
      "training loss: 34863953744350.938\n",
      "validation loss: 30708046920633.184\n",
      "epoch: 15\n",
      "training loss: 34737089940238.598\n",
      "validation loss: 30543818825834.586\n",
      "epoch: 16\n",
      "training loss: 34703313315173.38\n",
      "validation loss: 30522860750960.117\n",
      "epoch: 17\n",
      "training loss: 34589799564650.26\n",
      "validation loss: 30375462087960.137\n",
      "epoch: 18\n",
      "training loss: 34484191290639.246\n",
      "validation loss: 30236299490815.25\n",
      "epoch: 19\n",
      "training loss: 34385909564709.508\n",
      "validation loss: 30104809917239.586\n",
      "epoch: 20\n",
      "training loss: 34294417885156.11\n",
      "validation loss: 29980470270638.18\n",
      "epoch: 21\n",
      "training loss: 34209219068986.805\n",
      "validation loss: 29862794519145.363\n",
      "epoch: 22\n",
      "training loss: 34129852371538.0\n",
      "validation loss: 29751331024112.4\n",
      "epoch: 23\n",
      "training loss: 34055890817050.035\n",
      "validation loss: 29645660062758.625\n",
      "epoch: 24\n",
      "training loss: 33986938724750.816\n",
      "validation loss: 29545391530818.047\n",
      "epoch: 25\n",
      "training loss: 33922629415904.367\n",
      "validation loss: 29450162812048.89\n",
      "epoch: 26\n",
      "training loss: 33862513638949.996\n",
      "validation loss: 29356706240657.754\n",
      "epoch: 27\n",
      "training loss: 33827472148776.156\n",
      "validation loss: 29362543802798.438\n",
      "epoch: 28\n",
      "training loss: 33776522520233.605\n",
      "validation loss: 29284195853644.066\n",
      "epoch: 29\n",
      "training loss: 33728930061849.49\n",
      "validation loss: 29209566493983.527\n",
      "epoch: 30\n",
      "training loss: 33684447915932.53\n",
      "validation loss: 29138407297000.945\n",
      "epoch: 31\n",
      "training loss: 33642847412129.117\n",
      "validation loss: 29070487330373.938\n",
      "epoch: 32\n",
      "training loss: 33603916728148.195\n",
      "validation loss: 29005591895994.88\n",
      "epoch: 33\n",
      "training loss: 33567459649189.137\n",
      "validation loss: 28943521361608.754\n",
      "epoch: 34\n",
      "training loss: 33533294418683.113\n",
      "validation loss: 28884090077516.945\n",
      "epoch: 35\n",
      "training loss: 33501252673628.055\n",
      "validation loss: 28827125372118.766\n",
      "epoch: 36\n",
      "training loss: 33471178458291.81\n",
      "validation loss: 28772466620519.7\n",
      "epoch: 37\n",
      "training loss: 33442927310516.0\n",
      "validation loss: 28719964380858.754\n",
      "epoch: 38\n",
      "training loss: 33416365415277.754\n",
      "validation loss: 28669479593400.18\n",
      "epoch: 39\n",
      "training loss: 33391368820559.395\n",
      "validation loss: 28620882837798.094\n",
      "epoch: 40\n",
      "training loss: 33367822710940.527\n",
      "validation loss: 28574053644279.688\n",
      "epoch: 41\n",
      "training loss: 33345620734664.51\n",
      "validation loss: 28528879854804.754\n",
      "epoch: 42\n",
      "training loss: 33324664380243.543\n",
      "validation loss: 28485257030548.492\n",
      "epoch: 43\n",
      "training loss: 33304862398956.76\n",
      "validation loss: 28443087902322.59\n",
      "epoch: 44\n",
      "training loss: 33286130269863.47\n",
      "validation loss: 28402281860797.816\n",
      "epoch: 45\n",
      "training loss: 33268389704202.477\n",
      "validation loss: 28362754483621.203\n",
      "epoch: 46\n",
      "training loss: 33251568186278.77\n",
      "validation loss: 28324427096734.516\n",
      "epoch: 47\n",
      "training loss: 33235598548151.54\n",
      "validation loss: 28287226367397.55\n",
      "epoch: 48\n",
      "training loss: 33220418575632.324\n",
      "validation loss: 28251083926603.344\n",
      "epoch: 49\n",
      "training loss: 33205970643102.137\n",
      "validation loss: 28215936018741.266\n",
      "epoch: 50\n",
      "training loss: 33192159664065.555\n",
      "validation loss: 28181723176510.797\n",
      "epoch: 51\n",
      "training loss: 33179018179889.45\n",
      "validation loss: 28148387614122.29\n",
      "epoch: 52\n",
      "training loss: 33166460178985.11\n",
      "validation loss: 28115879904577.82\n",
      "epoch: 53\n",
      "training loss: 33154443246748.504\n",
      "validation loss: 28084151718328.67\n",
      "epoch: 54\n",
      "training loss: 33142928098293.84\n",
      "validation loss: 28053157931800.598\n",
      "epoch: 55\n",
      "training loss: 33131878347778.375\n",
      "validation loss: 28022856403550.74\n",
      "epoch: 56\n",
      "training loss: 33121260294714.746\n",
      "validation loss: 27993207766476.668\n",
      "epoch: 57\n",
      "training loss: 33111042726020.53\n",
      "validation loss: 27964175234910.723\n",
      "epoch: 58\n",
      "training loss: 33101196732646.605\n",
      "validation loss: 27935724425518.465\n",
      "epoch: 59\n",
      "training loss: 33091695539710.89\n",
      "validation loss: 27907823190998.883\n",
      "epoch: 60\n",
      "training loss: 33082514349143.51\n",
      "validation loss: 27880441465657.375\n",
      "epoch: 61\n",
      "training loss: 33073630193922.24\n",
      "validation loss: 27853551121990.598\n",
      "epoch: 62\n",
      "training loss: 33065021803044.902\n",
      "validation loss: 27827125837484.78\n",
      "epoch: 63\n",
      "training loss: 33056669476448.41\n",
      "validation loss: 27801140970888.035\n",
      "epoch: 64\n",
      "training loss: 33048554969141.996\n",
      "validation loss: 27775573447270.49\n",
      "epoch: 65\n",
      "training loss: 33040661383876.49\n",
      "validation loss: 27750401651236.89\n",
      "epoch: 66\n",
      "training loss: 33032973071720.88\n",
      "validation loss: 27725605327702.156\n",
      "epoch: 67\n",
      "training loss: 33025475539964.21\n",
      "validation loss: 27701165489683.55\n",
      "epoch: 68\n",
      "training loss: 33018155366803.414\n",
      "validation loss: 27677064332603.242\n",
      "epoch: 69\n",
      "training loss: 33011000122317.316\n",
      "validation loss: 27653285154631.434\n",
      "epoch: 70\n",
      "training loss: 33003998295264.035\n",
      "validation loss: 27629812282634.953\n",
      "epoch: 71\n",
      "training loss: 32997139225272.98\n",
      "validation loss: 27606631003327.625\n",
      "epoch: 72\n",
      "training loss: 32990413040034.188\n",
      "validation loss: 27583727499248.277\n",
      "epoch: 73\n",
      "training loss: 32983810597116.97\n",
      "validation loss: 27561088789219.316\n",
      "epoch: 74\n",
      "training loss: 32977323430076.965\n",
      "validation loss: 27538702672964.35\n",
      "epoch: 75\n",
      "training loss: 32970943698535.867\n",
      "validation loss: 27516557679586.273\n",
      "epoch: 76\n",
      "training loss: 32964664141941.086\n",
      "validation loss: 27494643019629.457\n",
      "epoch: 77\n",
      "training loss: 32958478036734.496\n",
      "validation loss: 27472948540469.332\n",
      "epoch: 78\n",
      "training loss: 32952379156679.066\n",
      "validation loss: 27451464684791.523\n",
      "epoch: 79\n",
      "training loss: 32946361736110.832\n",
      "validation loss: 27430182451939.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80\n",
      "training loss: 32940420435900.664\n",
      "validation loss: 27409093361928.91\n",
      "epoch: 81\n",
      "training loss: 32934550311926.375\n",
      "validation loss: 27388189421930.355\n",
      "epoch: 82\n",
      "training loss: 32928746785870.047\n",
      "validation loss: 27367463095058.66\n",
      "epoch: 83\n",
      "training loss: 32923005618169.44\n",
      "validation loss: 27346907271291.207\n",
      "epoch: 84\n",
      "training loss: 32917322882964.71\n",
      "validation loss: 27326515240372.402\n",
      "epoch: 85\n",
      "training loss: 32911694944893.453\n",
      "validation loss: 27306280666560.723\n",
      "epoch: 86\n",
      "training loss: 32906118437597.906\n",
      "validation loss: 27286197565088.62\n",
      "epoch: 87\n",
      "training loss: 32900590243818.13\n",
      "validation loss: 27266260280214.16\n",
      "epoch: 88\n",
      "training loss: 32895107476954.242\n",
      "validation loss: 27246463464752.066\n",
      "epoch: 89\n",
      "training loss: 32889667463989.668\n",
      "validation loss: 27226802060980.363\n",
      "epoch: 90\n",
      "training loss: 32884267729674.79\n",
      "validation loss: 27207271282825.508\n",
      "epoch: 91\n",
      "training loss: 32878905981878.35\n",
      "validation loss: 27187866599236.71\n",
      "epoch: 92\n",
      "training loss: 32873580098020.465\n",
      "validation loss: 27168583718665.977\n",
      "epoch: 93\n",
      "training loss: 32868288112507.47\n",
      "validation loss: 27149418574576.625\n",
      "epoch: 94\n",
      "training loss: 32863028205094.8\n",
      "validation loss: 27130367311908.676\n",
      "epoch: 95\n",
      "training loss: 32857798690109.54\n",
      "validation loss: 27111426274434.363\n",
      "epoch: 96\n",
      "training loss: 32852598006469.07\n",
      "validation loss: 27092591992942.082\n",
      "epoch: 97\n",
      "training loss: 32847424708437.43\n",
      "validation loss: 27073861174191.297\n",
      "epoch: 98\n",
      "training loss: 32842277457064.645\n",
      "validation loss: 27055230690585.16\n",
      "epoch: 99\n",
      "training loss: 32837155012258.91\n",
      "validation loss: 27036697570511.336\n",
      "epoch: 100\n",
      "training loss: 32832056225444.883\n",
      "validation loss: 27018258989305.023\n",
      "epoch: 101\n",
      "training loss: 32826980032764.79\n",
      "validation loss: 26999912260791.57\n",
      "epoch: 102\n",
      "training loss: 32821925448782.37\n",
      "validation loss: 26981654829368.95\n",
      "epoch: 103\n",
      "training loss: 32816891560652.625\n",
      "validation loss: 26963484262593.203\n",
      "epoch: 104\n",
      "training loss: 32811877522722.812\n",
      "validation loss: 26945398244232.82\n",
      "epoch: 105\n",
      "training loss: 32806882551533.062\n",
      "validation loss: 26927394567759.97\n",
      "epoch: 106\n",
      "training loss: 32801905921187.082\n",
      "validation loss: 26909471130249.24\n",
      "epoch: 107\n",
      "training loss: 32796946959065.453\n",
      "validation loss: 26891625926656.277\n",
      "epoch: 108\n",
      "training loss: 32792005041856.582\n",
      "validation loss: 26873857044450.855\n",
      "epoch: 109\n",
      "training loss: 32787079591881.473\n",
      "validation loss: 26856162658580.582\n",
      "epoch: 110\n",
      "training loss: 32782170073690.96\n",
      "validation loss: 26838541026743.168\n",
      "epoch: 111\n",
      "training loss: 32777275990914.98\n",
      "validation loss: 26820990484946.754\n",
      "epoch: 112\n",
      "training loss: 32772396883345.586\n",
      "validation loss: 26803509443339.19\n",
      "epoch: 113\n",
      "training loss: 32767532324236.117\n",
      "validation loss: 26786096382288.402\n",
      "epoch: 114\n",
      "training loss: 32762681917800.855\n",
      "validation loss: 26768749848697.516\n",
      "epoch: 115\n",
      "training loss: 32757845296900.11\n",
      "validation loss: 26751468452539.152\n",
      "epoch: 116\n",
      "training loss: 32753022120897.164\n",
      "validation loss: 26734250863594.69\n",
      "epoch: 117\n",
      "training loss: 32748212073674.383\n",
      "validation loss: 26717095808385.19\n",
      "epoch: 118\n",
      "training loss: 32743414861796.64\n",
      "validation loss: 26700002067281.434\n",
      "epoch: 119\n",
      "training loss: 32738630212811.227\n",
      "validation loss: 26682968471781.746\n",
      "epoch: 120\n",
      "training loss: 32733857873674.145\n",
      "validation loss: 26665993901946.617\n",
      "epoch: 121\n",
      "training loss: 32729097609293.45\n",
      "validation loss: 26649077283980.258\n",
      "epoch: 122\n",
      "training loss: 32724349201180.89\n",
      "validation loss: 26632217587949.69\n",
      "epoch: 123\n",
      "training loss: 32719612446204.01\n",
      "validation loss: 26615413825632.656\n",
      "epoch: 124\n",
      "training loss: 32714887155431.08\n",
      "validation loss: 26598665048486.25\n",
      "epoch: 125\n",
      "training loss: 32710173153062.125\n",
      "validation loss: 26581970345728.79\n",
      "epoch: 126\n",
      "training loss: 32705470275439.63\n",
      "validation loss: 26565328842527.684\n",
      "epoch: 127\n",
      "training loss: 32700778370132.97\n",
      "validation loss: 26548739698286.93\n",
      "epoch: 128\n",
      "training loss: 32696097295091.242\n",
      "validation loss: 26532202105027.98\n",
      "epoch: 129\n",
      "training loss: 32691426917859.246\n",
      "validation loss: 26515715285858.26\n",
      "epoch: 130\n",
      "training loss: 32686767114852.05\n",
      "validation loss: 26499278493522.07\n",
      "epoch: 131\n",
      "training loss: 32682117770683.848\n",
      "validation loss: 26482891009028.746\n",
      "epoch: 132\n",
      "training loss: 32677478777546.895\n",
      "validation loss: 26466552140353.58\n",
      "epoch: 133\n",
      "training loss: 32672850034637.0\n",
      "validation loss: 26450261221206.992\n",
      "epoch: 134\n",
      "training loss: 32668231447622.027\n",
      "validation loss: 26434017609868.004\n",
      "epoch: 135\n",
      "training loss: 32663622928150.15\n",
      "validation loss: 26417820688078.168\n",
      "epoch: 136\n",
      "training loss: 32659024393395.13\n",
      "validation loss: 26401669859992.344\n",
      "epoch: 137\n",
      "training loss: 32654435765635.55\n",
      "validation loss: 26385564551183.11\n",
      "epoch: 138\n",
      "training loss: 32649856971865.797\n",
      "validation loss: 26369504207695.58\n",
      "epoch: 139\n",
      "training loss: 32645287943436.164\n",
      "validation loss: 26353488295149.855\n",
      "epoch: 140\n",
      "training loss: 32640728615720.12\n",
      "validation loss: 26337516297888.195\n",
      "epoch: 141\n",
      "training loss: 32636178927806.59\n",
      "validation loss: 26321587718164.54\n",
      "epoch: 142\n",
      "training loss: 32631638822215.4\n",
      "validation loss: 26305702075373.918\n",
      "epoch: 143\n",
      "training loss: 32627108244634.24\n",
      "validation loss: 26289858905319.46\n",
      "epoch: 144\n",
      "training loss: 32622587143675.49\n",
      "validation loss: 26274057759515.02\n",
      "epoch: 145\n",
      "training loss: 32618075470651.367\n",
      "validation loss: 26258298204521.312\n",
      "epoch: 146\n",
      "training loss: 32613573179366.145\n",
      "validation loss: 26242579821313.84\n",
      "epoch: 147\n",
      "training loss: 32609080225924.082\n",
      "validation loss: 26226902204680.797\n",
      "epoch: 148\n",
      "training loss: 32604596568551.94\n",
      "validation loss: 26211264962649.37\n",
      "epoch: 149\n",
      "training loss: 32600122167434.96\n",
      "validation loss: 26195667715938.87\n",
      "epoch: 150\n",
      "training loss: 32595656984565.336\n",
      "validation loss: 26180110097439.273\n",
      "epoch: 151\n",
      "training loss: 32591200983602.168\n",
      "validation loss: 26164591751713.902\n",
      "epoch: 152\n",
      "training loss: 32586754129742.18\n",
      "validation loss: 26149112334524.82\n",
      "epoch: 153\n",
      "training loss: 32582316389600.215\n",
      "validation loss: 26133671512379.867\n",
      "epoch: 154\n",
      "training loss: 32577887731098.945\n",
      "validation loss: 26118268962100.254\n",
      "epoch: 155\n",
      "training loss: 32573468123366.97\n",
      "validation loss: 26102904370407.484\n",
      "epoch: 156\n",
      "training loss: 32569057536644.77\n",
      "validation loss: 26087577433528.81\n",
      "epoch: 157\n",
      "training loss: 32564655942197.86\n",
      "validation loss: 26072287856820.15\n",
      "epoch: 158\n",
      "training loss: 32560263312236.633\n",
      "validation loss: 26057035354405.656\n",
      "epoch: 159\n",
      "training loss: 32555879619842.414\n",
      "validation loss: 26041819648833.066\n",
      "epoch: 160\n",
      "training loss: 32551504838899.17\n",
      "validation loss: 26026640470744.062\n",
      "epoch: 161\n",
      "training loss: 32547138944030.582\n",
      "validation loss: 26011497558558.95\n",
      "epoch: 162\n",
      "training loss: 32542781910541.9\n",
      "validation loss: 25996390658174.832\n",
      "epoch: 163\n",
      "training loss: 32538433714366.414\n",
      "validation loss: 25981319522676.805\n",
      "epoch: 164\n",
      "training loss: 32534094332016.027\n",
      "validation loss: 25966283912061.324\n",
      "epoch: 165\n",
      "training loss: 32529763740535.773\n",
      "validation loss: 25951283592971.434\n",
      "epoch: 166\n",
      "training loss: 32525441917461.754\n",
      "validation loss: 25936318338442.97\n",
      "epoch: 167\n",
      "training loss: 32521128840782.535\n",
      "validation loss: 25921387927661.55\n",
      "epoch: 168\n",
      "training loss: 32516824488903.484\n",
      "validation loss: 25906492145729.59\n",
      "epoch: 169\n",
      "training loss: 32512528840613.94\n",
      "validation loss: 25891630783443.035\n",
      "epoch: 170\n",
      "training loss: 32508241875056.996\n",
      "validation loss: 25876803637077.242\n",
      "epoch: 171\n",
      "training loss: 32503963571701.7\n",
      "validation loss: 25862010508181.715\n",
      "epoch: 172\n",
      "training loss: 32499693910317.41\n",
      "validation loss: 25847251203383.2\n",
      "epoch: 173\n",
      "training loss: 32495432870950.242\n",
      "validation loss: 25832525534196.76\n",
      "epoch: 174\n",
      "training loss: 32491180433901.45\n",
      "validation loss: 25817833316844.594\n",
      "epoch: 175\n",
      "training loss: 32486936579707.414\n",
      "validation loss: 25803174372082.027\n",
      "epoch: 176\n",
      "training loss: 32482701289121.414\n",
      "validation loss: 25788548525030.59\n",
      "epoch: 177\n",
      "training loss: 32478474543096.727\n",
      "validation loss: 25773955605017.746\n",
      "epoch: 178\n",
      "training loss: 32474256322771.23\n",
      "validation loss: 25759395445422.953\n",
      "epoch: 179\n",
      "training loss: 32470046609453.16\n",
      "validation loss: 25744867883529.934\n",
      "epoch: 180\n",
      "training loss: 32465845384608.145\n",
      "validation loss: 25730372760384.66\n",
      "epoch: 181\n",
      "training loss: 32461652629847.18\n",
      "validation loss: 25715909920659.023\n",
      "epoch: 182\n",
      "training loss: 32457468326915.77\n",
      "validation loss: 25701479212519.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 183\n",
      "training loss: 32453292457683.785\n",
      "validation loss: 25687080487502.723\n",
      "epoch: 184\n",
      "training loss: 32449125004136.355\n",
      "validation loss: 25672713600391.61\n",
      "epoch: 185\n",
      "training loss: 32444965948365.387\n",
      "validation loss: 25658378409101.91\n",
      "epoch: 186\n",
      "training loss: 32440815272561.902\n",
      "validation loss: 25644074774569.023\n",
      "epoch: 187\n",
      "training loss: 32436672959008.973\n",
      "validation loss: 25629802560640.785\n",
      "epoch: 188\n",
      "training loss: 32432538990075.27\n",
      "validation loss: 25615561633974.11\n",
      "epoch: 189\n",
      "training loss: 32428413348209.246\n",
      "validation loss: 25601351863935.613\n",
      "epoch: 190\n",
      "training loss: 32424296015933.684\n",
      "validation loss: 25587173122506.05\n",
      "epoch: 191\n",
      "training loss: 32420186975840.895\n",
      "validation loss: 25573025284188.367\n",
      "epoch: 192\n",
      "training loss: 32416086210588.21\n",
      "validation loss: 25558908225919.277\n",
      "epoch: 193\n",
      "training loss: 32411993702893.93\n",
      "validation loss: 25544821826984.15\n",
      "epoch: 194\n",
      "training loss: 32407909435533.656\n",
      "validation loss: 25530765968935.16\n",
      "epoch: 195\n",
      "training loss: 32403833391336.93\n",
      "validation loss: 25516740535512.535\n",
      "epoch: 196\n",
      "training loss: 32399765553184.22\n",
      "validation loss: 25502745412568.75\n",
      "epoch: 197\n",
      "training loss: 32395705904004.12\n",
      "validation loss: 25488780487995.547\n",
      "epoch: 198\n",
      "training loss: 32391654426770.91\n",
      "validation loss: 25474845651653.727\n",
      "epoch: 199\n",
      "training loss: 32387611104502.25\n",
      "validation loss: 25460940795305.566\n",
      "epoch: 200\n",
      "training loss: 32383575920257.215\n",
      "validation loss: 25447065812549.758\n",
      "epoch: 201\n",
      "training loss: 32379548857134.4\n",
      "validation loss: 25433220598758.734\n",
      "epoch: 202\n",
      "training loss: 32375529898270.297\n",
      "validation loss: 25419405051018.39\n",
      "epoch: 203\n",
      "training loss: 32371519026837.84\n",
      "validation loss: 25405619068070.023\n",
      "epoch: 204\n",
      "training loss: 32367516226045.047\n",
      "validation loss: 25391862550254.34\n",
      "epoch: 205\n",
      "training loss: 32363521479133.855\n",
      "validation loss: 25378135399457.703\n",
      "epoch: 206\n",
      "training loss: 32359534769379.066\n",
      "validation loss: 25364437519060.176\n",
      "epoch: 207\n",
      "training loss: 32355556080087.395\n",
      "validation loss: 25350768813885.645\n",
      "epoch: 208\n",
      "training loss: 32351585394596.67\n",
      "validation loss: 25337129190153.668\n",
      "epoch: 209\n",
      "training loss: 32347622696275.074\n",
      "validation loss: 25323518555433.156\n",
      "epoch: 210\n",
      "training loss: 32343667968520.516\n",
      "validation loss: 25309936818597.703\n",
      "epoch: 211\n",
      "training loss: 32339721194760.082\n",
      "validation loss: 25296383889782.625\n",
      "epoch: 212\n",
      "training loss: 32335782358449.49\n",
      "validation loss: 25282859680343.45\n",
      "epoch: 213\n",
      "training loss: 32331851443072.754\n",
      "validation loss: 25269364102816.12\n",
      "epoch: 214\n",
      "training loss: 32327928432141.766\n",
      "validation loss: 25255897070878.406\n",
      "epoch: 215\n",
      "training loss: 32324013309195.977\n",
      "validation loss: 25242458499312.934\n",
      "epoch: 216\n",
      "training loss: 32320106057802.207\n",
      "validation loss: 25229048303971.445\n",
      "epoch: 217\n",
      "training loss: 32316206661554.344\n",
      "validation loss: 25215666401740.387\n",
      "epoch: 218\n",
      "training loss: 32312315104073.258\n",
      "validation loss: 25202312710507.754\n",
      "epoch: 219\n",
      "training loss: 32308431369006.6\n",
      "validation loss: 25188987149131.125\n",
      "epoch: 220\n",
      "training loss: 32304555440028.727\n",
      "validation loss: 25175689637406.89\n",
      "epoch: 221\n",
      "training loss: 32300687300840.61\n",
      "validation loss: 25162420096040.527\n",
      "epoch: 222\n",
      "training loss: 32296826935169.816\n",
      "validation loss: 25149178446618.008\n",
      "epoch: 223\n",
      "training loss: 32292974326770.426\n",
      "validation loss: 25135964611578.24\n",
      "epoch: 224\n",
      "training loss: 32289129459423.086\n",
      "validation loss: 25122778514186.434\n",
      "epoch: 225\n",
      "training loss: 32285292316934.992\n",
      "validation loss: 25109620078508.547\n",
      "epoch: 226\n",
      "training loss: 32281462883139.9\n",
      "validation loss: 25096489229386.504\n",
      "epoch: 227\n",
      "training loss: 32277641141898.203\n",
      "validation loss: 25083385892414.473\n",
      "epoch: 228\n",
      "training loss: 32273827077096.938\n",
      "validation loss: 25070309993915.81\n",
      "epoch: 229\n",
      "training loss: 32270020672649.902\n",
      "validation loss: 25057261460921.02\n",
      "epoch: 230\n",
      "training loss: 32266221912497.668\n",
      "validation loss: 25044240221146.34\n",
      "epoch: 231\n",
      "training loss: 32262430780607.688\n",
      "validation loss: 25031246202973.23\n",
      "epoch: 232\n",
      "training loss: 32258647260974.324\n",
      "validation loss: 25018279335428.473\n",
      "epoch: 233\n",
      "training loss: 32254871337618.965\n",
      "validation loss: 25005339548165.105\n",
      "epoch: 234\n",
      "training loss: 32251102994589.777\n",
      "validation loss: 24992426771443.934\n",
      "epoch: 235\n",
      "training loss: 32247342215960.676\n",
      "validation loss: 24979540936115.773\n",
      "epoch: 236\n",
      "training loss: 32243588985815.062\n",
      "validation loss: 24966681973604.273\n",
      "epoch: 237\n",
      "training loss: 32239843275162.293\n",
      "validation loss: 24953849815889.344\n",
      "epoch: 238\n",
      "training loss: 32235553529410.707\n",
      "validation loss: 24941389514102.516\n",
      "epoch: 239\n",
      "training loss: 32231813434691.195\n",
      "validation loss: 24928619362808.07\n",
      "epoch: 240\n",
      "training loss: 32228080828031.742\n",
      "validation loss: 24915875312333.277\n",
      "epoch: 241\n",
      "training loss: 32232124843359.95\n",
      "validation loss: 24922597455491.746\n",
      "epoch: 242\n",
      "training loss: 32228491616004.336\n",
      "validation loss: 24909948381551.297\n",
      "epoch: 243\n",
      "training loss: 32224865703379.18\n",
      "validation loss: 24897330811882.4\n",
      "epoch: 244\n",
      "training loss: 32221247084533.465\n",
      "validation loss: 24884744478719.742\n",
      "epoch: 245\n",
      "training loss: 32217635739104.33\n",
      "validation loss: 24872189122446.723\n",
      "epoch: 246\n",
      "training loss: 32214031647270.156\n",
      "validation loss: 24859664491281.035\n",
      "epoch: 247\n",
      "training loss: 32210434789707.324\n",
      "validation loss: 24847170340973.348\n",
      "epoch: 248\n",
      "training loss: 32206845147550.336\n",
      "validation loss: 24834706434518.617\n",
      "epoch: 249\n",
      "training loss: 32203262702355.145\n",
      "validation loss: 24822272541879.25\n",
      "epoch: 250\n",
      "training loss: 32199687436065.312\n",
      "validation loss: 24809868439719.75\n",
      "epoch: 251\n",
      "training loss: 32196119330980.945\n",
      "validation loss: 24797493911152.168\n",
      "epoch: 252\n",
      "training loss: 32192558369729.97\n",
      "validation loss: 24785148745492.008\n",
      "epoch: 253\n",
      "training loss: 32189004535241.875\n",
      "validation loss: 24772832738024.047\n",
      "epoch: 254\n",
      "training loss: 32185457810723.406\n",
      "validation loss: 24760545689777.617\n",
      "epoch: 255\n",
      "training loss: 32181918179636.26\n",
      "validation loss: 24748287407310.984\n",
      "epoch: 256\n",
      "training loss: 32178385625676.65\n",
      "validation loss: 24736057702504.383\n",
      "epoch: 257\n",
      "training loss: 32174860132756.406\n",
      "validation loss: 24723856392361.367\n",
      "epoch: 258\n",
      "training loss: 32171341684985.7\n",
      "validation loss: 24711683298817.996\n",
      "epoch: 259\n",
      "training loss: 32167830266657.16\n",
      "validation loss: 24699538248559.723\n",
      "epoch: 260\n",
      "training loss: 32164325862231.27\n",
      "validation loss: 24687421072845.492\n",
      "epoch: 261\n",
      "training loss: 32160828456322.98\n",
      "validation loss: 24675331607338.746\n",
      "epoch: 262\n",
      "training loss: 32157338033689.44\n",
      "validation loss: 24663269691945.133\n",
      "epoch: 263\n",
      "training loss: 32153854579218.77\n",
      "validation loss: 24651235170656.59\n",
      "epoch: 264\n",
      "training loss: 32150378077919.668\n",
      "validation loss: 24639227891401.49\n",
      "epoch: 265\n",
      "training loss: 32146908514912.074\n",
      "validation loss: 24627247705900.65\n",
      "epoch: 266\n",
      "training loss: 32143445875418.42\n",
      "validation loss: 24615294469528.95\n",
      "epoch: 267\n",
      "training loss: 32139990144755.824\n",
      "validation loss: 24603368041182.31\n",
      "epoch: 268\n",
      "training loss: 32136541308328.76\n",
      "validation loss: 24591468283149.824\n",
      "epoch: 269\n",
      "training loss: 32133099351622.51\n",
      "validation loss: 24579595060990.81\n",
      "epoch: 270\n",
      "training loss: 32129664260197.156\n",
      "validation loss: 24567748243416.613\n",
      "epoch: 271\n",
      "training loss: 32126236019682.02\n",
      "validation loss: 24555927702176.945\n",
      "epoch: 272\n",
      "training loss: 32122814615770.652\n",
      "validation loss: 24544133311950.535\n",
      "epoch: 273\n",
      "training loss: 32119400034216.336\n",
      "validation loss: 24532364950240.08\n",
      "epoch: 274\n",
      "training loss: 32115992260827.86\n",
      "validation loss: 24520622497270.99\n",
      "epoch: 275\n",
      "training loss: 32112591281465.777\n",
      "validation loss: 24508905835894.23\n",
      "epoch: 276\n",
      "training loss: 32109197082038.906\n",
      "validation loss: 24497214851492.707\n",
      "epoch: 277\n",
      "training loss: 32105809648501.36\n",
      "validation loss: 24485549431891.184\n",
      "epoch: 278\n",
      "training loss: 32102428966849.56\n",
      "validation loss: 24473909467269.715\n",
      "epoch: 279\n",
      "training loss: 32099055023119.8\n",
      "validation loss: 24462294850080.23\n",
      "epoch: 280\n",
      "training loss: 32095687803385.914\n",
      "validation loss: 24450705474966.39\n",
      "epoch: 281\n",
      "training loss: 32092327293757.16\n",
      "validation loss: 24439141238686.324\n",
      "epoch: 282\n",
      "training loss: 32088973480376.387\n",
      "validation loss: 24427602040038.348\n",
      "epoch: 283\n",
      "training loss: 32085626349418.35\n",
      "validation loss: 24416087779789.48\n",
      "epoch: 284\n",
      "training loss: 32082285887088.176\n",
      "validation loss: 24404598360606.566\n",
      "epoch: 285\n",
      "training loss: 32078952079620.023\n",
      "validation loss: 24393133686990.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 286\n",
      "training loss: 32075624913275.895\n",
      "validation loss: 24381693665210.105\n",
      "epoch: 287\n",
      "training loss: 32072304374344.527\n",
      "validation loss: 24370278203245.402\n",
      "epoch: 288\n",
      "training loss: 32068990449140.457\n",
      "validation loss: 24358887210723.777\n",
      "epoch: 289\n",
      "training loss: 32065683124003.184\n",
      "validation loss: 24347520598865.418\n",
      "epoch: 290\n",
      "training loss: 32062382385296.426\n",
      "validation loss: 24336178280428.043\n",
      "epoch: 291\n",
      "training loss: 32059088219407.426\n",
      "validation loss: 24324860169654.082\n",
      "epoch: 292\n",
      "training loss: 32055800612746.438\n",
      "validation loss: 24313566182219.863\n",
      "epoch: 293\n",
      "training loss: 32052519551746.207\n",
      "validation loss: 24302296235186.688\n",
      "epoch: 294\n",
      "training loss: 32049245022861.547\n",
      "validation loss: 24291050246953.664\n",
      "epoch: 295\n",
      "training loss: 32045977012568.953\n",
      "validation loss: 24279828137212.31\n",
      "epoch: 296\n",
      "training loss: 32042715507366.355\n",
      "validation loss: 24268629826902.832\n",
      "epoch: 297\n",
      "training loss: 32039460493772.777\n",
      "validation loss: 24257455238172.008\n",
      "epoch: 298\n",
      "training loss: 32036211958328.19\n",
      "validation loss: 24246304294332.637\n",
      "epoch: 299\n",
      "training loss: 32032969887593.32\n",
      "validation loss: 24235176919824.41\n",
      "epoch: 300\n",
      "training loss: 32029734268149.51\n",
      "validation loss: 24224073040176.344\n",
      "epoch: 301\n",
      "training loss: 32026505086598.62\n",
      "validation loss: 24212992581970.453\n",
      "epoch: 302\n",
      "training loss: 32023282329562.98\n",
      "validation loss: 24201935472806.855\n",
      "epoch: 303\n",
      "training loss: 32020065983685.297\n",
      "validation loss: 24190901641270.1\n",
      "epoch: 304\n",
      "training loss: 32016856035628.668\n",
      "validation loss: 24179891016896.754\n",
      "epoch: 305\n",
      "training loss: 32013652472076.594\n",
      "validation loss: 24168903530144.176\n",
      "epoch: 306\n",
      "training loss: 32010455279732.953\n",
      "validation loss: 24157939112360.39\n",
      "epoch: 307\n",
      "training loss: 32007264445322.082\n",
      "validation loss: 24146997695755.117\n",
      "epoch: 308\n",
      "training loss: 32004079955588.79\n",
      "validation loss: 24136079213371.816\n",
      "epoch: 309\n",
      "training loss: 32000901797298.44\n",
      "validation loss: 24125183599060.76\n",
      "epoch: 310\n",
      "training loss: 31997729957237.016\n",
      "validation loss: 24114310787453.12\n",
      "epoch: 311\n",
      "training loss: 31994564422211.19\n",
      "validation loss: 24103460713935.938\n",
      "epoch: 312\n",
      "training loss: 31991405179048.434\n",
      "validation loss: 24092633314628.035\n",
      "epoch: 313\n",
      "training loss: 31988252214597.105\n",
      "validation loss: 24081828526356.805\n",
      "epoch: 314\n",
      "training loss: 31985105515726.535\n",
      "validation loss: 24071046286635.848\n",
      "epoch: 315\n",
      "training loss: 31981965069327.164\n",
      "validation loss: 24060286533643.414\n",
      "epoch: 316\n",
      "training loss: 31978830862310.625\n",
      "validation loss: 24049549206201.605\n",
      "epoch: 317\n",
      "training loss: 31975702881609.832\n",
      "validation loss: 24038834243756.344\n",
      "epoch: 318\n",
      "training loss: 31972581114179.156\n",
      "validation loss: 24028141586358.105\n",
      "epoch: 319\n",
      "training loss: 31969465546994.504\n",
      "validation loss: 24017471174643.273\n",
      "epoch: 320\n",
      "training loss: 31966356167053.406\n",
      "validation loss: 24006822949816.24\n",
      "epoch: 321\n",
      "training loss: 31963252961375.184\n",
      "validation loss: 23996196853632.1\n",
      "epoch: 322\n",
      "training loss: 31960155917001.03\n",
      "validation loss: 23985592828380.01\n",
      "epoch: 323\n",
      "training loss: 31957065020994.133\n",
      "validation loss: 23975010816867.1\n",
      "epoch: 324\n",
      "training loss: 31953980260439.777\n",
      "validation loss: 23964450762403.06\n",
      "epoch: 325\n",
      "training loss: 31950901622445.473\n",
      "validation loss: 23953912608785.117\n",
      "epoch: 326\n",
      "training loss: 31947829094141.055\n",
      "validation loss: 23943396300283.727\n",
      "epoch: 327\n",
      "training loss: 31944762662678.78\n",
      "validation loss: 23932901781628.695\n",
      "epoch: 328\n",
      "training loss: 31941702315233.45\n",
      "validation loss: 23922428997995.777\n",
      "epoch: 329\n",
      "training loss: 31938648039002.484\n",
      "validation loss: 23911977894993.824\n",
      "epoch: 330\n",
      "training loss: 31935599821206.074\n",
      "validation loss: 23901548418652.336\n",
      "epoch: 331\n",
      "training loss: 31932557649087.19\n",
      "validation loss: 23891140515409.49\n",
      "epoch: 332\n",
      "training loss: 31929521509911.785\n",
      "validation loss: 23880754132100.574\n",
      "epoch: 333\n",
      "training loss: 31926491390968.777\n",
      "validation loss: 23870389215946.883\n",
      "epoch: 334\n",
      "training loss: 31923467279570.19\n",
      "validation loss: 23860045714544.945\n",
      "epoch: 335\n",
      "training loss: 31920449163051.273\n",
      "validation loss: 23849723575856.18\n",
      "epoch: 336\n",
      "training loss: 31917437028770.508\n",
      "validation loss: 23839422748196.953\n",
      "epoch: 337\n",
      "training loss: 31914430864109.71\n",
      "validation loss: 23829143180228.85\n",
      "epoch: 338\n",
      "training loss: 31911430656474.13\n",
      "validation loss: 23818884820949.496\n",
      "epoch: 339\n",
      "training loss: 31908436393292.504\n",
      "validation loss: 23808647619683.566\n",
      "epoch: 340\n",
      "training loss: 31905448062017.1\n",
      "validation loss: 23798431526074.098\n",
      "epoch: 341\n",
      "training loss: 31902465650123.848\n",
      "validation loss: 23788236490074.27\n",
      "epoch: 342\n",
      "training loss: 31899489145112.312\n",
      "validation loss: 23778062461939.273\n",
      "epoch: 343\n",
      "training loss: 31896518534505.816\n",
      "validation loss: 23767909392218.617\n",
      "epoch: 344\n",
      "training loss: 31893553805851.477\n",
      "validation loss: 23757777231748.652\n",
      "epoch: 345\n",
      "training loss: 31890594946720.246\n",
      "validation loss: 23747665931645.367\n",
      "epoch: 346\n",
      "training loss: 31887641944706.96\n",
      "validation loss: 23737575443297.4\n",
      "epoch: 347\n",
      "training loss: 31884694787430.418\n",
      "validation loss: 23727505718359.39\n",
      "epoch: 348\n",
      "training loss: 31881753462533.37\n",
      "validation loss: 23717456708745.473\n",
      "epoch: 349\n",
      "training loss: 31878817957682.594\n",
      "validation loss: 23707428366623.074\n",
      "epoch: 350\n",
      "training loss: 31875888260568.938\n",
      "validation loss: 23697420644406.844\n",
      "epoch: 351\n",
      "training loss: 31872964358907.324\n",
      "validation loss: 23687433494752.902\n",
      "epoch: 352\n",
      "training loss: 31870046240436.8\n",
      "validation loss: 23677466870553.19\n",
      "epoch: 353\n",
      "training loss: 31867133892920.562\n",
      "validation loss: 23667520724930.1\n",
      "epoch: 354\n",
      "training loss: 31864227304145.98\n",
      "validation loss: 23657595011231.23\n",
      "epoch: 355\n",
      "training loss: 31861326461924.62\n",
      "validation loss: 23647689683024.39\n",
      "epoch: 356\n",
      "training loss: 31858431354092.277\n",
      "validation loss: 23637804694092.71\n",
      "epoch: 357\n",
      "training loss: 31855541968508.977\n",
      "validation loss: 23627939998429.973\n",
      "epoch: 358\n",
      "training loss: 31852658293059.0\n",
      "validation loss: 23618095550236.08\n",
      "epoch: 359\n",
      "training loss: 31849780315650.89\n",
      "validation loss: 23608271303912.69\n",
      "epoch: 360\n",
      "training loss: 31846908024217.49\n",
      "validation loss: 23598467214059.03\n",
      "epoch: 361\n",
      "training loss: 31844041406715.9\n",
      "validation loss: 23588683235467.785\n",
      "epoch: 362\n",
      "training loss: 31841180451127.54\n",
      "validation loss: 23578919323121.21\n",
      "epoch: 363\n",
      "training loss: 31838325145458.133\n",
      "validation loss: 23569175432187.324\n",
      "epoch: 364\n",
      "training loss: 31835475477737.68\n",
      "validation loss: 23559451518016.258\n",
      "epoch: 365\n",
      "training loss: 31832631436020.516\n",
      "validation loss: 23549747536136.707\n",
      "epoch: 366\n",
      "training loss: 31829793008385.28\n",
      "validation loss: 23540063442252.57\n",
      "epoch: 367\n",
      "training loss: 31826960182934.91\n",
      "validation loss: 23530399192239.594\n",
      "epoch: 368\n",
      "training loss: 31824132947796.633\n",
      "validation loss: 23520754742142.23\n",
      "epoch: 369\n",
      "training loss: 31821311291121.984\n",
      "validation loss: 23511130048170.58\n",
      "epoch: 370\n",
      "training loss: 31818495201086.793\n",
      "validation loss: 23501525066697.38\n",
      "epoch: 371\n",
      "training loss: 31815684665891.145\n",
      "validation loss: 23491939754255.18\n",
      "epoch: 372\n",
      "training loss: 31812879673759.414\n",
      "validation loss: 23482374067533.582\n",
      "epoch: 373\n",
      "training loss: 31810080212940.227\n",
      "validation loss: 23472827963376.523\n",
      "epoch: 374\n",
      "training loss: 31807286271706.438\n",
      "validation loss: 23463301398779.754\n",
      "epoch: 375\n",
      "training loss: 31804497838355.164\n",
      "validation loss: 23453794330888.277\n",
      "epoch: 376\n",
      "training loss: 31801714901207.7\n",
      "validation loss: 23444306716994.016\n",
      "epoch: 377\n",
      "training loss: 31798937448609.56\n",
      "validation loss: 23434838514533.43\n",
      "epoch: 378\n",
      "training loss: 31796165468930.434\n",
      "validation loss: 23425389681085.258\n",
      "epoch: 379\n",
      "training loss: 31793398950564.156\n",
      "validation loss: 23415960174368.395\n",
      "epoch: 380\n",
      "training loss: 31790637881928.72\n",
      "validation loss: 23406549952239.76\n",
      "epoch: 381\n",
      "training loss: 31787882251466.23\n",
      "validation loss: 23397158972692.26\n",
      "epoch: 382\n",
      "training loss: 31785132047642.87\n",
      "validation loss: 23387787193852.832\n",
      "epoch: 383\n",
      "training loss: 31782387258948.91\n",
      "validation loss: 23378434573980.54\n",
      "epoch: 384\n",
      "training loss: 31779647873898.66\n",
      "validation loss: 23369101071464.742\n",
      "epoch: 385\n",
      "training loss: 31776913881030.465\n",
      "validation loss: 23359786644823.34\n",
      "epoch: 386\n",
      "training loss: 31774185268906.637\n",
      "validation loss: 23350491252700.996\n",
      "epoch: 387\n",
      "training loss: 31771462026113.473\n",
      "validation loss: 23341214853867.543\n",
      "epoch: 388\n",
      "training loss: 31768744141261.2\n",
      "validation loss: 23331957407216.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 389\n",
      "training loss: 31766031602983.96\n",
      "validation loss: 23322718871762.7\n",
      "epoch: 390\n",
      "training loss: 31763324399939.773\n",
      "validation loss: 23313499206642.453\n",
      "epoch: 391\n",
      "training loss: 31760622520810.52\n",
      "validation loss: 23304298371110.406\n",
      "epoch: 392\n",
      "training loss: 31757925954301.855\n",
      "validation loss: 23295116324538.99\n",
      "epoch: 393\n",
      "training loss: 31755234689143.293\n",
      "validation loss: 23285953026416.867\n",
      "epoch: 394\n",
      "training loss: 31752548714088.027\n",
      "validation loss: 23276808436347.637\n",
      "epoch: 395\n",
      "training loss: 31749868017913.023\n",
      "validation loss: 23267682514048.523\n",
      "epoch: 396\n",
      "training loss: 31747192589418.906\n",
      "validation loss: 23258575219349.156\n",
      "epoch: 397\n",
      "training loss: 31744522417429.96\n",
      "validation loss: 23249486512190.383\n",
      "epoch: 398\n",
      "training loss: 31741857490794.098\n",
      "validation loss: 23240416352623.07\n",
      "epoch: 399\n",
      "training loss: 31739197798382.8\n",
      "validation loss: 23231364700807.02\n",
      "epoch: 400\n",
      "training loss: 31736543329091.105\n",
      "validation loss: 23222331517009.836\n",
      "epoch: 401\n",
      "training loss: 31733894071837.547\n",
      "validation loss: 23213316761605.9\n",
      "epoch: 402\n",
      "training loss: 31731250015564.145\n",
      "validation loss: 23204320395075.312\n",
      "epoch: 403\n",
      "training loss: 31728611149236.355\n",
      "validation loss: 23195342378002.945\n",
      "epoch: 404\n",
      "training loss: 31725977461843.02\n",
      "validation loss: 23186382671077.438\n",
      "epoch: 405\n",
      "training loss: 31723348942396.355\n",
      "validation loss: 23177441235090.27\n",
      "epoch: 406\n",
      "training loss: 31720725579931.887\n",
      "validation loss: 23168518030934.855\n",
      "epoch: 407\n",
      "training loss: 31718107363508.438\n",
      "validation loss: 23159613019605.688\n",
      "epoch: 408\n",
      "training loss: 31715494282208.062\n",
      "validation loss: 23150726162197.438\n",
      "epoch: 409\n",
      "training loss: 31712886325136.035\n",
      "validation loss: 23141857419904.164\n",
      "epoch: 410\n",
      "training loss: 31710283481420.766\n",
      "validation loss: 23133006754018.48\n",
      "epoch: 411\n",
      "training loss: 31707685740213.83\n",
      "validation loss: 23124174125930.805\n",
      "epoch: 412\n",
      "training loss: 31705093090689.836\n",
      "validation loss: 23115359497128.562\n",
      "epoch: 413\n",
      "training loss: 31702505522046.484\n",
      "validation loss: 23106562829195.492\n",
      "epoch: 414\n",
      "training loss: 31699923023504.46\n",
      "validation loss: 23097784083810.883\n",
      "epoch: 415\n",
      "training loss: 31697345584307.402\n",
      "validation loss: 23089023222748.918\n",
      "epoch: 416\n",
      "training loss: 31694773193721.867\n",
      "validation loss: 23080280207877.957\n",
      "epoch: 417\n",
      "training loss: 31692205841037.312\n",
      "validation loss: 23071555001159.918\n",
      "epoch: 418\n",
      "training loss: 31689643515566.01\n",
      "validation loss: 23062847564649.613\n",
      "epoch: 419\n",
      "training loss: 31687086206643.03\n",
      "validation loss: 23054157860494.13\n",
      "epoch: 420\n",
      "training loss: 31684533903626.21\n",
      "validation loss: 23045485850932.227\n",
      "epoch: 421\n",
      "training loss: 31681986595896.09\n",
      "validation loss: 23036831498293.75\n",
      "epoch: 422\n",
      "training loss: 31679444272855.867\n",
      "validation loss: 23028194764999.03\n",
      "epoch: 423\n",
      "training loss: 31676906923931.363\n",
      "validation loss: 23019575613558.363\n",
      "epoch: 424\n",
      "training loss: 31674374538571.008\n",
      "validation loss: 23010974006571.44\n",
      "epoch: 425\n",
      "training loss: 31671847106245.746\n",
      "validation loss: 23002389906726.81\n",
      "epoch: 426\n",
      "training loss: 31669324616449.03\n",
      "validation loss: 22993823276801.402\n",
      "epoch: 427\n",
      "training loss: 31666807058696.75\n",
      "validation loss: 22985274079659.957\n",
      "epoch: 428\n",
      "training loss: 31664294422527.227\n",
      "validation loss: 22976742278254.61\n",
      "epoch: 429\n",
      "training loss: 31661786697501.137\n",
      "validation loss: 22968227835624.34\n",
      "epoch: 430\n",
      "training loss: 31659283873201.473\n",
      "validation loss: 22959730714894.562\n",
      "epoch: 431\n",
      "training loss: 31656785939233.52\n",
      "validation loss: 22951250879276.617\n",
      "epoch: 432\n",
      "training loss: 31654292885224.785\n",
      "validation loss: 22942788292067.367\n",
      "epoch: 433\n",
      "training loss: 31651804700824.992\n",
      "validation loss: 22934342916648.74\n",
      "epoch: 434\n",
      "training loss: 31649321375705.98\n",
      "validation loss: 22925914716487.305\n",
      "epoch: 435\n",
      "training loss: 31646842899561.72\n",
      "validation loss: 22917503655133.87\n",
      "epoch: 436\n",
      "training loss: 31644369262108.22\n",
      "validation loss: 22909109696223.06\n",
      "epoch: 437\n",
      "training loss: 31641900453083.52\n",
      "validation loss: 22900732803472.92\n",
      "epoch: 438\n",
      "training loss: 31639436462247.633\n",
      "validation loss: 22892372940684.547\n",
      "epoch: 439\n",
      "training loss: 31636977279382.49\n",
      "validation loss: 22884030071741.68\n",
      "epoch: 440\n",
      "training loss: 31634522894291.906\n",
      "validation loss: 22875704160610.355\n",
      "epoch: 441\n",
      "training loss: 31632073296801.555\n",
      "validation loss: 22867395171338.53\n",
      "epoch: 442\n",
      "training loss: 31629628476758.89\n",
      "validation loss: 22859103068055.723\n",
      "epoch: 443\n",
      "training loss: 31627188424033.125\n",
      "validation loss: 22850827814972.68\n",
      "epoch: 444\n",
      "training loss: 31624753128515.156\n",
      "validation loss: 22842569376381.008\n",
      "epoch: 445\n",
      "training loss: 31622322580117.574\n",
      "validation loss: 22834327716652.863\n",
      "epoch: 446\n",
      "training loss: 31619896768774.58\n",
      "validation loss: 22826102800240.613\n",
      "epoch: 447\n",
      "training loss: 31617475684441.945\n",
      "validation loss: 22817894591676.527\n",
      "epoch: 448\n",
      "training loss: 31615059317096.984\n",
      "validation loss: 22809703055572.426\n",
      "epoch: 449\n",
      "training loss: 31612647656738.477\n",
      "validation loss: 22801528156619.42\n",
      "epoch: 450\n",
      "training loss: 31610240693386.652\n",
      "validation loss: 22793369859587.57\n",
      "epoch: 451\n",
      "training loss: 31607838417083.164\n",
      "validation loss: 22785228129325.59\n",
      "epoch: 452\n",
      "training loss: 31605440817890.992\n",
      "validation loss: 22777102930760.582\n",
      "epoch: 453\n",
      "training loss: 31603047885894.418\n",
      "validation loss: 22768994228897.71\n",
      "epoch: 454\n",
      "training loss: 31600659611199.016\n",
      "validation loss: 22760901988819.95\n",
      "epoch: 455\n",
      "training loss: 31598275983931.57\n",
      "validation loss: 22752826175687.797\n",
      "epoch: 456\n",
      "training loss: 31595896994240.055\n",
      "validation loss: 22744766754738.984\n",
      "epoch: 457\n",
      "training loss: 31593522632293.555\n",
      "validation loss: 22736723691288.227\n",
      "epoch: 458\n",
      "training loss: 31591152888282.246\n",
      "validation loss: 22728696950726.957\n",
      "epoch: 459\n",
      "training loss: 31588787752417.387\n",
      "validation loss: 22720686498523.055\n",
      "epoch: 460\n",
      "training loss: 31586427214931.195\n",
      "validation loss: 22712692300220.582\n",
      "epoch: 461\n",
      "training loss: 31584071266076.867\n",
      "validation loss: 22704714321439.582\n",
      "epoch: 462\n",
      "training loss: 31581719896128.508\n",
      "validation loss: 22696752527875.74\n",
      "epoch: 463\n",
      "training loss: 31579373095381.09\n",
      "validation loss: 22688806885300.234\n",
      "epoch: 464\n",
      "training loss: 31577030854150.43\n",
      "validation loss: 22680877359559.434\n",
      "epoch: 465\n",
      "training loss: 31574693162773.11\n",
      "validation loss: 22672963916574.68\n",
      "epoch: 466\n",
      "training loss: 31572360011606.457\n",
      "validation loss: 22665066522342.023\n",
      "epoch: 467\n",
      "training loss: 31570031391028.477\n",
      "validation loss: 22657185142932.07\n",
      "epoch: 468\n",
      "training loss: 31567707291437.867\n",
      "validation loss: 22649319744489.652\n",
      "epoch: 469\n",
      "training loss: 31565387703253.89\n",
      "validation loss: 22641470293233.68\n",
      "epoch: 470\n",
      "training loss: 31563072616916.41\n",
      "validation loss: 22633636755456.867\n",
      "epoch: 471\n",
      "training loss: 31560762022885.78\n",
      "validation loss: 22625819097525.547\n",
      "epoch: 472\n",
      "training loss: 31558455911642.855\n",
      "validation loss: 22618017285879.445\n",
      "epoch: 473\n",
      "training loss: 31556154273688.914\n",
      "validation loss: 22610231287031.457\n",
      "epoch: 474\n",
      "training loss: 31553857099545.63\n",
      "validation loss: 22602461067567.434\n",
      "epoch: 475\n",
      "training loss: 31551564379755.02\n",
      "validation loss: 22594706594145.984\n",
      "epoch: 476\n",
      "training loss: 31549276104879.426\n",
      "validation loss: 22586967833498.273\n",
      "epoch: 477\n",
      "training loss: 31546992265501.426\n",
      "validation loss: 22579244752427.8\n",
      "epoch: 478\n",
      "training loss: 31544712852223.83\n",
      "validation loss: 22571537317810.19\n",
      "epoch: 479\n",
      "training loss: 31542437855669.617\n",
      "validation loss: 22563845496593.02\n",
      "epoch: 480\n",
      "training loss: 31540167266481.92\n",
      "validation loss: 22556169255795.598\n",
      "epoch: 481\n",
      "training loss: 31537901075323.938\n",
      "validation loss: 22548508562508.793\n",
      "epoch: 482\n",
      "training loss: 31535639272878.93\n",
      "validation loss: 22540863383894.797\n",
      "epoch: 483\n",
      "training loss: 31533381849850.164\n",
      "validation loss: 22533233687186.992\n",
      "epoch: 484\n",
      "training loss: 31531128796960.855\n",
      "validation loss: 22525619439689.68\n",
      "epoch: 485\n",
      "training loss: 31528880104954.156\n",
      "validation loss: 22518020608777.977\n",
      "epoch: 486\n",
      "training loss: 31526635764593.09\n",
      "validation loss: 22510437161897.57\n",
      "epoch: 487\n",
      "training loss: 31524395766660.516\n",
      "validation loss: 22502869066564.566\n",
      "epoch: 488\n",
      "training loss: 31522160101959.074\n",
      "validation loss: 22495316290365.273\n",
      "epoch: 489\n",
      "training loss: 31519928761311.18\n",
      "validation loss: 22487778800956.06\n",
      "epoch: 490\n",
      "training loss: 31517701735558.934\n",
      "validation loss: 22480256566063.12\n",
      "epoch: 491\n",
      "training loss: 31515479015564.125\n",
      "validation loss: 22472749553482.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 492\n",
      "training loss: 31513260592208.14\n",
      "validation loss: 22465257731079.184\n",
      "epoch: 493\n",
      "training loss: 31511046456391.977\n",
      "validation loss: 22457781066788.285\n",
      "epoch: 494\n",
      "training loss: 31508836599036.152\n",
      "validation loss: 22450319528613.547\n",
      "epoch: 495\n",
      "training loss: 31506631011080.69\n",
      "validation loss: 22442873084627.8\n",
      "epoch: 496\n",
      "training loss: 31504429683485.07\n",
      "validation loss: 22435441702972.69\n",
      "epoch: 497\n",
      "training loss: 31502232607228.195\n",
      "validation loss: 22428025351858.5\n",
      "epoch: 498\n",
      "training loss: 31500039773308.324\n",
      "validation loss: 22420623999563.97\n",
      "epoch: 499\n",
      "training loss: 31497851172743.062\n",
      "validation loss: 22413237614436.133\n",
      "epoch: 500\n",
      "training loss: 31495666796569.316\n",
      "validation loss: 22405866164890.15\n",
      "epoch: 501\n",
      "training loss: 31493486635843.207\n",
      "validation loss: 22398509619409.14\n",
      "epoch: 502\n",
      "training loss: 31491310681640.094\n",
      "validation loss: 22391167946544.008\n",
      "epoch: 503\n",
      "training loss: 31489138925054.504\n",
      "validation loss: 22383841114913.316\n",
      "epoch: 504\n",
      "training loss: 31486971357200.074\n",
      "validation loss: 22376529093203.06\n",
      "epoch: 505\n",
      "training loss: 31484807969209.527\n",
      "validation loss: 22369231850166.555\n",
      "epoch: 506\n",
      "training loss: 31482648752234.645\n",
      "validation loss: 22361949354624.27\n",
      "epoch: 507\n",
      "training loss: 31480493697446.203\n",
      "validation loss: 22354681575463.633\n",
      "epoch: 508\n",
      "training loss: 31478342796033.926\n",
      "validation loss: 22347428481638.918\n",
      "epoch: 509\n",
      "training loss: 31476196039206.5\n",
      "validation loss: 22340190042171.05\n",
      "epoch: 510\n",
      "training loss: 31474053418191.44\n",
      "validation loss: 22332966226147.473\n",
      "epoch: 511\n",
      "training loss: 31471914924235.15\n",
      "validation loss: 22325757002721.97\n",
      "epoch: 512\n",
      "training loss: 31469780548602.79\n",
      "validation loss: 22318562341114.527\n",
      "epoch: 513\n",
      "training loss: 31467650282578.32\n",
      "validation loss: 22311382210611.17\n",
      "epoch: 514\n",
      "training loss: 31465524117464.387\n",
      "validation loss: 22304216580563.816\n",
      "epoch: 515\n",
      "training loss: 31463402044582.344\n",
      "validation loss: 22297065420390.09\n",
      "epoch: 516\n",
      "training loss: 31461284055272.164\n",
      "validation loss: 22289928699573.223\n",
      "epoch: 517\n",
      "training loss: 31459170140892.414\n",
      "validation loss: 22282806387661.85\n",
      "epoch: 518\n",
      "training loss: 31457060292820.234\n",
      "validation loss: 22275698454269.918\n",
      "epoch: 519\n",
      "training loss: 31454954502451.29\n",
      "validation loss: 22268604869076.465\n",
      "epoch: 520\n",
      "training loss: 31452852761199.703\n",
      "validation loss: 22261525601825.516\n",
      "epoch: 521\n",
      "training loss: 31450755060498.055\n",
      "validation loss: 22254460622325.93\n",
      "epoch: 522\n",
      "training loss: 31448661391797.316\n",
      "validation loss: 22247409900451.242\n",
      "epoch: 523\n",
      "training loss: 31446571746566.82\n",
      "validation loss: 22240373406139.51\n",
      "epoch: 524\n",
      "training loss: 31444486116294.215\n",
      "validation loss: 22233351109393.184\n",
      "epoch: 525\n",
      "training loss: 31442404492485.438\n",
      "validation loss: 22226342980278.94\n",
      "epoch: 526\n",
      "training loss: 31440326866664.668\n",
      "validation loss: 22219348988927.56\n",
      "epoch: 527\n",
      "training loss: 31438253230374.277\n",
      "validation loss: 22212369105533.754\n",
      "epoch: 528\n",
      "training loss: 31436183575174.816\n",
      "validation loss: 22205403300356.055\n",
      "epoch: 529\n",
      "training loss: 31434117892644.945\n",
      "validation loss: 22198451543716.633\n",
      "epoch: 530\n",
      "training loss: 31432056174381.414\n",
      "validation loss: 22191513806001.184\n",
      "epoch: 531\n",
      "training loss: 31429998411999.027\n",
      "validation loss: 22184590057658.766\n",
      "epoch: 532\n",
      "training loss: 31427944597130.586\n",
      "validation loss: 22177680269201.664\n",
      "epoch: 533\n",
      "training loss: 31425894721426.855\n",
      "validation loss: 22170784411205.254\n",
      "epoch: 534\n",
      "training loss: 31423848776556.55\n",
      "validation loss: 22163902454307.85\n",
      "epoch: 535\n",
      "training loss: 31421806754206.26\n",
      "validation loss: 22157034369210.59\n",
      "epoch: 536\n",
      "training loss: 31419768646080.43\n",
      "validation loss: 22150180126677.24\n",
      "epoch: 537\n",
      "training loss: 31417734443901.332\n",
      "validation loss: 22143339697534.117\n",
      "epoch: 538\n",
      "training loss: 31415704139408.98\n",
      "validation loss: 22136513052669.91\n",
      "epoch: 539\n",
      "training loss: 31413677724361.168\n",
      "validation loss: 22129700163035.562\n",
      "epoch: 540\n",
      "training loss: 31411655190533.35\n",
      "validation loss: 22122900999644.105\n",
      "epoch: 541\n",
      "training loss: 31409636529718.69\n",
      "validation loss: 22116115533570.566\n",
      "epoch: 542\n",
      "training loss: 31407621733727.93\n",
      "validation loss: 22109343735951.773\n",
      "epoch: 543\n",
      "training loss: 31405610794389.402\n",
      "validation loss: 22102585577986.273\n",
      "epoch: 544\n",
      "training loss: 31403603703549.01\n",
      "validation loss: 22095841030934.152\n",
      "epoch: 545\n",
      "training loss: 31401600453070.156\n",
      "validation loss: 22089110066116.93\n",
      "epoch: 546\n",
      "training loss: 31399601034833.707\n",
      "validation loss: 22082392654917.402\n",
      "epoch: 547\n",
      "training loss: 31397605440737.957\n",
      "validation loss: 22075688768779.527\n",
      "epoch: 548\n",
      "training loss: 31395613662698.613\n",
      "validation loss: 22068998379208.258\n",
      "epoch: 549\n",
      "training loss: 31393625692648.754\n",
      "validation loss: 22062321457769.457\n",
      "epoch: 550\n",
      "training loss: 31391641522538.742\n",
      "validation loss: 22055657976089.707\n",
      "epoch: 551\n",
      "training loss: 31389661144336.24\n",
      "validation loss: 22049007905856.215\n",
      "epoch: 552\n",
      "training loss: 31387684550026.17\n",
      "validation loss: 22042371218816.67\n",
      "epoch: 553\n",
      "training loss: 31385711731610.65\n",
      "validation loss: 22035747886779.105\n",
      "epoch: 554\n",
      "training loss: 31383742681108.98\n",
      "validation loss: 22029137881611.766\n",
      "epoch: 555\n",
      "training loss: 31381777390557.586\n",
      "validation loss: 22022541175242.984\n",
      "epoch: 556\n",
      "training loss: 31379815852009.992\n",
      "validation loss: 22015957739661.04\n",
      "epoch: 557\n",
      "training loss: 31377858057536.797\n",
      "validation loss: 22009387546914.04\n",
      "epoch: 558\n",
      "training loss: 31375903999225.61\n",
      "validation loss: 22002830569109.773\n",
      "epoch: 559\n",
      "training loss: 31373953669181.04\n",
      "validation loss: 21996286778415.586\n",
      "epoch: 560\n",
      "training loss: 31372007059524.637\n",
      "validation loss: 21989756147058.26\n",
      "epoch: 561\n",
      "training loss: 31370064162394.875\n",
      "validation loss: 21983238647323.85\n",
      "epoch: 562\n",
      "training loss: 31368124969947.098\n",
      "validation loss: 21976734251557.625\n",
      "epoch: 563\n",
      "training loss: 31366189474353.492\n",
      "validation loss: 21970242932163.855\n",
      "epoch: 564\n",
      "training loss: 31364257667803.07\n",
      "validation loss: 21963764661605.727\n",
      "epoch: 565\n",
      "training loss: 31362329542501.58\n",
      "validation loss: 21957299412405.234\n",
      "epoch: 566\n",
      "training loss: 31360405090671.527\n",
      "validation loss: 21950847157142.992\n",
      "epoch: 567\n",
      "training loss: 31358484304552.11\n",
      "validation loss: 21944407868458.156\n",
      "epoch: 568\n",
      "training loss: 31356567176399.18\n",
      "validation loss: 21937981519048.285\n",
      "epoch: 569\n",
      "training loss: 31354653698485.223\n",
      "validation loss: 21931568081669.223\n",
      "epoch: 570\n",
      "training loss: 31352743863099.31\n",
      "validation loss: 21925167529134.92\n",
      "epoch: 571\n",
      "training loss: 31350837662547.074\n",
      "validation loss: 21918779834317.387\n",
      "epoch: 572\n",
      "training loss: 31348935089150.656\n",
      "validation loss: 21912404970146.504\n",
      "epoch: 573\n",
      "training loss: 31347036135248.676\n",
      "validation loss: 21906042909609.914\n",
      "epoch: 574\n",
      "training loss: 31345140793196.203\n",
      "validation loss: 21899693625752.918\n",
      "epoch: 575\n",
      "training loss: 31343249055364.727\n",
      "validation loss: 21893357091678.332\n",
      "epoch: 576\n",
      "training loss: 31341360914142.113\n",
      "validation loss: 21887033280546.36\n",
      "epoch: 577\n",
      "training loss: 31339476361932.547\n",
      "validation loss: 21880722165574.465\n",
      "epoch: 578\n",
      "training loss: 31337595391156.547\n",
      "validation loss: 21874423720037.258\n",
      "epoch: 579\n",
      "training loss: 31335717994250.887\n",
      "validation loss: 21868137917266.383\n",
      "epoch: 580\n",
      "training loss: 31333844163668.566\n",
      "validation loss: 21861864730650.363\n",
      "epoch: 581\n",
      "training loss: 31331973891878.8\n",
      "validation loss: 21855604133634.508\n",
      "epoch: 582\n",
      "training loss: 31330107171366.973\n",
      "validation loss: 21849356099720.766\n",
      "epoch: 583\n",
      "training loss: 31328243994634.59\n",
      "validation loss: 21843120602467.605\n",
      "epoch: 584\n",
      "training loss: 31326384354199.24\n",
      "validation loss: 21836897615489.926\n",
      "epoch: 585\n",
      "training loss: 31324528242594.586\n",
      "validation loss: 21830687112458.87\n",
      "epoch: 586\n",
      "training loss: 31322675652370.33\n",
      "validation loss: 21824489067101.797\n",
      "epoch: 587\n",
      "training loss: 31320826576092.13\n",
      "validation loss: 21818303453202.047\n",
      "epoch: 588\n",
      "training loss: 31318981006341.63\n",
      "validation loss: 21812130244598.918\n",
      "epoch: 589\n",
      "training loss: 31317138935716.402\n",
      "validation loss: 21805969415187.492\n",
      "epoch: 590\n",
      "training loss: 31315300356829.875\n",
      "validation loss: 21799820938918.523\n",
      "epoch: 591\n",
      "training loss: 31313465262311.35\n",
      "validation loss: 21793684789798.324\n",
      "epoch: 592\n",
      "training loss: 31311633644805.953\n",
      "validation loss: 21787560941888.66\n",
      "epoch: 593\n",
      "training loss: 31309805496974.594\n",
      "validation loss: 21781449369306.582\n",
      "epoch: 594\n",
      "training loss: 31307980811493.91\n",
      "validation loss: 21775350046224.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 595\n",
      "training loss: 31306159581056.297\n",
      "validation loss: 21769262946869.367\n",
      "epoch: 596\n",
      "training loss: 31304341798369.8\n",
      "validation loss: 21763188045523.883\n",
      "epoch: 597\n",
      "training loss: 31302527456158.125\n",
      "validation loss: 21757125316525.05\n",
      "epoch: 598\n",
      "training loss: 31300716547160.598\n",
      "validation loss: 21751074734264.742\n",
      "epoch: 599\n",
      "training loss: 31298909064132.125\n",
      "validation loss: 21745036273189.426\n",
      "epoch: 600\n",
      "training loss: 31297104999843.164\n",
      "validation loss: 21739009907800.06\n",
      "epoch: 601\n",
      "training loss: 31295304347079.684\n",
      "validation loss: 21732995612651.98\n",
      "epoch: 602\n",
      "training loss: 31293507098643.125\n",
      "validation loss: 21726993362354.754\n",
      "epoch: 603\n",
      "training loss: 31291713247350.4\n",
      "validation loss: 21721003131572.105\n",
      "epoch: 604\n",
      "training loss: 31289922786033.805\n",
      "validation loss: 21715024895021.758\n",
      "epoch: 605\n",
      "training loss: 31288135707541.05\n",
      "validation loss: 21709058627475.348\n",
      "epoch: 606\n",
      "training loss: 31286352004735.176\n",
      "validation loss: 21703104303758.293\n",
      "epoch: 607\n",
      "training loss: 31284571670494.535\n",
      "validation loss: 21697161898749.68\n",
      "epoch: 608\n",
      "training loss: 31282794697712.785\n",
      "validation loss: 21691231387382.168\n",
      "epoch: 609\n",
      "training loss: 31281021079298.797\n",
      "validation loss: 21685312744641.812\n",
      "epoch: 610\n",
      "training loss: 31279250808176.688\n",
      "validation loss: 21679405945568.035\n",
      "epoch: 611\n",
      "training loss: 31277483877285.75\n",
      "validation loss: 21673510965253.44\n",
      "epoch: 612\n",
      "training loss: 31275720279580.406\n",
      "validation loss: 21667627778843.742\n",
      "epoch: 613\n",
      "training loss: 31273960008030.246\n",
      "validation loss: 21661756361537.62\n",
      "epoch: 614\n",
      "training loss: 31272203055619.895\n",
      "validation loss: 21655896688586.625\n",
      "epoch: 615\n",
      "training loss: 31270449415349.06\n",
      "validation loss: 21650048735295.08\n",
      "epoch: 616\n",
      "training loss: 31268699080232.445\n",
      "validation loss: 21644212477019.902\n",
      "epoch: 617\n",
      "training loss: 31266952043299.78\n",
      "validation loss: 21638387889170.58\n",
      "epoch: 618\n",
      "training loss: 31265208297595.695\n",
      "validation loss: 21632574947208.98\n",
      "epoch: 619\n",
      "training loss: 31263467836179.805\n",
      "validation loss: 21626773626649.277\n",
      "epoch: 620\n",
      "training loss: 31261730652126.58\n",
      "validation loss: 21620983903057.85\n",
      "epoch: 621\n",
      "training loss: 31259996738525.344\n",
      "validation loss: 21615205752053.13\n",
      "epoch: 622\n",
      "training loss: 31258266088480.277\n",
      "validation loss: 21609439149305.508\n",
      "epoch: 623\n",
      "training loss: 31256538695110.33\n",
      "validation loss: 21603684070537.258\n",
      "epoch: 624\n",
      "training loss: 31254814551549.215\n",
      "validation loss: 21597940491522.36\n",
      "epoch: 625\n",
      "training loss: 31253093650945.4\n",
      "validation loss: 21592208388086.43\n",
      "epoch: 626\n",
      "training loss: 31251375986462.027\n",
      "validation loss: 21586487736106.59\n",
      "epoch: 627\n",
      "training loss: 31249661551276.918\n",
      "validation loss: 21580778511511.395\n",
      "epoch: 628\n",
      "training loss: 31247950338582.535\n",
      "validation loss: 21575080690280.688\n",
      "epoch: 629\n",
      "training loss: 31246242341585.934\n",
      "validation loss: 21569394248445.465\n",
      "epoch: 630\n",
      "training loss: 31244537553508.754\n",
      "validation loss: 21563719162087.836\n",
      "epoch: 631\n",
      "training loss: 31242835967587.164\n",
      "validation loss: 21558055407340.86\n",
      "epoch: 632\n",
      "training loss: 31241137577071.86\n",
      "validation loss: 21552402960388.453\n",
      "epoch: 633\n",
      "training loss: 31239442375227.996\n",
      "validation loss: 21546761797465.26\n",
      "epoch: 634\n",
      "training loss: 31237750355335.188\n",
      "validation loss: 21541131894856.594\n",
      "epoch: 635\n",
      "training loss: 31236061510687.473\n",
      "validation loss: 21535513228898.277\n",
      "epoch: 636\n",
      "training loss: 31234375834593.26\n",
      "validation loss: 21529905775976.56\n",
      "epoch: 637\n",
      "training loss: 31232693320375.312\n",
      "validation loss: 21524309512527.99\n",
      "epoch: 638\n",
      "training loss: 31231013961370.74\n",
      "validation loss: 21518724415039.32\n",
      "epoch: 639\n",
      "training loss: 31229337750930.914\n",
      "validation loss: 21513150460047.402\n",
      "epoch: 640\n",
      "training loss: 31227664682421.484\n",
      "validation loss: 21507587624139.098\n",
      "epoch: 641\n",
      "training loss: 31225994749222.34\n",
      "validation loss: 21502035883951.098\n",
      "epoch: 642\n",
      "training loss: 31224327944727.543\n",
      "validation loss: 21496495216169.934\n",
      "epoch: 643\n",
      "training loss: 31222664262345.348\n",
      "validation loss: 21490965597531.723\n",
      "epoch: 644\n",
      "training loss: 31221003695498.152\n",
      "validation loss: 21485447004822.21\n",
      "epoch: 645\n",
      "training loss: 31219346237622.44\n",
      "validation loss: 21479939414876.555\n",
      "epoch: 646\n",
      "training loss: 31217691882168.793\n",
      "validation loss: 21474442804579.277\n",
      "epoch: 647\n",
      "training loss: 31216040622601.83\n",
      "validation loss: 21468957150864.12\n",
      "epoch: 648\n",
      "training loss: 31214392452400.184\n",
      "validation loss: 21463482430714.0\n",
      "epoch: 649\n",
      "training loss: 31212747365056.492\n",
      "validation loss: 21458018621160.812\n",
      "epoch: 650\n",
      "training loss: 31211105354077.34\n",
      "validation loss: 21452565699285.41\n",
      "epoch: 651\n",
      "training loss: 31209466412983.234\n",
      "validation loss: 21447123642217.46\n",
      "epoch: 652\n",
      "training loss: 31207830535308.58\n",
      "validation loss: 21441692427135.32\n",
      "epoch: 653\n",
      "training loss: 31206197714601.65\n",
      "validation loss: 21436272031265.99\n",
      "epoch: 654\n",
      "training loss: 31204567944424.56\n",
      "validation loss: 21430862431884.945\n",
      "epoch: 655\n",
      "training loss: 31202941218353.242\n",
      "validation loss: 21425463606316.08\n",
      "epoch: 656\n",
      "training loss: 31201317529977.37\n",
      "validation loss: 21420075531931.58\n",
      "epoch: 657\n",
      "training loss: 31199696872900.402\n",
      "validation loss: 21414698186151.812\n",
      "epoch: 658\n",
      "training loss: 31198079240739.5\n",
      "validation loss: 21409331546445.254\n",
      "epoch: 659\n",
      "training loss: 31196464627125.5\n",
      "validation loss: 21403975590328.348\n",
      "epoch: 660\n",
      "training loss: 31194853025702.94\n",
      "validation loss: 21398630295365.44\n",
      "epoch: 661\n",
      "training loss: 31193244430129.938\n",
      "validation loss: 21393295639168.633\n",
      "epoch: 662\n",
      "training loss: 31191638834078.242\n",
      "validation loss: 21387971599397.72\n",
      "epoch: 663\n",
      "training loss: 31190036231233.152\n",
      "validation loss: 21382658153760.066\n",
      "epoch: 664\n",
      "training loss: 31188436615293.53\n",
      "validation loss: 21377355280010.516\n",
      "epoch: 665\n",
      "training loss: 31186839979971.742\n",
      "validation loss: 21372062955951.266\n",
      "epoch: 666\n",
      "training loss: 31185246318993.63\n",
      "validation loss: 21366781159431.81\n",
      "epoch: 667\n",
      "training loss: 31183655626098.492\n",
      "validation loss: 21361509868348.78\n",
      "epoch: 668\n",
      "training loss: 31182067895039.06\n",
      "validation loss: 21356249060645.9\n",
      "epoch: 669\n",
      "training loss: 31180483119581.453\n",
      "validation loss: 21350998714313.83\n",
      "epoch: 670\n",
      "training loss: 31178901293505.152\n",
      "validation loss: 21345758807390.12\n",
      "epoch: 671\n",
      "training loss: 31177322410603.004\n",
      "validation loss: 21340529317959.082\n",
      "epoch: 672\n",
      "training loss: 31175746464681.137\n",
      "validation loss: 21335310224151.676\n",
      "epoch: 673\n",
      "training loss: 31174173449558.97\n",
      "validation loss: 21330101504145.434\n",
      "epoch: 674\n",
      "training loss: 31172603359069.17\n",
      "validation loss: 21324903136164.344\n",
      "epoch: 675\n",
      "training loss: 31171036187057.633\n",
      "validation loss: 21319715098478.78\n",
      "epoch: 676\n",
      "training loss: 31169471927383.46\n",
      "validation loss: 21314537369405.344\n",
      "epoch: 677\n",
      "training loss: 31167910573918.887\n",
      "validation loss: 21309369927306.836\n",
      "epoch: 678\n",
      "training loss: 31166352120549.316\n",
      "validation loss: 21304212750592.1\n",
      "epoch: 679\n",
      "training loss: 31164796561173.26\n",
      "validation loss: 21299065817715.957\n",
      "epoch: 680\n",
      "training loss: 31163243889702.3\n",
      "validation loss: 21293929107179.086\n",
      "epoch: 681\n",
      "training loss: 31161694100061.074\n",
      "validation loss: 21288802597527.953\n",
      "epoch: 682\n",
      "training loss: 31160147186187.242\n",
      "validation loss: 21283686267354.676\n",
      "epoch: 683\n",
      "training loss: 31158603142031.473\n",
      "validation loss: 21278580095296.953\n",
      "epoch: 684\n",
      "training loss: 31157061961557.395\n",
      "validation loss: 21273484060037.965\n",
      "epoch: 685\n",
      "training loss: 31155523638741.59\n",
      "validation loss: 21268398140306.258\n",
      "epoch: 686\n",
      "training loss: 31153988167573.535\n",
      "validation loss: 21263322314875.664\n",
      "epoch: 687\n",
      "training loss: 31152455542055.598\n",
      "validation loss: 21258256562565.2\n",
      "epoch: 688\n",
      "training loss: 31150925756203.016\n",
      "validation loss: 21253200862238.965\n",
      "epoch: 689\n",
      "training loss: 31149398804043.848\n",
      "validation loss: 21248155192806.06\n",
      "epoch: 690\n",
      "training loss: 31147874679618.953\n",
      "validation loss: 21243119533220.45\n",
      "epoch: 691\n",
      "training loss: 31146353376981.953\n",
      "validation loss: 21238093862480.926\n",
      "epoch: 692\n",
      "training loss: 31144834890199.26\n",
      "validation loss: 21233078159630.965\n",
      "epoch: 693\n",
      "training loss: 31143319213349.957\n",
      "validation loss: 21228072403758.65\n",
      "epoch: 694\n",
      "training loss: 31141806340525.844\n",
      "validation loss: 21223076573996.574\n",
      "epoch: 695\n",
      "training loss: 31140296265831.375\n",
      "validation loss: 21218090649521.742\n",
      "epoch: 696\n",
      "training loss: 31138788983383.664\n",
      "validation loss: 21213114609555.49\n",
      "epoch: 697\n",
      "training loss: 31137284487312.402\n",
      "validation loss: 21208148433363.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 698\n",
      "training loss: 31135782771759.89\n",
      "validation loss: 21203192100254.984\n",
      "epoch: 699\n",
      "training loss: 31134283830880.97\n",
      "validation loss: 21198245589584.12\n",
      "epoch: 700\n",
      "training loss: 31132787658843.016\n",
      "validation loss: 21193308880748.395\n",
      "epoch: 701\n",
      "training loss: 31131294249825.91\n",
      "validation loss: 21188381953189.3\n",
      "epoch: 702\n",
      "training loss: 31129803598022.01\n",
      "validation loss: 21183464786392.082\n",
      "epoch: 703\n",
      "training loss: 31128315697636.11\n",
      "validation loss: 21178557359885.625\n",
      "epoch: 704\n",
      "training loss: 31126830542885.426\n",
      "validation loss: 21173659653242.4\n",
      "epoch: 705\n",
      "training loss: 31125348127999.586\n",
      "validation loss: 21168771646078.312\n",
      "epoch: 706\n",
      "training loss: 31123868447220.566\n",
      "validation loss: 21163893318052.688\n",
      "epoch: 707\n",
      "training loss: 31122391494802.695\n",
      "validation loss: 21159024648868.11\n",
      "epoch: 708\n",
      "training loss: 31120917265012.61\n",
      "validation loss: 21154165618270.34\n",
      "epoch: 709\n",
      "training loss: 31119445752129.242\n",
      "validation loss: 21149316206048.27\n",
      "epoch: 710\n",
      "training loss: 31117976950443.777\n",
      "validation loss: 21144476392033.77\n",
      "epoch: 711\n",
      "training loss: 31116510854259.64\n",
      "validation loss: 21139646156101.625\n",
      "epoch: 712\n",
      "training loss: 31115047457892.46\n",
      "validation loss: 21134825478169.453\n",
      "epoch: 713\n",
      "training loss: 31113586755670.05\n",
      "validation loss: 21130014338197.598\n",
      "epoch: 714\n",
      "training loss: 31112128741932.395\n",
      "validation loss: 21125212716189.03\n",
      "epoch: 715\n",
      "training loss: 31110673411031.574\n",
      "validation loss: 21120420592189.273\n",
      "epoch: 716\n",
      "training loss: 31109220757331.81\n",
      "validation loss: 21115637946286.31\n",
      "epoch: 717\n",
      "training loss: 31107770775209.37\n",
      "validation loss: 21110864758610.465\n",
      "epoch: 718\n",
      "training loss: 31106323459052.598\n",
      "validation loss: 21106101009334.367\n",
      "epoch: 719\n",
      "training loss: 31104878803261.844\n",
      "validation loss: 21101346678672.793\n",
      "epoch: 720\n",
      "training loss: 31103436802249.47\n",
      "validation loss: 21096601746882.637\n",
      "epoch: 721\n",
      "training loss: 31101997450439.797\n",
      "validation loss: 21091866194262.79\n",
      "epoch: 722\n",
      "training loss: 31100560742269.13\n",
      "validation loss: 21087140001154.027\n",
      "epoch: 723\n",
      "training loss: 31099126672185.633\n",
      "validation loss: 21082423147938.984\n",
      "epoch: 724\n",
      "training loss: 31097695234649.44\n",
      "validation loss: 21077715615042.0\n",
      "epoch: 725\n",
      "training loss: 31096266424132.5\n",
      "validation loss: 21073017382929.055\n",
      "epoch: 726\n",
      "training loss: 31094840235118.633\n",
      "validation loss: 21068328432107.707\n",
      "epoch: 727\n",
      "training loss: 31093416662103.473\n",
      "validation loss: 21063648743126.95\n",
      "epoch: 728\n",
      "training loss: 31091995699594.438\n",
      "validation loss: 21058978296577.16\n",
      "epoch: 729\n",
      "training loss: 31090577342110.742\n",
      "validation loss: 21054317073090.008\n",
      "epoch: 730\n",
      "training loss: 31089161584183.305\n",
      "validation loss: 21049665053338.344\n",
      "epoch: 731\n",
      "training loss: 31087748420354.8\n",
      "validation loss: 21045022218036.137\n",
      "epoch: 732\n",
      "training loss: 31086337845179.566\n",
      "validation loss: 21040388547938.38\n",
      "epoch: 733\n",
      "training loss: 31084929853223.633\n",
      "validation loss: 21035764023840.992\n",
      "epoch: 734\n",
      "training loss: 31083524439064.66\n",
      "validation loss: 21031148626580.742\n",
      "epoch: 735\n",
      "training loss: 31082121597291.91\n",
      "validation loss: 21026542337035.152\n",
      "epoch: 736\n",
      "training loss: 31080721322506.28\n",
      "validation loss: 21021945136122.406\n",
      "epoch: 737\n",
      "training loss: 31079323609320.184\n",
      "validation loss: 21017357004801.28\n",
      "epoch: 738\n",
      "training loss: 31077928452357.605\n",
      "validation loss: 21012777924071.047\n",
      "epoch: 739\n",
      "training loss: 31076535846254.05\n",
      "validation loss: 21008207874971.38\n",
      "epoch: 740\n",
      "training loss: 31075145785656.508\n",
      "validation loss: 21003646838582.273\n",
      "epoch: 741\n",
      "training loss: 31073758265223.434\n",
      "validation loss: 20999094796023.96\n",
      "epoch: 742\n",
      "training loss: 31072373279624.723\n",
      "validation loss: 20994551728456.844\n",
      "epoch: 743\n",
      "training loss: 31070990823541.707\n",
      "validation loss: 20990017617081.344\n",
      "epoch: 744\n",
      "training loss: 31069610891667.086\n",
      "validation loss: 20985492443137.902\n",
      "epoch: 745\n",
      "training loss: 31068233478704.96\n",
      "validation loss: 20980976187906.824\n",
      "epoch: 746\n",
      "training loss: 31066858579370.742\n",
      "validation loss: 20976468832708.227\n",
      "epoch: 747\n",
      "training loss: 31065486188391.188\n",
      "validation loss: 20971970358901.945\n",
      "epoch: 748\n",
      "training loss: 31064116300504.344\n",
      "validation loss: 20967480747887.46\n",
      "epoch: 749\n",
      "training loss: 31062748910459.504\n",
      "validation loss: 20962999981103.793\n",
      "epoch: 750\n",
      "training loss: 31061384013017.27\n",
      "validation loss: 20958528040029.434\n",
      "epoch: 751\n",
      "training loss: 31060021602949.395\n",
      "validation loss: 20954064906182.246\n",
      "epoch: 752\n",
      "training loss: 31058661675038.875\n",
      "validation loss: 20949610561119.414\n",
      "epoch: 753\n",
      "training loss: 31057304224079.87\n",
      "validation loss: 20945164986437.297\n",
      "epoch: 754\n",
      "training loss: 31055949244877.695\n",
      "validation loss: 20940728163771.406\n",
      "epoch: 755\n",
      "training loss: 31054596732248.78\n",
      "validation loss: 20936300074796.29\n",
      "epoch: 756\n",
      "training loss: 31053246681020.668\n",
      "validation loss: 20931880701225.465\n",
      "epoch: 757\n",
      "training loss: 31051899086031.977\n",
      "validation loss: 20927470024811.305\n",
      "epoch: 758\n",
      "training loss: 31050553942132.38\n",
      "validation loss: 20923068027345.0\n",
      "epoch: 759\n",
      "training loss: 31049211244182.59\n",
      "validation loss: 20918674690656.44\n",
      "epoch: 760\n",
      "training loss: 31047870987054.305\n",
      "validation loss: 20914289996614.145\n",
      "epoch: 761\n",
      "training loss: 31046533165630.223\n",
      "validation loss: 20909913927125.156\n",
      "epoch: 762\n",
      "training loss: 31045197774804.01\n",
      "validation loss: 20905546464135.016\n",
      "epoch: 763\n",
      "training loss: 31043864809480.246\n",
      "validation loss: 20901187589627.625\n",
      "epoch: 764\n",
      "training loss: 31042534264574.44\n",
      "validation loss: 20896837285625.17\n",
      "epoch: 765\n",
      "training loss: 31041206135012.99\n",
      "validation loss: 20892495534188.074\n",
      "epoch: 766\n",
      "training loss: 31039880415733.137\n",
      "validation loss: 20888162317414.89\n",
      "epoch: 767\n",
      "training loss: 31038557101682.996\n",
      "validation loss: 20883837617442.2\n",
      "epoch: 768\n",
      "training loss: 31037236187821.492\n",
      "validation loss: 20879521416444.582\n",
      "epoch: 769\n",
      "training loss: 31035917669118.332\n",
      "validation loss: 20875213696634.477\n",
      "epoch: 770\n",
      "training loss: 31034601540554.004\n",
      "validation loss: 20870914440262.152\n",
      "epoch: 771\n",
      "training loss: 31033287797119.742\n",
      "validation loss: 20866623629615.598\n",
      "epoch: 772\n",
      "training loss: 31031976433817.52\n",
      "validation loss: 20862341247020.445\n",
      "epoch: 773\n",
      "training loss: 31030667445660.0\n",
      "validation loss: 20858067274839.895\n",
      "epoch: 774\n",
      "training loss: 31029360827670.523\n",
      "validation loss: 20853801695474.633\n",
      "epoch: 775\n",
      "training loss: 31028056574883.094\n",
      "validation loss: 20849544491362.73\n",
      "epoch: 776\n",
      "training loss: 31026754682342.344\n",
      "validation loss: 20845295644979.598\n",
      "epoch: 777\n",
      "training loss: 31025455145103.53\n",
      "validation loss: 20841055138837.88\n",
      "epoch: 778\n",
      "training loss: 31024157958232.484\n",
      "validation loss: 20836822955487.402\n",
      "epoch: 779\n",
      "training loss: 31022863116805.598\n",
      "validation loss: 20832599077515.047\n",
      "epoch: 780\n",
      "training loss: 31021570615909.82\n",
      "validation loss: 20828383487544.73\n",
      "epoch: 781\n",
      "training loss: 31020280450642.617\n",
      "validation loss: 20824176168237.27\n",
      "epoch: 782\n",
      "training loss: 31018992616111.94\n",
      "validation loss: 20819977102290.336\n",
      "epoch: 783\n",
      "training loss: 31017707107436.23\n",
      "validation loss: 20815786272438.363\n",
      "epoch: 784\n",
      "training loss: 31016423919744.37\n",
      "validation loss: 20811603661452.492\n",
      "epoch: 785\n",
      "training loss: 31015143048175.676\n",
      "validation loss: 20807429252140.457\n",
      "epoch: 786\n",
      "training loss: 31013864487879.867\n",
      "validation loss: 20803263027346.508\n",
      "epoch: 787\n",
      "training loss: 31012588234017.047\n",
      "validation loss: 20799104969951.367\n",
      "epoch: 788\n",
      "training loss: 31011314281757.695\n",
      "validation loss: 20794955062872.14\n",
      "epoch: 789\n",
      "training loss: 31010042626282.605\n",
      "validation loss: 20790813289062.207\n",
      "epoch: 790\n",
      "training loss: 31008773262782.92\n",
      "validation loss: 20786679631511.18\n",
      "epoch: 791\n",
      "training loss: 31007506186460.05\n",
      "validation loss: 20782554073244.797\n",
      "epoch: 792\n",
      "training loss: 31006241392525.684\n",
      "validation loss: 20778436597324.887\n",
      "epoch: 793\n",
      "training loss: 31004978876201.777\n",
      "validation loss: 20774327186849.234\n",
      "epoch: 794\n",
      "training loss: 31003718632720.492\n",
      "validation loss: 20770225824951.555\n",
      "epoch: 795\n",
      "training loss: 31002460657324.21\n",
      "validation loss: 20766132494801.383\n",
      "epoch: 796\n",
      "training loss: 31001204945265.51\n",
      "validation loss: 20762047179604.02\n",
      "epoch: 797\n",
      "training loss: 30999951491807.117\n",
      "validation loss: 20757969862600.434\n",
      "epoch: 798\n",
      "training loss: 30998700292221.883\n",
      "validation loss: 20753900527067.215\n",
      "epoch: 799\n",
      "training loss: 30997451341792.81\n",
      "validation loss: 20749839156316.44\n",
      "epoch: 800\n",
      "training loss: 30996204635812.98\n",
      "validation loss: 20745785733695.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 801\n",
      "training loss: 30994960169585.56\n",
      "validation loss: 20741740242587.844\n",
      "epoch: 802\n",
      "training loss: 30993717938423.742\n",
      "validation loss: 20737702666411.168\n",
      "epoch: 803\n",
      "training loss: 30992477937650.805\n",
      "validation loss: 20733672988619.098\n",
      "epoch: 804\n",
      "training loss: 30991240162599.99\n",
      "validation loss: 20729651192700.234\n",
      "epoch: 805\n",
      "training loss: 30990004608614.535\n",
      "validation loss: 20725637262178.215\n",
      "epoch: 806\n",
      "training loss: 30988771271047.66\n",
      "validation loss: 20721631180611.727\n",
      "epoch: 807\n",
      "training loss: 30987540145262.54\n",
      "validation loss: 20717632931594.336\n",
      "epoch: 808\n",
      "training loss: 30986311226632.246\n",
      "validation loss: 20713642498754.48\n",
      "epoch: 809\n",
      "training loss: 30985084510539.758\n",
      "validation loss: 20709659865755.363\n",
      "epoch: 810\n",
      "training loss: 30983859992377.96\n",
      "validation loss: 20705685016294.867\n",
      "epoch: 811\n",
      "training loss: 30982637667549.566\n",
      "validation loss: 20701717934105.53\n",
      "epoch: 812\n",
      "training loss: 30981417531467.164\n",
      "validation loss: 20697758602954.418\n",
      "epoch: 813\n",
      "training loss: 30980199579553.133\n",
      "validation loss: 20693807006643.066\n",
      "epoch: 814\n",
      "training loss: 30978983807239.66\n",
      "validation loss: 20689863129007.42\n",
      "epoch: 815\n",
      "training loss: 30977770209968.707\n",
      "validation loss: 20685926953917.773\n",
      "epoch: 816\n",
      "training loss: 30976558783191.97\n",
      "validation loss: 20681998465278.637\n",
      "epoch: 817\n",
      "training loss: 30975349522370.926\n",
      "validation loss: 20678077647028.72\n",
      "epoch: 818\n",
      "training loss: 30974142422976.715\n",
      "validation loss: 20674164483140.824\n",
      "epoch: 819\n",
      "training loss: 30972937480490.2\n",
      "validation loss: 20670258957621.824\n",
      "epoch: 820\n",
      "training loss: 30971734690401.91\n",
      "validation loss: 20666361054512.504\n",
      "epoch: 821\n",
      "training loss: 30970534048212.01\n",
      "validation loss: 20662470757887.574\n",
      "epoch: 822\n",
      "training loss: 30969335549430.312\n",
      "validation loss: 20658588051855.55\n",
      "epoch: 823\n",
      "training loss: 30968139189576.223\n",
      "validation loss: 20654712920558.695\n",
      "epoch: 824\n",
      "training loss: 30966944964178.754\n",
      "validation loss: 20650845348172.92\n",
      "epoch: 825\n",
      "training loss: 30965752868776.47\n",
      "validation loss: 20646985318907.766\n",
      "epoch: 826\n",
      "training loss: 30964562898917.47\n",
      "validation loss: 20643132817006.27\n",
      "epoch: 827\n",
      "training loss: 30963375050159.41\n",
      "validation loss: 20639287826744.95\n",
      "epoch: 828\n",
      "training loss: 30962189318069.438\n",
      "validation loss: 20635450332433.695\n",
      "epoch: 829\n",
      "training loss: 30961005698224.184\n",
      "validation loss: 20631620318415.7\n",
      "epoch: 830\n",
      "training loss: 30959824186209.742\n",
      "validation loss: 20627797769067.434\n",
      "epoch: 831\n",
      "training loss: 30958644777621.652\n",
      "validation loss: 20623982668798.484\n",
      "epoch: 832\n",
      "training loss: 30957467468064.88\n",
      "validation loss: 20620175002051.574\n",
      "epoch: 833\n",
      "training loss: 30956292253153.793\n",
      "validation loss: 20616374753302.45\n",
      "epoch: 834\n",
      "training loss: 30955119128512.15\n",
      "validation loss: 20612581907059.797\n",
      "epoch: 835\n",
      "training loss: 30953948089773.062\n",
      "validation loss: 20608796447865.19\n",
      "epoch: 836\n",
      "training loss: 30952779132578.98\n",
      "validation loss: 20605018360293.04\n",
      "epoch: 837\n",
      "training loss: 30951612252581.7\n",
      "validation loss: 20601247628950.49\n",
      "epoch: 838\n",
      "training loss: 30950447445442.3\n",
      "validation loss: 20597484238477.348\n",
      "epoch: 839\n",
      "training loss: 30949284706831.133\n",
      "validation loss: 20593728173546.03\n",
      "epoch: 840\n",
      "training loss: 30948124032427.85\n",
      "validation loss: 20589979418861.508\n",
      "epoch: 841\n",
      "training loss: 30946965417921.305\n",
      "validation loss: 20586237959161.2\n",
      "epoch: 842\n",
      "training loss: 30945808859009.605\n",
      "validation loss: 20582503779214.92\n",
      "epoch: 843\n",
      "training loss: 30944654351400.047\n",
      "validation loss: 20578776863824.824\n",
      "epoch: 844\n",
      "training loss: 30943501890809.105\n",
      "validation loss: 20575057197825.31\n",
      "epoch: 845\n",
      "training loss: 30942351472962.418\n",
      "validation loss: 20571344766082.965\n",
      "epoch: 846\n",
      "training loss: 30941203093594.79\n",
      "validation loss: 20567639553496.5\n",
      "epoch: 847\n",
      "training loss: 30940056748450.12\n",
      "validation loss: 20563941544996.69\n",
      "epoch: 848\n",
      "training loss: 30938912433281.42\n",
      "validation loss: 20560250725546.26\n",
      "epoch: 849\n",
      "training loss: 30937770143850.766\n",
      "validation loss: 20556567080139.87\n",
      "epoch: 850\n",
      "training loss: 30936629875929.332\n",
      "validation loss: 20552890593804.03\n",
      "epoch: 851\n",
      "training loss: 30935491625297.28\n",
      "validation loss: 20549221251596.996\n",
      "epoch: 852\n",
      "training loss: 30934355387743.785\n",
      "validation loss: 20545559038608.773\n",
      "epoch: 853\n",
      "training loss: 30933221159066.906\n",
      "validation loss: 20541903939960.96\n",
      "epoch: 854\n",
      "training loss: 30932088935073.363\n",
      "validation loss: 20538255940806.754\n",
      "epoch: 855\n",
      "training loss: 30930958711577.027\n",
      "validation loss: 20534615026330.793\n",
      "epoch: 856\n",
      "training loss: 30929830484378.39\n",
      "validation loss: 20530981181749.02\n",
      "epoch: 857\n",
      "training loss: 30928704217892.492\n",
      "validation loss: 20527354392307.105\n",
      "epoch: 858\n",
      "training loss: 30928777483922.844\n",
      "validation loss: 20524935702495.51\n",
      "epoch: 859\n",
      "training loss: 30927657586515.305\n",
      "validation loss: 20521323827267.01\n",
      "epoch: 860\n",
      "training loss: 30926539656359.543\n",
      "validation loss: 20517719261484.324\n",
      "epoch: 861\n",
      "training loss: 30925423689207.87\n",
      "validation loss: 20514121978362.656\n",
      "epoch: 862\n",
      "training loss: 30924309681083.47\n",
      "validation loss: 20510531951625.64\n",
      "epoch: 863\n",
      "training loss: 30923197628014.11\n",
      "validation loss: 20506949155465.07\n",
      "epoch: 864\n",
      "training loss: 30922087526032.293\n",
      "validation loss: 20503373564524.543\n",
      "epoch: 865\n",
      "training loss: 30920979371175.305\n",
      "validation loss: 20499805153883.74\n",
      "epoch: 866\n",
      "training loss: 30919873159485.344\n",
      "validation loss: 20496243899043.242\n",
      "epoch: 867\n",
      "training loss: 30918768887009.676\n",
      "validation loss: 20492689775910.043\n",
      "epoch: 868\n",
      "training loss: 30917666549800.65\n",
      "validation loss: 20489142760783.4\n",
      "epoch: 869\n",
      "training loss: 30916566143915.816\n",
      "validation loss: 20485602830341.336\n",
      "epoch: 870\n",
      "training loss: 30915467665418.06\n",
      "validation loss: 20482069961627.664\n",
      "epoch: 871\n",
      "training loss: 30914371110375.65\n",
      "validation loss: 20478544132039.383\n",
      "epoch: 872\n",
      "training loss: 30913276474862.32\n",
      "validation loss: 20475025319314.65\n",
      "epoch: 873\n",
      "training loss: 30912183754957.402\n",
      "validation loss: 20471513501521.11\n",
      "epoch: 874\n",
      "training loss: 30911092946745.855\n",
      "validation loss: 20468008657044.734\n",
      "epoch: 875\n",
      "training loss: 30910004046318.375\n",
      "validation loss: 20464510764579.02\n",
      "epoch: 876\n",
      "training loss: 30908917049771.45\n",
      "validation loss: 20461019803114.59\n",
      "epoch: 877\n",
      "training loss: 30907831953207.438\n",
      "validation loss: 20457535751929.22\n",
      "epoch: 878\n",
      "training loss: 30906748752734.66\n",
      "validation loss: 20454058590578.184\n",
      "epoch: 879\n",
      "training loss: 30905667444467.457\n",
      "validation loss: 20450588298885.008\n",
      "epoch: 880\n",
      "training loss: 30904588024526.215\n",
      "validation loss: 20447124856932.49\n",
      "epoch: 881\n",
      "training loss: 30903510489037.496\n",
      "validation loss: 20443668245054.145\n",
      "epoch: 882\n",
      "training loss: 30902434834134.066\n",
      "validation loss: 20440218443825.91\n",
      "epoch: 883\n",
      "training loss: 30901361055954.92\n",
      "validation loss: 20436775434058.14\n",
      "epoch: 884\n",
      "training loss: 30900289150645.402\n",
      "validation loss: 20433339196787.94\n",
      "epoch: 885\n",
      "training loss: 30899219114357.223\n",
      "validation loss: 20429909713271.785\n",
      "epoch: 886\n",
      "training loss: 30898150943248.5\n",
      "validation loss: 20426486964978.348\n",
      "epoch: 887\n",
      "training loss: 30897084633483.84\n",
      "validation loss: 20423070933581.695\n",
      "epoch: 888\n",
      "training loss: 30896020181234.36\n",
      "validation loss: 20419661600954.6\n",
      "epoch: 889\n",
      "training loss: 30894957582677.742\n",
      "validation loss: 20416258949162.254\n",
      "epoch: 890\n",
      "training loss: 30893896833998.273\n",
      "validation loss: 20412862960456.074\n",
      "epoch: 891\n",
      "training loss: 30892837931386.895\n",
      "validation loss: 20409473617267.8\n",
      "epoch: 892\n",
      "training loss: 30891780871041.223\n",
      "validation loss: 20406090902203.855\n",
      "epoch: 893\n",
      "training loss: 30890725649165.61\n",
      "validation loss: 20402714798039.816\n",
      "epoch: 894\n",
      "training loss: 30889672261971.156\n",
      "validation loss: 20399345287715.15\n",
      "epoch: 895\n",
      "training loss: 30888620705675.77\n",
      "validation loss: 20395982354328.133\n",
      "epoch: 896\n",
      "training loss: 30887570976504.18\n",
      "validation loss: 20392625981130.938\n",
      "epoch: 897\n",
      "training loss: 30886523070687.945\n",
      "validation loss: 20389276151524.934\n",
      "epoch: 898\n",
      "training loss: 30885476984465.547\n",
      "validation loss: 20385932849056.133\n",
      "epoch: 899\n",
      "training loss: 30884432714082.363\n",
      "validation loss: 20382596057410.824\n",
      "epoch: 900\n",
      "training loss: 30883390255790.695\n",
      "validation loss: 20379265760411.344\n",
      "epoch: 901\n",
      "training loss: 30882349605849.836\n",
      "validation loss: 20375941942012.016\n",
      "epoch: 902\n",
      "training loss: 30881310760526.016\n",
      "validation loss: 20372624586295.223\n",
      "epoch: 903\n",
      "training loss: 30880273716092.516\n",
      "validation loss: 20369313677467.652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 904\n",
      "training loss: 30879238468829.62\n",
      "validation loss: 20366009199856.695\n",
      "epoch: 905\n",
      "training loss: 30878205015024.664\n",
      "validation loss: 20362711137906.875\n",
      "epoch: 906\n",
      "training loss: 30877173350972.04\n",
      "validation loss: 20359419476176.54\n",
      "epoch: 907\n",
      "training loss: 30876143472973.227\n",
      "validation loss: 20356134199334.586\n",
      "epoch: 908\n",
      "training loss: 30875115377336.79\n",
      "validation loss: 20352855292157.332\n",
      "epoch: 909\n",
      "training loss: 30874089060378.4\n",
      "validation loss: 20349582739525.5\n",
      "epoch: 910\n",
      "training loss: 30873064518420.86\n",
      "validation loss: 20346316526421.34\n",
      "epoch: 911\n",
      "training loss: 30872041747794.1\n",
      "validation loss: 20343056637925.76\n",
      "epoch: 912\n",
      "training loss: 30871020744835.195\n",
      "validation loss: 20339803059215.71\n",
      "epoch: 913\n",
      "training loss: 30870001505888.38\n",
      "validation loss: 20336555775561.555\n",
      "epoch: 914\n",
      "training loss: 30868984027305.04\n",
      "validation loss: 20333314772324.52\n",
      "epoch: 915\n",
      "training loss: 30867968305443.723\n",
      "validation loss: 20330080034954.35\n",
      "epoch: 916\n",
      "training loss: 30866954336670.203\n",
      "validation loss: 20326851548986.973\n",
      "epoch: 917\n",
      "training loss: 30865942117357.39\n",
      "validation loss: 20323629300042.188\n",
      "epoch: 918\n",
      "training loss: 30864931643885.414\n",
      "validation loss: 20320413273821.594\n",
      "epoch: 919\n",
      "training loss: 30863922912641.582\n",
      "validation loss: 20317203456106.457\n",
      "epoch: 920\n",
      "training loss: 30862915920020.414\n",
      "validation loss: 20313999832755.72\n",
      "epoch: 921\n",
      "training loss: 30861910662423.633\n",
      "validation loss: 20310802389704.07\n",
      "epoch: 922\n",
      "training loss: 30860907136260.18\n",
      "validation loss: 20307611112960.06\n",
      "epoch: 923\n",
      "training loss: 30859905337946.176\n",
      "validation loss: 20304425988604.32\n",
      "epoch: 924\n",
      "training loss: 30858905263904.965\n",
      "validation loss: 20301247002787.83\n",
      "epoch: 925\n",
      "training loss: 30857906910567.12\n",
      "validation loss: 20298074141730.25\n",
      "epoch: 926\n",
      "training loss: 30856910274370.406\n",
      "validation loss: 20294907391718.305\n",
      "epoch: 927\n",
      "training loss: 30855915351759.81\n",
      "validation loss: 20291746739104.23\n",
      "epoch: 928\n",
      "training loss: 30854922139187.516\n",
      "validation loss: 20288592170304.293\n",
      "epoch: 929\n",
      "training loss: 30853930633112.93\n",
      "validation loss: 20285443671797.32\n",
      "epoch: 930\n",
      "training loss: 30852940830002.656\n",
      "validation loss: 20282301230123.336\n",
      "epoch: 931\n",
      "training loss: 30851952726330.523\n",
      "validation loss: 20279164831882.2\n",
      "epoch: 932\n",
      "training loss: 30850966318577.535\n",
      "validation loss: 20276034463732.32\n",
      "epoch: 933\n",
      "training loss: 30849981603231.9\n",
      "validation loss: 20272910112389.43\n",
      "epoch: 934\n",
      "training loss: 30848998576789.03\n",
      "validation loss: 20269791764625.33\n",
      "epoch: 935\n",
      "training loss: 30848017235751.53\n",
      "validation loss: 20266679407266.79\n",
      "epoch: 936\n",
      "training loss: 30847037576629.168\n",
      "validation loss: 20263573027194.367\n",
      "epoch: 937\n",
      "training loss: 30846059595938.926\n",
      "validation loss: 20260472611341.414\n",
      "epoch: 938\n",
      "training loss: 30845083290204.94\n",
      "validation loss: 20257378146692.94\n",
      "epoch: 939\n",
      "training loss: 30844108655958.527\n",
      "validation loss: 20254289620284.664\n",
      "epoch: 940\n",
      "training loss: 30843135689738.133\n",
      "validation loss: 20251207019202.027\n",
      "epoch: 941\n",
      "training loss: 30842164388089.42\n",
      "validation loss: 20248130330579.266\n",
      "epoch: 942\n",
      "training loss: 30841194747565.15\n",
      "validation loss: 20245059541598.508\n",
      "epoch: 943\n",
      "training loss: 30840226764725.246\n",
      "validation loss: 20241994639488.902\n",
      "epoch: 944\n",
      "training loss: 30839260436136.777\n",
      "validation loss: 20238935611525.78\n",
      "epoch: 945\n",
      "training loss: 30838295758373.938\n",
      "validation loss: 20235882445029.832\n",
      "epoch: 946\n",
      "training loss: 30837332728018.02\n",
      "validation loss: 20232835127366.344\n",
      "epoch: 947\n",
      "training loss: 30836371341657.45\n",
      "validation loss: 20229793645944.41\n",
      "epoch: 948\n",
      "training loss: 30835411595887.746\n",
      "validation loss: 20226757988216.26\n",
      "epoch: 949\n",
      "training loss: 30834453487311.547\n",
      "validation loss: 20223728141676.508\n",
      "epoch: 950\n",
      "training loss: 30833497012538.543\n",
      "validation loss: 20220704093861.46\n",
      "epoch: 951\n",
      "training loss: 30832542168185.52\n",
      "validation loss: 20217685832348.51\n",
      "epoch: 952\n",
      "training loss: 30831588950876.336\n",
      "validation loss: 20214673344755.477\n",
      "epoch: 953\n",
      "training loss: 30830637357241.887\n",
      "validation loss: 20211666618739.98\n",
      "epoch: 954\n",
      "training loss: 30829687383920.14\n",
      "validation loss: 20208665641998.88\n",
      "epoch: 955\n",
      "training loss: 30828739027556.09\n",
      "validation loss: 20205670402267.656\n",
      "epoch: 956\n",
      "training loss: 30827792284801.766\n",
      "validation loss: 20202680887319.938\n",
      "epoch: 957\n",
      "training loss: 30826847152316.21\n",
      "validation loss: 20199697084966.875\n",
      "epoch: 958\n",
      "training loss: 30825903626765.477\n",
      "validation loss: 20196718983056.688\n",
      "epoch: 959\n",
      "training loss: 30824961704822.617\n",
      "validation loss: 20193746569474.156\n",
      "epoch: 960\n",
      "training loss: 30824021383167.656\n",
      "validation loss: 20190779832140.113\n",
      "epoch: 961\n",
      "training loss: 30823082658487.62\n",
      "validation loss: 20187818759011.0\n",
      "epoch: 962\n",
      "training loss: 30822145527476.49\n",
      "validation loss: 20184863338078.43\n",
      "epoch: 963\n",
      "training loss: 30821209986835.184\n",
      "validation loss: 20181913557368.715\n",
      "epoch: 964\n",
      "training loss: 30820276033271.586\n",
      "validation loss: 20178969404942.492\n",
      "epoch: 965\n",
      "training loss: 30819343663500.5\n",
      "validation loss: 20176030868894.266\n",
      "epoch: 966\n",
      "training loss: 30818412874243.664\n",
      "validation loss: 20173097937352.043\n",
      "epoch: 967\n",
      "training loss: 30817483662229.69\n",
      "validation loss: 20170170598476.98\n",
      "epoch: 968\n",
      "training loss: 30816556024194.133\n",
      "validation loss: 20167248840462.93\n",
      "epoch: 969\n",
      "training loss: 30815629956879.414\n",
      "validation loss: 20164332651536.195\n",
      "epoch: 970\n",
      "training loss: 30814705457034.797\n",
      "validation loss: 20161422019955.074\n",
      "epoch: 971\n",
      "training loss: 30813782521416.457\n",
      "validation loss: 20158516934009.613\n",
      "epoch: 972\n",
      "training loss: 30812861146787.395\n",
      "validation loss: 20155617382021.23\n",
      "epoch: 973\n",
      "training loss: 30811941329917.434\n",
      "validation loss: 20152723352342.42\n",
      "epoch: 974\n",
      "training loss: 30811023067583.258\n",
      "validation loss: 20149834833356.457\n",
      "epoch: 975\n",
      "training loss: 30810106356568.336\n",
      "validation loss: 20146951813477.094\n",
      "epoch: 976\n",
      "training loss: 30809191193662.957\n",
      "validation loss: 20144074281148.27\n",
      "epoch: 977\n",
      "training loss: 30808277575664.17\n",
      "validation loss: 20141202224843.836\n",
      "epoch: 978\n",
      "training loss: 30807365499375.83\n",
      "validation loss: 20138335633067.312\n",
      "epoch: 979\n",
      "training loss: 30806454961608.535\n",
      "validation loss: 20135474494351.574\n",
      "epoch: 980\n",
      "training loss: 30805545959179.66\n",
      "validation loss: 20132618797258.656\n",
      "epoch: 981\n",
      "training loss: 30804638488913.285\n",
      "validation loss: 20129768530379.5\n",
      "epoch: 982\n",
      "training loss: 30803732547640.24\n",
      "validation loss: 20126923682333.684\n",
      "epoch: 983\n",
      "training loss: 30802828132198.055\n",
      "validation loss: 20124084241769.2\n",
      "epoch: 984\n",
      "training loss: 30801925239430.973\n",
      "validation loss: 20121250197362.27\n",
      "epoch: 985\n",
      "training loss: 30801023866189.914\n",
      "validation loss: 20118421537817.094\n",
      "epoch: 986\n",
      "training loss: 30800124009332.48\n",
      "validation loss: 20115598251865.633\n",
      "epoch: 987\n",
      "training loss: 30799225665722.93\n",
      "validation loss: 20112780328267.44\n",
      "epoch: 988\n",
      "training loss: 30798328832232.176\n",
      "validation loss: 20109967755809.44\n",
      "epoch: 989\n",
      "training loss: 30797433505737.758\n",
      "validation loss: 20107160523305.723\n",
      "epoch: 990\n",
      "training loss: 30796539683123.855\n",
      "validation loss: 20104358619597.375\n",
      "epoch: 991\n",
      "training loss: 30795647361281.266\n",
      "validation loss: 20101562033552.32\n",
      "epoch: 992\n",
      "training loss: 30794756537107.34\n",
      "validation loss: 20098770754065.066\n",
      "epoch: 993\n",
      "training loss: 30793867207506.05\n",
      "validation loss: 20095984770056.63\n",
      "epoch: 994\n",
      "training loss: 30792979369387.95\n",
      "validation loss: 20093204070474.305\n",
      "epoch: 995\n",
      "training loss: 30792093019670.12\n",
      "validation loss: 20090428644291.504\n",
      "epoch: 996\n",
      "training loss: 30791208155276.207\n",
      "validation loss: 20087658480507.633\n",
      "epoch: 997\n",
      "training loss: 30790324773136.39\n",
      "validation loss: 20084893568147.883\n",
      "epoch: 998\n",
      "training loss: 30789442870187.36\n",
      "validation loss: 20082133896263.152\n",
      "epoch: 999\n",
      "training loss: 30788562443372.316\n",
      "validation loss: 20079379453929.832\n",
      "epoch: 1000\n",
      "training loss: 30787683489640.96\n",
      "validation loss: 20076630230249.7\n",
      "epoch: 1001\n",
      "training loss: 30786806005949.47\n",
      "validation loss: 20073886214349.76\n",
      "epoch: 1002\n",
      "training loss: 30785929989260.484\n",
      "validation loss: 20071147395382.13\n",
      "epoch: 1003\n",
      "training loss: 30785055436543.105\n",
      "validation loss: 20068413762523.88\n",
      "epoch: 1004\n",
      "training loss: 30784182344772.88\n",
      "validation loss: 20065685304976.945\n",
      "epoch: 1005\n",
      "training loss: 30783310710931.766\n",
      "validation loss: 20062962011967.95\n",
      "epoch: 1006\n",
      "training loss: 30782440532008.152\n",
      "validation loss: 20060243872748.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1007\n",
      "training loss: 30781571804996.84\n",
      "validation loss: 20057530876593.133\n",
      "epoch: 1008\n",
      "training loss: 30780704526898.99\n",
      "validation loss: 20054823012803.06\n",
      "epoch: 1009\n",
      "training loss: 30779838694722.16\n",
      "validation loss: 20052120270702.16\n",
      "epoch: 1010\n",
      "training loss: 30778974305480.277\n",
      "validation loss: 20049422639638.844\n",
      "epoch: 1011\n",
      "training loss: 30778111356193.605\n",
      "validation loss: 20046730108985.54\n",
      "epoch: 1012\n",
      "training loss: 30777249843888.74\n",
      "validation loss: 20044042668138.562\n",
      "epoch: 1013\n",
      "training loss: 30776389765598.613\n",
      "validation loss: 20041360306518.04\n",
      "epoch: 1014\n",
      "training loss: 30775531118362.477\n",
      "validation loss: 20038683013567.805\n",
      "epoch: 1015\n",
      "training loss: 30774673899225.844\n",
      "validation loss: 20036010778755.29\n",
      "epoch: 1016\n",
      "training loss: 30773818105240.562\n",
      "validation loss: 20033343591571.406\n",
      "epoch: 1017\n",
      "training loss: 30772963733464.715\n",
      "validation loss: 20030681441530.484\n",
      "epoch: 1018\n",
      "training loss: 30772110780962.668\n",
      "validation loss: 20028024318170.17\n",
      "epoch: 1019\n",
      "training loss: 30771259244805.004\n",
      "validation loss: 20025372211051.312\n",
      "epoch: 1020\n",
      "training loss: 30770409122068.56\n",
      "validation loss: 20022725109757.887\n",
      "epoch: 1021\n",
      "training loss: 30769560409836.402\n",
      "validation loss: 20020083003896.9\n",
      "epoch: 1022\n",
      "training loss: 30768713105197.785\n",
      "validation loss: 20017445883098.33\n",
      "epoch: 1023\n",
      "training loss: 30767867205248.156\n",
      "validation loss: 20014813737014.97\n",
      "epoch: 1024\n",
      "training loss: 30767022707089.156\n",
      "validation loss: 20012186555322.44\n",
      "epoch: 1025\n",
      "training loss: 30766179607828.598\n",
      "validation loss: 20009564327719.027\n",
      "epoch: 1026\n",
      "training loss: 30765337904580.43\n",
      "validation loss: 20006947043925.64\n",
      "epoch: 1027\n",
      "training loss: 30764497594464.758\n",
      "validation loss: 20004334693685.707\n",
      "epoch: 1028\n",
      "training loss: 30763658674607.82\n",
      "validation loss: 20001727266765.145\n",
      "epoch: 1029\n",
      "training loss: 30762821142141.957\n",
      "validation loss: 19999124752952.2\n",
      "epoch: 1030\n",
      "training loss: 30761984994205.62\n",
      "validation loss: 19996527142057.457\n",
      "epoch: 1031\n",
      "training loss: 30761150227943.367\n",
      "validation loss: 19993934423913.695\n",
      "epoch: 1032\n",
      "training loss: 30760316840505.805\n",
      "validation loss: 19991346588375.87\n",
      "epoch: 1033\n",
      "training loss: 30759484829049.633\n",
      "validation loss: 19988763625321.004\n",
      "epoch: 1034\n",
      "training loss: 30758654190737.59\n",
      "validation loss: 19986185524648.156\n",
      "epoch: 1035\n",
      "training loss: 30757824922738.44\n",
      "validation loss: 19983612276278.273\n",
      "epoch: 1036\n",
      "training loss: 30756997022227.023\n",
      "validation loss: 19981043870154.2\n",
      "epoch: 1037\n",
      "training loss: 30756170486384.133\n",
      "validation loss: 19978480296240.58\n",
      "epoch: 1038\n",
      "training loss: 30755345312396.61\n",
      "validation loss: 19975921544523.777\n",
      "epoch: 1039\n",
      "training loss: 30754521497457.266\n",
      "validation loss: 19973367605011.84\n",
      "epoch: 1040\n",
      "training loss: 30753699038764.89\n",
      "validation loss: 19970818467734.406\n",
      "epoch: 1041\n",
      "training loss: 30752877933524.234\n",
      "validation loss: 19968274122742.63\n",
      "epoch: 1042\n",
      "training loss: 30752058178946.004\n",
      "validation loss: 19965734560109.17\n",
      "epoch: 1043\n",
      "training loss: 30751239772246.84\n",
      "validation loss: 19963199769928.08\n",
      "epoch: 1044\n",
      "training loss: 30750422710649.32\n",
      "validation loss: 19960669742314.742\n",
      "epoch: 1045\n",
      "training loss: 30749606991381.926\n",
      "validation loss: 19958144467405.85\n",
      "epoch: 1046\n",
      "training loss: 30748792611679.04\n",
      "validation loss: 19955623935359.297\n",
      "epoch: 1047\n",
      "training loss: 30747979568780.93\n",
      "validation loss: 19953108136354.164\n",
      "epoch: 1048\n",
      "training loss: 30747167859933.754\n",
      "validation loss: 19950597060590.605\n",
      "epoch: 1049\n",
      "training loss: 30746357482389.523\n",
      "validation loss: 19948090698289.85\n",
      "epoch: 1050\n",
      "training loss: 30745548433406.094\n",
      "validation loss: 19945589039694.09\n",
      "epoch: 1051\n",
      "training loss: 30744740710247.176\n",
      "validation loss: 19943092075066.453\n",
      "epoch: 1052\n",
      "training loss: 30743934310182.293\n",
      "validation loss: 19940599794690.957\n",
      "epoch: 1053\n",
      "training loss: 30743129230486.797\n",
      "validation loss: 19938112188872.406\n",
      "epoch: 1054\n",
      "training loss: 30742325468441.81\n",
      "validation loss: 19935629247936.383\n",
      "epoch: 1055\n",
      "training loss: 30741523021334.285\n",
      "validation loss: 19933150962229.16\n",
      "epoch: 1056\n",
      "training loss: 30740721886456.92\n",
      "validation loss: 19930677322117.69\n",
      "epoch: 1057\n",
      "training loss: 30739922061108.188\n",
      "validation loss: 19928208317989.48\n",
      "epoch: 1058\n",
      "training loss: 30739123542592.316\n",
      "validation loss: 19925743940252.61\n",
      "epoch: 1059\n",
      "training loss: 30738326328219.277\n",
      "validation loss: 19923284179335.62\n",
      "epoch: 1060\n",
      "training loss: 30737530415304.758\n",
      "validation loss: 19920829025687.53\n",
      "epoch: 1061\n",
      "training loss: 30736735801170.16\n",
      "validation loss: 19918378469777.69\n",
      "epoch: 1062\n",
      "training loss: 30735942483142.61\n",
      "validation loss: 19915932502095.824\n",
      "epoch: 1063\n",
      "training loss: 30735150458554.91\n",
      "validation loss: 19913491113151.914\n",
      "epoch: 1064\n",
      "training loss: 30734359724745.523\n",
      "validation loss: 19911054293476.184\n",
      "epoch: 1065\n",
      "training loss: 30733570279058.617\n",
      "validation loss: 19908622033619.023\n",
      "epoch: 1066\n",
      "training loss: 30732782118843.99\n",
      "validation loss: 19906194324150.945\n",
      "epoch: 1067\n",
      "training loss: 30731995241457.09\n",
      "validation loss: 19903771155662.566\n",
      "epoch: 1068\n",
      "training loss: 30731209644258.98\n",
      "validation loss: 19901352518764.5\n",
      "epoch: 1069\n",
      "training loss: 30730425324616.37\n",
      "validation loss: 19898938404087.36\n",
      "epoch: 1070\n",
      "training loss: 30729642279901.555\n",
      "validation loss: 19896528802281.688\n",
      "epoch: 1071\n",
      "training loss: 30728860507492.44\n",
      "validation loss: 19894123704017.895\n",
      "epoch: 1072\n",
      "training loss: 30728080004772.5\n",
      "validation loss: 19891723099986.25\n",
      "epoch: 1073\n",
      "training loss: 30727300769130.777\n",
      "validation loss: 19889326980896.785\n",
      "epoch: 1074\n",
      "training loss: 30726522797961.89\n",
      "validation loss: 19886935337479.273\n",
      "epoch: 1075\n",
      "training loss: 30725746088665.984\n",
      "validation loss: 19884548160483.203\n",
      "epoch: 1076\n",
      "training loss: 30724970638648.766\n",
      "validation loss: 19882165440677.67\n",
      "epoch: 1077\n",
      "training loss: 30724196445321.434\n",
      "validation loss: 19879787168851.39\n",
      "epoch: 1078\n",
      "training loss: 30723423506100.723\n",
      "validation loss: 19877413335812.617\n",
      "epoch: 1079\n",
      "training loss: 30722651818408.85\n",
      "validation loss: 19875043932389.145\n",
      "epoch: 1080\n",
      "training loss: 30721881379673.527\n",
      "validation loss: 19872678949428.17\n",
      "epoch: 1081\n",
      "training loss: 30721112187327.965\n",
      "validation loss: 19870318377796.36\n",
      "epoch: 1082\n",
      "training loss: 30720344238810.793\n",
      "validation loss: 19867962208379.703\n",
      "epoch: 1083\n",
      "training loss: 30719577531566.117\n",
      "validation loss: 19865610432083.535\n",
      "epoch: 1084\n",
      "training loss: 30718812063043.492\n",
      "validation loss: 19863263039832.477\n",
      "epoch: 1085\n",
      "training loss: 30718047830697.9\n",
      "validation loss: 19860920022570.36\n",
      "epoch: 1086\n",
      "training loss: 30717284831989.723\n",
      "validation loss: 19858581371260.21\n",
      "epoch: 1087\n",
      "training loss: 30716523064384.766\n",
      "validation loss: 19856247076884.227\n",
      "epoch: 1088\n",
      "training loss: 30715762525354.23\n",
      "validation loss: 19853917130443.688\n",
      "epoch: 1089\n",
      "training loss: 30715003212374.688\n",
      "validation loss: 19851591522958.91\n",
      "epoch: 1090\n",
      "training loss: 30714245122928.086\n",
      "validation loss: 19849270245469.258\n",
      "epoch: 1091\n",
      "training loss: 30713488254501.74\n",
      "validation loss: 19846953289033.05\n",
      "epoch: 1092\n",
      "training loss: 30712732604588.312\n",
      "validation loss: 19844640644727.543\n",
      "epoch: 1093\n",
      "training loss: 30711978170685.78\n",
      "validation loss: 19842332303648.844\n",
      "epoch: 1094\n",
      "training loss: 30711224950297.477\n",
      "validation loss: 19840028256911.957\n",
      "epoch: 1095\n",
      "training loss: 30710472940932.035\n",
      "validation loss: 19837728495650.633\n",
      "epoch: 1096\n",
      "training loss: 30709722140103.395\n",
      "validation loss: 19835433011017.41\n",
      "epoch: 1097\n",
      "training loss: 30708972545330.777\n",
      "validation loss: 19833141794183.527\n",
      "epoch: 1098\n",
      "training loss: 30708224154138.707\n",
      "validation loss: 19830854836338.918\n",
      "epoch: 1099\n",
      "training loss: 30707476964056.94\n",
      "validation loss: 19828572128692.11\n",
      "epoch: 1100\n",
      "training loss: 30706730972620.54\n",
      "validation loss: 19826293662470.246\n",
      "epoch: 1101\n",
      "training loss: 30705986177369.766\n",
      "validation loss: 19824019428919.008\n",
      "epoch: 1102\n",
      "training loss: 30705242575850.14\n",
      "validation loss: 19821749419302.582\n",
      "epoch: 1103\n",
      "training loss: 30704500165612.406\n",
      "validation loss: 19819483624903.63\n",
      "epoch: 1104\n",
      "training loss: 30703758944212.523\n",
      "validation loss: 19817222037023.227\n",
      "epoch: 1105\n",
      "training loss: 30703018909211.637\n",
      "validation loss: 19814964646980.824\n",
      "epoch: 1106\n",
      "training loss: 30702280058176.09\n",
      "validation loss: 19812711446114.234\n",
      "epoch: 1107\n",
      "training loss: 30701542388677.42\n",
      "validation loss: 19810462425779.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1108\n",
      "training loss: 30700805898292.305\n",
      "validation loss: 19808217577351.14\n",
      "epoch: 1109\n",
      "training loss: 30700070584602.598\n",
      "validation loss: 19805976892221.586\n",
      "epoch: 1110\n",
      "training loss: 30699336445195.305\n",
      "validation loss: 19803740361801.656\n",
      "epoch: 1111\n",
      "training loss: 30698603477662.543\n",
      "validation loss: 19801507977520.254\n",
      "epoch: 1112\n",
      "training loss: 30697871679601.57\n",
      "validation loss: 19799279730824.375\n",
      "epoch: 1113\n",
      "training loss: 30697141048614.75\n",
      "validation loss: 19797055613179.098\n",
      "epoch: 1114\n",
      "training loss: 30696411582309.56\n",
      "validation loss: 19794835616067.49\n",
      "epoch: 1115\n",
      "training loss: 30695683278298.55\n",
      "validation loss: 19792619730990.625\n",
      "epoch: 1116\n",
      "training loss: 30694956134199.363\n",
      "validation loss: 19790407949467.523\n",
      "epoch: 1117\n",
      "training loss: 30694230147634.707\n",
      "validation loss: 19788200263035.08\n",
      "epoch: 1118\n",
      "training loss: 30693505316232.35\n",
      "validation loss: 19785996663248.086\n",
      "epoch: 1119\n",
      "training loss: 30692781637625.113\n",
      "validation loss: 19783797141679.125\n",
      "epoch: 1120\n",
      "training loss: 30692059109450.84\n",
      "validation loss: 19781601689918.582\n",
      "epoch: 1121\n",
      "training loss: 30691337729352.4\n",
      "validation loss: 19779410299574.598\n",
      "epoch: 1122\n",
      "training loss: 30690617494977.707\n",
      "validation loss: 19777222962273.004\n",
      "epoch: 1123\n",
      "training loss: 30689898403979.64\n",
      "validation loss: 19775039669657.316\n",
      "epoch: 1124\n",
      "training loss: 30689180454016.09\n",
      "validation loss: 19772860413388.664\n",
      "epoch: 1125\n",
      "training loss: 30688463642749.934\n",
      "validation loss: 19770685185145.777\n",
      "epoch: 1126\n",
      "training loss: 30687747967849.016\n",
      "validation loss: 19768513976624.953\n",
      "epoch: 1127\n",
      "training loss: 30687033426986.14\n",
      "validation loss: 19766346779539.99\n",
      "epoch: 1128\n",
      "training loss: 30686320017839.074\n",
      "validation loss: 19764183585622.16\n",
      "epoch: 1129\n",
      "training loss: 30685607738090.492\n",
      "validation loss: 19762024386620.18\n",
      "epoch: 1130\n",
      "training loss: 30684896585428.047\n",
      "validation loss: 19759869174300.19\n",
      "epoch: 1131\n",
      "training loss: 30684186557544.27\n",
      "validation loss: 19757717940445.668\n",
      "epoch: 1132\n",
      "training loss: 30683477652136.613\n",
      "validation loss: 19755570676857.426\n",
      "epoch: 1133\n",
      "training loss: 30682769866907.44\n",
      "validation loss: 19753427375353.566\n",
      "epoch: 1134\n",
      "training loss: 30682063199563.984\n",
      "validation loss: 19751288027769.45\n",
      "epoch: 1135\n",
      "training loss: 30681357647818.367\n",
      "validation loss: 19749152625957.652\n",
      "epoch: 1136\n",
      "training loss: 30680653209387.56\n",
      "validation loss: 19747021161787.92\n",
      "epoch: 1137\n",
      "training loss: 30679949881993.418\n",
      "validation loss: 19744893627147.145\n",
      "epoch: 1138\n",
      "training loss: 30679247663362.617\n",
      "validation loss: 19742770013939.32\n",
      "epoch: 1139\n",
      "training loss: 30678546551226.688\n",
      "validation loss: 19740650314085.51\n",
      "epoch: 1140\n",
      "training loss: 30677846543321.95\n",
      "validation loss: 19738534519523.793\n",
      "epoch: 1141\n",
      "training loss: 30677147637389.598\n",
      "validation loss: 19736422622209.27\n",
      "epoch: 1142\n",
      "training loss: 30676449831175.562\n",
      "validation loss: 19734314614113.965\n",
      "epoch: 1143\n",
      "training loss: 30675753122430.633\n",
      "validation loss: 19732210487226.85\n",
      "epoch: 1144\n",
      "training loss: 30675057508910.34\n",
      "validation loss: 19730110233553.77\n",
      "epoch: 1145\n",
      "training loss: 30674362988374.98\n",
      "validation loss: 19728013845117.414\n",
      "epoch: 1146\n",
      "training loss: 30673669558589.656\n",
      "validation loss: 19725921313957.277\n",
      "epoch: 1147\n",
      "training loss: 30672977217324.188\n",
      "validation loss: 19723832632129.656\n",
      "epoch: 1148\n",
      "training loss: 30672285962353.145\n",
      "validation loss: 19721747791707.56\n",
      "epoch: 1149\n",
      "training loss: 30671595791455.848\n",
      "validation loss: 19719666784780.71\n",
      "epoch: 1150\n",
      "training loss: 30670906702416.31\n",
      "validation loss: 19717589603455.496\n",
      "epoch: 1151\n",
      "training loss: 30670218693023.28\n",
      "validation loss: 19715516239854.938\n",
      "epoch: 1152\n",
      "training loss: 30669531761070.2\n",
      "validation loss: 19713446686118.645\n",
      "epoch: 1153\n",
      "training loss: 30668845904355.207\n",
      "validation loss: 19711380934402.793\n",
      "epoch: 1154\n",
      "training loss: 30668161120681.11\n",
      "validation loss: 19709318976880.086\n",
      "epoch: 1155\n",
      "training loss: 30667477407855.41\n",
      "validation loss: 19707260805739.715\n",
      "epoch: 1156\n",
      "training loss: 30666794763690.25\n",
      "validation loss: 19705206413187.312\n",
      "epoch: 1157\n",
      "training loss: 30666113186002.44\n",
      "validation loss: 19703155791444.96\n",
      "epoch: 1158\n",
      "training loss: 30665432672613.42\n",
      "validation loss: 19701108932751.09\n",
      "epoch: 1159\n",
      "training loss: 30664753221349.258\n",
      "validation loss: 19699065829360.484\n",
      "epoch: 1160\n",
      "training loss: 30664074830040.676\n",
      "validation loss: 19697026473544.258\n",
      "epoch: 1161\n",
      "training loss: 30663397496522.96\n",
      "validation loss: 19694990857589.8\n",
      "epoch: 1162\n",
      "training loss: 30662721218636.04\n",
      "validation loss: 19692958973800.73\n",
      "epoch: 1163\n",
      "training loss: 30662045994224.414\n",
      "validation loss: 19690930814496.883\n",
      "epoch: 1164\n",
      "training loss: 30661371821137.17\n",
      "validation loss: 19688906372014.273\n",
      "epoch: 1165\n",
      "training loss: 30660698697227.973\n",
      "validation loss: 19686885638705.047\n",
      "epoch: 1166\n",
      "training loss: 30660026620355.043\n",
      "validation loss: 19684868606937.45\n",
      "epoch: 1167\n",
      "training loss: 30659355588381.16\n",
      "validation loss: 19682855269095.83\n",
      "epoch: 1168\n",
      "training loss: 30658685599173.65\n",
      "validation loss: 19680845617580.516\n",
      "epoch: 1169\n",
      "training loss: 30658016650604.344\n",
      "validation loss: 19678839644807.9\n",
      "epoch: 1170\n",
      "training loss: 30657348740549.652\n",
      "validation loss: 19676837343210.29\n",
      "epoch: 1171\n",
      "training loss: 30656681866890.438\n",
      "validation loss: 19674838705235.953\n",
      "epoch: 1172\n",
      "training loss: 30656016027512.113\n",
      "validation loss: 19672843723349.055\n",
      "epoch: 1173\n",
      "training loss: 30655351220304.57\n",
      "validation loss: 19670852390029.62\n",
      "epoch: 1174\n",
      "training loss: 30654687443162.176\n",
      "validation loss: 19668864697773.492\n",
      "epoch: 1175\n",
      "training loss: 30654024693983.785\n",
      "validation loss: 19666880639092.348\n",
      "epoch: 1176\n",
      "training loss: 30653362970672.72\n",
      "validation loss: 19664900206513.58\n",
      "epoch: 1177\n",
      "training loss: 30652702271136.746\n",
      "validation loss: 19662923392580.37\n",
      "epoch: 1178\n",
      "training loss: 30652042593288.09\n",
      "validation loss: 19660950189851.54\n",
      "epoch: 1179\n",
      "training loss: 30651383935043.414\n",
      "validation loss: 19658980590901.613\n",
      "epoch: 1180\n",
      "training loss: 30650726294323.793\n",
      "validation loss: 19657014588320.72\n",
      "epoch: 1181\n",
      "training loss: 30650069669054.75\n",
      "validation loss: 19655052174714.59\n",
      "epoch: 1182\n",
      "training loss: 30649414057166.17\n",
      "validation loss: 19653093342704.52\n",
      "epoch: 1183\n",
      "training loss: 30648759456592.395\n",
      "validation loss: 19651138084927.348\n",
      "epoch: 1184\n",
      "training loss: 30648105865272.105\n",
      "validation loss: 19649186394035.38\n",
      "epoch: 1185\n",
      "training loss: 30647453281148.395\n",
      "validation loss: 19647238262696.418\n",
      "epoch: 1186\n",
      "training loss: 30646801702168.71\n",
      "validation loss: 19645293683593.676\n",
      "epoch: 1187\n",
      "training loss: 30646151126284.87\n",
      "validation loss: 19643352649425.76\n",
      "epoch: 1188\n",
      "training loss: 30645501551453.047\n",
      "validation loss: 19641415152906.668\n",
      "epoch: 1189\n",
      "training loss: 30644852975633.742\n",
      "validation loss: 19639481186765.684\n",
      "epoch: 1190\n",
      "training loss: 30644205396791.81\n",
      "validation loss: 19637550743747.42\n",
      "epoch: 1191\n",
      "training loss: 30643558812896.402\n",
      "validation loss: 19635623816611.76\n",
      "epoch: 1192\n",
      "training loss: 30642913221921.027\n",
      "validation loss: 19633700398133.824\n",
      "epoch: 1193\n",
      "training loss: 30642268621843.46\n",
      "validation loss: 19631780481103.918\n",
      "epoch: 1194\n",
      "training loss: 30641625010645.78\n",
      "validation loss: 19629864058327.516\n",
      "epoch: 1195\n",
      "training loss: 30640982386314.38\n",
      "validation loss: 19627951122625.234\n",
      "epoch: 1196\n",
      "training loss: 30640340746839.895\n",
      "validation loss: 19626041666832.805\n",
      "epoch: 1197\n",
      "training loss: 30639700090217.266\n",
      "validation loss: 19624135683801.023\n",
      "epoch: 1198\n",
      "training loss: 30639060414445.645\n",
      "validation loss: 19622233166395.707\n",
      "epoch: 1199\n",
      "training loss: 30638421717528.492\n",
      "validation loss: 19620334107497.715\n",
      "epoch: 1200\n",
      "training loss: 30637783997473.465\n",
      "validation loss: 19618438500002.863\n",
      "epoch: 1201\n",
      "training loss: 30637147252292.473\n",
      "validation loss: 19616546336821.902\n",
      "epoch: 1202\n",
      "training loss: 30636511480001.65\n",
      "validation loss: 19614657610880.51\n",
      "epoch: 1203\n",
      "training loss: 30635876678621.332\n",
      "validation loss: 19612772315119.254\n",
      "epoch: 1204\n",
      "training loss: 30635242846176.066\n",
      "validation loss: 19610890442493.53\n",
      "epoch: 1205\n",
      "training loss: 30634609980694.61\n",
      "validation loss: 19609011985973.55\n",
      "epoch: 1206\n",
      "training loss: 30633978080209.887\n",
      "validation loss: 19607136938544.34\n",
      "epoch: 1207\n",
      "training loss: 30633347142759.01\n",
      "validation loss: 19605265293205.65\n",
      "epoch: 1208\n",
      "training loss: 30632717166383.273\n",
      "validation loss: 19603397042971.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1209\n",
      "training loss: 30632088149128.1\n",
      "validation loss: 19601532180872.457\n",
      "epoch: 1210\n",
      "training loss: 30631460089043.086\n",
      "validation loss: 19599670699950.953\n",
      "epoch: 1211\n",
      "training loss: 30630832984181.97\n",
      "validation loss: 19597812593265.926\n",
      "epoch: 1212\n",
      "training loss: 30630206832602.613\n",
      "validation loss: 19595957853890.445\n",
      "epoch: 1213\n",
      "training loss: 30629581632367.027\n",
      "validation loss: 19594106474912.13\n",
      "epoch: 1214\n",
      "training loss: 30628957381541.312\n",
      "validation loss: 19592258449433.145\n",
      "epoch: 1215\n",
      "training loss: 30628334078195.68\n",
      "validation loss: 19590413770570.152\n",
      "epoch: 1216\n",
      "training loss: 30627711720404.453\n",
      "validation loss: 19588572431454.305\n",
      "epoch: 1217\n",
      "training loss: 30627090306246.035\n",
      "validation loss: 19586734425231.17\n",
      "epoch: 1218\n",
      "training loss: 30626469833802.918\n",
      "validation loss: 19584899745060.75\n",
      "epoch: 1219\n",
      "training loss: 30625850301161.645\n",
      "validation loss: 19583068384117.434\n",
      "epoch: 1220\n",
      "training loss: 30625231706412.848\n",
      "validation loss: 19581240335589.94\n",
      "epoch: 1221\n",
      "training loss: 30624614047651.207\n",
      "validation loss: 19579415592681.32\n",
      "epoch: 1222\n",
      "training loss: 30623997322975.434\n",
      "validation loss: 19577594148608.906\n",
      "epoch: 1223\n",
      "training loss: 30623381530488.297\n",
      "validation loss: 19575775996604.3\n",
      "epoch: 1224\n",
      "training loss: 30622766668296.58\n",
      "validation loss: 19573961129913.33\n",
      "epoch: 1225\n",
      "training loss: 30622152734511.098\n",
      "validation loss: 19572149541796.02\n",
      "epoch: 1226\n",
      "training loss: 30621539727246.66\n",
      "validation loss: 19570341225526.566\n",
      "epoch: 1227\n",
      "training loss: 30620927644622.09\n",
      "validation loss: 19568536174393.297\n",
      "epoch: 1228\n",
      "training loss: 30620316484760.215\n",
      "validation loss: 19566734381698.645\n",
      "epoch: 1229\n",
      "training loss: 30619706245787.83\n",
      "validation loss: 19564935840759.14\n",
      "epoch: 1230\n",
      "training loss: 30619096925835.715\n",
      "validation loss: 19563140544905.348\n",
      "epoch: 1231\n",
      "training loss: 30618488523038.613\n",
      "validation loss: 19561348487481.85\n",
      "epoch: 1232\n",
      "training loss: 30617881035535.246\n",
      "validation loss: 19559559661847.22\n",
      "epoch: 1233\n",
      "training loss: 30617274461468.26\n",
      "validation loss: 19557774061373.99\n",
      "epoch: 1234\n",
      "training loss: 30616668798984.26\n",
      "validation loss: 19555991679448.613\n",
      "epoch: 1235\n",
      "training loss: 30616064046233.793\n",
      "validation loss: 19554212509471.46\n",
      "epoch: 1236\n",
      "training loss: 30615460201371.312\n",
      "validation loss: 19552436544856.76\n",
      "epoch: 1237\n",
      "training loss: 30614857262555.2\n",
      "validation loss: 19550663779032.582\n",
      "epoch: 1238\n",
      "training loss: 30614255227947.75\n",
      "validation loss: 19548894205440.8\n",
      "epoch: 1239\n",
      "training loss: 30613654095715.152\n",
      "validation loss: 19547127817537.082\n",
      "epoch: 1240\n",
      "training loss: 30613053864027.49\n",
      "validation loss: 19545364608790.836\n",
      "epoch: 1241\n",
      "training loss: 30612454531058.734\n",
      "validation loss: 19543604572685.203\n",
      "epoch: 1242\n",
      "training loss: 30611856094986.727\n",
      "validation loss: 19541847702717.016\n",
      "epoch: 1243\n",
      "training loss: 30611258553993.188\n",
      "validation loss: 19540093992396.773\n",
      "epoch: 1244\n",
      "training loss: 30610661906263.68\n",
      "validation loss: 19538343435248.59\n",
      "epoch: 1245\n",
      "training loss: 30610066149987.625\n",
      "validation loss: 19536596024810.215\n",
      "epoch: 1246\n",
      "training loss: 30609471283358.293\n",
      "validation loss: 19534851754632.965\n",
      "epoch: 1247\n",
      "training loss: 30608877304572.79\n",
      "validation loss: 19533110618281.695\n",
      "epoch: 1248\n",
      "training loss: 30608284211832.035\n",
      "validation loss: 19531372609334.793\n",
      "epoch: 1249\n",
      "training loss: 30607692003340.773\n",
      "validation loss: 19529637721384.137\n",
      "epoch: 1250\n",
      "training loss: 30607100677307.56\n",
      "validation loss: 19527905948035.05\n",
      "epoch: 1251\n",
      "training loss: 30606510231944.754\n",
      "validation loss: 19526177282906.32\n",
      "epoch: 1252\n",
      "training loss: 30605920665468.5\n",
      "validation loss: 19524451719630.105\n",
      "epoch: 1253\n",
      "training loss: 30605331976098.742\n",
      "validation loss: 19522729251851.97\n",
      "epoch: 1254\n",
      "training loss: 30604744162059.19\n",
      "validation loss: 19521009873230.79\n",
      "epoch: 1255\n",
      "training loss: 30604157221577.324\n",
      "validation loss: 19519293577438.82\n",
      "epoch: 1256\n",
      "training loss: 30603571152884.39\n",
      "validation loss: 19517580358161.54\n",
      "epoch: 1257\n",
      "training loss: 30602985954215.387\n",
      "validation loss: 19515870209097.742\n",
      "epoch: 1258\n",
      "training loss: 30602401623809.05\n",
      "validation loss: 19514163123959.42\n",
      "epoch: 1259\n",
      "training loss: 30601818159907.863\n",
      "validation loss: 19512459096471.805\n",
      "epoch: 1260\n",
      "training loss: 30601235560758.04\n",
      "validation loss: 19510758120373.28\n",
      "epoch: 1261\n",
      "training loss: 30600653824609.508\n",
      "validation loss: 19509060189415.395\n",
      "epoch: 1262\n",
      "training loss: 30600072949715.902\n",
      "validation loss: 19507365297362.816\n",
      "epoch: 1263\n",
      "training loss: 30599492934334.57\n",
      "validation loss: 19505673437993.305\n",
      "epoch: 1264\n",
      "training loss: 30598913776726.574\n",
      "validation loss: 19503984605097.688\n",
      "epoch: 1265\n",
      "training loss: 30598335475156.625\n",
      "validation loss: 19502298792479.832\n",
      "epoch: 1266\n",
      "training loss: 30597758027893.164\n",
      "validation loss: 19500615993956.62\n",
      "epoch: 1267\n",
      "training loss: 30597181433208.266\n",
      "validation loss: 19498936203357.92\n",
      "epoch: 1268\n",
      "training loss: 30596605689377.684\n",
      "validation loss: 19497259414526.55\n",
      "epoch: 1269\n",
      "training loss: 30596030794680.83\n",
      "validation loss: 19495585621318.258\n",
      "epoch: 1270\n",
      "training loss: 30595456747400.785\n",
      "validation loss: 19493914817601.695\n",
      "epoch: 1271\n",
      "training loss: 30594883545824.23\n",
      "validation loss: 19492246997258.38\n",
      "epoch: 1272\n",
      "training loss: 30594311188241.52\n",
      "validation loss: 19490582154182.68\n",
      "epoch: 1273\n",
      "training loss: 30593739672946.613\n",
      "validation loss: 19488920282281.79\n",
      "epoch: 1274\n",
      "training loss: 30593168998237.1\n",
      "validation loss: 19487261375475.68\n",
      "epoch: 1275\n",
      "training loss: 30592599162414.18\n",
      "validation loss: 19485605427697.11\n",
      "epoch: 1276\n",
      "training loss: 30592030163782.633\n",
      "validation loss: 19483952432891.55\n",
      "epoch: 1277\n",
      "training loss: 30591462000650.875\n",
      "validation loss: 19482302385017.19\n",
      "epoch: 1278\n",
      "training loss: 30590894671330.875\n",
      "validation loss: 19480655278044.926\n",
      "epoch: 1279\n",
      "training loss: 30590328174138.207\n",
      "validation loss: 19479011105958.27\n",
      "epoch: 1280\n",
      "training loss: 30589762507391.992\n",
      "validation loss: 19477369862753.39\n",
      "epoch: 1281\n",
      "training loss: 30589197669414.95\n",
      "validation loss: 19475731542439.055\n",
      "epoch: 1282\n",
      "training loss: 30588633658533.312\n",
      "validation loss: 19474096139036.6\n",
      "epoch: 1283\n",
      "training loss: 30588070473076.91\n",
      "validation loss: 19472463646579.92\n",
      "epoch: 1284\n",
      "training loss: 30587508111379.074\n",
      "validation loss: 19470834059115.418\n",
      "epoch: 1285\n",
      "training loss: 30586946571776.703\n",
      "validation loss: 19469207370702.008\n",
      "epoch: 1286\n",
      "training loss: 30586385852610.184\n",
      "validation loss: 19467583575411.062\n",
      "epoch: 1287\n",
      "training loss: 30585825952223.473\n",
      "validation loss: 19465962667326.406\n",
      "epoch: 1288\n",
      "training loss: 30585266868963.992\n",
      "validation loss: 19464344640544.27\n",
      "epoch: 1289\n",
      "training loss: 30584708601182.695\n",
      "validation loss: 19462729489173.277\n",
      "epoch: 1290\n",
      "training loss: 30584151147234.02\n",
      "validation loss: 19461117207334.43\n",
      "epoch: 1291\n",
      "training loss: 30583594505475.9\n",
      "validation loss: 19459507789161.05\n",
      "epoch: 1292\n",
      "training loss: 30583038674269.746\n",
      "validation loss: 19457901228798.77\n",
      "epoch: 1293\n",
      "training loss: 30582483651980.45\n",
      "validation loss: 19456297520405.504\n",
      "epoch: 1294\n",
      "training loss: 30581929436976.355\n",
      "validation loss: 19454696658151.445\n",
      "epoch: 1295\n",
      "training loss: 30581376027629.29\n",
      "validation loss: 19453098636219.016\n",
      "epoch: 1296\n",
      "training loss: 30580823422314.504\n",
      "validation loss: 19451503448802.832\n",
      "epoch: 1297\n",
      "training loss: 30580271619410.707\n",
      "validation loss: 19449911090109.684\n",
      "epoch: 1298\n",
      "training loss: 30579720617300.062\n",
      "validation loss: 19448321554358.543\n",
      "epoch: 1299\n",
      "training loss: 30579170414368.137\n",
      "validation loss: 19446734835780.484\n",
      "epoch: 1300\n",
      "training loss: 30578621009003.918\n",
      "validation loss: 19445150928618.703\n",
      "epoch: 1301\n",
      "training loss: 30578072399599.844\n",
      "validation loss: 19443569827128.457\n",
      "epoch: 1302\n",
      "training loss: 30577524584551.72\n",
      "validation loss: 19441991525577.08\n",
      "epoch: 1303\n",
      "training loss: 30576977562258.77\n",
      "validation loss: 19440416018243.902\n",
      "epoch: 1304\n",
      "training loss: 30576431331123.62\n",
      "validation loss: 19438843299420.29\n",
      "epoch: 1305\n",
      "training loss: 30575885889552.258\n",
      "validation loss: 19437273363409.54\n",
      "epoch: 1306\n",
      "training loss: 30575341235954.07\n",
      "validation loss: 19435706204526.938\n",
      "epoch: 1307\n",
      "training loss: 30574797368741.816\n",
      "validation loss: 19434141817099.68\n",
      "epoch: 1308\n",
      "training loss: 30574254286331.598\n",
      "validation loss: 19432580195466.844\n",
      "epoch: 1309\n",
      "training loss: 30573711987142.902\n",
      "validation loss: 19431021333979.418\n",
      "epoch: 1310\n",
      "training loss: 30573170469598.543\n",
      "validation loss: 19429465227000.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1311\n",
      "training loss: 30572629732124.69\n",
      "validation loss: 19427911868903.883\n",
      "epoch: 1312\n",
      "training loss: 30572089773150.85\n",
      "validation loss: 19426361254076.844\n",
      "epoch: 1313\n",
      "training loss: 30571550591109.844\n",
      "validation loss: 19424813376917.316\n",
      "epoch: 1314\n",
      "training loss: 30571012184437.836\n",
      "validation loss: 19423268231835.26\n",
      "epoch: 1315\n",
      "training loss: 30570474551574.27\n",
      "validation loss: 19421725813252.38\n",
      "epoch: 1316\n",
      "training loss: 30569937690961.938\n",
      "validation loss: 19420186115602.027\n",
      "epoch: 1317\n",
      "training loss: 30569401601046.902\n",
      "validation loss: 19418649133329.27\n",
      "epoch: 1318\n",
      "training loss: 30568866280278.53\n",
      "validation loss: 19417114860890.824\n",
      "epoch: 1319\n",
      "training loss: 30568331727109.465\n",
      "validation loss: 19415583292754.996\n",
      "epoch: 1320\n",
      "training loss: 30567797939995.656\n",
      "validation loss: 19414054423401.74\n",
      "epoch: 1321\n",
      "training loss: 30567264917396.29\n",
      "validation loss: 19412528247322.535\n",
      "epoch: 1322\n",
      "training loss: 30566732657773.832\n",
      "validation loss: 19411004759020.45\n",
      "epoch: 1323\n",
      "training loss: 30566201159594.016\n",
      "validation loss: 19409483953010.062\n",
      "epoch: 1324\n",
      "training loss: 30565670421325.812\n",
      "validation loss: 19407965823817.434\n",
      "epoch: 1325\n",
      "training loss: 30565140441441.43\n",
      "validation loss: 19406450365980.133\n",
      "epoch: 1326\n",
      "training loss: 30564611218416.34\n",
      "validation loss: 19404937574047.17\n",
      "epoch: 1327\n",
      "training loss: 30564082750729.215\n",
      "validation loss: 19403427442578.98\n",
      "epoch: 1328\n",
      "training loss: 30563555036861.977\n",
      "validation loss: 19401919966147.406\n",
      "epoch: 1329\n",
      "training loss: 30563028075299.74\n",
      "validation loss: 19400415139335.664\n",
      "epoch: 1330\n",
      "training loss: 30562501864530.836\n",
      "validation loss: 19398912956738.324\n",
      "epoch: 1331\n",
      "training loss: 30561976403046.816\n",
      "validation loss: 19397413412961.31\n",
      "epoch: 1332\n",
      "training loss: 30561451689342.4\n",
      "validation loss: 19395916502621.816\n",
      "epoch: 1333\n",
      "training loss: 30560927721915.508\n",
      "validation loss: 19394422220348.348\n",
      "epoch: 1334\n",
      "training loss: 30560404499267.254\n",
      "validation loss: 19392930560780.656\n",
      "epoch: 1335\n",
      "training loss: 30559882019901.902\n",
      "validation loss: 19391441518569.74\n",
      "epoch: 1336\n",
      "training loss: 30559360282326.895\n",
      "validation loss: 19389955088377.793\n",
      "epoch: 1337\n",
      "training loss: 30558839285052.855\n",
      "validation loss: 19388471264878.207\n",
      "epoch: 1338\n",
      "training loss: 30558319026593.54\n",
      "validation loss: 19386990042755.535\n",
      "epoch: 1339\n",
      "training loss: 30557799505465.848\n",
      "validation loss: 19385511416705.48\n",
      "epoch: 1340\n",
      "training loss: 30557280720189.84\n",
      "validation loss: 19384035381434.85\n",
      "epoch: 1341\n",
      "training loss: 30556762669288.703\n",
      "validation loss: 19382561931661.54\n",
      "epoch: 1342\n",
      "training loss: 30556245351288.742\n",
      "validation loss: 19381091062114.52\n",
      "epoch: 1343\n",
      "training loss: 30555728764719.402\n",
      "validation loss: 19379622767533.836\n",
      "epoch: 1344\n",
      "training loss: 30555212908113.215\n",
      "validation loss: 19378157042670.49\n",
      "epoch: 1345\n",
      "training loss: 30554697780005.855\n",
      "validation loss: 19376693882286.54\n",
      "epoch: 1346\n",
      "training loss: 30554183378936.07\n",
      "validation loss: 19375233281155.004\n",
      "epoch: 1347\n",
      "training loss: 30553669703445.707\n",
      "validation loss: 19373775234059.844\n",
      "epoch: 1348\n",
      "training loss: 30553156752079.71\n",
      "validation loss: 19372319735795.965\n",
      "epoch: 1349\n",
      "training loss: 30552644523386.11\n",
      "validation loss: 19370866781169.16\n",
      "epoch: 1350\n",
      "training loss: 30552133015915.98\n",
      "validation loss: 19369416364996.113\n",
      "epoch: 1351\n",
      "training loss: 30551622228223.504\n",
      "validation loss: 19367968482104.387\n",
      "epoch: 1352\n",
      "training loss: 30551112158865.89\n",
      "validation loss: 19366523127332.348\n",
      "epoch: 1353\n",
      "training loss: 30550602806403.434\n",
      "validation loss: 19365080295529.19\n",
      "epoch: 1354\n",
      "training loss: 30550094169399.45\n",
      "validation loss: 19363639981554.91\n",
      "epoch: 1355\n",
      "training loss: 30549586246420.31\n",
      "validation loss: 19362202180280.254\n",
      "epoch: 1356\n",
      "training loss: 30549079036035.414\n",
      "validation loss: 19360766886586.715\n",
      "epoch: 1357\n",
      "training loss: 30548572536817.215\n",
      "validation loss: 19359334095366.523\n",
      "epoch: 1358\n",
      "training loss: 30548066747341.15\n",
      "validation loss: 19357903801522.59\n",
      "epoch: 1359\n",
      "training loss: 30547561666185.688\n",
      "validation loss: 19356475999968.52\n",
      "epoch: 1360\n",
      "training loss: 30547057291932.33\n",
      "validation loss: 19355050685628.562\n",
      "epoch: 1361\n",
      "training loss: 30546553623165.543\n",
      "validation loss: 19353627853437.6\n",
      "epoch: 1362\n",
      "training loss: 30546050658472.805\n",
      "validation loss: 19352207498341.11\n",
      "epoch: 1363\n",
      "training loss: 30545548396444.598\n",
      "validation loss: 19350789615295.18\n",
      "epoch: 1364\n",
      "training loss: 30545046835674.375\n",
      "validation loss: 19349374199266.445\n",
      "epoch: 1365\n",
      "training loss: 30544545974758.56\n",
      "validation loss: 19347961245232.086\n",
      "epoch: 1366\n",
      "training loss: 30544045812296.55\n",
      "validation loss: 19346550748179.81\n",
      "epoch: 1367\n",
      "training loss: 30543546346890.73\n",
      "validation loss: 19345142703107.8\n",
      "epoch: 1368\n",
      "training loss: 30543047577146.414\n",
      "validation loss: 19343737105024.746\n",
      "epoch: 1369\n",
      "training loss: 30542549501671.88\n",
      "validation loss: 19342333948949.76\n",
      "epoch: 1370\n",
      "training loss: 30542052119078.344\n",
      "validation loss: 19340933229912.395\n",
      "epoch: 1371\n",
      "training loss: 30541555427979.973\n",
      "validation loss: 19339534942952.62\n",
      "epoch: 1372\n",
      "training loss: 30541059426993.875\n",
      "validation loss: 19338139083120.777\n",
      "epoch: 1373\n",
      "training loss: 30540564114740.043\n",
      "validation loss: 19336745645477.57\n",
      "epoch: 1374\n",
      "training loss: 30540069489841.445\n",
      "validation loss: 19335354625094.074\n",
      "epoch: 1375\n",
      "training loss: 30539575550923.92\n",
      "validation loss: 19333966017051.64\n",
      "epoch: 1376\n",
      "training loss: 30539082296616.242\n",
      "validation loss: 19332579816441.94\n",
      "epoch: 1377\n",
      "training loss: 30538589725550.07\n",
      "validation loss: 19331196018366.926\n",
      "epoch: 1378\n",
      "training loss: 30538097836359.965\n",
      "validation loss: 19329814617938.793\n",
      "epoch: 1379\n",
      "training loss: 30537606627683.383\n",
      "validation loss: 19328435610279.977\n",
      "epoch: 1380\n",
      "training loss: 30537116098160.652\n",
      "validation loss: 19327058990523.125\n",
      "epoch: 1381\n",
      "training loss: 30536626246434.984\n",
      "validation loss: 19325684753811.066\n",
      "epoch: 1382\n",
      "training loss: 30536137071152.46\n",
      "validation loss: 19324312895296.797\n",
      "epoch: 1383\n",
      "training loss: 30535648570962.023\n",
      "validation loss: 19322943410143.473\n",
      "epoch: 1384\n",
      "training loss: 30535160744515.477\n",
      "validation loss: 19321576293524.35\n",
      "epoch: 1385\n",
      "training loss: 30534673590467.492\n",
      "validation loss: 19320211540622.81\n",
      "epoch: 1386\n",
      "training loss: 30534187107475.547\n",
      "validation loss: 19318849146632.305\n",
      "epoch: 1387\n",
      "training loss: 30533701294200.01\n",
      "validation loss: 19317489106756.363\n",
      "epoch: 1388\n",
      "training loss: 30533216149304.04\n",
      "validation loss: 19316131416208.516\n",
      "epoch: 1389\n",
      "training loss: 30532731671453.65\n",
      "validation loss: 19314776070212.34\n",
      "epoch: 1390\n",
      "training loss: 30532247859317.66\n",
      "validation loss: 19313423064001.418\n",
      "epoch: 1391\n",
      "training loss: 30531764711567.72\n",
      "validation loss: 19312072392819.273\n",
      "epoch: 1392\n",
      "training loss: 30531282226878.285\n",
      "validation loss: 19310724051919.41\n",
      "epoch: 1393\n",
      "training loss: 30530800403926.605\n",
      "validation loss: 19309378036565.258\n",
      "epoch: 1394\n",
      "training loss: 30530319241392.74\n",
      "validation loss: 19308034342030.152\n",
      "epoch: 1395\n",
      "training loss: 30529838737959.53\n",
      "validation loss: 19306692963597.324\n",
      "epoch: 1396\n",
      "training loss: 30529358892312.617\n",
      "validation loss: 19305353896559.883\n",
      "epoch: 1397\n",
      "training loss: 30528879703140.402\n",
      "validation loss: 19304017136220.773\n",
      "epoch: 1398\n",
      "training loss: 30528401169134.086\n",
      "validation loss: 19302682677892.773\n",
      "epoch: 1399\n",
      "training loss: 30527923288987.61\n",
      "validation loss: 19301350516898.453\n",
      "epoch: 1400\n",
      "training loss: 30527446061397.707\n",
      "validation loss: 19300020648570.203\n",
      "epoch: 1401\n",
      "training loss: 30526969485063.836\n",
      "validation loss: 19298693068250.152\n",
      "epoch: 1402\n",
      "training loss: 30526493558688.223\n",
      "validation loss: 19297367771290.164\n",
      "epoch: 1403\n",
      "training loss: 30526018280975.836\n",
      "validation loss: 19296044753051.863\n",
      "epoch: 1404\n",
      "training loss: 30525543650634.383\n",
      "validation loss: 19294724008906.543\n",
      "epoch: 1405\n",
      "training loss: 30525069666374.31\n",
      "validation loss: 19293405534235.215\n",
      "epoch: 1406\n",
      "training loss: 30524596326908.777\n",
      "validation loss: 19292089324428.52\n",
      "epoch: 1407\n",
      "training loss: 30524123630953.664\n",
      "validation loss: 19290775374886.75\n",
      "epoch: 1408\n",
      "training loss: 30523651577227.59\n",
      "validation loss: 19289463681019.812\n",
      "epoch: 1409\n",
      "training loss: 30523180164451.848\n",
      "validation loss: 19288154238247.24\n",
      "epoch: 1410\n",
      "training loss: 30522709391350.48\n",
      "validation loss: 19286847041998.13\n",
      "epoch: 1411\n",
      "training loss: 30522239256650.17\n",
      "validation loss: 19285542087711.133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1412\n",
      "training loss: 30521769759080.344\n",
      "validation loss: 19284239370834.453\n",
      "epoch: 1413\n",
      "training loss: 30521300897373.082\n",
      "validation loss: 19282938886825.797\n",
      "epoch: 1414\n",
      "training loss: 30520832670263.17\n",
      "validation loss: 19281640631152.395\n",
      "epoch: 1415\n",
      "training loss: 30520365076488.035\n",
      "validation loss: 19280344599290.934\n",
      "epoch: 1416\n",
      "training loss: 30519898114787.82\n",
      "validation loss: 19279050786727.582\n",
      "epoch: 1417\n",
      "training loss: 30519431783905.28\n",
      "validation loss: 19277759188957.92\n",
      "epoch: 1418\n",
      "training loss: 30518966082585.86\n",
      "validation loss: 19276469801486.965\n",
      "epoch: 1419\n",
      "training loss: 30518501009577.668\n",
      "validation loss: 19275182619829.152\n",
      "epoch: 1420\n",
      "training loss: 30518036563631.402\n",
      "validation loss: 19273897639508.25\n",
      "epoch: 1421\n",
      "training loss: 30517572743500.477\n",
      "validation loss: 19272614856057.42\n",
      "epoch: 1422\n",
      "training loss: 30517109547940.883\n",
      "validation loss: 19271334265019.16\n",
      "epoch: 1423\n",
      "training loss: 30516646975711.258\n",
      "validation loss: 19270055861945.27\n",
      "epoch: 1424\n",
      "training loss: 30516185025572.883\n",
      "validation loss: 19268779642396.87\n",
      "epoch: 1425\n",
      "training loss: 30515723696289.633\n",
      "validation loss: 19267505601944.363\n",
      "epoch: 1426\n",
      "training loss: 30515262986628.004\n",
      "validation loss: 19266233736167.395\n",
      "epoch: 1427\n",
      "training loss: 30514802895357.09\n",
      "validation loss: 19264964040654.86\n",
      "epoch: 1428\n",
      "training loss: 30514343421248.61\n",
      "validation loss: 19263696511004.87\n",
      "epoch: 1429\n",
      "training loss: 30513884563076.848\n",
      "validation loss: 19262431142824.75\n",
      "epoch: 1430\n",
      "training loss: 30513426319618.707\n",
      "validation loss: 19261167931731.0\n",
      "epoch: 1431\n",
      "training loss: 30512968689653.66\n",
      "validation loss: 19259906873349.285\n",
      "epoch: 1432\n",
      "training loss: 30512511671963.742\n",
      "validation loss: 19258647963314.41\n",
      "epoch: 1433\n",
      "training loss: 30512055265333.613\n",
      "validation loss: 19257391197270.316\n",
      "epoch: 1434\n",
      "training loss: 30511599468550.453\n",
      "validation loss: 19256136570870.035\n",
      "epoch: 1435\n",
      "training loss: 30511144280404.008\n",
      "validation loss: 19254884079775.684\n",
      "epoch: 1436\n",
      "training loss: 30510689699686.61\n",
      "validation loss: 19253633719658.438\n",
      "epoch: 1437\n",
      "training loss: 30510235725193.113\n",
      "validation loss: 19252385486198.56\n",
      "epoch: 1438\n",
      "training loss: 30509782355720.94\n",
      "validation loss: 19251139375085.3\n",
      "epoch: 1439\n",
      "training loss: 30509329590070.047\n",
      "validation loss: 19249895382016.938\n",
      "epoch: 1440\n",
      "training loss: 30508877427042.918\n",
      "validation loss: 19248653502700.727\n",
      "epoch: 1441\n",
      "training loss: 30508425865444.582\n",
      "validation loss: 19247413732852.91\n",
      "epoch: 1442\n",
      "training loss: 30507974904082.566\n",
      "validation loss: 19246176068198.66\n",
      "epoch: 1443\n",
      "training loss: 30507524541766.945\n",
      "validation loss: 19244940504472.09\n",
      "epoch: 1444\n",
      "training loss: 30507074777310.297\n",
      "validation loss: 19243707037416.254\n",
      "epoch: 1445\n",
      "training loss: 30506625609527.71\n",
      "validation loss: 19242475662783.06\n",
      "epoch: 1446\n",
      "training loss: 30506177037236.766\n",
      "validation loss: 19241246376333.31\n",
      "epoch: 1447\n",
      "training loss: 30505729059257.555\n",
      "validation loss: 19240019173836.668\n",
      "epoch: 1448\n",
      "training loss: 30505281674412.65\n",
      "validation loss: 19238794051071.63\n",
      "epoch: 1449\n",
      "training loss: 30504834881527.117\n",
      "validation loss: 19237571003825.508\n",
      "epoch: 1450\n",
      "training loss: 30504388679428.52\n",
      "validation loss: 19236350027894.418\n",
      "epoch: 1451\n",
      "training loss: 30503943066946.848\n",
      "validation loss: 19235131119083.28\n",
      "epoch: 1452\n",
      "training loss: 30503498042914.64\n",
      "validation loss: 19233914273205.746\n",
      "epoch: 1453\n",
      "training loss: 30503053606166.816\n",
      "validation loss: 19232699486084.22\n",
      "epoch: 1454\n",
      "training loss: 30502609755540.832\n",
      "validation loss: 19231486753549.844\n",
      "epoch: 1455\n",
      "training loss: 30502166489876.535\n",
      "validation loss: 19230276071442.46\n",
      "epoch: 1456\n",
      "training loss: 30501723808016.28\n",
      "validation loss: 19229067435610.613\n",
      "epoch: 1457\n",
      "training loss: 30501281708804.812\n",
      "validation loss: 19227860841911.492\n",
      "epoch: 1458\n",
      "training loss: 30500840191089.348\n",
      "validation loss: 19226656286210.957\n",
      "epoch: 1459\n",
      "training loss: 30500399253719.535\n",
      "validation loss: 19225453764383.5\n",
      "epoch: 1460\n",
      "training loss: 30499958895547.46\n",
      "validation loss: 19224253272312.24\n",
      "epoch: 1461\n",
      "training loss: 30499519115427.605\n",
      "validation loss: 19223054805888.88\n",
      "epoch: 1462\n",
      "training loss: 30499079912216.89\n",
      "validation loss: 19221858361013.695\n",
      "epoch: 1463\n",
      "training loss: 30498641284774.645\n",
      "validation loss: 19220663933595.555\n",
      "epoch: 1464\n",
      "training loss: 30498203231962.617\n",
      "validation loss: 19219471519551.848\n",
      "epoch: 1465\n",
      "training loss: 30497765752644.934\n",
      "validation loss: 19218281114808.477\n",
      "epoch: 1466\n",
      "training loss: 30497328845688.152\n",
      "validation loss: 19217092715299.887\n",
      "epoch: 1467\n",
      "training loss: 30496892509961.19\n",
      "validation loss: 19215906316968.977\n",
      "epoch: 1468\n",
      "training loss: 30496456744335.383\n",
      "validation loss: 19214721915767.137\n",
      "epoch: 1469\n",
      "training loss: 30496021547684.42\n",
      "validation loss: 19213539507654.203\n",
      "epoch: 1470\n",
      "training loss: 30495586918884.4\n",
      "validation loss: 19212359088598.46\n",
      "epoch: 1471\n",
      "training loss: 30495152856813.754\n",
      "validation loss: 19211180654576.59\n",
      "epoch: 1472\n",
      "training loss: 30494719360353.32\n",
      "validation loss: 19210004201573.695\n",
      "epoch: 1473\n",
      "training loss: 30494286428386.277\n",
      "validation loss: 19208829725583.242\n",
      "epoch: 1474\n",
      "training loss: 30493854059798.17\n",
      "validation loss: 19207657222607.08\n",
      "epoch: 1475\n",
      "training loss: 30493422253476.89\n",
      "validation loss: 19206486688655.387\n",
      "epoch: 1476\n",
      "training loss: 30492991008312.688\n",
      "validation loss: 19205318119746.67\n",
      "epoch: 1477\n",
      "training loss: 30492560323198.13\n",
      "validation loss: 19204151511907.77\n",
      "epoch: 1478\n",
      "training loss: 30492130197028.137\n",
      "validation loss: 19202986861173.797\n",
      "epoch: 1479\n",
      "training loss: 30491700628699.98\n",
      "validation loss: 19201824163588.15\n",
      "epoch: 1480\n",
      "training loss: 30491271617113.23\n",
      "validation loss: 19200663415202.48\n",
      "epoch: 1481\n",
      "training loss: 30490843161169.79\n",
      "validation loss: 19199504612076.69\n",
      "epoch: 1482\n",
      "training loss: 30490415259773.87\n",
      "validation loss: 19198347750278.895\n",
      "epoch: 1483\n",
      "training loss: 30489987911832.016\n",
      "validation loss: 19197192825885.406\n",
      "epoch: 1484\n",
      "training loss: 30489561116253.062\n",
      "validation loss: 19196039834980.77\n",
      "epoch: 1485\n",
      "training loss: 30489134871948.156\n",
      "validation loss: 19194888773657.64\n",
      "epoch: 1486\n",
      "training loss: 30488709177830.734\n",
      "validation loss: 19193739638016.87\n",
      "epoch: 1487\n",
      "training loss: 30488284032816.535\n",
      "validation loss: 19192592424167.438\n",
      "epoch: 1488\n",
      "training loss: 30487859435823.566\n",
      "validation loss: 19191447128226.434\n",
      "epoch: 1489\n",
      "training loss: 30487435385772.14\n",
      "validation loss: 19190303746319.06\n",
      "epoch: 1490\n",
      "training loss: 30487011881584.84\n",
      "validation loss: 19189162274578.605\n",
      "epoch: 1491\n",
      "training loss: 30486588922186.52\n",
      "validation loss: 19188022709146.41\n",
      "epoch: 1492\n",
      "training loss: 30486166506504.293\n",
      "validation loss: 19186885046171.895\n",
      "epoch: 1493\n",
      "training loss: 30485744633467.566\n",
      "validation loss: 19185749281812.49\n",
      "epoch: 1494\n",
      "training loss: 30485323302007.973\n",
      "validation loss: 19184615412233.656\n",
      "epoch: 1495\n",
      "training loss: 30484902511059.41\n",
      "validation loss: 19183483433608.84\n",
      "epoch: 1496\n",
      "training loss: 30484482259558.027\n",
      "validation loss: 19182353342119.49\n",
      "epoch: 1497\n",
      "training loss: 30484062546442.22\n",
      "validation loss: 19181225133955.008\n",
      "epoch: 1498\n",
      "training loss: 30483643370652.613\n",
      "validation loss: 19180098805312.74\n",
      "epoch: 1499\n",
      "training loss: 30483224731132.09\n",
      "validation loss: 19178974352397.99\n",
      "epoch: 1500\n",
      "training loss: 30482806626825.723\n",
      "validation loss: 19177851771423.934\n",
      "epoch: 1501\n",
      "training loss: 30482389056680.855\n",
      "validation loss: 19176731058611.703\n",
      "epoch: 1502\n",
      "training loss: 30481972019647.004\n",
      "validation loss: 19175612210190.27\n",
      "epoch: 1503\n",
      "training loss: 30481555514675.957\n",
      "validation loss: 19174495222396.484\n",
      "epoch: 1504\n",
      "training loss: 30481139540721.652\n",
      "validation loss: 19173380091475.047\n",
      "epoch: 1505\n",
      "training loss: 30480724096740.28\n",
      "validation loss: 19172266813678.496\n",
      "epoch: 1506\n",
      "training loss: 30480309181690.223\n",
      "validation loss: 19171155385267.16\n",
      "epoch: 1507\n",
      "training loss: 30479894794532.035\n",
      "validation loss: 19170045802509.215\n",
      "epoch: 1508\n",
      "training loss: 30479480934228.484\n",
      "validation loss: 19168938061680.566\n",
      "epoch: 1509\n",
      "training loss: 30479067599744.53\n",
      "validation loss: 19167832159064.92\n",
      "epoch: 1510\n",
      "training loss: 30478654790047.305\n",
      "validation loss: 19166728090953.72\n",
      "epoch: 1511\n",
      "training loss: 30478242504106.113\n",
      "validation loss: 19165625853646.152\n",
      "epoch: 1512\n",
      "training loss: 30477830740892.45\n",
      "validation loss: 19164525443449.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1513\n",
      "training loss: 30477419499379.96\n",
      "validation loss: 19163426856677.184\n",
      "epoch: 1514\n",
      "training loss: 30477008778544.46\n",
      "validation loss: 19162330089652.67\n",
      "epoch: 1515\n",
      "training loss: 30476598577363.934\n",
      "validation loss: 19161235138705.504\n",
      "epoch: 1516\n",
      "training loss: 30476188894818.516\n",
      "validation loss: 19160142000173.293\n",
      "epoch: 1517\n",
      "training loss: 30475779729890.49\n",
      "validation loss: 19159050670401.28\n",
      "epoch: 1518\n",
      "training loss: 30475371081564.27\n",
      "validation loss: 19157961145742.324\n",
      "epoch: 1519\n",
      "training loss: 30474962948826.434\n",
      "validation loss: 19156873422556.883\n",
      "epoch: 1520\n",
      "training loss: 30474555330665.688\n",
      "validation loss: 19155787497213.008\n",
      "epoch: 1521\n",
      "training loss: 30474148226072.867\n",
      "validation loss: 19154703366086.316\n",
      "epoch: 1522\n",
      "training loss: 30473741634040.934\n",
      "validation loss: 19153621025559.99\n",
      "epoch: 1523\n",
      "training loss: 30473335553564.984\n",
      "validation loss: 19152540472024.742\n",
      "epoch: 1524\n",
      "training loss: 30472929983642.203\n",
      "validation loss: 19151461701878.812\n",
      "epoch: 1525\n",
      "training loss: 30472524923271.93\n",
      "validation loss: 19150384711527.94\n",
      "epoch: 1526\n",
      "training loss: 30472120371455.594\n",
      "validation loss: 19149309497385.383\n",
      "epoch: 1527\n",
      "training loss: 30471716327196.71\n",
      "validation loss: 19148236055871.83\n",
      "epoch: 1528\n",
      "training loss: 30471312789500.926\n",
      "validation loss: 19147164383415.47\n",
      "epoch: 1529\n",
      "training loss: 30470909757375.97\n",
      "validation loss: 19146094476451.914\n",
      "epoch: 1530\n",
      "training loss: 30470507229831.656\n",
      "validation loss: 19145026331424.223\n",
      "epoch: 1531\n",
      "training loss: 30470105205879.887\n",
      "validation loss: 19143959944782.84\n",
      "epoch: 1532\n",
      "training loss: 30469703684534.664\n",
      "validation loss: 19142895312985.625\n",
      "epoch: 1533\n",
      "training loss: 30469302664812.055\n",
      "validation loss: 19141832432497.82\n",
      "epoch: 1534\n",
      "training loss: 30468902145730.184\n",
      "validation loss: 19140771299792.035\n",
      "epoch: 1535\n",
      "training loss: 30468502126309.273\n",
      "validation loss: 19139711911348.22\n",
      "epoch: 1536\n",
      "training loss: 30468102605571.586\n",
      "validation loss: 19138654263653.652\n",
      "epoch: 1537\n",
      "training loss: 30467703582541.465\n",
      "validation loss: 19137598353202.953\n",
      "epoch: 1538\n",
      "training loss: 30467305056245.3\n",
      "validation loss: 19136544176498.027\n",
      "epoch: 1539\n",
      "training loss: 30466907025711.53\n",
      "validation loss: 19135491730048.082\n",
      "epoch: 1540\n",
      "training loss: 30466509489970.645\n",
      "validation loss: 19134441010369.582\n",
      "epoch: 1541\n",
      "training loss: 30466112448055.176\n",
      "validation loss: 19133392013986.27\n",
      "epoch: 1542\n",
      "training loss: 30465715898999.68\n",
      "validation loss: 19132344737429.11\n",
      "epoch: 1543\n",
      "training loss: 30465319841840.773\n",
      "validation loss: 19131299177236.3\n",
      "epoch: 1544\n",
      "training loss: 30464924275617.086\n",
      "validation loss: 19130255329953.258\n",
      "epoch: 1545\n",
      "training loss: 30464529199369.27\n",
      "validation loss: 19129213192132.59\n",
      "epoch: 1546\n",
      "training loss: 30464134612140.008\n",
      "validation loss: 19128172760334.07\n",
      "epoch: 1547\n",
      "training loss: 30463740512973.996\n",
      "validation loss: 19127134031124.676\n",
      "epoch: 1548\n",
      "training loss: 30463346900917.93\n",
      "validation loss: 19126097001078.496\n",
      "epoch: 1549\n",
      "training loss: 30462953775020.543\n",
      "validation loss: 19125061666776.773\n",
      "epoch: 1550\n",
      "training loss: 30462561134332.54\n",
      "validation loss: 19124028024807.875\n",
      "epoch: 1551\n",
      "training loss: 30462168977906.64\n",
      "validation loss: 19122996071767.258\n",
      "epoch: 1552\n",
      "training loss: 30461777304797.566\n",
      "validation loss: 19121965804257.484\n",
      "epoch: 1553\n",
      "training loss: 30461386114062.01\n",
      "validation loss: 19120937218888.195\n",
      "epoch: 1554\n",
      "training loss: 30460995404758.67\n",
      "validation loss: 19119910312276.074\n",
      "epoch: 1555\n",
      "training loss: 30460605175948.22\n",
      "validation loss: 19118885081044.85\n",
      "epoch: 1556\n",
      "training loss: 30460215426693.3\n",
      "validation loss: 19117861521825.31\n",
      "epoch: 1557\n",
      "training loss: 30459826156058.543\n",
      "validation loss: 19116839631255.234\n",
      "epoch: 1558\n",
      "training loss: 30459437363110.547\n",
      "validation loss: 19115819405979.406\n",
      "epoch: 1559\n",
      "training loss: 30459049046917.87\n",
      "validation loss: 19114800842649.598\n",
      "epoch: 1560\n",
      "training loss: 30458661206551.03\n",
      "validation loss: 19113783937924.543\n",
      "epoch: 1561\n",
      "training loss: 30458273841082.504\n",
      "validation loss: 19112768688469.965\n",
      "epoch: 1562\n",
      "training loss: 30457886949586.727\n",
      "validation loss: 19111755090958.484\n",
      "epoch: 1563\n",
      "training loss: 30457500531140.074\n",
      "validation loss: 19110743142069.676\n",
      "epoch: 1564\n",
      "training loss: 30457114584820.875\n",
      "validation loss: 19109732838490.027\n",
      "epoch: 1565\n",
      "training loss: 30456729109709.395\n",
      "validation loss: 19108724176912.914\n",
      "epoch: 1566\n",
      "training loss: 30456344104887.836\n",
      "validation loss: 19107717154038.6\n",
      "epoch: 1567\n",
      "training loss: 30455959569440.336\n",
      "validation loss: 19106711766574.23\n",
      "epoch: 1568\n",
      "training loss: 30455575502452.94\n",
      "validation loss: 19105708011233.78\n",
      "epoch: 1569\n",
      "training loss: 30455191903013.645\n",
      "validation loss: 19104705884738.062\n",
      "epoch: 1570\n",
      "training loss: 30454808770212.36\n",
      "validation loss: 19103705383814.75\n",
      "epoch: 1571\n",
      "training loss: 30454426103140.89\n",
      "validation loss: 19102706505198.3\n",
      "epoch: 1572\n",
      "training loss: 30454043900892.98\n",
      "validation loss: 19101709245629.977\n",
      "epoch: 1573\n",
      "training loss: 30453662162564.273\n",
      "validation loss: 19100713601857.812\n",
      "epoch: 1574\n",
      "training loss: 30453280887252.297\n",
      "validation loss: 19099719570636.625\n",
      "epoch: 1575\n",
      "training loss: 30452900074056.5\n",
      "validation loss: 19098727148727.96\n",
      "epoch: 1576\n",
      "training loss: 30452519722078.24\n",
      "validation loss: 19097736332900.152\n",
      "epoch: 1577\n",
      "training loss: 30452139830420.723\n",
      "validation loss: 19096747119928.207\n",
      "epoch: 1578\n",
      "training loss: 30451760398189.066\n",
      "validation loss: 19095759506593.87\n",
      "epoch: 1579\n",
      "training loss: 30451381424490.285\n",
      "validation loss: 19094773489685.574\n",
      "epoch: 1580\n",
      "training loss: 30451002908433.242\n",
      "validation loss: 19093789065998.44\n",
      "epoch: 1581\n",
      "training loss: 30450624849128.707\n",
      "validation loss: 19092806232334.25\n",
      "epoch: 1582\n",
      "training loss: 30450247245689.28\n",
      "validation loss: 19091824985501.45\n",
      "epoch: 1583\n",
      "training loss: 30449870097229.48\n",
      "validation loss: 19090845322315.117\n",
      "epoch: 1584\n",
      "training loss: 30449493402865.656\n",
      "validation loss: 19089867239596.965\n",
      "epoch: 1585\n",
      "training loss: 30449117161716.02\n",
      "validation loss: 19088890734175.312\n",
      "epoch: 1586\n",
      "training loss: 30448741372900.633\n",
      "validation loss: 19087915802885.07\n",
      "epoch: 1587\n",
      "training loss: 30448366035541.434\n",
      "validation loss: 19086942442567.742\n",
      "epoch: 1588\n",
      "training loss: 30447991148762.176\n",
      "validation loss: 19085970650071.406\n",
      "epoch: 1589\n",
      "training loss: 30447616711688.473\n",
      "validation loss: 19085000422250.688\n",
      "epoch: 1590\n",
      "training loss: 30447242723447.79\n",
      "validation loss: 19084031755966.766\n",
      "epoch: 1591\n",
      "training loss: 30446869183169.4\n",
      "validation loss: 19083064648087.332\n",
      "epoch: 1592\n",
      "training loss: 30446496089984.426\n",
      "validation loss: 19082099095486.61\n",
      "epoch: 1593\n",
      "training loss: 30446123443025.805\n",
      "validation loss: 19081135095045.312\n",
      "epoch: 1594\n",
      "training loss: 30445751241428.33\n",
      "validation loss: 19080172643650.65\n",
      "epoch: 1595\n",
      "training loss: 30445379484328.574\n",
      "validation loss: 19079211738196.293\n",
      "epoch: 1596\n",
      "training loss: 30445008170864.94\n",
      "validation loss: 19078252375582.387\n",
      "epoch: 1597\n",
      "training loss: 30444637300177.66\n",
      "validation loss: 19077294552715.516\n",
      "epoch: 1598\n",
      "training loss: 30444266871408.746\n",
      "validation loss: 19076338266508.69\n",
      "epoch: 1599\n",
      "training loss: 30443896883702.035\n",
      "validation loss: 19075383513881.36\n",
      "epoch: 1600\n",
      "training loss: 30443527336203.164\n",
      "validation loss: 19074430291759.37\n",
      "epoch: 1601\n",
      "training loss: 30443158228059.555\n",
      "validation loss: 19073478597074.94\n",
      "epoch: 1602\n",
      "training loss: 30442789558420.43\n",
      "validation loss: 19072528426766.703\n",
      "epoch: 1603\n",
      "training loss: 30442421326436.79\n",
      "validation loss: 19071579777779.625\n",
      "epoch: 1604\n",
      "training loss: 30442053531261.434\n",
      "validation loss: 19070632647065.047\n",
      "epoch: 1605\n",
      "training loss: 30441686172048.957\n",
      "validation loss: 19069687031580.64\n",
      "epoch: 1606\n",
      "training loss: 30441319247955.69\n",
      "validation loss: 19068742928290.39\n",
      "epoch: 1607\n",
      "training loss: 30440952758139.77\n",
      "validation loss: 19067800334164.613\n",
      "epoch: 1608\n",
      "training loss: 30440586701761.098\n",
      "validation loss: 19066859246179.92\n",
      "epoch: 1609\n",
      "training loss: 30440221077981.33\n",
      "validation loss: 19065919661319.188\n",
      "epoch: 1610\n",
      "training loss: 30439855885963.906\n",
      "validation loss: 19064981576571.586\n",
      "epoch: 1611\n",
      "training loss: 30439491124873.992\n",
      "validation loss: 19064044988932.527\n",
      "epoch: 1612\n",
      "training loss: 30439126793878.55\n",
      "validation loss: 19063109895403.7\n",
      "epoch: 1613\n",
      "training loss: 30438762892146.26\n",
      "validation loss: 19062176292992.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1614\n",
      "training loss: 30438399418847.555\n",
      "validation loss: 19061244178714.477\n",
      "epoch: 1615\n",
      "training loss: 30438036373154.633\n",
      "validation loss: 19060313549588.54\n",
      "epoch: 1616\n",
      "training loss: 30437673754241.41\n",
      "validation loss: 19059384402641.676\n",
      "epoch: 1617\n",
      "training loss: 30437311561283.543\n",
      "validation loss: 19058456734906.582\n",
      "epoch: 1618\n",
      "training loss: 30436949793458.418\n",
      "validation loss: 19057530543422.105\n",
      "epoch: 1619\n",
      "training loss: 30436588449945.164\n",
      "validation loss: 19056605825233.285\n",
      "epoch: 1620\n",
      "training loss: 30436227529924.63\n",
      "validation loss: 19055682577391.254\n",
      "epoch: 1621\n",
      "training loss: 30435867032579.375\n",
      "validation loss: 19054760796953.32\n",
      "epoch: 1622\n",
      "training loss: 30435506957093.7\n",
      "validation loss: 19053840480982.875\n",
      "epoch: 1623\n",
      "training loss: 30435147302653.59\n",
      "validation loss: 19052921626549.414\n",
      "epoch: 1624\n",
      "training loss: 30434788068446.754\n",
      "validation loss: 19052004230728.523\n",
      "epoch: 1625\n",
      "training loss: 30434429253662.617\n",
      "validation loss: 19051088290601.87\n",
      "epoch: 1626\n",
      "training loss: 30434070857492.297\n",
      "validation loss: 19050173803257.195\n",
      "epoch: 1627\n",
      "training loss: 30433712879128.613\n",
      "validation loss: 19049260765788.26\n",
      "epoch: 1628\n",
      "training loss: 30433355317766.074\n",
      "validation loss: 19048349175294.902\n",
      "epoch: 1629\n",
      "training loss: 30432998172600.902\n",
      "validation loss: 19047439028882.938\n",
      "epoch: 1630\n",
      "training loss: 30432641442830.99\n",
      "validation loss: 19046530323664.242\n",
      "epoch: 1631\n",
      "training loss: 30432285127655.914\n",
      "validation loss: 19045623056756.664\n",
      "epoch: 1632\n",
      "training loss: 30431929226276.945\n",
      "validation loss: 19044717225284.043\n",
      "epoch: 1633\n",
      "training loss: 30431573737897.027\n",
      "validation loss: 19043812826376.2\n",
      "epoch: 1634\n",
      "training loss: 30431218661720.766\n",
      "validation loss: 19042909857168.902\n",
      "epoch: 1635\n",
      "training loss: 30430863996954.445\n",
      "validation loss: 19042008314803.88\n",
      "epoch: 1636\n",
      "training loss: 30430509742806.043\n",
      "validation loss: 19041108196428.785\n",
      "epoch: 1637\n",
      "training loss: 30430155898485.152\n",
      "validation loss: 19040209499197.227\n",
      "epoch: 1638\n",
      "training loss: 30429802463203.06\n",
      "validation loss: 19039312220268.69\n",
      "epoch: 1639\n",
      "training loss: 30429449436172.71\n",
      "validation loss: 19038416356808.562\n",
      "epoch: 1640\n",
      "training loss: 30429096816608.68\n",
      "validation loss: 19037521905988.133\n",
      "epoch: 1641\n",
      "training loss: 30428744603727.207\n",
      "validation loss: 19036628864984.562\n",
      "epoch: 1642\n",
      "training loss: 30428392796746.176\n",
      "validation loss: 19035737230980.863\n",
      "epoch: 1643\n",
      "training loss: 30428041394885.113\n",
      "validation loss: 19034847001165.914\n",
      "epoch: 1644\n",
      "training loss: 30427690397365.19\n",
      "validation loss: 19033958172734.41\n",
      "epoch: 1645\n",
      "training loss: 30427339803409.2\n",
      "validation loss: 19033070742886.883\n",
      "epoch: 1646\n",
      "training loss: 30426989612241.582\n",
      "validation loss: 19032184708829.684\n",
      "epoch: 1647\n",
      "training loss: 30426639823088.387\n",
      "validation loss: 19031300067774.945\n",
      "epoch: 1648\n",
      "training loss: 30426290435177.305\n",
      "validation loss: 19030416816940.59\n",
      "epoch: 1649\n",
      "training loss: 30425941447737.645\n",
      "validation loss: 19029534953550.336\n",
      "epoch: 1650\n",
      "training loss: 30425592860000.336\n",
      "validation loss: 19028654474833.664\n",
      "epoch: 1651\n",
      "training loss: 30425244671197.914\n",
      "validation loss: 19027775378025.773\n",
      "epoch: 1652\n",
      "training loss: 30424896880564.54\n",
      "validation loss: 19026897660367.637\n",
      "epoch: 1653\n",
      "training loss: 30424549487335.953\n",
      "validation loss: 19026021319105.93\n",
      "epoch: 1654\n",
      "training loss: 30424202490749.543\n",
      "validation loss: 19025146351493.066\n",
      "epoch: 1655\n",
      "training loss: 30423855890044.258\n",
      "validation loss: 19024272754787.145\n",
      "epoch: 1656\n",
      "training loss: 30423509684460.656\n",
      "validation loss: 19023400526251.953\n",
      "epoch: 1657\n",
      "training loss: 30423163873240.918\n",
      "validation loss: 19022529663156.99\n",
      "epoch: 1658\n",
      "training loss: 30422818455628.766\n",
      "validation loss: 19021660162777.37\n",
      "epoch: 1659\n",
      "training loss: 30422473430869.56\n",
      "validation loss: 19020792022393.92\n",
      "epoch: 1660\n",
      "training loss: 30422128798210.184\n",
      "validation loss: 19019925239293.05\n",
      "epoch: 1661\n",
      "training loss: 30421784556899.168\n",
      "validation loss: 19019059810766.85\n",
      "epoch: 1662\n",
      "training loss: 30421440706186.58\n",
      "validation loss: 19018195734113.0\n",
      "epoch: 1663\n",
      "training loss: 30421097245324.062\n",
      "validation loss: 19017333006634.793\n",
      "epoch: 1664\n",
      "training loss: 30420754173564.848\n",
      "validation loss: 19016471625641.145\n",
      "epoch: 1665\n",
      "training loss: 30420411490163.72\n",
      "validation loss: 19015611588446.504\n",
      "epoch: 1666\n",
      "training loss: 30420069194377.023\n",
      "validation loss: 19014752892370.94\n",
      "epoch: 1667\n",
      "training loss: 30419727285462.676\n",
      "validation loss: 19013895534740.055\n",
      "epoch: 1668\n",
      "training loss: 30419385762680.145\n",
      "validation loss: 19013039512885.008\n",
      "epoch: 1669\n",
      "training loss: 30419044625290.47\n",
      "validation loss: 19012184824142.5\n",
      "epoch: 1670\n",
      "training loss: 30418703872556.203\n",
      "validation loss: 19011331465854.746\n",
      "epoch: 1671\n",
      "training loss: 30418363503741.473\n",
      "validation loss: 19010479435369.473\n",
      "epoch: 1672\n",
      "training loss: 30418023518111.94\n",
      "validation loss: 19009628730039.918\n",
      "epoch: 1673\n",
      "training loss: 30417683914934.824\n",
      "validation loss: 19008779347224.82\n",
      "epoch: 1674\n",
      "training loss: 30417344693478.86\n",
      "validation loss: 19007931284288.363\n",
      "epoch: 1675\n",
      "training loss: 30417005853014.32\n",
      "validation loss: 19007084538600.234\n",
      "epoch: 1676\n",
      "training loss: 30416667392813.01\n",
      "validation loss: 19006239107535.555\n",
      "epoch: 1677\n",
      "training loss: 30416329312148.28\n",
      "validation loss: 19005394988474.906\n",
      "epoch: 1678\n",
      "training loss: 30415991610294.973\n",
      "validation loss: 19004552178804.266\n",
      "epoch: 1679\n",
      "training loss: 30415654286529.48\n",
      "validation loss: 19003710675915.086\n",
      "epoch: 1680\n",
      "training loss: 30415317340129.7\n",
      "validation loss: 19002870477204.19\n",
      "epoch: 1681\n",
      "training loss: 30414980770375.027\n",
      "validation loss: 19002031580073.812\n",
      "epoch: 1682\n",
      "training loss: 30414644576546.414\n",
      "validation loss: 19001193981931.58\n",
      "epoch: 1683\n",
      "training loss: 30414308757926.266\n",
      "validation loss: 19000357680190.47\n",
      "epoch: 1684\n",
      "training loss: 30413973313798.54\n",
      "validation loss: 18999522672268.86\n",
      "epoch: 1685\n",
      "training loss: 30413638243448.664\n",
      "validation loss: 18998688955590.457\n",
      "epoch: 1686\n",
      "training loss: 30413303546163.574\n",
      "validation loss: 18997856527584.316\n",
      "epoch: 1687\n",
      "training loss: 30412969221231.707\n",
      "validation loss: 18997025385684.83\n",
      "epoch: 1688\n",
      "training loss: 30412635267942.992\n",
      "validation loss: 18996195527331.676\n",
      "epoch: 1689\n",
      "training loss: 30412301685588.832\n",
      "validation loss: 18995366949969.902\n",
      "epoch: 1690\n",
      "training loss: 30411968473462.13\n",
      "validation loss: 18994539651049.785\n",
      "epoch: 1691\n",
      "training loss: 30411635630857.27\n",
      "validation loss: 18993713628026.95\n",
      "epoch: 1692\n",
      "training loss: 30411303157070.1\n",
      "validation loss: 18992888878362.242\n",
      "epoch: 1693\n",
      "training loss: 30410971051397.977\n",
      "validation loss: 18992065399521.816\n",
      "epoch: 1694\n",
      "training loss: 30410639313139.695\n",
      "validation loss: 18991243188977.043\n",
      "epoch: 1695\n",
      "training loss: 30410307941595.547\n",
      "validation loss: 18990422244204.547\n",
      "epoch: 1696\n",
      "training loss: 30409976936067.273\n",
      "validation loss: 18989602562686.188\n",
      "epoch: 1697\n",
      "training loss: 30409646295858.08\n",
      "validation loss: 18988784141909.04\n",
      "epoch: 1698\n",
      "training loss: 30409316020272.65\n",
      "validation loss: 18987966979365.38\n",
      "epoch: 1699\n",
      "training loss: 30408986108617.086\n",
      "validation loss: 18987151072552.71\n",
      "epoch: 1700\n",
      "training loss: 30408656560199.008\n",
      "validation loss: 18986336418973.68\n",
      "epoch: 1701\n",
      "training loss: 30408327374327.42\n",
      "validation loss: 18985523016136.137\n",
      "epoch: 1702\n",
      "training loss: 30407998550312.816\n",
      "validation loss: 18984710861553.074\n",
      "epoch: 1703\n",
      "training loss: 30407670087467.125\n",
      "validation loss: 18983899952742.676\n",
      "epoch: 1704\n",
      "training loss: 30407341985103.71\n",
      "validation loss: 18983090287228.223\n",
      "epoch: 1705\n",
      "training loss: 30407014242537.39\n",
      "validation loss: 18982281862538.16\n",
      "epoch: 1706\n",
      "training loss: 30406686859084.395\n",
      "validation loss: 18981474676206.05\n",
      "epoch: 1707\n",
      "training loss: 30406359834062.41\n",
      "validation loss: 18980668725770.555\n",
      "epoch: 1708\n",
      "training loss: 30406033166790.543\n",
      "validation loss: 18979864008775.42\n",
      "epoch: 1709\n",
      "training loss: 30405706856589.316\n",
      "validation loss: 18979060522769.523\n",
      "epoch: 1710\n",
      "training loss: 30405380902780.707\n",
      "validation loss: 18978258265306.79\n",
      "epoch: 1711\n",
      "training loss: 30405055304688.07\n",
      "validation loss: 18977457233946.22\n",
      "epoch: 1712\n",
      "training loss: 30404730061636.21\n",
      "validation loss: 18976657426251.86\n",
      "epoch: 1713\n",
      "training loss: 30404405172951.34\n",
      "validation loss: 18975858839792.816\n",
      "epoch: 1714\n",
      "training loss: 30404080637961.074\n",
      "validation loss: 18975061472143.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1715\n",
      "training loss: 30403756455994.445\n",
      "validation loss: 18974265320882.254\n",
      "epoch: 1716\n",
      "training loss: 30403432626381.883\n",
      "validation loss: 18973470383594.07\n",
      "epoch: 1717\n",
      "training loss: 30403109148455.24\n",
      "validation loss: 18972676657867.86\n",
      "epoch: 1718\n",
      "training loss: 30402786021547.734\n",
      "validation loss: 18971884141297.79\n",
      "epoch: 1719\n",
      "training loss: 30402463244994.02\n",
      "validation loss: 18971092831483.02\n",
      "epoch: 1720\n",
      "training loss: 30402140818130.1\n",
      "validation loss: 18970302726027.676\n",
      "epoch: 1721\n",
      "training loss: 30401818740293.41\n",
      "validation loss: 18969513822540.84\n",
      "epoch: 1722\n",
      "training loss: 30401497010822.754\n",
      "validation loss: 18968726118636.562\n",
      "epoch: 1723\n",
      "training loss: 30401175629058.312\n",
      "validation loss: 18967939611933.81\n",
      "epoch: 1724\n",
      "training loss: 30400854594341.668\n",
      "validation loss: 18967154300056.5\n",
      "epoch: 1725\n",
      "training loss: 30400533906015.76\n",
      "validation loss: 18966370180633.46\n",
      "epoch: 1726\n",
      "training loss: 30400213563424.934\n",
      "validation loss: 18965587251298.434\n",
      "epoch: 1727\n",
      "training loss: 30399893565914.863\n",
      "validation loss: 18964805509690.06\n",
      "epoch: 1728\n",
      "training loss: 30399573912832.633\n",
      "validation loss: 18964024953451.863\n",
      "epoch: 1729\n",
      "training loss: 30399254603526.68\n",
      "validation loss: 18963245580232.25\n",
      "epoch: 1730\n",
      "training loss: 30398935637346.805\n",
      "validation loss: 18962467387684.492\n",
      "epoch: 1731\n",
      "training loss: 30398617013644.164\n",
      "validation loss: 18961690373466.73\n",
      "epoch: 1732\n",
      "training loss: 30398298731771.28\n",
      "validation loss: 18960914535241.938\n",
      "epoch: 1733\n",
      "training loss: 30397980791082.035\n",
      "validation loss: 18960139870677.934\n",
      "epoch: 1734\n",
      "training loss: 30397663190931.645\n",
      "validation loss: 18959366377447.36\n",
      "epoch: 1735\n",
      "training loss: 30397345930676.703\n",
      "validation loss: 18958594053227.684\n",
      "epoch: 1736\n",
      "training loss: 30397029009675.113\n",
      "validation loss: 18957822895701.17\n",
      "epoch: 1737\n",
      "training loss: 30396712427286.164\n",
      "validation loss: 18957052902554.887\n",
      "epoch: 1738\n",
      "training loss: 30396396182870.453\n",
      "validation loss: 18956284071480.695\n",
      "epoch: 1739\n",
      "training loss: 30396080275789.94\n",
      "validation loss: 18955516400175.207\n",
      "epoch: 1740\n",
      "training loss: 30395764705407.9\n",
      "validation loss: 18954749886339.836\n",
      "epoch: 1741\n",
      "training loss: 30395449471088.957\n",
      "validation loss: 18953984527680.73\n",
      "epoch: 1742\n",
      "training loss: 30395134572199.04\n",
      "validation loss: 18953220321908.79\n",
      "epoch: 1743\n",
      "training loss: 30394820008105.434\n",
      "validation loss: 18952457266739.652\n",
      "epoch: 1744\n",
      "training loss: 30394505778176.73\n",
      "validation loss: 18951695359893.684\n",
      "epoch: 1745\n",
      "training loss: 30394191881782.867\n",
      "validation loss: 18950934599095.96\n",
      "epoch: 1746\n",
      "training loss: 30393878318295.047\n",
      "validation loss: 18950174982076.266\n",
      "epoch: 1747\n",
      "training loss: 30393565087085.848\n",
      "validation loss: 18949416506569.094\n",
      "epoch: 1748\n",
      "training loss: 30393252187529.113\n",
      "validation loss: 18948659170313.605\n",
      "epoch: 1749\n",
      "training loss: 30392939619000.04\n",
      "validation loss: 18947902971053.656\n",
      "epoch: 1750\n",
      "training loss: 30392627380875.094\n",
      "validation loss: 18947147906537.75\n",
      "epoch: 1751\n",
      "training loss: 30392315472532.07\n",
      "validation loss: 18946393974519.082\n",
      "epoch: 1752\n",
      "training loss: 30392003893350.047\n",
      "validation loss: 18945641172755.46\n",
      "epoch: 1753\n",
      "training loss: 30391692642709.418\n",
      "validation loss: 18944889499009.34\n",
      "epoch: 1754\n",
      "training loss: 30391381719991.867\n",
      "validation loss: 18944138951047.81\n",
      "epoch: 1755\n",
      "training loss: 30391071124580.36\n",
      "validation loss: 18943389526642.566\n",
      "epoch: 1756\n",
      "training loss: 30390760855859.188\n",
      "validation loss: 18942641223569.945\n",
      "epoch: 1757\n",
      "training loss: 30390450913213.875\n",
      "validation loss: 18941894039610.844\n",
      "epoch: 1758\n",
      "training loss: 30390141296031.28\n",
      "validation loss: 18941147972550.77\n",
      "epoch: 1759\n",
      "training loss: 30389832003699.516\n",
      "validation loss: 18940403020179.8\n",
      "epoch: 1760\n",
      "training loss: 30389523035607.992\n",
      "validation loss: 18939659180292.598\n",
      "epoch: 1761\n",
      "training loss: 30389214391147.38\n",
      "validation loss: 18938916450688.36\n",
      "epoch: 1762\n",
      "training loss: 30388906069709.64\n",
      "validation loss: 18938174829170.863\n",
      "epoch: 1763\n",
      "training loss: 30388598070687.992\n",
      "validation loss: 18937434313548.4\n",
      "epoch: 1764\n",
      "training loss: 30388290393476.914\n",
      "validation loss: 18936694901633.812\n",
      "epoch: 1765\n",
      "training loss: 30387983037472.19\n",
      "validation loss: 18935956591244.465\n",
      "epoch: 1766\n",
      "training loss: 30387676002070.836\n",
      "validation loss: 18935219380202.223\n",
      "epoch: 1767\n",
      "training loss: 30387369286671.117\n",
      "validation loss: 18934483266333.457\n",
      "epoch: 1768\n",
      "training loss: 30387062890672.6\n",
      "validation loss: 18933748247469.04\n",
      "epoch: 1769\n",
      "training loss: 30386756813476.06\n",
      "validation loss: 18933014321444.324\n",
      "epoch: 1770\n",
      "training loss: 30386451054483.56\n",
      "validation loss: 18932281486099.133\n",
      "epoch: 1771\n",
      "training loss: 30386145613098.387\n",
      "validation loss: 18931549739277.76\n",
      "epoch: 1772\n",
      "training loss: 30385840488725.098\n",
      "validation loss: 18930819078828.945\n",
      "epoch: 1773\n",
      "training loss: 30385535680769.477\n",
      "validation loss: 18930089502605.9\n",
      "epoch: 1774\n",
      "training loss: 30385231188638.56\n",
      "validation loss: 18929361008466.242\n",
      "epoch: 1775\n",
      "training loss: 30384927011740.62\n",
      "validation loss: 18928633594272.035\n",
      "epoch: 1776\n",
      "training loss: 30384623149485.156\n",
      "validation loss: 18927907257889.758\n",
      "epoch: 1777\n",
      "training loss: 30384319601282.92\n",
      "validation loss: 18927181997190.3\n",
      "epoch: 1778\n",
      "training loss: 30384016366545.89\n",
      "validation loss: 18926457810048.94\n",
      "epoch: 1779\n",
      "training loss: 30383713444687.26\n",
      "validation loss: 18925734694345.367\n",
      "epoch: 1780\n",
      "training loss: 30383410835121.47\n",
      "validation loss: 18925012647963.637\n",
      "epoch: 1781\n",
      "training loss: 30383108537264.156\n",
      "validation loss: 18924291668792.184\n",
      "epoch: 1782\n",
      "training loss: 30382806550532.215\n",
      "validation loss: 18923571754723.797\n",
      "epoch: 1783\n",
      "training loss: 30382504874343.707\n",
      "validation loss: 18922852903655.637\n",
      "epoch: 1784\n",
      "training loss: 30382203508117.977\n",
      "validation loss: 18922135113489.19\n",
      "epoch: 1785\n",
      "training loss: 30381902451275.523\n",
      "validation loss: 18921418382130.285\n",
      "epoch: 1786\n",
      "training loss: 30381601703238.086\n",
      "validation loss: 18920702707489.08\n",
      "epoch: 1787\n",
      "training loss: 30381301263428.598\n",
      "validation loss: 18919988087480.055\n",
      "epoch: 1788\n",
      "training loss: 30381001131271.21\n",
      "validation loss: 18919274520021.973\n",
      "epoch: 1789\n",
      "training loss: 30380701306191.27\n",
      "validation loss: 18918562003037.94\n",
      "epoch: 1790\n",
      "training loss: 30380401787615.332\n",
      "validation loss: 18917850534455.312\n",
      "epoch: 1791\n",
      "training loss: 30380102574971.133\n",
      "validation loss: 18917140112205.742\n",
      "epoch: 1792\n",
      "training loss: 30379803667687.625\n",
      "validation loss: 18916430734225.16\n",
      "epoch: 1793\n",
      "training loss: 30379505065194.93\n",
      "validation loss: 18915722398453.766\n",
      "epoch: 1794\n",
      "training loss: 30379206766924.383\n",
      "validation loss: 18915015102835.977\n",
      "epoch: 1795\n",
      "training loss: 30378908772308.492\n",
      "validation loss: 18914308845320.496\n",
      "epoch: 1796\n",
      "training loss: 30378611080780.953\n",
      "validation loss: 18913603623860.242\n",
      "epoch: 1797\n",
      "training loss: 30378313691776.645\n",
      "validation loss: 18912899436412.367\n",
      "epoch: 1798\n",
      "training loss: 30378016604731.637\n",
      "validation loss: 18912196280938.25\n",
      "epoch: 1799\n",
      "training loss: 30377719819083.16\n",
      "validation loss: 18911494155403.465\n",
      "epoch: 1800\n",
      "training loss: 30377423334269.63\n",
      "validation loss: 18910793057777.785\n",
      "epoch: 1801\n",
      "training loss: 30377127149730.625\n",
      "validation loss: 18910092986035.19\n",
      "epoch: 1802\n",
      "training loss: 30376831264906.906\n",
      "validation loss: 18909393938153.848\n",
      "epoch: 1803\n",
      "training loss: 30376535679240.402\n",
      "validation loss: 18908695912116.066\n",
      "epoch: 1804\n",
      "training loss: 30376240392174.176\n",
      "validation loss: 18907998905908.367\n",
      "epoch: 1805\n",
      "training loss: 30375945403152.51\n",
      "validation loss: 18907302917521.38\n",
      "epoch: 1806\n",
      "training loss: 30375650711620.793\n",
      "validation loss: 18906607944949.918\n",
      "epoch: 1807\n",
      "training loss: 30375356317025.605\n",
      "validation loss: 18905913986192.93\n",
      "epoch: 1808\n",
      "training loss: 30375062218814.668\n",
      "validation loss: 18905221039253.465\n",
      "epoch: 1809\n",
      "training loss: 30374768416436.848\n",
      "validation loss: 18904529102138.746\n",
      "epoch: 1810\n",
      "training loss: 30374474909342.18\n",
      "validation loss: 18903838172860.06\n",
      "epoch: 1811\n",
      "training loss: 30374181696981.836\n",
      "validation loss: 18903148249432.824\n",
      "epoch: 1812\n",
      "training loss: 30373888778808.137\n",
      "validation loss: 18902459329876.55\n",
      "epoch: 1813\n",
      "training loss: 30373596154274.543\n",
      "validation loss: 18901771412214.816\n",
      "epoch: 1814\n",
      "training loss: 30373303822835.67\n",
      "validation loss: 18901084494475.33\n",
      "epoch: 1815\n",
      "training loss: 30373011783947.242\n",
      "validation loss: 18900398574689.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1816\n",
      "training loss: 30372720037066.133\n",
      "validation loss: 18899713650894.066\n",
      "epoch: 1817\n",
      "training loss: 30372428581650.37\n",
      "validation loss: 18899029721127.973\n",
      "epoch: 1818\n",
      "training loss: 30372137417159.082\n",
      "validation loss: 18898346783435.42\n",
      "epoch: 1819\n",
      "training loss: 30371846543052.547\n",
      "validation loss: 18897664835864.363\n",
      "epoch: 1820\n",
      "training loss: 30371555958792.152\n",
      "validation loss: 18896983876466.76\n",
      "epoch: 1821\n",
      "training loss: 30371265663840.418\n",
      "validation loss: 18896303903298.6\n",
      "epoch: 1822\n",
      "training loss: 30370975657660.99\n",
      "validation loss: 18895624914419.89\n",
      "epoch: 1823\n",
      "training loss: 30370685939718.617\n",
      "validation loss: 18894946907894.62\n",
      "epoch: 1824\n",
      "training loss: 30370396509479.188\n",
      "validation loss: 18894269881790.793\n",
      "epoch: 1825\n",
      "training loss: 30370107366409.684\n",
      "validation loss: 18893593834180.395\n",
      "epoch: 1826\n",
      "training loss: 30369818509978.207\n",
      "validation loss: 18892918763139.37\n",
      "epoch: 1827\n",
      "training loss: 30369529939653.96\n",
      "validation loss: 18892244666747.664\n",
      "epoch: 1828\n",
      "training loss: 30369241654907.28\n",
      "validation loss: 18891571543089.156\n",
      "epoch: 1829\n",
      "training loss: 30368953655209.57\n",
      "validation loss: 18890899390251.688\n",
      "epoch: 1830\n",
      "training loss: 30368665940033.367\n",
      "validation loss: 18890228206327.043\n",
      "epoch: 1831\n",
      "training loss: 30368378508852.293\n",
      "validation loss: 18889557989410.94\n",
      "epoch: 1832\n",
      "training loss: 30368091361141.066\n",
      "validation loss: 18888888737603.023\n",
      "epoch: 1833\n",
      "training loss: 30367804496375.508\n",
      "validation loss: 18888220449006.86\n",
      "epoch: 1834\n",
      "training loss: 30367517914032.53\n",
      "validation loss: 18887553121729.938\n",
      "epoch: 1835\n",
      "training loss: 30367231613590.125\n",
      "validation loss: 18886886753883.64\n",
      "epoch: 1836\n",
      "training loss: 30366945594527.387\n",
      "validation loss: 18886221343583.227\n",
      "epoch: 1837\n",
      "training loss: 30366659856324.496\n",
      "validation loss: 18885556888947.87\n",
      "epoch: 1838\n",
      "training loss: 30366374398462.703\n",
      "validation loss: 18884893388100.61\n",
      "epoch: 1839\n",
      "training loss: 30366089220424.363\n",
      "validation loss: 18884230839168.348\n",
      "epoch: 1840\n",
      "training loss: 30365804321692.87\n",
      "validation loss: 18883569240281.855\n",
      "epoch: 1841\n",
      "training loss: 30365519701752.742\n",
      "validation loss: 18882908589575.754\n",
      "epoch: 1842\n",
      "training loss: 30365235360089.54\n",
      "validation loss: 18882248885188.523\n",
      "epoch: 1843\n",
      "training loss: 30364951296189.92\n",
      "validation loss: 18881590125262.48\n",
      "epoch: 1844\n",
      "training loss: 30364667509541.566\n",
      "validation loss: 18880932307943.746\n",
      "epoch: 1845\n",
      "training loss: 30364383999633.29\n",
      "validation loss: 18880275431382.29\n",
      "epoch: 1846\n",
      "training loss: 30364100765954.918\n",
      "validation loss: 18879619493731.883\n",
      "epoch: 1847\n",
      "training loss: 30363817807997.367\n",
      "validation loss: 18878964493150.098\n",
      "epoch: 1848\n",
      "training loss: 30363535125252.6\n",
      "validation loss: 18878310427798.32\n",
      "epoch: 1849\n",
      "training loss: 30363252717213.656\n",
      "validation loss: 18877657295841.707\n",
      "epoch: 1850\n",
      "training loss: 30362970583374.605\n",
      "validation loss: 18877005095449.21\n",
      "epoch: 1851\n",
      "training loss: 30362688723230.61\n",
      "validation loss: 18876353824793.55\n",
      "epoch: 1852\n",
      "training loss: 30362407136277.832\n",
      "validation loss: 18875703482051.203\n",
      "epoch: 1853\n",
      "training loss: 30362125822013.523\n",
      "validation loss: 18875054065402.434\n",
      "epoch: 1854\n",
      "training loss: 30361844779935.977\n",
      "validation loss: 18874405573031.23\n",
      "epoch: 1855\n",
      "training loss: 30361564009544.527\n",
      "validation loss: 18873758003125.312\n",
      "epoch: 1856\n",
      "training loss: 30361283510339.535\n",
      "validation loss: 18873111353876.168\n",
      "epoch: 1857\n",
      "training loss: 30361003281822.43\n",
      "validation loss: 18872465623478.992\n",
      "epoch: 1858\n",
      "training loss: 30360723323495.668\n",
      "validation loss: 18871820810132.695\n",
      "epoch: 1859\n",
      "training loss: 30360443634862.72\n",
      "validation loss: 18871176912039.906\n",
      "epoch: 1860\n",
      "training loss: 30360164215428.137\n",
      "validation loss: 18870533927406.953\n",
      "epoch: 1861\n",
      "training loss: 30359885064697.46\n",
      "validation loss: 18869891854443.855\n",
      "epoch: 1862\n",
      "training loss: 30359606182177.285\n",
      "validation loss: 18869250691364.336\n",
      "epoch: 1863\n",
      "training loss: 30359327567375.22\n",
      "validation loss: 18868610436385.77\n",
      "epoch: 1864\n",
      "training loss: 30359049219799.902\n",
      "validation loss: 18867971087729.242\n",
      "epoch: 1865\n",
      "training loss: 30358771138961.008\n",
      "validation loss: 18867332643619.465\n",
      "epoch: 1866\n",
      "training loss: 30358493324369.195\n",
      "validation loss: 18866695102284.812\n",
      "epoch: 1867\n",
      "training loss: 30358215775536.19\n",
      "validation loss: 18866058461957.336\n",
      "epoch: 1868\n",
      "training loss: 30357938491974.707\n",
      "validation loss: 18865422720872.71\n",
      "epoch: 1869\n",
      "training loss: 30357661473198.473\n",
      "validation loss: 18864787877270.227\n",
      "epoch: 1870\n",
      "training loss: 30357384718722.23\n",
      "validation loss: 18864153929392.832\n",
      "epoch: 1871\n",
      "training loss: 30357108228061.754\n",
      "validation loss: 18863520875487.082\n",
      "epoch: 1872\n",
      "training loss: 30356832000733.785\n",
      "validation loss: 18862888713803.13\n",
      "epoch: 1873\n",
      "training loss: 30356556036256.11\n",
      "validation loss: 18862257442594.74\n",
      "epoch: 1874\n",
      "training loss: 30356280334147.49\n",
      "validation loss: 18861627060119.273\n",
      "epoch: 1875\n",
      "training loss: 30356004893927.71\n",
      "validation loss: 18860997564637.684\n",
      "epoch: 1876\n",
      "training loss: 30355729715117.53\n",
      "validation loss: 18860368954414.504\n",
      "epoch: 1877\n",
      "training loss: 30355454797238.742\n",
      "validation loss: 18859741227717.836\n",
      "epoch: 1878\n",
      "training loss: 30355180139814.105\n",
      "validation loss: 18859114382819.344\n",
      "epoch: 1879\n",
      "training loss: 30354905742367.37\n",
      "validation loss: 18858488417994.25\n",
      "epoch: 1880\n",
      "training loss: 30354631604423.3\n",
      "validation loss: 18857863331521.344\n",
      "epoch: 1881\n",
      "training loss: 30354357725507.63\n",
      "validation loss: 18857239121682.938\n",
      "epoch: 1882\n",
      "training loss: 30354084105147.086\n",
      "validation loss: 18856615786764.887\n",
      "epoch: 1883\n",
      "training loss: 30353810742869.38\n",
      "validation loss: 18855993325056.586\n",
      "epoch: 1884\n",
      "training loss: 30353537638203.21\n",
      "validation loss: 18855371734850.93\n",
      "epoch: 1885\n",
      "training loss: 30353264790678.24\n",
      "validation loss: 18854751014444.332\n",
      "epoch: 1886\n",
      "training loss: 30352992199825.133\n",
      "validation loss: 18854131162136.73\n",
      "epoch: 1887\n",
      "training loss: 30352719865175.52\n",
      "validation loss: 18853512176231.543\n",
      "epoch: 1888\n",
      "training loss: 30352447786261.992\n",
      "validation loss: 18852894055035.688\n",
      "epoch: 1889\n",
      "training loss: 30352175962618.133\n",
      "validation loss: 18852276796859.562\n",
      "epoch: 1890\n",
      "training loss: 30351904393778.492\n",
      "validation loss: 18851660400017.03\n",
      "epoch: 1891\n",
      "training loss: 30351633079278.582\n",
      "validation loss: 18851044862825.46\n",
      "epoch: 1892\n",
      "training loss: 30351362018654.875\n",
      "validation loss: 18850430183605.64\n",
      "epoch: 1893\n",
      "training loss: 30351091211444.816\n",
      "validation loss: 18849816360681.836\n",
      "epoch: 1894\n",
      "training loss: 30350820657186.824\n",
      "validation loss: 18849203392381.77\n",
      "epoch: 1895\n",
      "training loss: 30350550355420.246\n",
      "validation loss: 18848591277036.57\n",
      "epoch: 1896\n",
      "training loss: 30350280305685.414\n",
      "validation loss: 18847980012980.844\n",
      "epoch: 1897\n",
      "training loss: 30350010507523.613\n",
      "validation loss: 18847369598552.598\n",
      "epoch: 1898\n",
      "training loss: 30349740960477.062\n",
      "validation loss: 18846760032093.246\n",
      "epoch: 1899\n",
      "training loss: 30349471664088.96\n",
      "validation loss: 18846151311947.64\n",
      "epoch: 1900\n",
      "training loss: 30349202617903.434\n",
      "validation loss: 18845543436464.027\n",
      "epoch: 1901\n",
      "training loss: 30348933821465.57\n",
      "validation loss: 18844936403994.055\n",
      "epoch: 1902\n",
      "training loss: 30348665274321.383\n",
      "validation loss: 18844330212892.74\n",
      "epoch: 1903\n",
      "training loss: 30348396976017.86\n",
      "validation loss: 18843724861518.504\n",
      "epoch: 1904\n",
      "training loss: 30348128926102.902\n",
      "validation loss: 18843120348233.145\n",
      "epoch: 1905\n",
      "training loss: 30347861124125.37\n",
      "validation loss: 18842516671401.82\n",
      "epoch: 1906\n",
      "training loss: 30347593569635.047\n",
      "validation loss: 18841913829393.05\n",
      "epoch: 1907\n",
      "training loss: 30347326262182.664\n",
      "validation loss: 18841311820578.727\n",
      "epoch: 1908\n",
      "training loss: 30347059201319.89\n",
      "validation loss: 18840710643334.066\n",
      "epoch: 1909\n",
      "training loss: 30346792386599.297\n",
      "validation loss: 18840110296037.63\n",
      "epoch: 1910\n",
      "training loss: 30346525817574.418\n",
      "validation loss: 18839510777071.33\n",
      "epoch: 1911\n",
      "training loss: 30346259493799.69\n",
      "validation loss: 18838912084820.38\n",
      "epoch: 1912\n",
      "training loss: 30345993414830.504\n",
      "validation loss: 18838314217673.34\n",
      "epoch: 1913\n",
      "training loss: 30345727580223.145\n",
      "validation loss: 18837717174022.066\n",
      "epoch: 1914\n",
      "training loss: 30345461989534.84\n",
      "validation loss: 18837120952261.727\n",
      "epoch: 1915\n",
      "training loss: 30345196642323.723\n",
      "validation loss: 18836525550790.79\n",
      "epoch: 1916\n",
      "training loss: 30344931538148.86\n",
      "validation loss: 18835930968011.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1917\n",
      "training loss: 30344666676570.215\n",
      "validation loss: 18835337202327.418\n",
      "epoch: 1918\n",
      "training loss: 30344402057148.68\n",
      "validation loss: 18834744252148.35\n",
      "epoch: 1919\n",
      "training loss: 30344137679446.055\n",
      "validation loss: 18834152115885.4\n",
      "epoch: 1920\n",
      "training loss: 30343873543025.05\n",
      "validation loss: 18833560791953.42\n",
      "epoch: 1921\n",
      "training loss: 30343609647449.293\n",
      "validation loss: 18832970278770.53\n",
      "epoch: 1922\n",
      "training loss: 30343345992283.29\n",
      "validation loss: 18832380574758.094\n",
      "epoch: 1923\n",
      "training loss: 30343082577092.484\n",
      "validation loss: 18831791678340.72\n",
      "epoch: 1924\n",
      "training loss: 30342819401443.19\n",
      "validation loss: 18831203587946.25\n",
      "epoch: 1925\n",
      "training loss: 30342556464902.656\n",
      "validation loss: 18830616302005.773\n",
      "epoch: 1926\n",
      "training loss: 30342293767039.016\n",
      "validation loss: 18830029818953.594\n",
      "epoch: 1927\n",
      "training loss: 30342031307421.277\n",
      "validation loss: 18829444137227.22\n",
      "epoch: 1928\n",
      "training loss: 30341769085619.38\n",
      "validation loss: 18828859255267.383\n",
      "epoch: 1929\n",
      "training loss: 30341507101204.125\n",
      "validation loss: 18828275171518.023\n",
      "epoch: 1930\n",
      "training loss: 30341245353747.22\n",
      "validation loss: 18827691884426.27\n",
      "epoch: 1931\n",
      "training loss: 30340983842821.273\n",
      "validation loss: 18827109392442.438\n",
      "epoch: 1932\n",
      "training loss: 30340722567999.742\n",
      "validation loss: 18826527694020.027\n",
      "epoch: 1933\n",
      "training loss: 30340461528857.023\n",
      "validation loss: 18825946787615.73\n",
      "epoch: 1934\n",
      "training loss: 30340200724968.348\n",
      "validation loss: 18825366671689.39\n",
      "epoch: 1935\n",
      "training loss: 30339940155909.848\n",
      "validation loss: 18824787344704.02\n",
      "epoch: 1936\n",
      "training loss: 30339679821258.555\n",
      "validation loss: 18824208805125.8\n",
      "epoch: 1937\n",
      "training loss: 30339419720592.332\n",
      "validation loss: 18823631051424.03\n",
      "epoch: 1938\n",
      "training loss: 30339159853489.953\n",
      "validation loss: 18823054082071.2\n",
      "epoch: 1939\n",
      "training loss: 30338900219531.074\n",
      "validation loss: 18822477895542.906\n",
      "epoch: 1940\n",
      "training loss: 30338640818296.184\n",
      "validation loss: 18821902490317.863\n",
      "epoch: 1941\n",
      "training loss: 30338381649366.684\n",
      "validation loss: 18821327864877.945\n",
      "epoch: 1942\n",
      "training loss: 30338122712324.812\n",
      "validation loss: 18820754017708.12\n",
      "epoch: 1943\n",
      "training loss: 30337864006753.695\n",
      "validation loss: 18820180947296.484\n",
      "epoch: 1944\n",
      "training loss: 30337605532237.31\n",
      "validation loss: 18819608652134.207\n",
      "epoch: 1945\n",
      "training loss: 30337347288360.504\n",
      "validation loss: 18819037130715.586\n",
      "epoch: 1946\n",
      "training loss: 30337089274708.98\n",
      "validation loss: 18818466381538.008\n",
      "epoch: 1947\n",
      "training loss: 30336831490869.32\n",
      "validation loss: 18817896403101.926\n",
      "epoch: 1948\n",
      "training loss: 30336573936428.926\n",
      "validation loss: 18817327193910.88\n",
      "epoch: 1949\n",
      "training loss: 30336316610976.082\n",
      "validation loss: 18816758752471.48\n",
      "epoch: 1950\n",
      "training loss: 30336059514099.934\n",
      "validation loss: 18816191077293.42\n",
      "epoch: 1951\n",
      "training loss: 30335802645390.457\n",
      "validation loss: 18815624166889.434\n",
      "epoch: 1952\n",
      "training loss: 30335546004438.492\n",
      "validation loss: 18815058019775.31\n",
      "epoch: 1953\n",
      "training loss: 30335289590835.715\n",
      "validation loss: 18814492634469.89\n",
      "epoch: 1954\n",
      "training loss: 30335033404174.664\n",
      "validation loss: 18813928009495.055\n",
      "epoch: 1955\n",
      "training loss: 30334777444048.71\n",
      "validation loss: 18813364143375.703\n",
      "epoch: 1956\n",
      "training loss: 30334521710052.074\n",
      "validation loss: 18812801034639.773\n",
      "epoch: 1957\n",
      "training loss: 30334266201779.83\n",
      "validation loss: 18812238681818.25\n",
      "epoch: 1958\n",
      "training loss: 30334010918827.848\n",
      "validation loss: 18811677083445.07\n",
      "epoch: 1959\n",
      "training loss: 30333755860792.875\n",
      "validation loss: 18811116238057.246\n",
      "epoch: 1960\n",
      "training loss: 30333501027272.496\n",
      "validation loss: 18810556144194.746\n",
      "epoch: 1961\n",
      "training loss: 30333246417865.113\n",
      "validation loss: 18809996800400.562\n",
      "epoch: 1962\n",
      "training loss: 30332992032169.965\n",
      "validation loss: 18809438205220.656\n",
      "epoch: 1963\n",
      "training loss: 30332737869787.117\n",
      "validation loss: 18808880357203.977\n",
      "epoch: 1964\n",
      "training loss: 30332483930317.477\n",
      "validation loss: 18808323254902.47\n",
      "epoch: 1965\n",
      "training loss: 30332230213362.766\n",
      "validation loss: 18807766896871.016\n",
      "epoch: 1966\n",
      "training loss: 30331976718525.53\n",
      "validation loss: 18807211281667.5\n",
      "epoch: 1967\n",
      "training loss: 30331723445409.16\n",
      "validation loss: 18806656407852.727\n",
      "epoch: 1968\n",
      "training loss: 30331470393617.84\n",
      "validation loss: 18806102273990.484\n",
      "epoch: 1969\n",
      "training loss: 30331217562756.59\n",
      "validation loss: 18805548878647.484\n",
      "epoch: 1970\n",
      "training loss: 30330964952431.242\n",
      "validation loss: 18804996220393.4\n",
      "epoch: 1971\n",
      "training loss: 30330712562248.45\n",
      "validation loss: 18804444297800.812\n",
      "epoch: 1972\n",
      "training loss: 30330460391815.688\n",
      "validation loss: 18803893109445.25\n",
      "epoch: 1973\n",
      "training loss: 30330208440741.223\n",
      "validation loss: 18803342653905.164\n",
      "epoch: 1974\n",
      "training loss: 30329956708634.16\n",
      "validation loss: 18802792929761.902\n",
      "epoch: 1975\n",
      "training loss: 30329705195104.395\n",
      "validation loss: 18802243935599.73\n",
      "epoch: 1976\n",
      "training loss: 30329453899762.64\n",
      "validation loss: 18801695670005.832\n",
      "epoch: 1977\n",
      "training loss: 30329202822220.4\n",
      "validation loss: 18801148131570.266\n",
      "epoch: 1978\n",
      "training loss: 30328951962090.004\n",
      "validation loss: 18800601318885.984\n",
      "epoch: 1979\n",
      "training loss: 30328701318984.566\n",
      "validation loss: 18800055230548.84\n",
      "epoch: 1980\n",
      "training loss: 30328450892518.023\n",
      "validation loss: 18799509865157.56\n",
      "epoch: 1981\n",
      "training loss: 30328200682305.094\n",
      "validation loss: 18798965221313.723\n",
      "epoch: 1982\n",
      "training loss: 30327950687961.297\n",
      "validation loss: 18798421297621.812\n",
      "epoch: 1983\n",
      "training loss: 30327700909102.957\n",
      "validation loss: 18797878092689.13\n",
      "epoch: 1984\n",
      "training loss: 30327451345347.168\n",
      "validation loss: 18797335605125.863\n",
      "epoch: 1985\n",
      "training loss: 30327201996311.844\n",
      "validation loss: 18796793833545.03\n",
      "epoch: 1986\n",
      "training loss: 30326952861615.688\n",
      "validation loss: 18796252776562.52\n",
      "epoch: 1987\n",
      "training loss: 30326703940878.18\n",
      "validation loss: 18795712432797.016\n",
      "epoch: 1988\n",
      "training loss: 30326455233719.59\n",
      "validation loss: 18795172800870.07\n",
      "epoch: 1989\n",
      "training loss: 30326206739760.98\n",
      "validation loss: 18794633879406.03\n",
      "epoch: 1990\n",
      "training loss: 30325958458624.188\n",
      "validation loss: 18794095667032.086\n",
      "epoch: 1991\n",
      "training loss: 30325710389931.832\n",
      "validation loss: 18793558162378.227\n",
      "epoch: 1992\n",
      "training loss: 30325462533307.332\n",
      "validation loss: 18793021364077.266\n",
      "epoch: 1993\n",
      "training loss: 30325214888374.875\n",
      "validation loss: 18792485270764.805\n",
      "epoch: 1994\n",
      "training loss: 30324967454759.418\n",
      "validation loss: 18791949881079.23\n",
      "epoch: 1995\n",
      "training loss: 30324720232086.69\n",
      "validation loss: 18791415193661.746\n",
      "epoch: 1996\n",
      "training loss: 30324473219983.227\n",
      "validation loss: 18790881207156.316\n",
      "epoch: 1997\n",
      "training loss: 30324226418076.293\n",
      "validation loss: 18790347920209.688\n",
      "epoch: 1998\n",
      "training loss: 30323979825993.957\n",
      "validation loss: 18789815331471.395\n",
      "epoch: 1999\n",
      "training loss: 30323733443365.047\n",
      "validation loss: 18789283439593.715\n",
      "Mean absolute error: $6050248423.60\n",
      "Nodes: 75\n",
      "Learning Rate: 1e-08\n",
      "epoch: 0\n",
      "training loss: 37090762102677.62\n",
      "validation loss: 33520924246209.535\n",
      "epoch: 1\n",
      "training loss: 37047928134992.266\n",
      "validation loss: 33342330627612.695\n",
      "epoch: 2\n",
      "training loss: 37005932518292.59\n",
      "validation loss: 33351928439009.58\n",
      "epoch: 3\n",
      "training loss: 36951204550616.086\n",
      "validation loss: 33407754007614.844\n",
      "epoch: 4\n",
      "training loss: 36909682992668.13\n",
      "validation loss: 33369602774070.15\n",
      "epoch: 5\n",
      "training loss: 36866177004298.695\n",
      "validation loss: 33501384339122.465\n",
      "epoch: 6\n",
      "training loss: 36810971033270.5\n",
      "validation loss: 33375163407871.33\n",
      "epoch: 7\n",
      "training loss: 36814314200642.81\n",
      "validation loss: 33259633765981.434\n",
      "epoch: 8\n",
      "training loss: 36733586560586.15\n",
      "validation loss: 33032897139024.098\n",
      "epoch: 9\n",
      "training loss: 36688578932953.016\n",
      "validation loss: 33165331650709.938\n",
      "epoch: 10\n",
      "training loss: 36680382943440.63\n",
      "validation loss: 33219894017020.45\n",
      "epoch: 11\n",
      "training loss: 36635587747412.94\n",
      "validation loss: 33061453179682.555\n",
      "epoch: 12\n",
      "training loss: 36556176181300.3\n",
      "validation loss: 33176106049712.098\n",
      "epoch: 13\n",
      "training loss: 36483496098748.26\n",
      "validation loss: 33231213371022.973\n",
      "epoch: 14\n",
      "training loss: 36489102255579.28\n",
      "validation loss: 32671955058669.32\n",
      "epoch: 15\n",
      "training loss: 36422639379439.516\n",
      "validation loss: 32510744983218.812\n",
      "epoch: 16\n",
      "training loss: 36408709942753.6\n",
      "validation loss: 32671237132248.24\n",
      "epoch: 17\n",
      "training loss: 36392587805501.67\n",
      "validation loss: 32760493557794.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\n",
      "training loss: 36329867975211.66\n",
      "validation loss: 32486120428239.348\n",
      "epoch: 19\n",
      "training loss: 36315550318062.93\n",
      "validation loss: 32374240963081.88\n",
      "epoch: 20\n",
      "training loss: 36228138123871.07\n",
      "validation loss: 32348626138601.234\n",
      "epoch: 21\n",
      "training loss: 36184623752773.82\n",
      "validation loss: 32312537492492.027\n",
      "epoch: 22\n",
      "training loss: 36134219664050.734\n",
      "validation loss: 32399876079637.81\n",
      "epoch: 23\n",
      "training loss: 36056967588814.61\n",
      "validation loss: 32268497250444.25\n",
      "epoch: 24\n",
      "training loss: 36014215269801.17\n",
      "validation loss: 32483396032744.5\n",
      "epoch: 25\n",
      "training loss: 35981451880934.47\n",
      "validation loss: 32487299065952.906\n",
      "epoch: 26\n",
      "training loss: 35931064851162.32\n",
      "validation loss: 32381553033445.566\n",
      "epoch: 27\n",
      "training loss: 35868991767980.836\n",
      "validation loss: 32589890001226.383\n",
      "epoch: 28\n",
      "training loss: 35832846794029.266\n",
      "validation loss: 32480780792816.246\n",
      "epoch: 29\n",
      "training loss: 35705534625127.11\n",
      "validation loss: 32818170165178.34\n",
      "epoch: 30\n",
      "training loss: 35637329342447.38\n",
      "validation loss: 33000762256039.242\n",
      "epoch: 31\n",
      "training loss: 35575133731430.95\n",
      "validation loss: 32718892761145.426\n",
      "epoch: 32\n",
      "training loss: 35592712245746.7\n",
      "validation loss: 32744089175209.855\n",
      "epoch: 33\n",
      "training loss: 35514613261498.56\n",
      "validation loss: 32699809676077.72\n",
      "epoch: 34\n",
      "training loss: 35525202422239.73\n",
      "validation loss: 32607125824483.812\n",
      "epoch: 35\n",
      "training loss: 35449495555896.19\n",
      "validation loss: 32310786636187.16\n",
      "epoch: 36\n",
      "training loss: 35415212531285.66\n",
      "validation loss: 32197645531911.613\n",
      "epoch: 37\n",
      "training loss: 35363280388836.62\n",
      "validation loss: 32028610016248.125\n",
      "epoch: 38\n",
      "training loss: 35379967859334.734\n",
      "validation loss: 31891718182550.363\n",
      "epoch: 39\n",
      "training loss: 35409725035111.93\n",
      "validation loss: 32222805388650.227\n",
      "epoch: 40\n",
      "training loss: 35315488971118.95\n",
      "validation loss: 31915040169221.285\n",
      "epoch: 41\n",
      "training loss: 35265727732275.62\n",
      "validation loss: 31585449763717.195\n",
      "epoch: 42\n",
      "training loss: 35206664906001.11\n",
      "validation loss: 31434706348686.816\n",
      "epoch: 43\n",
      "training loss: 35215305702585.305\n",
      "validation loss: 31422606324836.04\n",
      "epoch: 44\n",
      "training loss: 35131341606217.367\n",
      "validation loss: 31173236078273.066\n",
      "epoch: 45\n",
      "training loss: 35132598546725.688\n",
      "validation loss: 31157659704727.234\n",
      "epoch: 46\n",
      "training loss: 35011479254538.11\n",
      "validation loss: 31282327744996.414\n",
      "epoch: 47\n",
      "training loss: 34948051804836.324\n",
      "validation loss: 31261340766210.875\n",
      "epoch: 48\n",
      "training loss: 34870112585864.957\n",
      "validation loss: 31509456530916.066\n",
      "epoch: 49\n",
      "training loss: 34902348934308.54\n",
      "validation loss: 31077481834880.066\n",
      "epoch: 50\n",
      "training loss: 34808217089855.91\n",
      "validation loss: 30908808756680.844\n",
      "epoch: 51\n",
      "training loss: 34790249610979.12\n",
      "validation loss: 31475107827002.676\n",
      "epoch: 52\n",
      "training loss: 34745105548261.027\n",
      "validation loss: 31563960656387.816\n",
      "epoch: 53\n",
      "training loss: 34715148275205.855\n",
      "validation loss: 31373358368761.543\n",
      "epoch: 54\n",
      "training loss: 34707208791671.92\n",
      "validation loss: 32013092363828.68\n",
      "epoch: 55\n",
      "training loss: 34760986646009.85\n",
      "validation loss: 31691825912890.84\n",
      "epoch: 56\n",
      "training loss: 34719309343269.277\n",
      "validation loss: 31445790963149.38\n",
      "epoch: 57\n",
      "training loss: 34674647509634.016\n",
      "validation loss: 31333776003656.043\n",
      "epoch: 58\n",
      "training loss: 34721298752190.555\n",
      "validation loss: 31327578345993.875\n",
      "epoch: 59\n",
      "training loss: 34842318951987.805\n",
      "validation loss: 30365822906504.207\n",
      "epoch: 60\n",
      "training loss: 34859139441820.58\n",
      "validation loss: 30378962383394.973\n",
      "epoch: 61\n",
      "training loss: 34793933348110.992\n",
      "validation loss: 30375044575379.207\n",
      "epoch: 62\n",
      "training loss: 34780622824647.477\n",
      "validation loss: 30827631729896.04\n",
      "epoch: 63\n",
      "training loss: 34755629232691.35\n",
      "validation loss: 30800173151895.89\n",
      "epoch: 64\n",
      "training loss: 34751267310429.965\n",
      "validation loss: 30568190691068.96\n",
      "epoch: 65\n",
      "training loss: 34688838918928.39\n",
      "validation loss: 30543352023432.414\n",
      "epoch: 66\n",
      "training loss: 34654034618425.758\n",
      "validation loss: 30628209812850.984\n",
      "epoch: 67\n",
      "training loss: 34662237593516.152\n",
      "validation loss: 30595619654144.625\n",
      "epoch: 68\n",
      "training loss: 34601816070805.816\n",
      "validation loss: 30444866827376.37\n",
      "epoch: 69\n",
      "training loss: 34530713220326.49\n",
      "validation loss: 30208281870566.16\n",
      "epoch: 70\n",
      "training loss: 34535652237130.645\n",
      "validation loss: 30103315292117.902\n",
      "epoch: 71\n",
      "training loss: 34504155507476.863\n",
      "validation loss: 30151939120828.61\n",
      "epoch: 72\n",
      "training loss: 34459586285051.707\n",
      "validation loss: 30293045026301.01\n",
      "epoch: 73\n",
      "training loss: 34410553565813.938\n",
      "validation loss: 29990110047278.04\n",
      "epoch: 74\n",
      "training loss: 34400052428692.76\n",
      "validation loss: 30056991337764.477\n",
      "epoch: 75\n",
      "training loss: 34389360031868.145\n",
      "validation loss: 29900648555579.016\n",
      "epoch: 76\n",
      "training loss: 34346587489472.38\n",
      "validation loss: 29771108254641.348\n",
      "epoch: 77\n",
      "training loss: 34292654953827.777\n",
      "validation loss: 29934364231486.336\n",
      "epoch: 78\n",
      "training loss: 34319338247171.824\n",
      "validation loss: 29732899562132.902\n",
      "epoch: 79\n",
      "training loss: 34286449026256.344\n",
      "validation loss: 29610440696066.71\n",
      "epoch: 80\n",
      "training loss: 34328090402596.137\n",
      "validation loss: 29659723348628.38\n",
      "epoch: 81\n",
      "training loss: 34355063594358.676\n",
      "validation loss: 29448328148485.05\n",
      "epoch: 82\n",
      "training loss: 34324129665708.695\n",
      "validation loss: 29479082613543.35\n",
      "epoch: 83\n",
      "training loss: 34288888298070.332\n",
      "validation loss: 29309873078017.06\n",
      "epoch: 84\n",
      "training loss: 34266782781145.76\n",
      "validation loss: 29229541927174.723\n",
      "epoch: 85\n",
      "training loss: 34229956451088.047\n",
      "validation loss: 29209990130597.926\n",
      "epoch: 86\n",
      "training loss: 34182816784264.67\n",
      "validation loss: 29083898830024.594\n",
      "epoch: 87\n",
      "training loss: 34143578935155.977\n",
      "validation loss: 28983632579213.46\n",
      "epoch: 88\n",
      "training loss: 34155389453663.676\n",
      "validation loss: 28798207308698.023\n",
      "epoch: 89\n",
      "training loss: 34109453775066.496\n",
      "validation loss: 28701343730648.496\n",
      "epoch: 90\n",
      "training loss: 34087914977555.844\n",
      "validation loss: 28661163403388.043\n",
      "epoch: 91\n",
      "training loss: 34068242125184.68\n",
      "validation loss: 28646918232020.63\n",
      "epoch: 92\n",
      "training loss: 34034054886297.125\n",
      "validation loss: 28682865021599.492\n",
      "epoch: 93\n",
      "training loss: 34026825458394.12\n",
      "validation loss: 28564103868573.3\n",
      "epoch: 94\n",
      "training loss: 34005150011175.793\n",
      "validation loss: 28503075596264.707\n",
      "epoch: 95\n",
      "training loss: 33985163623726.574\n",
      "validation loss: 28468031243493.418\n",
      "epoch: 96\n",
      "training loss: 33987593823431.84\n",
      "validation loss: 28299461179057.89\n",
      "epoch: 97\n",
      "training loss: 33966021335978.18\n",
      "validation loss: 28214979617041.18\n",
      "epoch: 98\n",
      "training loss: 33946281904464.234\n",
      "validation loss: 28178264705060.887\n",
      "epoch: 99\n",
      "training loss: 33926732206503.945\n",
      "validation loss: 28141705400800.934\n",
      "epoch: 100\n",
      "training loss: 33907331668284.66\n",
      "validation loss: 28105400007592.84\n",
      "epoch: 101\n",
      "training loss: 33888081587497.24\n",
      "validation loss: 28069297800217.71\n",
      "epoch: 102\n",
      "training loss: 33868980791275.688\n",
      "validation loss: 28033397508186.434\n",
      "epoch: 103\n",
      "training loss: 33850028115884.05\n",
      "validation loss: 27997697869676.004\n",
      "epoch: 104\n",
      "training loss: 33831222406532.023\n",
      "validation loss: 27962197630996.76\n",
      "epoch: 105\n",
      "training loss: 33812562489501.15\n",
      "validation loss: 27926895544824.777\n",
      "epoch: 106\n",
      "training loss: 33794023131760.598\n",
      "validation loss: 27891466334638.824\n",
      "epoch: 107\n",
      "training loss: 33775651277733.51\n",
      "validation loss: 27856555273735.76\n",
      "epoch: 108\n",
      "training loss: 33757421863922.8\n",
      "validation loss: 27821838717558.47\n",
      "epoch: 109\n",
      "training loss: 33739333779948.273\n",
      "validation loss: 27787315462480.266\n",
      "epoch: 110\n",
      "training loss: 33721385924084.562\n",
      "validation loss: 27752984318889.246\n",
      "epoch: 111\n",
      "training loss: 33703577203191.797\n",
      "validation loss: 27718844119676.38\n",
      "epoch: 112\n",
      "training loss: 33685906532638.855\n",
      "validation loss: 27684893751571.777\n",
      "epoch: 113\n",
      "training loss: 33668372836128.246\n",
      "validation loss: 27651132338618.605\n",
      "epoch: 114\n",
      "training loss: 33650975022081.18\n",
      "validation loss: 27617562974395.07\n",
      "epoch: 115\n",
      "training loss: 33633718691292.47\n",
      "validation loss: 27584506936164.85\n",
      "epoch: 116\n",
      "training loss: 33616589541420.027\n",
      "validation loss: 27551304942183.297\n",
      "epoch: 117\n",
      "training loss: 33599593142691.168\n",
      "validation loss: 27518286829475.062\n",
      "epoch: 118\n",
      "training loss: 33582724256666.29\n",
      "validation loss: 27485451460403.98\n",
      "epoch: 119\n",
      "training loss: 33566041828371.9\n",
      "validation loss: 27453525508080.215\n",
      "epoch: 120\n",
      "training loss: 33549412694559.57\n",
      "validation loss: 27421056328002.89\n",
      "epoch: 121\n",
      "training loss: 33532885368176.234\n",
      "validation loss: 27371353043548.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 122\n",
      "training loss: 33516538368933.684\n",
      "validation loss: 27339148551537.895\n",
      "epoch: 123\n",
      "training loss: 33500318040190.523\n",
      "validation loss: 27307121729734.97\n",
      "epoch: 124\n",
      "training loss: 33484223394715.113\n",
      "validation loss: 27275271483002.293\n",
      "epoch: 125\n",
      "training loss: 33468253452622.074\n",
      "validation loss: 27243596723605.273\n",
      "epoch: 126\n",
      "training loss: 33452407241667.844\n",
      "validation loss: 27212096371168.973\n",
      "epoch: 127\n",
      "training loss: 33436683797188.887\n",
      "validation loss: 27180769352623.11\n",
      "epoch: 128\n",
      "training loss: 33421082162041.516\n",
      "validation loss: 27149614602149.004\n",
      "epoch: 129\n",
      "training loss: 33405601386540.22\n",
      "validation loss: 27118631061135.773\n",
      "epoch: 130\n",
      "training loss: 33390240528380.94\n",
      "validation loss: 27087817678185.758\n",
      "epoch: 131\n",
      "training loss: 33374998652256.426\n",
      "validation loss: 27057173409436.22\n",
      "epoch: 132\n",
      "training loss: 33359872015497.195\n",
      "validation loss: 27026697220167.82\n",
      "epoch: 133\n",
      "training loss: 33345596475288.566\n",
      "validation loss: 26953087001710.42\n",
      "epoch: 134\n",
      "training loss: 33330719062995.258\n",
      "validation loss: 26922723238644.23\n",
      "epoch: 135\n",
      "training loss: 33315956916470.363\n",
      "validation loss: 26892525730351.09\n",
      "epoch: 136\n",
      "training loss: 33301309136723.145\n",
      "validation loss: 26862493465136.008\n",
      "epoch: 137\n",
      "training loss: 33286774831694.242\n",
      "validation loss: 26832625432131.7\n",
      "epoch: 138\n",
      "training loss: 33272353116282.363\n",
      "validation loss: 26802920627274.777\n",
      "epoch: 139\n",
      "training loss: 33258043112289.883\n",
      "validation loss: 26773378053256.664\n",
      "epoch: 140\n",
      "training loss: 33243843948368.977\n",
      "validation loss: 26743996719474.78\n",
      "epoch: 141\n",
      "training loss: 33229754759968.227\n",
      "validation loss: 26714775641984.29\n",
      "epoch: 142\n",
      "training loss: 33215774689279.586\n",
      "validation loss: 26685713843450.043\n",
      "epoch: 143\n",
      "training loss: 33201902885185.777\n",
      "validation loss: 26656810353099.004\n",
      "epoch: 144\n",
      "training loss: 33188138503208.094\n",
      "validation loss: 26628064206673.05\n",
      "epoch: 145\n",
      "training loss: 33174480705454.617\n",
      "validation loss: 26599474446382.082\n",
      "epoch: 146\n",
      "training loss: 33160928660568.793\n",
      "validation loss: 26571040120857.574\n",
      "epoch: 147\n",
      "training loss: 33147481543678.39\n",
      "validation loss: 26542760285106.375\n",
      "epoch: 148\n",
      "training loss: 33134138536344.703\n",
      "validation loss: 26514634000465.035\n",
      "epoch: 149\n",
      "training loss: 33120898826511.74\n",
      "validation loss: 26486660334554.26\n",
      "epoch: 150\n",
      "training loss: 33107761608454.1\n",
      "validation loss: 26458838361233.973\n",
      "epoch: 151\n",
      "training loss: 33094726082713.848\n",
      "validation loss: 26431167160558.438\n",
      "epoch: 152\n",
      "training loss: 33081791455857.82\n",
      "validation loss: 26403645818731.777\n",
      "epoch: 153\n",
      "training loss: 33068956665489.887\n",
      "validation loss: 26376273428061.89\n",
      "epoch: 154\n",
      "training loss: 33055764170093.777\n",
      "validation loss: 26351548523067.527\n",
      "epoch: 155\n",
      "training loss: 33043123778583.234\n",
      "validation loss: 26324480468520.71\n",
      "epoch: 156\n",
      "training loss: 33030581211395.895\n",
      "validation loss: 26297558587240.34\n",
      "epoch: 157\n",
      "training loss: 33018135706073.562\n",
      "validation loss: 26270782084260.613\n",
      "epoch: 158\n",
      "training loss: 33005786505472.445\n",
      "validation loss: 26244150081503.734\n",
      "epoch: 159\n",
      "training loss: 32993532858354.844\n",
      "validation loss: 26217661706744.06\n",
      "epoch: 160\n",
      "training loss: 32981374019343.004\n",
      "validation loss: 26191316093566.387\n",
      "epoch: 161\n",
      "training loss: 32969309248873.367\n",
      "validation loss: 26165112381324.55\n",
      "epoch: 162\n",
      "training loss: 32957337813151.004\n",
      "validation loss: 26139049715100.336\n",
      "epoch: 163\n",
      "training loss: 32945458984104.094\n",
      "validation loss: 26113127245662.67\n",
      "epoch: 164\n",
      "training loss: 32933672039337.234\n",
      "validation loss: 26087344129427.258\n",
      "epoch: 165\n",
      "training loss: 32921976262076.156\n",
      "validation loss: 26061699528416.42\n",
      "epoch: 166\n",
      "training loss: 32910370940989.86\n",
      "validation loss: 26036192610219.242\n",
      "epoch: 167\n",
      "training loss: 32898855317880.5\n",
      "validation loss: 26010822547951.008\n",
      "epoch: 168\n",
      "training loss: 32887259010692.094\n",
      "validation loss: 25986287351747.008\n",
      "epoch: 169\n",
      "training loss: 32875648901582.883\n",
      "validation loss: 25962258337225.67\n",
      "epoch: 170\n",
      "training loss: 32864395463048.55\n",
      "validation loss: 25937297074965.867\n",
      "epoch: 171\n",
      "training loss: 32853229034970.438\n",
      "validation loss: 25912469417991.84\n",
      "epoch: 172\n",
      "training loss: 32842148938583.316\n",
      "validation loss: 25887774566328.734\n",
      "epoch: 173\n",
      "training loss: 32831154500416.36\n",
      "validation loss: 25863211725302.305\n",
      "epoch: 174\n",
      "training loss: 32820245052250.664\n",
      "validation loss: 25838780105502.36\n",
      "epoch: 175\n",
      "training loss: 32809419931073.42\n",
      "validation loss: 25814478922746.81\n",
      "epoch: 176\n",
      "training loss: 32798678478997.945\n",
      "validation loss: 25790307398046.445\n",
      "epoch: 177\n",
      "training loss: 32788020041749.203\n",
      "validation loss: 25766264757570.11\n",
      "epoch: 178\n",
      "training loss: 32777431117904.992\n",
      "validation loss: 25742355409256.234\n",
      "epoch: 179\n",
      "training loss: 32766936687882.914\n",
      "validation loss: 25718570486030.215\n",
      "epoch: 180\n",
      "training loss: 32756523346630.066\n",
      "validation loss: 25694909808110.086\n",
      "epoch: 181\n",
      "training loss: 32746190462314.96\n",
      "validation loss: 25671374970677.99\n",
      "epoch: 182\n",
      "training loss: 32735937407295.594\n",
      "validation loss: 25647965225233.414\n",
      "epoch: 183\n",
      "training loss: 32725763558826.105\n",
      "validation loss: 25624679828274.293\n",
      "epoch: 184\n",
      "training loss: 32715668299018.566\n",
      "validation loss: 25601518041290.93\n",
      "epoch: 185\n",
      "training loss: 32705651014805.105\n",
      "validation loss: 25578479130776.64\n",
      "epoch: 186\n",
      "training loss: 32695711097900.316\n",
      "validation loss: 25555562368267.195\n",
      "epoch: 187\n",
      "training loss: 32685847944763.92\n",
      "validation loss: 25532767030432.184\n",
      "epoch: 188\n",
      "training loss: 32676060956563.727\n",
      "validation loss: 25510092399266.293\n",
      "epoch: 189\n",
      "training loss: 32666349539138.902\n",
      "validation loss: 25487537762485.957\n",
      "epoch: 190\n",
      "training loss: 32656713102963.45\n",
      "validation loss: 25465102414386.293\n",
      "epoch: 191\n",
      "training loss: 32647151063109.902\n",
      "validation loss: 25442785657847.777\n",
      "epoch: 192\n",
      "training loss: 32637662839212.957\n",
      "validation loss: 25420586809653.49\n",
      "epoch: 193\n",
      "training loss: 32628247855432.367\n",
      "validation loss: 25398505217407.93\n",
      "epoch: 194\n",
      "training loss: 32618905540409.797\n",
      "validation loss: 25376540330786.277\n",
      "epoch: 195\n",
      "training loss: 32609635327163.293\n",
      "validation loss: 25354692176955.78\n",
      "epoch: 196\n",
      "training loss: 32600436651330.84\n",
      "validation loss: 25332965253852.33\n",
      "epoch: 197\n",
      "training loss: 32591317542763.72\n",
      "validation loss: 25311416484152.945\n",
      "epoch: 198\n",
      "training loss: 32582260285776.902\n",
      "validation loss: 25289910487085.547\n",
      "epoch: 199\n",
      "training loss: 32573263933001.496\n",
      "validation loss: 25268483578710.89\n",
      "epoch: 200\n",
      "training loss: 32564641870278.293\n",
      "validation loss: 25272538213445.914\n",
      "epoch: 201\n",
      "training loss: 32555788749601.133\n",
      "validation loss: 25251408535479.72\n",
      "epoch: 202\n",
      "training loss: 32547003881731.195\n",
      "validation loss: 25230389640167.527\n",
      "epoch: 203\n",
      "training loss: 32538286734395.633\n",
      "validation loss: 25209480878038.65\n",
      "epoch: 204\n",
      "training loss: 32529636773252.633\n",
      "validation loss: 25188681603849.004\n",
      "epoch: 205\n",
      "training loss: 32498056790967.367\n",
      "validation loss: 25382083152508.1\n",
      "epoch: 206\n",
      "training loss: 32520824354573.29\n",
      "validation loss: 25333835833887.03\n",
      "epoch: 207\n",
      "training loss: 32517103505056.16\n",
      "validation loss: 25248817347645.31\n",
      "epoch: 208\n",
      "training loss: 32508758213075.832\n",
      "validation loss: 25228625602097.746\n",
      "epoch: 209\n",
      "training loss: 32500477035273.88\n",
      "validation loss: 25208538326226.65\n",
      "epoch: 210\n",
      "training loss: 32492259472974.957\n",
      "validation loss: 25188554927382.25\n",
      "epoch: 211\n",
      "training loss: 32484105031330.094\n",
      "validation loss: 25168674809787.42\n",
      "epoch: 212\n",
      "training loss: 32476013218911.61\n",
      "validation loss: 25148897378616.047\n",
      "epoch: 213\n",
      "training loss: 32468177550714.83\n",
      "validation loss: 25133346396046.727\n",
      "epoch: 214\n",
      "training loss: 32458684815835.414\n",
      "validation loss: 25006364078851.66\n",
      "epoch: 215\n",
      "training loss: 32450789045979.566\n",
      "validation loss: 24986740392029.848\n",
      "epoch: 216\n",
      "training loss: 32442953993469.688\n",
      "validation loss: 24967218278968.06\n",
      "epoch: 217\n",
      "training loss: 32435179185226.35\n",
      "validation loss: 24947797150899.434\n",
      "epoch: 218\n",
      "training loss: 32427464151855.406\n",
      "validation loss: 24928476422741.05\n",
      "epoch: 219\n",
      "training loss: 32419808427619.445\n",
      "validation loss: 24909255513103.258\n",
      "epoch: 220\n",
      "training loss: 32412211550409.46\n",
      "validation loss: 24890133844286.68\n",
      "epoch: 221\n",
      "training loss: 32404673061716.656\n",
      "validation loss: 24871110842271.72\n",
      "epoch: 222\n",
      "training loss: 32397192506604.492\n",
      "validation loss: 24852185936703.43\n",
      "epoch: 223\n",
      "training loss: 32389769433680.87\n",
      "validation loss: 24833358560873.406\n",
      "epoch: 224\n",
      "training loss: 32382403395070.58\n",
      "validation loss: 24814628151699.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 225\n",
      "training loss: 32375093946387.887\n",
      "validation loss: 24795994149705.52\n",
      "epoch: 226\n",
      "training loss: 32367840646709.445\n",
      "validation loss: 24777455998997.01\n",
      "epoch: 227\n",
      "training loss: 32360643058547.24\n",
      "validation loss: 24759013147240.582\n",
      "epoch: 228\n",
      "training loss: 32353500747821.91\n",
      "validation loss: 24740665045639.793\n",
      "epoch: 229\n",
      "training loss: 32346413283836.184\n",
      "validation loss: 24722411148912.11\n",
      "epoch: 230\n",
      "training loss: 32339380239248.48\n",
      "validation loss: 24704250915265.457\n",
      "epoch: 231\n",
      "training loss: 32332401190046.875\n",
      "validation loss: 24686183806374.848\n",
      "epoch: 232\n",
      "training loss: 32325475715523.055\n",
      "validation loss: 24668209287358.895\n",
      "epoch: 233\n",
      "training loss: 32318603398246.684\n",
      "validation loss: 24650326826756.418\n",
      "epoch: 234\n",
      "training loss: 32311783824039.78\n",
      "validation loss: 24632535896503.17\n",
      "epoch: 235\n",
      "training loss: 32305016581951.434\n",
      "validation loss: 24614835971908.605\n",
      "epoch: 236\n",
      "training loss: 32298301264232.645\n",
      "validation loss: 24597226531632.695\n",
      "epoch: 237\n",
      "training loss: 32291637466311.4\n",
      "validation loss: 24579707057663.027\n",
      "epoch: 238\n",
      "training loss: 32285024786767.87\n",
      "validation loss: 24562277035291.87\n",
      "epoch: 239\n",
      "training loss: 32278462827309.926\n",
      "validation loss: 24544935953093.42\n",
      "epoch: 240\n",
      "training loss: 32271951192748.703\n",
      "validation loss: 24527683302901.293\n",
      "epoch: 241\n",
      "training loss: 32265489490974.49\n",
      "validation loss: 24510518579785.98\n",
      "epoch: 242\n",
      "training loss: 32259077332932.656\n",
      "validation loss: 24493441282032.555\n",
      "epoch: 243\n",
      "training loss: 32252714332599.934\n",
      "validation loss: 24476450911118.555\n",
      "epoch: 244\n",
      "training loss: 32246400106960.742\n",
      "validation loss: 24459546971691.918\n",
      "epoch: 245\n",
      "training loss: 32240134275983.793\n",
      "validation loss: 24442728971549.168\n",
      "epoch: 246\n",
      "training loss: 32233916462598.805\n",
      "validation loss: 24425996421613.645\n",
      "epoch: 247\n",
      "training loss: 32227746292673.46\n",
      "validation loss: 24409348835913.984\n",
      "epoch: 248\n",
      "training loss: 32221623394990.504\n",
      "validation loss: 24392785731562.703\n",
      "epoch: 249\n",
      "training loss: 32215547401225.04\n",
      "validation loss: 24376306628734.914\n",
      "epoch: 250\n",
      "training loss: 32209517945921.977\n",
      "validation loss: 24359911050647.234\n",
      "epoch: 251\n",
      "training loss: 32203534666473.684\n",
      "validation loss: 24343598523536.816\n",
      "epoch: 252\n",
      "training loss: 32197597203097.82\n",
      "validation loss: 24327368576640.527\n",
      "epoch: 253\n",
      "training loss: 32191705198815.27\n",
      "validation loss: 24311220742174.31\n",
      "epoch: 254\n",
      "training loss: 32185858299428.363\n",
      "validation loss: 24295154555312.625\n",
      "epoch: 255\n",
      "training loss: 32180056153499.176\n",
      "validation loss: 24279169554168.117\n",
      "epoch: 256\n",
      "training loss: 32174298412327.996\n",
      "validation loss: 24263265279771.363\n",
      "epoch: 257\n",
      "training loss: 32168584729932.047\n",
      "validation loss: 24247441276050.83\n",
      "epoch: 258\n",
      "training loss: 32162914763024.258\n",
      "validation loss: 24231697089812.875\n",
      "epoch: 259\n",
      "training loss: 32157288170992.266\n",
      "validation loss: 24216032270722.02\n",
      "epoch: 260\n",
      "training loss: 32151704615877.543\n",
      "validation loss: 24200446371281.266\n",
      "epoch: 261\n",
      "training loss: 32146163762354.633\n",
      "validation loss: 24184938946812.582\n",
      "epoch: 262\n",
      "training loss: 32140665277710.438\n",
      "validation loss: 24169509555437.555\n",
      "epoch: 263\n",
      "training loss: 32135208831823.316\n",
      "validation loss: 24154157758058.145\n",
      "epoch: 264\n",
      "training loss: 32129794097140.26\n",
      "validation loss: 24138883118337.594\n",
      "epoch: 265\n",
      "training loss: 32124420748639.688\n",
      "validation loss: 24123685202681.44\n",
      "epoch: 266\n",
      "training loss: 32119088463296.97\n",
      "validation loss: 24108563580218.586\n",
      "epoch: 267\n",
      "training loss: 32113171667690.26\n",
      "validation loss: 24100888377030.582\n",
      "epoch: 268\n",
      "training loss: 32107806666949.57\n",
      "validation loss: 24087157070174.47\n",
      "epoch: 269\n",
      "training loss: 32102591608894.49\n",
      "validation loss: 24072267040952.207\n",
      "epoch: 270\n",
      "training loss: 32097416375901.32\n",
      "validation loss: 24057451572338.363\n",
      "epoch: 271\n",
      "training loss: 32092280657704.844\n",
      "validation loss: 24042710247105.207\n",
      "epoch: 272\n",
      "training loss: 32087184146457.0\n",
      "validation loss: 24028042650635.906\n",
      "epoch: 273\n",
      "training loss: 32082126536708.117\n",
      "validation loss: 24013448370906.625\n",
      "epoch: 274\n",
      "training loss: 32077107525388.188\n",
      "validation loss: 23998926998468.78\n",
      "epoch: 275\n",
      "training loss: 32072126811788.324\n",
      "validation loss: 23984478126431.44\n",
      "epoch: 276\n",
      "training loss: 32067184097542.438\n",
      "validation loss: 23970101350443.832\n",
      "epoch: 277\n",
      "training loss: 32062279086608.875\n",
      "validation loss: 23955796268677.99\n",
      "epoch: 278\n",
      "training loss: 32057411485252.406\n",
      "validation loss: 23941562481811.492\n",
      "epoch: 279\n",
      "training loss: 32052581002026.21\n",
      "validation loss: 23927399593010.38\n",
      "epoch: 280\n",
      "training loss: 32047787347754.05\n",
      "validation loss: 23913307207912.14\n",
      "epoch: 281\n",
      "training loss: 32043030235512.59\n",
      "validation loss: 23899284934608.86\n",
      "epoch: 282\n",
      "training loss: 32038309380613.84\n",
      "validation loss: 23885332383630.445\n",
      "epoch: 283\n",
      "training loss: 32033624500587.766\n",
      "validation loss: 23871449167928.027\n",
      "epoch: 284\n",
      "training loss: 32028975315164.97\n",
      "validation loss: 23857634902857.445\n",
      "epoch: 285\n",
      "training loss: 32024361546259.574\n",
      "validation loss: 23843889206162.836\n",
      "epoch: 286\n",
      "training loss: 32019782917952.207\n",
      "validation loss: 23830211697960.367\n",
      "epoch: 287\n",
      "training loss: 32015239156473.117\n",
      "validation loss: 23816602000722.094\n",
      "epoch: 288\n",
      "training loss: 32010729990185.418\n",
      "validation loss: 23803059739259.906\n",
      "epoch: 289\n",
      "training loss: 32006255149568.496\n",
      "validation loss: 23789584540709.594\n",
      "epoch: 290\n",
      "training loss: 32001814367201.53\n",
      "validation loss: 23776176034515.043\n",
      "epoch: 291\n",
      "training loss: 31997407377747.09\n",
      "validation loss: 23762833852412.52\n",
      "epoch: 292\n",
      "training loss: 31993033917934.934\n",
      "validation loss: 23749557628415.09\n",
      "epoch: 293\n",
      "training loss: 31988693726545.906\n",
      "validation loss: 23736346998797.14\n",
      "epoch: 294\n",
      "training loss: 31984386544395.96\n",
      "validation loss: 23723201602079.01\n",
      "epoch: 295\n",
      "training loss: 31980112114320.285\n",
      "validation loss: 23710121079011.688\n",
      "epoch: 296\n",
      "training loss: 31975870181157.61\n",
      "validation loss: 23697105072561.73\n",
      "epoch: 297\n",
      "training loss: 31971660491734.54\n",
      "validation loss: 23684153227896.168\n",
      "epoch: 298\n",
      "training loss: 31967482794850.12\n",
      "validation loss: 23671265192367.57\n",
      "epoch: 299\n",
      "training loss: 31963336841260.46\n",
      "validation loss: 23658440615499.223\n",
      "epoch: 300\n",
      "training loss: 31959222383663.46\n",
      "validation loss: 23645679148970.375\n",
      "epoch: 301\n",
      "training loss: 31955139176683.71\n",
      "validation loss: 23632980446601.66\n",
      "epoch: 302\n",
      "training loss: 31951086976857.49\n",
      "validation loss: 23620344164340.52\n",
      "epoch: 303\n",
      "training loss: 31947065542617.824\n",
      "validation loss: 23607769960246.81\n",
      "epoch: 304\n",
      "training loss: 31943074634279.758\n",
      "validation loss: 23595257494478.46\n",
      "epoch: 305\n",
      "training loss: 31939114014025.695\n",
      "validation loss: 23582806429277.3\n",
      "epoch: 306\n",
      "training loss: 31935183445890.81\n",
      "validation loss: 23570416428954.875\n",
      "epoch: 307\n",
      "training loss: 31931282695748.63\n",
      "validation loss: 23558087159878.477\n",
      "epoch: 308\n",
      "training loss: 31927411531296.74\n",
      "validation loss: 23545818290457.188\n",
      "epoch: 309\n",
      "training loss: 31923569722042.53\n",
      "validation loss: 23533609491128.055\n",
      "epoch: 310\n",
      "training loss: 31919757039289.156\n",
      "validation loss: 23521460434342.395\n",
      "epoch: 311\n",
      "training loss: 31915973256121.48\n",
      "validation loss: 23509370794552.094\n",
      "epoch: 312\n",
      "training loss: 31912218147392.21\n",
      "validation loss: 23497340248196.125\n",
      "epoch: 313\n",
      "training loss: 31908491489708.188\n",
      "validation loss: 23485368473687.07\n",
      "epoch: 314\n",
      "training loss: 31904793061416.625\n",
      "validation loss: 23473455151397.766\n",
      "epoch: 315\n",
      "training loss: 31901122642591.63\n",
      "validation loss: 23461599963648.043\n",
      "epoch: 316\n",
      "training loss: 31897480015020.69\n",
      "validation loss: 23449802594691.566\n",
      "epoch: 317\n",
      "training loss: 31893864962191.37\n",
      "validation loss: 23438062730702.715\n",
      "epoch: 318\n",
      "training loss: 31890277269278.062\n",
      "validation loss: 23426380059763.65\n",
      "epoch: 319\n",
      "training loss: 31886716723128.812\n",
      "validation loss: 23414754271851.336\n",
      "epoch: 320\n",
      "training loss: 31883183112252.293\n",
      "validation loss: 23403185058824.8\n",
      "epoch: 321\n",
      "training loss: 31879676226804.902\n",
      "validation loss: 23391672114412.35\n",
      "epoch: 322\n",
      "training loss: 31876195858577.895\n",
      "validation loss: 23380215134198.965\n",
      "epoch: 323\n",
      "training loss: 31872741800984.63\n",
      "validation loss: 23368813815613.7\n",
      "epoch: 324\n",
      "training loss: 31869313849047.977\n",
      "validation loss: 23357467857917.266\n",
      "epoch: 325\n",
      "training loss: 31865911799387.742\n",
      "validation loss: 23346176962189.633\n",
      "epoch: 326\n",
      "training loss: 31862535450208.25\n",
      "validation loss: 23334940831317.69\n",
      "epoch: 327\n",
      "training loss: 31859184601285.984\n",
      "validation loss: 23323759169983.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 328\n",
      "training loss: 31855859053957.348\n",
      "validation loss: 23312631684650.08\n",
      "epoch: 329\n",
      "training loss: 31852558611106.5\n",
      "validation loss: 23301558083553.46\n",
      "epoch: 330\n",
      "training loss: 31849283077153.316\n",
      "validation loss: 23290538076686.65\n",
      "epoch: 331\n",
      "training loss: 31846032258041.406\n",
      "validation loss: 23279571375789.777\n",
      "epoch: 332\n",
      "training loss: 31842805961226.266\n",
      "validation loss: 23268657694337.926\n",
      "epoch: 333\n",
      "training loss: 31839603995663.457\n",
      "validation loss: 23257796747529.395\n",
      "epoch: 334\n",
      "training loss: 31836426171796.965\n",
      "validation loss: 23246988252274.117\n",
      "epoch: 335\n",
      "training loss: 31833272301547.598\n",
      "validation loss: 23236231927182.145\n",
      "epoch: 336\n",
      "training loss: 31830142198301.453\n",
      "validation loss: 23225527492552.215\n",
      "epoch: 337\n",
      "training loss: 31827035676898.51\n",
      "validation loss: 23214874670360.438\n",
      "epoch: 338\n",
      "training loss: 31823952553621.34\n",
      "validation loss: 23204273184249.062\n",
      "epoch: 339\n",
      "training loss: 31820892646183.81\n",
      "validation loss: 23193722759515.438\n",
      "epoch: 340\n",
      "training loss: 31817855773719.99\n",
      "validation loss: 23183223123101.0\n",
      "epoch: 341\n",
      "training loss: 31814841756773.023\n",
      "validation loss: 23172774003580.477\n",
      "epoch: 342\n",
      "training loss: 31811850417284.24\n",
      "validation loss: 23162375131151.25\n",
      "epoch: 343\n",
      "training loss: 31808881578582.15\n",
      "validation loss: 23152026237622.9\n",
      "epoch: 344\n",
      "training loss: 31805935065371.742\n",
      "validation loss: 23141727056406.97\n",
      "epoch: 345\n",
      "training loss: 31803010703723.703\n",
      "validation loss: 23131477322507.094\n",
      "epoch: 346\n",
      "training loss: 31800108321063.785\n",
      "validation loss: 23121276772509.414\n",
      "epoch: 347\n",
      "training loss: 31797227746162.258\n",
      "validation loss: 23111125144573.51\n",
      "epoch: 348\n",
      "training loss: 31794368809123.418\n",
      "validation loss: 23101022178423.934\n",
      "epoch: 349\n",
      "training loss: 31791531341375.22\n",
      "validation loss: 23090967615342.61\n",
      "epoch: 350\n",
      "training loss: 31788715175658.96\n",
      "validation loss: 23080961198162.332\n",
      "epoch: 351\n",
      "training loss: 31785920146019.023\n",
      "validation loss: 23071002671261.906\n",
      "epoch: 352\n",
      "training loss: 31783146087792.758\n",
      "validation loss: 23061091780563.555\n",
      "epoch: 353\n",
      "training loss: 31780392837600.36\n",
      "validation loss: 23051228273533.68\n",
      "epoch: 354\n",
      "training loss: 31777660233334.95\n",
      "validation loss: 23041411899188.695\n",
      "epoch: 355\n",
      "training loss: 31774948114152.582\n",
      "validation loss: 23031642408108.547\n",
      "epoch: 356\n",
      "training loss: 31772256320462.438\n",
      "validation loss: 23021919552462.555\n",
      "epoch: 357\n",
      "training loss: 31769584693917.086\n",
      "validation loss: 23012243086055.395\n",
      "epoch: 358\n",
      "training loss: 31766933077402.73\n",
      "validation loss: 23002612764407.273\n",
      "epoch: 359\n",
      "training loss: 31764301315029.652\n",
      "validation loss: 22993028344894.9\n",
      "epoch: 360\n",
      "training loss: 31761689252122.637\n",
      "validation loss: 22983489587005.996\n",
      "epoch: 361\n",
      "training loss: 31759096735211.496\n",
      "validation loss: 22973996252818.84\n",
      "epoch: 362\n",
      "training loss: 31756523612021.67\n",
      "validation loss: 22964548107962.4\n",
      "epoch: 363\n",
      "training loss: 31753969731464.832\n",
      "validation loss: 22955144923699.24\n",
      "epoch: 364\n",
      "training loss: 31751434943629.453\n",
      "validation loss: 22945786481955.625\n",
      "epoch: 365\n",
      "training loss: 31748919099771.21\n",
      "validation loss: 22936472589372.03\n",
      "epoch: 366\n",
      "training loss: 31746422052302.14\n",
      "validation loss: 22927203125553.082\n",
      "epoch: 367\n",
      "training loss: 31743943654772.766\n",
      "validation loss: 22917978269477.44\n",
      "epoch: 368\n",
      "training loss: 31741483761768.57\n",
      "validation loss: 22908800225745.516\n",
      "epoch: 369\n",
      "training loss: 31739042214143.375\n",
      "validation loss: 22899679157733.727\n",
      "epoch: 370\n",
      "training loss: 31737860277693.117\n",
      "validation loss: 22904081000786.656\n",
      "epoch: 371\n",
      "training loss: 31716294212544.36\n",
      "validation loss: 22915589766962.652\n",
      "epoch: 372\n",
      "training loss: 31747670557099.773\n",
      "validation loss: 23172810217941.023\n",
      "epoch: 373\n",
      "training loss: 31744927336460.637\n",
      "validation loss: 23163168143391.45\n",
      "epoch: 374\n",
      "training loss: 31742191532801.855\n",
      "validation loss: 23153572243016.387\n",
      "epoch: 375\n",
      "training loss: 31722686162638.465\n",
      "validation loss: 23155629329874.37\n",
      "epoch: 376\n",
      "training loss: 31714804128819.645\n",
      "validation loss: 23149040965331.79\n",
      "epoch: 377\n",
      "training loss: 31712133832371.695\n",
      "validation loss: 23140426405763.297\n",
      "epoch: 378\n",
      "training loss: 31709738310340.133\n",
      "validation loss: 23130377979113.023\n",
      "epoch: 379\n",
      "training loss: 31700518486568.55\n",
      "validation loss: 23126159810717.855\n",
      "epoch: 380\n",
      "training loss: 31676466053173.523\n",
      "validation loss: 23125749889638.598\n",
      "epoch: 381\n",
      "training loss: 31688928338646.234\n",
      "validation loss: 23113662419610.52\n",
      "epoch: 382\n",
      "training loss: 31686003381252.195\n",
      "validation loss: 23104763670162.65\n",
      "epoch: 383\n",
      "training loss: 31650432139883.996\n",
      "validation loss: 23101415978941.395\n",
      "epoch: 384\n",
      "training loss: 31646330257632.22\n",
      "validation loss: 23093005414591.656\n",
      "epoch: 385\n",
      "training loss: 31636513944884.918\n",
      "validation loss: 23084686890405.87\n",
      "epoch: 386\n",
      "training loss: 31689298691149.82\n",
      "validation loss: 23070606522964.023\n",
      "epoch: 387\n",
      "training loss: 31686793619042.082\n",
      "validation loss: 23061799273326.01\n",
      "epoch: 388\n",
      "training loss: 31684306049345.734\n",
      "validation loss: 23053034310764.395\n",
      "epoch: 389\n",
      "training loss: 31681835846630.18\n",
      "validation loss: 23044311413739.973\n",
      "epoch: 390\n",
      "training loss: 31679382870168.42\n",
      "validation loss: 23035630361820.996\n",
      "epoch: 391\n",
      "training loss: 31676862655509.258\n",
      "validation loss: 23026991202991.914\n",
      "epoch: 392\n",
      "training loss: 31665362546142.496\n",
      "validation loss: 23020231574494.18\n",
      "epoch: 393\n",
      "training loss: 31660987524452.22\n",
      "validation loss: 23012008211727.58\n",
      "epoch: 394\n",
      "training loss: 31658579028157.453\n",
      "validation loss: 23003596727934.95\n",
      "epoch: 395\n",
      "training loss: 31656186944939.21\n",
      "validation loss: 22995225672182.195\n",
      "epoch: 396\n",
      "training loss: 31653811148074.527\n",
      "validation loss: 22986894832277.277\n",
      "epoch: 397\n",
      "training loss: 31651451511819.656\n",
      "validation loss: 22978603996935.96\n",
      "epoch: 398\n",
      "training loss: 31649107911401.805\n",
      "validation loss: 22970352955747.145\n",
      "epoch: 399\n",
      "training loss: 31646780223009.32\n",
      "validation loss: 22962141499135.668\n",
      "epoch: 400\n",
      "training loss: 31644468323772.508\n",
      "validation loss: 22953969418322.203\n",
      "epoch: 401\n",
      "training loss: 31642172091628.99\n",
      "validation loss: 22945836505280.156\n",
      "epoch: 402\n",
      "training loss: 31639891378676.723\n",
      "validation loss: 22937742552690.156\n",
      "epoch: 403\n",
      "training loss: 31638189299368.035\n",
      "validation loss: 22929688444874.965\n",
      "epoch: 404\n",
      "training loss: 31658966240830.32\n",
      "validation loss: 22903928879095.156\n",
      "epoch: 405\n",
      "training loss: 31656768093102.2\n",
      "validation loss: 22895707514148.285\n",
      "epoch: 406\n",
      "training loss: 31654585392429.555\n",
      "validation loss: 22887525007067.863\n",
      "epoch: 407\n",
      "training loss: 31652418019145.273\n",
      "validation loss: 22879381148593.85\n",
      "epoch: 408\n",
      "training loss: 31650265854509.996\n",
      "validation loss: 22871275729853.816\n",
      "epoch: 409\n",
      "training loss: 31648128780704.87\n",
      "validation loss: 22863208542297.15\n",
      "epoch: 410\n",
      "training loss: 31646006680824.516\n",
      "validation loss: 22855179377632.516\n",
      "epoch: 411\n",
      "training loss: 31643899438869.82\n",
      "validation loss: 22847188027773.582\n",
      "epoch: 412\n",
      "training loss: 31641806939741.01\n",
      "validation loss: 22839234284801.69\n",
      "epoch: 413\n",
      "training loss: 31639729069230.633\n",
      "validation loss: 22831317940959.082\n",
      "epoch: 414\n",
      "training loss: 31637665714016.65\n",
      "validation loss: 22823438788695.49\n",
      "epoch: 415\n",
      "training loss: 31635616761655.543\n",
      "validation loss: 22815596620805.355\n",
      "epoch: 416\n",
      "training loss: 31633582100575.55\n",
      "validation loss: 22807791230718.973\n",
      "epoch: 417\n",
      "training loss: 31631561620069.875\n",
      "validation loss: 22800022413056.445\n",
      "epoch: 418\n",
      "training loss: 31629555210289.953\n",
      "validation loss: 22792289964639.574\n",
      "epoch: 419\n",
      "training loss: 31627562762238.8\n",
      "validation loss: 22784593686323.664\n",
      "epoch: 420\n",
      "training loss: 31625584167764.37\n",
      "validation loss: 22776933386358.57\n",
      "epoch: 421\n",
      "training loss: 31623619319552.9\n",
      "validation loss: 22769308886758.67\n",
      "epoch: 422\n",
      "training loss: 31621668111122.145\n",
      "validation loss: 22761720036027.625\n",
      "epoch: 423\n",
      "training loss: 31619730436814.176\n",
      "validation loss: 22754166736595.695\n",
      "epoch: 424\n",
      "training loss: 31617806191785.863\n",
      "validation loss: 22746649010511.516\n",
      "epoch: 425\n",
      "training loss: 31615895271982.31\n",
      "validation loss: 22739167178105.957\n",
      "epoch: 426\n",
      "training loss: 31613997573751.25\n",
      "validation loss: 22731722359311.438\n",
      "epoch: 427\n",
      "training loss: 31612106902407.87\n",
      "validation loss: 22724316110909.973\n",
      "epoch: 428\n",
      "training loss: 31616716470135.57\n",
      "validation loss: 22680365142882.41\n",
      "epoch: 429\n",
      "training loss: 31614874416424.535\n",
      "validation loss: 22673034256879.094\n",
      "epoch: 430\n",
      "training loss: 31613045074905.074\n",
      "validation loss: 22665737870490.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 431\n",
      "training loss: 31611228347064.418\n",
      "validation loss: 22658475845013.934\n",
      "epoch: 432\n",
      "training loss: 31609424135153.406\n",
      "validation loss: 22651248079592.125\n",
      "epoch: 433\n",
      "training loss: 31607632342179.53\n",
      "validation loss: 22644054576073.195\n",
      "epoch: 434\n",
      "training loss: 31605852871896.746\n",
      "validation loss: 22636895710502.047\n",
      "epoch: 435\n",
      "training loss: 31604085628762.848\n",
      "validation loss: 22629774227782.535\n",
      "epoch: 436\n",
      "training loss: 31602330514558.35\n",
      "validation loss: 22622787832172.64\n",
      "epoch: 437\n",
      "training loss: 31600553146196.133\n",
      "validation loss: 22616121558733.39\n",
      "epoch: 438\n",
      "training loss: 31598821961913.344\n",
      "validation loss: 22609094542889.08\n",
      "epoch: 439\n",
      "training loss: 31597102630692.95\n",
      "validation loss: 22602100438615.37\n",
      "epoch: 440\n",
      "training loss: 31595395060695.598\n",
      "validation loss: 22595139080720.902\n",
      "epoch: 441\n",
      "training loss: 31593699160785.05\n",
      "validation loss: 22588210304890.355\n",
      "epoch: 442\n",
      "training loss: 31592014840531.973\n",
      "validation loss: 22581313947734.75\n",
      "epoch: 443\n",
      "training loss: 31590342010208.438\n",
      "validation loss: 22574449846785.61\n",
      "epoch: 444\n",
      "training loss: 31588680580782.477\n",
      "validation loss: 22567617840489.14\n",
      "epoch: 445\n",
      "training loss: 31587030463912.72\n",
      "validation loss: 22560817768200.508\n",
      "epoch: 446\n",
      "training loss: 31585391571942.973\n",
      "validation loss: 22554049470178.09\n",
      "epoch: 447\n",
      "training loss: 31583763817896.96\n",
      "validation loss: 22547312787577.773\n",
      "epoch: 448\n",
      "training loss: 31582147115473.047\n",
      "validation loss: 22540607562447.34\n",
      "epoch: 449\n",
      "training loss: 31580541379038.965\n",
      "validation loss: 22533933637720.848\n",
      "epoch: 450\n",
      "training loss: 31578946523626.67\n",
      "validation loss: 22527290857213.06\n",
      "epoch: 451\n",
      "training loss: 31577362464927.15\n",
      "validation loss: 22520679065613.902\n",
      "epoch: 452\n",
      "training loss: 31575789119285.35\n",
      "validation loss: 22514098108482.996\n",
      "epoch: 453\n",
      "training loss: 31574226403695.074\n",
      "validation loss: 22507547832244.176\n",
      "epoch: 454\n",
      "training loss: 31572674235793.945\n",
      "validation loss: 22501028084180.074\n",
      "epoch: 455\n",
      "training loss: 31571132533858.473\n",
      "validation loss: 22494538712426.734\n",
      "epoch: 456\n",
      "training loss: 31569601216798.984\n",
      "validation loss: 22488079565968.277\n",
      "epoch: 457\n",
      "training loss: 31568080204154.85\n",
      "validation loss: 22481650494631.574\n",
      "epoch: 458\n",
      "training loss: 31566569416089.473\n",
      "validation loss: 22475251349080.965\n",
      "epoch: 459\n",
      "training loss: 31565068773385.54\n",
      "validation loss: 22468881980813.023\n",
      "epoch: 460\n",
      "training loss: 31563578197440.176\n",
      "validation loss: 22462542242151.36\n",
      "epoch: 461\n",
      "training loss: 31562097610260.19\n",
      "validation loss: 22456231986241.434\n",
      "epoch: 462\n",
      "training loss: 31560626934457.34\n",
      "validation loss: 22449951067045.418\n",
      "epoch: 463\n",
      "training loss: 31559166093243.645\n",
      "validation loss: 22443699339337.09\n",
      "epoch: 464\n",
      "training loss: 31557715010426.74\n",
      "validation loss: 22437476658696.78\n",
      "epoch: 465\n",
      "training loss: 31556273610405.24\n",
      "validation loss: 22431282881506.305\n",
      "epoch: 466\n",
      "training loss: 31554841818164.152\n",
      "validation loss: 22425117864943.977\n",
      "epoch: 467\n",
      "training loss: 31553419559270.37\n",
      "validation loss: 22418981466979.656\n",
      "epoch: 468\n",
      "training loss: 31552006759868.094\n",
      "validation loss: 22412873546369.742\n",
      "epoch: 469\n",
      "training loss: 31550603346674.39\n",
      "validation loss: 22406793962652.344\n",
      "epoch: 470\n",
      "training loss: 31549209246974.773\n",
      "validation loss: 22400742576142.35\n",
      "epoch: 471\n",
      "training loss: 31547824388618.754\n",
      "validation loss: 22394719247926.61\n",
      "epoch: 472\n",
      "training loss: 31546448700015.477\n",
      "validation loss: 22388723839859.08\n",
      "epoch: 473\n",
      "training loss: 31545082110129.387\n",
      "validation loss: 22382756214556.094\n",
      "epoch: 474\n",
      "training loss: 31543724548475.914\n",
      "validation loss: 22376816235391.56\n",
      "epoch: 475\n",
      "training loss: 31542375945117.203\n",
      "validation loss: 22370903766492.25\n",
      "epoch: 476\n",
      "training loss: 31541036230657.895\n",
      "validation loss: 22365018672733.12\n",
      "epoch: 477\n",
      "training loss: 31539705336240.883\n",
      "validation loss: 22359160819732.6\n",
      "epoch: 478\n",
      "training loss: 31538383193543.152\n",
      "validation loss: 22353330073847.996\n",
      "epoch: 479\n",
      "training loss: 31537069734771.65\n",
      "validation loss: 22347526302170.836\n",
      "epoch: 480\n",
      "training loss: 31535764892659.145\n",
      "validation loss: 22341749372522.312\n",
      "epoch: 481\n",
      "training loss: 31534468600460.184\n",
      "validation loss: 22335999153448.7\n",
      "epoch: 482\n",
      "training loss: 31533180791947.04\n",
      "validation loss: 22330275514216.836\n",
      "epoch: 483\n",
      "training loss: 31531901401405.656\n",
      "validation loss: 22324578324809.582\n",
      "epoch: 484\n",
      "training loss: 31530630363631.727\n",
      "validation loss: 22318907455921.355\n",
      "epoch: 485\n",
      "training loss: 31529367613926.664\n",
      "validation loss: 22313262778953.67\n",
      "epoch: 486\n",
      "training loss: 31528113088093.766\n",
      "validation loss: 22307644166010.668\n",
      "epoch: 487\n",
      "training loss: 31526866722434.26\n",
      "validation loss: 22302051489894.707\n",
      "epoch: 488\n",
      "training loss: 31525628453743.445\n",
      "validation loss: 22296484624101.945\n",
      "epoch: 489\n",
      "training loss: 31524398219306.895\n",
      "validation loss: 22290943442817.99\n",
      "epoch: 490\n",
      "training loss: 31523175956896.617\n",
      "validation loss: 22285427820913.484\n",
      "epoch: 491\n",
      "training loss: 31521961604767.312\n",
      "validation loss: 22279937633939.785\n",
      "epoch: 492\n",
      "training loss: 31520755101652.598\n",
      "validation loss: 22274472758124.586\n",
      "epoch: 493\n",
      "training loss: 31519556386761.33\n",
      "validation loss: 22269033070367.633\n",
      "epoch: 494\n",
      "training loss: 31518365399773.883\n",
      "validation loss: 22263618448236.363\n",
      "epoch: 495\n",
      "training loss: 31517182080838.52\n",
      "validation loss: 22258228769961.62\n",
      "epoch: 496\n",
      "training loss: 31516006370567.734\n",
      "validation loss: 22252863914433.324\n",
      "epoch: 497\n",
      "training loss: 31514838210034.68\n",
      "validation loss: 22247523761196.16\n",
      "epoch: 498\n",
      "training loss: 31513677540769.59\n",
      "validation loss: 22242208190445.27\n",
      "epoch: 499\n",
      "training loss: 31512524304756.207\n",
      "validation loss: 22236917083021.918\n",
      "epoch: 500\n",
      "training loss: 31511378444428.3\n",
      "validation loss: 22231650320409.17\n",
      "epoch: 501\n",
      "training loss: 31510239902666.16\n",
      "validation loss: 22226407784727.527\n",
      "epoch: 502\n",
      "training loss: 31509108622793.117\n",
      "validation loss: 22221189358730.53\n",
      "epoch: 503\n",
      "training loss: 31507984548572.145\n",
      "validation loss: 22215994925800.35\n",
      "epoch: 504\n",
      "training loss: 31506867624202.414\n",
      "validation loss: 22210824369943.324\n",
      "epoch: 505\n",
      "training loss: 31505757794315.938\n",
      "validation loss: 22205677575785.438\n",
      "epoch: 506\n",
      "training loss: 31504655003974.168\n",
      "validation loss: 22200554428567.715\n",
      "epoch: 507\n",
      "training loss: 31503559198664.746\n",
      "validation loss: 22195454814141.574\n",
      "epoch: 508\n",
      "training loss: 31502470324298.098\n",
      "validation loss: 22190378618964.02\n",
      "epoch: 509\n",
      "training loss: 31501388327204.25\n",
      "validation loss: 22185325730092.758\n",
      "epoch: 510\n",
      "training loss: 31500313154129.49\n",
      "validation loss: 22180296035181.086\n",
      "epoch: 511\n",
      "training loss: 31499244752233.203\n",
      "validation loss: 22175289422472.707\n",
      "epoch: 512\n",
      "training loss: 31498183069084.633\n",
      "validation loss: 22170305780796.184\n",
      "epoch: 513\n",
      "training loss: 31497128052659.715\n",
      "validation loss: 22165344999559.207\n",
      "epoch: 514\n",
      "training loss: 31496079651337.92\n",
      "validation loss: 22160406968742.48\n",
      "epoch: 515\n",
      "training loss: 31495037813899.13\n",
      "validation loss: 22155491578893.184\n",
      "epoch: 516\n",
      "training loss: 31494002489520.508\n",
      "validation loss: 22150598721117.953\n",
      "epoch: 517\n",
      "training loss: 31492973627773.45\n",
      "validation loss: 22145728287075.17\n",
      "epoch: 518\n",
      "training loss: 31491951178620.51\n",
      "validation loss: 22140880168966.535\n",
      "epoch: 519\n",
      "training loss: 31490935092412.37\n",
      "validation loss: 22136054259527.566\n",
      "epoch: 520\n",
      "training loss: 31489925319884.832\n",
      "validation loss: 22131250452016.97\n",
      "epoch: 521\n",
      "training loss: 31488921812155.812\n",
      "validation loss: 22126468640204.33\n",
      "epoch: 522\n",
      "training loss: 31487924520722.395\n",
      "validation loss: 22121708718355.965\n",
      "epoch: 523\n",
      "training loss: 31486933397457.902\n",
      "validation loss: 22116970581218.1\n",
      "epoch: 524\n",
      "training loss: 31485948394608.957\n",
      "validation loss: 22112254123996.824\n",
      "epoch: 525\n",
      "training loss: 31484969464792.586\n",
      "validation loss: 22107559242333.68\n",
      "epoch: 526\n",
      "training loss: 31483996560993.348\n",
      "validation loss: 22102885832275.64\n",
      "epoch: 527\n",
      "training loss: 31483029636560.504\n",
      "validation loss: 22098233790237.344\n",
      "epoch: 528\n",
      "training loss: 31482068645205.176\n",
      "validation loss: 22093603012953.047\n",
      "epoch: 529\n",
      "training loss: 31481113540997.523\n",
      "validation loss: 22088993397414.344\n",
      "epoch: 530\n",
      "training loss: 31480164278363.965\n",
      "validation loss: 22084404840787.918\n",
      "epoch: 531\n",
      "training loss: 31479220812084.445\n",
      "validation loss: 22079837240305.0\n",
      "epoch: 532\n",
      "training loss: 31478283097289.66\n",
      "validation loss: 22075290493109.656\n",
      "epoch: 533\n",
      "training loss: 31477351089458.336\n",
      "validation loss: 22070764496045.957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 534\n",
      "training loss: 31476424744414.54\n",
      "validation loss: 22066259145352.234\n",
      "epoch: 535\n",
      "training loss: 31475504018325.0\n",
      "validation loss: 22061774336209.6\n",
      "epoch: 536\n",
      "training loss: 31474588867696.45\n",
      "validation loss: 22057309962054.83\n",
      "epoch: 537\n",
      "training loss: 31473679249372.965\n",
      "validation loss: 22052865913497.023\n",
      "epoch: 538\n",
      "training loss: 31472775120533.35\n",
      "validation loss: 22048442076538.293\n",
      "epoch: 539\n",
      "training loss: 31471876438688.55\n",
      "validation loss: 22044038329505.395\n",
      "epoch: 540\n",
      "training loss: 31470983161679.0\n",
      "validation loss: 22039654537437.375\n",
      "epoch: 541\n",
      "training loss: 31470095247672.043\n",
      "validation loss: 22035290541042.945\n",
      "epoch: 542\n",
      "training loss: 31469212655159.207\n",
      "validation loss: 22030946132857.375\n",
      "epoch: 543\n",
      "training loss: 31468335342953.258\n",
      "validation loss: 22026620999042.316\n",
      "epoch: 544\n",
      "training loss: 31467463270184.082\n",
      "validation loss: 22022314551017.69\n",
      "epoch: 545\n",
      "training loss: 31466596396288.09\n",
      "validation loss: 22018025298969.574\n",
      "epoch: 546\n",
      "training loss: 31465734680923.18\n",
      "validation loss: 22013747344248.41\n",
      "epoch: 547\n",
      "training loss: 31464878074146.195\n",
      "validation loss: 22009438064794.46\n",
      "epoch: 548\n",
      "training loss: 31464036685178.508\n",
      "validation loss: 22004651605474.457\n",
      "epoch: 549\n",
      "training loss: 31463190225155.91\n",
      "validation loss: 22000444966389.566\n",
      "epoch: 550\n",
      "training loss: 31462348765794.91\n",
      "validation loss: 21996257494935.844\n",
      "epoch: 551\n",
      "training loss: 31461512268398.18\n",
      "validation loss: 21992089100916.84\n",
      "epoch: 552\n",
      "training loss: 31460680694568.547\n",
      "validation loss: 21987939694608.715\n",
      "epoch: 553\n",
      "training loss: 31459854006206.7\n",
      "validation loss: 21983809186757.45\n",
      "epoch: 554\n",
      "training loss: 31459032165508.85\n",
      "validation loss: 21979697488576.05\n",
      "epoch: 555\n",
      "training loss: 31458215134964.45\n",
      "validation loss: 21975604511741.824\n",
      "epoch: 556\n",
      "training loss: 31457402877353.91\n",
      "validation loss: 21971530168393.633\n",
      "epoch: 557\n",
      "training loss: 31456595355746.355\n",
      "validation loss: 21967474371129.188\n",
      "epoch: 558\n",
      "training loss: 31455792533497.406\n",
      "validation loss: 21963437033002.38\n",
      "epoch: 559\n",
      "training loss: 31454994374246.887\n",
      "validation loss: 21959418067520.53\n",
      "epoch: 560\n",
      "training loss: 31454200841916.69\n",
      "validation loss: 21955417388641.8\n",
      "epoch: 561\n",
      "training loss: 31453411900708.566\n",
      "validation loss: 21951434910772.53\n",
      "epoch: 562\n",
      "training loss: 31452627515101.92\n",
      "validation loss: 21947470548764.574\n",
      "epoch: 563\n",
      "training loss: 31451847649851.734\n",
      "validation loss: 21943524217912.746\n",
      "epoch: 564\n",
      "training loss: 31451072269986.34\n",
      "validation loss: 21939595833952.19\n",
      "epoch: 565\n",
      "training loss: 31450301340805.38\n",
      "validation loss: 21935685313055.824\n",
      "epoch: 566\n",
      "training loss: 31449534827877.625\n",
      "validation loss: 21931792571831.773\n",
      "epoch: 567\n",
      "training loss: 31448772697038.96\n",
      "validation loss: 21927917527320.83\n",
      "epoch: 568\n",
      "training loss: 31448014914390.266\n",
      "validation loss: 21924060096993.918\n",
      "epoch: 569\n",
      "training loss: 31447261446295.363\n",
      "validation loss: 21920220198749.61\n",
      "epoch: 570\n",
      "training loss: 31446512259378.996\n",
      "validation loss: 21916397750911.6\n",
      "epoch: 571\n",
      "training loss: 31445767320524.81\n",
      "validation loss: 21912592672226.26\n",
      "epoch: 572\n",
      "training loss: 31445026596873.285\n",
      "validation loss: 21908804881860.145\n",
      "epoch: 573\n",
      "training loss: 31444290055819.81\n",
      "validation loss: 21905034299397.586\n",
      "epoch: 574\n",
      "training loss: 31443557665012.652\n",
      "validation loss: 21901280844838.23\n",
      "epoch: 575\n",
      "training loss: 31442829392350.996\n",
      "validation loss: 21897544438594.63\n",
      "epoch: 576\n",
      "training loss: 31442105205982.96\n",
      "validation loss: 21893825001489.867\n",
      "epoch: 577\n",
      "training loss: 31441385074303.58\n",
      "validation loss: 21890122454755.152\n",
      "epoch: 578\n",
      "training loss: 31440668965952.72\n",
      "validation loss: 21886436720027.465\n",
      "epoch: 579\n",
      "training loss: 31439956849812.63\n",
      "validation loss: 21882767719347.184\n",
      "epoch: 580\n",
      "training loss: 31439248695004.02\n",
      "validation loss: 21879115375155.777\n",
      "epoch: 581\n",
      "training loss: 31438544470873.07\n",
      "validation loss: 21875479610293.44\n",
      "epoch: 582\n",
      "training loss: 31437844146852.477\n",
      "validation loss: 21871860347996.754\n",
      "epoch: 583\n",
      "training loss: 31437147632238.2\n",
      "validation loss: 21868257511895.75\n",
      "epoch: 584\n",
      "training loss: 31436425384831.18\n",
      "validation loss: 21866030706599.527\n",
      "epoch: 585\n",
      "training loss: 31435390254990.98\n",
      "validation loss: 21865037775167.96\n",
      "epoch: 586\n",
      "training loss: 31434704947087.535\n",
      "validation loss: 21861483284085.387\n",
      "epoch: 587\n",
      "training loss: 31434023393976.477\n",
      "validation loss: 21857944851962.13\n",
      "epoch: 588\n",
      "training loss: 31433345566816.16\n",
      "validation loss: 21854422471231.94\n",
      "epoch: 589\n",
      "training loss: 31432671436797.406\n",
      "validation loss: 21850916067831.977\n",
      "epoch: 590\n",
      "training loss: 31432000975334.465\n",
      "validation loss: 21847425568078.004\n",
      "epoch: 591\n",
      "training loss: 31431334154063.363\n",
      "validation loss: 21843950898662.227\n",
      "epoch: 592\n",
      "training loss: 31430670944840.117\n",
      "validation loss: 21840491986651.133\n",
      "epoch: 593\n",
      "training loss: 31430011319739.09\n",
      "validation loss: 21837048759483.355\n",
      "epoch: 594\n",
      "training loss: 31429355251051.223\n",
      "validation loss: 21833621144967.49\n",
      "epoch: 595\n",
      "training loss: 31428702711282.42\n",
      "validation loss: 21830209071280.02\n",
      "epoch: 596\n",
      "training loss: 31428053673151.867\n",
      "validation loss: 21826812466963.203\n",
      "epoch: 597\n",
      "training loss: 31427408109590.355\n",
      "validation loss: 21823431260922.94\n",
      "epoch: 598\n",
      "training loss: 31426765993738.645\n",
      "validation loss: 21820065382426.734\n",
      "epoch: 599\n",
      "training loss: 31426127298945.863\n",
      "validation loss: 21816714761101.61\n",
      "epoch: 600\n",
      "training loss: 31425491998767.85\n",
      "validation loss: 21813379326932.023\n",
      "epoch: 601\n",
      "training loss: 31424860066965.598\n",
      "validation loss: 21810059010257.89\n",
      "epoch: 602\n",
      "training loss: 31424231477503.61\n",
      "validation loss: 21806753741772.492\n",
      "epoch: 603\n",
      "training loss: 31423606204548.37\n",
      "validation loss: 21803463452520.49\n",
      "epoch: 604\n",
      "training loss: 31422984222466.734\n",
      "validation loss: 21800188073895.9\n",
      "epoch: 605\n",
      "training loss: 31422365505824.42\n",
      "validation loss: 21796927537640.156\n",
      "epoch: 606\n",
      "training loss: 31421750029384.418\n",
      "validation loss: 21793681775840.062\n",
      "epoch: 607\n",
      "training loss: 31421137768105.5\n",
      "validation loss: 21790450720925.88\n",
      "epoch: 608\n",
      "training loss: 31420528697140.684\n",
      "validation loss: 21787234305669.316\n",
      "epoch: 609\n",
      "training loss: 31419922791835.715\n",
      "validation loss: 21784032463181.668\n",
      "epoch: 610\n",
      "training loss: 31419320027727.59\n",
      "validation loss: 21780845126911.816\n",
      "epoch: 611\n",
      "training loss: 31418720380543.094\n",
      "validation loss: 21777672230644.348\n",
      "epoch: 612\n",
      "training loss: 31418123826197.25\n",
      "validation loss: 21774513708497.65\n",
      "epoch: 613\n",
      "training loss: 31417530340791.953\n",
      "validation loss: 21771369494922.008\n",
      "epoch: 614\n",
      "training loss: 31416939900614.46\n",
      "validation loss: 21768239524697.723\n",
      "epoch: 615\n",
      "training loss: 31416352482135.984\n",
      "validation loss: 21765123732933.266\n",
      "epoch: 616\n",
      "training loss: 31415768062010.242\n",
      "validation loss: 21762022055063.387\n",
      "epoch: 617\n",
      "training loss: 31415186617072.055\n",
      "validation loss: 21758934426847.3\n",
      "epoch: 618\n",
      "training loss: 31414608124335.94\n",
      "validation loss: 21755860784366.844\n",
      "epoch: 619\n",
      "training loss: 31414032560994.71\n",
      "validation loss: 21752801064024.652\n",
      "epoch: 620\n",
      "training loss: 31413459904418.117\n",
      "validation loss: 21749755202542.344\n",
      "epoch: 621\n",
      "training loss: 31412890132151.426\n",
      "validation loss: 21746723136958.734\n",
      "epoch: 622\n",
      "training loss: 31412323221914.125\n",
      "validation loss: 21743704804628.035\n",
      "epoch: 623\n",
      "training loss: 31411759151598.504\n",
      "validation loss: 21740700143218.11\n",
      "epoch: 624\n",
      "training loss: 31411197899268.37\n",
      "validation loss: 21737709090708.656\n",
      "epoch: 625\n",
      "training loss: 31410639443157.668\n",
      "validation loss: 21734731585389.49\n",
      "epoch: 626\n",
      "training loss: 31410083761669.22\n",
      "validation loss: 21731767565858.81\n",
      "epoch: 627\n",
      "training loss: 31409530833373.348\n",
      "validation loss: 21728816971021.43\n",
      "epoch: 628\n",
      "training loss: 31408980637006.633\n",
      "validation loss: 21725879740087.094\n",
      "epoch: 629\n",
      "training loss: 31408433151470.58\n",
      "validation loss: 21722955812568.74\n",
      "epoch: 630\n",
      "training loss: 31407888355830.37\n",
      "validation loss: 21720045128280.83\n",
      "epoch: 631\n",
      "training loss: 31407346229313.6\n",
      "validation loss: 21717147627337.617\n",
      "epoch: 632\n",
      "training loss: 31406806751308.965\n",
      "validation loss: 21714263250151.51\n",
      "epoch: 633\n",
      "training loss: 31406269901365.062\n",
      "validation loss: 21711391937431.395\n",
      "epoch: 634\n",
      "training loss: 31405735659189.152\n",
      "validation loss: 21708533630180.96\n",
      "epoch: 635\n",
      "training loss: 31405204004645.895\n",
      "validation loss: 21705688269697.074\n",
      "epoch: 636\n",
      "training loss: 31404674917756.137\n",
      "validation loss: 21702855797568.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 637\n",
      "training loss: 31404148378695.73\n",
      "validation loss: 21700036155672.4\n",
      "epoch: 638\n",
      "training loss: 31403624367794.29\n",
      "validation loss: 21697229286176.504\n",
      "epoch: 639\n",
      "training loss: 31403102865534.023\n",
      "validation loss: 21694435131533.7\n",
      "epoch: 640\n",
      "training loss: 31402583852548.56\n",
      "validation loss: 21691653634482.36\n",
      "epoch: 641\n",
      "training loss: 31402067309621.727\n",
      "validation loss: 21688884738044.332\n",
      "epoch: 642\n",
      "training loss: 31401553217686.445\n",
      "validation loss: 21686128385523.414\n",
      "epoch: 643\n",
      "training loss: 31401041557823.53\n",
      "validation loss: 21683384520503.75\n",
      "epoch: 644\n",
      "training loss: 31400532311260.562\n",
      "validation loss: 21680653086848.277\n",
      "epoch: 645\n",
      "training loss: 31400025459370.746\n",
      "validation loss: 21677934028697.22\n",
      "epoch: 646\n",
      "training loss: 31399520983671.785\n",
      "validation loss: 21675227290466.492\n",
      "epoch: 647\n",
      "training loss: 31399018865824.746\n",
      "validation loss: 21672532816846.21\n",
      "epoch: 648\n",
      "training loss: 31398519087632.96\n",
      "validation loss: 21669850552799.168\n",
      "epoch: 649\n",
      "training loss: 31398021631040.918\n",
      "validation loss: 21667180443559.31\n",
      "epoch: 650\n",
      "training loss: 31397526478133.17\n",
      "validation loss: 21664522434630.246\n",
      "epoch: 651\n",
      "training loss: 31397033611133.25\n",
      "validation loss: 21661876471783.777\n",
      "epoch: 652\n",
      "training loss: 31396543012402.58\n",
      "validation loss: 21659242501058.383\n",
      "epoch: 653\n",
      "training loss: 31396054664439.42\n",
      "validation loss: 21656620468757.77\n",
      "epoch: 654\n",
      "training loss: 31395568549877.78\n",
      "validation loss: 21654010321449.387\n",
      "epoch: 655\n",
      "training loss: 31395084651486.418\n",
      "validation loss: 21651412005963.02\n",
      "epoch: 656\n",
      "training loss: 31394602952167.72\n",
      "validation loss: 21648825469389.293\n",
      "epoch: 657\n",
      "training loss: 31394123434956.74\n",
      "validation loss: 21646250659078.254\n",
      "epoch: 658\n",
      "training loss: 31393646083020.1\n",
      "validation loss: 21643687522637.953\n",
      "epoch: 659\n",
      "training loss: 31393170879655.035\n",
      "validation loss: 21641136007933.023\n",
      "epoch: 660\n",
      "training loss: 31392697808288.344\n",
      "validation loss: 21638596063083.26\n",
      "epoch: 661\n",
      "training loss: 31392226852475.395\n",
      "validation loss: 21636067636462.242\n",
      "epoch: 662\n",
      "training loss: 31391757995899.1\n",
      "validation loss: 21633550676695.91\n",
      "epoch: 663\n",
      "training loss: 31391291222368.97\n",
      "validation loss: 21631045132661.2\n",
      "epoch: 664\n",
      "training loss: 31390826515820.105\n",
      "validation loss: 21628550953484.684\n",
      "epoch: 665\n",
      "training loss: 31390363860312.23\n",
      "validation loss: 21626068088541.156\n",
      "epoch: 666\n",
      "training loss: 31389903240028.727\n",
      "validation loss: 21623596487452.336\n",
      "epoch: 667\n",
      "training loss: 31389444639275.676\n",
      "validation loss: 21621136100085.47\n",
      "epoch: 668\n",
      "training loss: 31388988042480.895\n",
      "validation loss: 21618686876552.01\n",
      "epoch: 669\n",
      "training loss: 31388533434193.008\n",
      "validation loss: 21616248767206.285\n",
      "epoch: 670\n",
      "training loss: 31388080799080.492\n",
      "validation loss: 21613821722644.156\n",
      "epoch: 671\n",
      "training loss: 31387630121930.773\n",
      "validation loss: 21611405693701.71\n",
      "epoch: 672\n",
      "training loss: 31387181387649.28\n",
      "validation loss: 21609000631453.965\n",
      "epoch: 673\n",
      "training loss: 31386734581258.53\n",
      "validation loss: 21606606487213.555\n",
      "epoch: 674\n",
      "training loss: 31386289687897.246\n",
      "validation loss: 21604223212529.42\n",
      "epoch: 675\n",
      "training loss: 31385846692819.43\n",
      "validation loss: 21601850759185.566\n",
      "epoch: 676\n",
      "training loss: 31385405581393.465\n",
      "validation loss: 21599489079199.74\n",
      "epoch: 677\n",
      "training loss: 31384966339101.25\n",
      "validation loss: 21597138124822.18\n",
      "epoch: 678\n",
      "training loss: 31384528951537.305\n",
      "validation loss: 21594797848534.375\n",
      "epoch: 679\n",
      "training loss: 31384093404407.89\n",
      "validation loss: 21592468203047.766\n",
      "epoch: 680\n",
      "training loss: 31383659683530.184\n",
      "validation loss: 21590149141302.54\n",
      "epoch: 681\n",
      "training loss: 31383227774831.344\n",
      "validation loss: 21587840616466.367\n",
      "epoch: 682\n",
      "training loss: 31382797664347.727\n",
      "validation loss: 21585542581933.176\n",
      "epoch: 683\n",
      "training loss: 31382369338224.01\n",
      "validation loss: 21583254991321.94\n",
      "epoch: 684\n",
      "training loss: 31381942782712.344\n",
      "validation loss: 21580977798475.445\n",
      "epoch: 685\n",
      "training loss: 31381517984171.53\n",
      "validation loss: 21578710957459.086\n",
      "epoch: 686\n",
      "training loss: 31381094929066.207\n",
      "validation loss: 21576454422559.664\n",
      "epoch: 687\n",
      "training loss: 31380673603965.99\n",
      "validation loss: 21574208148284.2\n",
      "epoch: 688\n",
      "training loss: 31380253995544.703\n",
      "validation loss: 21571972089358.734\n",
      "epoch: 689\n",
      "training loss: 31379836090579.543\n",
      "validation loss: 21569746200727.17\n",
      "epoch: 690\n",
      "training loss: 31379419875950.285\n",
      "validation loss: 21567530437550.07\n",
      "epoch: 691\n",
      "training loss: 31379005338638.46\n",
      "validation loss: 21565324755203.496\n",
      "epoch: 692\n",
      "training loss: 31378592465726.645\n",
      "validation loss: 21563129109277.88\n",
      "epoch: 693\n",
      "training loss: 31378181244397.58\n",
      "validation loss: 21560943455576.844\n",
      "epoch: 694\n",
      "training loss: 31377771661933.445\n",
      "validation loss: 21558767750116.06\n",
      "epoch: 695\n",
      "training loss: 31377363705715.09\n",
      "validation loss: 21556601949122.11\n",
      "epoch: 696\n",
      "training loss: 31376957363221.258\n",
      "validation loss: 21554446009031.37\n",
      "epoch: 697\n",
      "training loss: 31376552622027.812\n",
      "validation loss: 21552299886488.875\n",
      "epoch: 698\n",
      "training loss: 31376149469807.027\n",
      "validation loss: 21550163538347.188\n",
      "epoch: 699\n",
      "training loss: 31375747894326.773\n",
      "validation loss: 21548036921665.316\n",
      "epoch: 700\n",
      "training loss: 31375347883449.844\n",
      "validation loss: 21545919993707.59\n",
      "epoch: 701\n",
      "training loss: 31374949425133.188\n",
      "validation loss: 21543812711942.574\n",
      "epoch: 702\n",
      "training loss: 31374552507427.152\n",
      "validation loss: 21541715034041.97\n",
      "epoch: 703\n",
      "training loss: 31374157118474.83\n",
      "validation loss: 21539626917879.527\n",
      "epoch: 704\n",
      "training loss: 31373763246511.24\n",
      "validation loss: 21537548321529.973\n",
      "epoch: 705\n",
      "training loss: 31373370879862.707\n",
      "validation loss: 21535479203267.94\n",
      "epoch: 706\n",
      "training loss: 31372980006946.13\n",
      "validation loss: 21533419521566.93\n",
      "epoch: 707\n",
      "training loss: 31372590616268.234\n",
      "validation loss: 21531369235098.17\n",
      "epoch: 708\n",
      "training loss: 31372202696424.902\n",
      "validation loss: 21529328302729.676\n",
      "epoch: 709\n",
      "training loss: 31371816236100.52\n",
      "validation loss: 21527296683525.1\n",
      "epoch: 710\n",
      "training loss: 31371431224067.24\n",
      "validation loss: 21525274336742.766\n",
      "epoch: 711\n",
      "training loss: 31371047649184.312\n",
      "validation loss: 21523261221834.6\n",
      "epoch: 712\n",
      "training loss: 31370665500397.418\n",
      "validation loss: 21521257298445.117\n",
      "epoch: 713\n",
      "training loss: 31370284766737.99\n",
      "validation loss: 21519262526410.38\n",
      "epoch: 714\n",
      "training loss: 31369905437322.555\n",
      "validation loss: 21517276865757.008\n",
      "epoch: 715\n",
      "training loss: 31369527501352.09\n",
      "validation loss: 21515300276701.168\n",
      "epoch: 716\n",
      "training loss: 31369150948111.305\n",
      "validation loss: 21513332719647.54\n",
      "epoch: 717\n",
      "training loss: 31368775766968.08\n",
      "validation loss: 21511374155188.355\n",
      "epoch: 718\n",
      "training loss: 31368401947372.76\n",
      "validation loss: 21509424544102.4\n",
      "epoch: 719\n",
      "training loss: 31368029478857.51\n",
      "validation loss: 21507483847353.992\n",
      "epoch: 720\n",
      "training loss: 31367658351035.766\n",
      "validation loss: 21505552026092.062\n",
      "epoch: 721\n",
      "training loss: 31367288553601.457\n",
      "validation loss: 21503629041649.13\n",
      "epoch: 722\n",
      "training loss: 31366920076328.527\n",
      "validation loss: 21501714855540.363\n",
      "epoch: 723\n",
      "training loss: 31366552909070.21\n",
      "validation loss: 21499809429462.6\n",
      "epoch: 724\n",
      "training loss: 31366187041758.496\n",
      "validation loss: 21497912725293.41\n",
      "epoch: 725\n",
      "training loss: 31365822464403.453\n",
      "validation loss: 21496024705090.13\n",
      "epoch: 726\n",
      "training loss: 31365459167092.664\n",
      "validation loss: 21494145331088.91\n",
      "epoch: 727\n",
      "training loss: 31365097139990.6\n",
      "validation loss: 21492274565703.805\n",
      "epoch: 728\n",
      "training loss: 31364736373338.023\n",
      "validation loss: 21490412371525.805\n",
      "epoch: 729\n",
      "training loss: 31364376857451.457\n",
      "validation loss: 21488558711321.934\n",
      "epoch: 730\n",
      "training loss: 31364018582722.484\n",
      "validation loss: 21486713548034.31\n",
      "epoch: 731\n",
      "training loss: 31363661539617.266\n",
      "validation loss: 21484876844779.242\n",
      "epoch: 732\n",
      "training loss: 31363305718675.918\n",
      "validation loss: 21483048564846.285\n",
      "epoch: 733\n",
      "training loss: 31362951110511.94\n",
      "validation loss: 21481228671697.387\n",
      "epoch: 734\n",
      "training loss: 31362597705811.66\n",
      "validation loss: 21479417128965.926\n",
      "epoch: 735\n",
      "training loss: 31362245495333.645\n",
      "validation loss: 21477613900455.883\n",
      "epoch: 736\n",
      "training loss: 31361894469908.17\n",
      "validation loss: 21475818950140.87\n",
      "epoch: 737\n",
      "training loss: 31361544620436.65\n",
      "validation loss: 21474032242163.324\n",
      "epoch: 738\n",
      "training loss: 31361195937891.066\n",
      "validation loss: 21472253740833.562\n",
      "epoch: 739\n",
      "training loss: 31360848413313.45\n",
      "validation loss: 21470483410628.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 740\n",
      "training loss: 31360502037815.35\n",
      "validation loss: 21468721216192.992\n",
      "epoch: 741\n",
      "training loss: 31360156802577.246\n",
      "validation loss: 21466967122334.516\n",
      "epoch: 742\n",
      "training loss: 31359812698848.016\n",
      "validation loss: 21465221094026.76\n",
      "epoch: 743\n",
      "training loss: 31359469717944.49\n",
      "validation loss: 21463483096406.574\n",
      "epoch: 744\n",
      "training loss: 31359127851250.797\n",
      "validation loss: 21461753094773.5\n",
      "epoch: 745\n",
      "training loss: 31358787090217.96\n",
      "validation loss: 21460031054588.99\n",
      "epoch: 746\n",
      "training loss: 31358447426363.293\n",
      "validation loss: 21458316941475.508\n",
      "epoch: 747\n",
      "training loss: 31358108851269.93\n",
      "validation loss: 21456610721215.785\n",
      "epoch: 748\n",
      "training loss: 31357771356586.305\n",
      "validation loss: 21454912359751.86\n",
      "epoch: 749\n",
      "training loss: 31357434934025.625\n",
      "validation loss: 21453221823184.367\n",
      "epoch: 750\n",
      "training loss: 31357099575365.41\n",
      "validation loss: 21451539077771.656\n",
      "epoch: 751\n",
      "training loss: 31356765272446.97\n",
      "validation loss: 21449864089928.992\n",
      "epoch: 752\n",
      "training loss: 31356432017174.875\n",
      "validation loss: 21448196826227.734\n",
      "epoch: 753\n",
      "training loss: 31356099801516.54\n",
      "validation loss: 21446537253394.55\n",
      "epoch: 754\n",
      "training loss: 31355768617501.68\n",
      "validation loss: 21444885338310.58\n",
      "epoch: 755\n",
      "training loss: 31355438457221.84\n",
      "validation loss: 21443241048010.668\n",
      "epoch: 756\n",
      "training loss: 31355109312829.926\n",
      "validation loss: 21441604349682.566\n",
      "epoch: 757\n",
      "training loss: 31354781176539.72\n",
      "validation loss: 21439975210666.125\n",
      "epoch: 758\n",
      "training loss: 31354454040625.406\n",
      "validation loss: 21438353598452.523\n",
      "epoch: 759\n",
      "training loss: 31354127897421.113\n",
      "validation loss: 21436739480683.484\n",
      "epoch: 760\n",
      "training loss: 31353802739320.438\n",
      "validation loss: 21435132825150.523\n",
      "epoch: 761\n",
      "training loss: 31353478558775.99\n",
      "validation loss: 21433533599794.13\n",
      "epoch: 762\n",
      "training loss: 31353155348298.92\n",
      "validation loss: 21431941772703.027\n",
      "epoch: 763\n",
      "training loss: 31352833100458.5\n",
      "validation loss: 21430357312113.438\n",
      "epoch: 764\n",
      "training loss: 31352511807881.63\n",
      "validation loss: 21428780186408.266\n",
      "epoch: 765\n",
      "training loss: 31352191463252.42\n",
      "validation loss: 21427210364116.39\n",
      "epoch: 766\n",
      "training loss: 31351872059311.742\n",
      "validation loss: 21425647813911.895\n",
      "epoch: 767\n",
      "training loss: 31351553588856.777\n",
      "validation loss: 21424092504613.33\n",
      "epoch: 768\n",
      "training loss: 31351236044740.586\n",
      "validation loss: 21422544405182.957\n",
      "epoch: 769\n",
      "training loss: 31350919419871.688\n",
      "validation loss: 21421003484726.04\n",
      "epoch: 770\n",
      "training loss: 31350603707213.598\n",
      "validation loss: 21419469712490.07\n",
      "epoch: 771\n",
      "training loss: 31350288899784.44\n",
      "validation loss: 21417943057864.086\n",
      "epoch: 772\n",
      "training loss: 31349974990656.508\n",
      "validation loss: 21416423490377.902\n",
      "epoch: 773\n",
      "training loss: 31349661972955.81\n",
      "validation loss: 21414910979701.42\n",
      "epoch: 774\n",
      "training loss: 31349349839861.715\n",
      "validation loss: 21413405495643.895\n",
      "epoch: 775\n",
      "training loss: 31349038584606.504\n",
      "validation loss: 21411907008153.22\n",
      "epoch: 776\n",
      "training loss: 31348728200474.95\n",
      "validation loss: 21410415487315.234\n",
      "epoch: 777\n",
      "training loss: 31348418680803.93\n",
      "validation loss: 21408930903352.98\n",
      "epoch: 778\n",
      "training loss: 31348110018982.02\n",
      "validation loss: 21407453226626.047\n",
      "epoch: 779\n",
      "training loss: 31347802208449.082\n",
      "validation loss: 21405982427629.84\n",
      "epoch: 780\n",
      "training loss: 31347495242695.87\n",
      "validation loss: 21404518476994.89\n",
      "epoch: 781\n",
      "training loss: 31347189115263.652\n",
      "validation loss: 21403061345486.168\n",
      "epoch: 782\n",
      "training loss: 31346883819743.785\n",
      "validation loss: 21401611004002.383\n",
      "epoch: 783\n",
      "training loss: 31346579349777.363\n",
      "validation loss: 21400167423575.312\n",
      "epoch: 784\n",
      "training loss: 31346275699054.8\n",
      "validation loss: 21398730575369.105\n",
      "epoch: 785\n",
      "training loss: 31345972861315.48\n",
      "validation loss: 21397300430679.613\n",
      "epoch: 786\n",
      "training loss: 31345670830347.324\n",
      "validation loss: 21395876960933.72\n",
      "epoch: 787\n",
      "training loss: 31345369599986.492\n",
      "validation loss: 21394460137688.633\n",
      "epoch: 788\n",
      "training loss: 31345069164116.926\n",
      "validation loss: 21393049932631.254\n",
      "epoch: 789\n",
      "training loss: 31344769516670.035\n",
      "validation loss: 21391646317577.48\n",
      "epoch: 790\n",
      "training loss: 31344470651624.32\n",
      "validation loss: 21390249264471.56\n",
      "epoch: 791\n",
      "training loss: 31344172563004.98\n",
      "validation loss: 21388858745385.4\n",
      "epoch: 792\n",
      "training loss: 31343875244883.57\n",
      "validation loss: 21387474732517.93\n",
      "epoch: 793\n",
      "training loss: 31343578691377.65\n",
      "validation loss: 21386097198194.426\n",
      "epoch: 794\n",
      "training loss: 31343282896650.418\n",
      "validation loss: 21384726114865.863\n",
      "epoch: 795\n",
      "training loss: 31342987854910.336\n",
      "validation loss: 21383361455108.23\n",
      "epoch: 796\n",
      "training loss: 31342693560410.83\n",
      "validation loss: 21382003191621.902\n",
      "epoch: 797\n",
      "training loss: 31342400007449.87\n",
      "validation loss: 21380651297230.957\n",
      "epoch: 798\n",
      "training loss: 31342107190369.707\n",
      "validation loss: 21379305744882.535\n",
      "epoch: 799\n",
      "training loss: 31341815103556.457\n",
      "validation loss: 21377966507646.156\n",
      "epoch: 800\n",
      "training loss: 31341523741439.8\n",
      "validation loss: 21376633558713.08\n",
      "epoch: 801\n",
      "training loss: 31341233098492.62\n",
      "validation loss: 21375306871395.61\n",
      "epoch: 802\n",
      "training loss: 31340943169230.71\n",
      "validation loss: 21373986419126.46\n",
      "epoch: 803\n",
      "training loss: 31340653948212.39\n",
      "validation loss: 21372672175458.06\n",
      "epoch: 804\n",
      "training loss: 31340365430038.2\n",
      "validation loss: 21371364114061.848\n",
      "epoch: 805\n",
      "training loss: 31340077609350.57\n",
      "validation loss: 21370062208727.613\n",
      "epoch: 806\n",
      "training loss: 31339790480833.51\n",
      "validation loss: 21368766433362.78\n",
      "epoch: 807\n",
      "training loss: 31339504039212.254\n",
      "validation loss: 21367476761991.684\n",
      "epoch: 808\n",
      "training loss: 31339218279252.977\n",
      "validation loss: 21366193168754.836\n",
      "epoch: 809\n",
      "training loss: 31338933195762.453\n",
      "validation loss: 21364915627908.156\n",
      "epoch: 810\n",
      "training loss: 31338648783587.734\n",
      "validation loss: 21363644113822.188\n",
      "epoch: 811\n",
      "training loss: 31338365037615.875\n",
      "validation loss: 21362378600981.273\n",
      "epoch: 812\n",
      "training loss: 31338081952773.58\n",
      "validation loss: 21361119063982.668\n",
      "epoch: 813\n",
      "training loss: 31337799524026.918\n",
      "validation loss: 21359865477535.617\n",
      "epoch: 814\n",
      "training loss: 31337517746381.02\n",
      "validation loss: 21358617816460.367\n",
      "epoch: 815\n",
      "training loss: 31337236614879.766\n",
      "validation loss: 21357376055687.004\n",
      "epoch: 816\n",
      "training loss: 31336956124605.473\n",
      "validation loss: 21356140170254.293\n",
      "epoch: 817\n",
      "training loss: 31336676270678.637\n",
      "validation loss: 21354910135308.258\n",
      "epoch: 818\n",
      "training loss: 31336397048257.59\n",
      "validation loss: 21353685926100.574\n",
      "epoch: 819\n",
      "training loss: 31336118452538.24\n",
      "validation loss: 21352467517986.707\n",
      "epoch: 820\n",
      "training loss: 31335840478753.75\n",
      "validation loss: 21351254886423.617\n",
      "epoch: 821\n",
      "training loss: 31335563122174.29\n",
      "validation loss: 21350048006966.957\n",
      "epoch: 822\n",
      "training loss: 31335286378106.695\n",
      "validation loss: 21348846855267.543\n",
      "epoch: 823\n",
      "training loss: 31335010241894.24\n",
      "validation loss: 21347651407066.777\n",
      "epoch: 824\n",
      "training loss: 31334734708916.293\n",
      "validation loss: 21346461638190.473\n",
      "epoch: 825\n",
      "training loss: 31334459774588.094\n",
      "validation loss: 21345277524540.465\n",
      "epoch: 826\n",
      "training loss: 31334185434360.434\n",
      "validation loss: 21344099042082.7\n",
      "epoch: 827\n",
      "training loss: 31333911683719.383\n",
      "validation loss: 21342926166829.727\n",
      "epoch: 828\n",
      "training loss: 31333638518186.02\n",
      "validation loss: 21341758874814.09\n",
      "epoch: 829\n",
      "training loss: 31333365933316.14\n",
      "validation loss: 21340597142045.91\n",
      "epoch: 997\n",
      "training loss: 31293114136566.926\n",
      "validation loss: 21208513157411.207\n",
      "epoch: 998\n",
      "training loss: 31292896813545.77\n",
      "validation loss: 21208024875905.645\n",
      "epoch: 999\n",
      "training loss: 31292679654074.316\n",
      "validation loss: 21207539346710.863\n",
      "epoch: 1000\n",
      "training loss: 31292462656979.223\n",
      "validation loss: 21207056558641.67\n",
      "epoch: 1001\n",
      "training loss: 31292245821096.24\n",
      "validation loss: 21206576500559.91\n",
      "epoch: 1002\n",
      "training loss: 31292029145270.176\n",
      "validation loss: 21206099161374.27\n",
      "epoch: 1003\n",
      "training loss: 31291812628354.78\n",
      "validation loss: 21205624530040.047\n",
      "epoch: 1004\n",
      "training loss: 31291596269212.703\n",
      "validation loss: 21205152595558.957\n",
      "epoch: 1005\n",
      "training loss: 31291380066715.41\n",
      "validation loss: 21204683346978.926\n",
      "epoch: 1006\n",
      "training loss: 31291164019743.152\n",
      "validation loss: 21204216773393.89\n",
      "epoch: 1007\n",
      "training loss: 31290948127184.836\n",
      "validation loss: 21203752863943.57\n",
      "epoch: 1008\n",
      "training loss: 31290732387937.996\n",
      "validation loss: 21203291607813.285\n",
      "epoch: 1009\n",
      "training loss: 31290516800908.723\n",
      "validation loss: 21202832994233.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1010\n",
      "training loss: 31290301365011.61\n",
      "validation loss: 21202377012480.836\n",
      "epoch: 1011\n",
      "training loss: 31290086079169.645\n",
      "validation loss: 21201923651875.46\n",
      "epoch: 1012\n",
      "training loss: 31289870942314.19\n",
      "validation loss: 21201472901783.277\n",
      "epoch: 1013\n",
      "training loss: 31289655953384.9\n",
      "validation loss: 21201024751614.547\n",
      "epoch: 1014\n",
      "training loss: 31289441111329.63\n",
      "validation loss: 21200579190823.926\n",
      "epoch: 1015\n",
      "training loss: 31289226415104.42\n",
      "validation loss: 21200136208910.242\n",
      "epoch: 1016\n",
      "training loss: 31289011863673.43\n",
      "validation loss: 21199695795416.344\n",
      "epoch: 1017\n",
      "training loss: 31288797456008.81\n",
      "validation loss: 21199257939928.855\n",
      "epoch: 1018\n",
      "training loss: 31288583191090.707\n",
      "validation loss: 21198822632078.023\n",
      "epoch: 1019\n",
      "training loss: 31288369067907.207\n",
      "validation loss: 21198389861537.508\n",
      "epoch: 1020\n",
      "training loss: 31288155085454.19\n",
      "validation loss: 21197959618024.168\n",
      "epoch: 1021\n",
      "training loss: 31287941242735.383\n",
      "validation loss: 21197531891297.926\n",
      "epoch: 1022\n",
      "training loss: 31287727538762.2\n",
      "validation loss: 21197106671161.5\n",
      "epoch: 1023\n",
      "training loss: 31287513972553.75\n",
      "validation loss: 21196683947460.293\n",
      "epoch: 1024\n",
      "training loss: 31287300543136.742\n",
      "validation loss: 21196263710082.145\n",
      "epoch: 1025\n",
      "training loss: 31287087249545.434\n",
      "validation loss: 21195845948957.168\n",
      "epoch: 1026\n",
      "training loss: 31286874090821.59\n",
      "validation loss: 21195430654057.55\n",
      "epoch: 1027\n",
      "training loss: 31286661066014.395\n",
      "validation loss: 21195017815397.406\n",
      "epoch: 1028\n",
      "training loss: 31286448174180.414\n",
      "validation loss: 21194607423032.516\n",
      "epoch: 1029\n",
      "training loss: 31286235414383.527\n",
      "validation loss: 21194199467060.24\n",
      "epoch: 1030\n",
      "training loss: 31286022785694.895\n",
      "validation loss: 21193793937619.234\n",
      "epoch: 1031\n",
      "training loss: 31285810287192.85\n",
      "validation loss: 21193390824889.336\n",
      "epoch: 1032\n",
      "training loss: 31285597917962.934\n",
      "validation loss: 21192990119091.367\n",
      "epoch: 1033\n",
      "training loss: 31285385677097.703\n",
      "validation loss: 21192591810486.93\n",
      "epoch: 1034\n",
      "training loss: 31285173563696.832\n",
      "validation loss: 21192195889378.26\n",
      "epoch: 1035\n",
      "training loss: 31284961576866.92\n",
      "validation loss: 21191802346108.02\n",
      "epoch: 1036\n",
      "training loss: 31284749715721.53\n",
      "validation loss: 21191411171059.133\n",
      "epoch: 1037\n",
      "training loss: 31284537979381.113\n",
      "validation loss: 21191022354654.613\n",
      "epoch: 1038\n",
      "training loss: 31284326366972.9\n",
      "validation loss: 21190635887357.38\n",
      "epoch: 1039\n",
      "training loss: 31284114877630.92\n",
      "validation loss: 21190251759670.055\n",
      "epoch: 1040\n",
      "training loss: 31283903510495.94\n",
      "validation loss: 21189869962134.848\n",
      "epoch: 1041\n",
      "training loss: 31283692264715.35\n",
      "validation loss: 21189490485333.33\n",
      "epoch: 1042\n",
      "training loss: 31283481139443.18\n",
      "validation loss: 21189113319886.285\n",
      "epoch: 1043\n",
      "training loss: 31283270133840.035\n",
      "validation loss: 21188738456453.535\n",
      "epoch: 1044\n",
      "training loss: 31283059247073.0\n",
      "validation loss: 21188365885733.746\n",
      "epoch: 1045\n",
      "training loss: 31282848478315.64\n",
      "validation loss: 21187995598464.31\n",
      "epoch: 1046\n",
      "training loss: 31282637826747.93\n",
      "validation loss: 21187627585421.09\n",
      "epoch: 1047\n",
      "training loss: 31282427291556.21\n",
      "validation loss: 21187261837418.344\n",
      "epoch: 1048\n",
      "training loss: 31282216871933.133\n",
      "validation loss: 21186898345308.5\n",
      "epoch: 1049\n",
      "training loss: 31282006567077.6\n",
      "validation loss: 21186537099981.996\n",
      "epoch: 1050\n",
      "training loss: 31281796376194.742\n",
      "validation loss: 21186178092367.11\n",
      "epoch: 1051\n",
      "training loss: 31281586298495.85\n",
      "validation loss: 21185821313429.83\n",
      "epoch: 1052\n",
      "training loss: 31281376333198.34\n",
      "validation loss: 21185466754173.633\n",
      "epoch: 1053\n",
      "training loss: 31281166479525.7\n",
      "validation loss: 21185114405639.37\n",
      "epoch: 1054\n",
      "training loss: 31280956736707.42\n",
      "validation loss: 21184764258905.074\n",
      "epoch: 1055\n",
      "training loss: 31280747103978.996\n",
      "validation loss: 21184416305085.785\n",
      "epoch: 1056\n",
      "training loss: 31280537580581.844\n",
      "validation loss: 21184070535333.438\n",
      "epoch: 1057\n",
      "training loss: 31280328165763.266\n",
      "validation loss: 21183726940836.645\n",
      "epoch: 1058\n",
      "training loss: 31280118858776.395\n",
      "validation loss: 21183385512820.574\n",
      "epoch: 1059\n",
      "training loss: 31279909658880.168\n",
      "validation loss: 21183046242546.76\n",
      "epoch: 1060\n",
      "training loss: 31279700565339.266\n",
      "validation loss: 21182709121312.984\n",
      "epoch: 1061\n",
      "training loss: 31279491577424.074\n",
      "validation loss: 21182374140453.062\n",
      "epoch: 1062\n",
      "training loss: 31279282694410.633\n",
      "validation loss: 21182041291336.74\n",
      "epoch: 1063\n",
      "training loss: 31279073915580.62\n",
      "validation loss: 21181710565369.504\n",
      "epoch: 1064\n",
      "training loss: 31278865240221.258\n",
      "validation loss: 21181381953992.42\n",
      "epoch: 1065\n",
      "training loss: 31278656667625.316\n",
      "validation loss: 21181055448682.027\n",
      "epoch: 1066\n",
      "training loss: 31278448197091.055\n",
      "validation loss: 21180731040950.1\n",
      "epoch: 1067\n",
      "training loss: 31278239827922.15\n",
      "validation loss: 21180408722343.582\n",
      "epoch: 1068\n",
      "training loss: 31278031559427.707\n",
      "validation loss: 21180088484444.363\n",
      "epoch: 1069\n",
      "training loss: 31277823390922.18\n",
      "validation loss: 21179770318869.18\n",
      "epoch: 1070\n",
      "training loss: 31277615321725.344\n",
      "validation loss: 21179454217269.434\n",
      "epoch: 1071\n",
      "training loss: 31277407351162.25\n",
      "validation loss: 21179140171331.027\n",
      "epoch: 1072\n",
      "training loss: 31277199478563.19\n",
      "validation loss: 21178828172774.266\n",
      "epoch: 1073\n",
      "training loss: 31276991703263.633\n",
      "validation loss: 21178518213353.645\n",
      "epoch: 1074\n",
      "training loss: 31276784024604.223\n",
      "validation loss: 21178210284857.746\n",
      "epoch: 1075\n",
      "training loss: 31276576441930.71\n",
      "validation loss: 21177904379109.066\n",
      "epoch: 1076\n",
      "training loss: 31276368954593.914\n",
      "validation loss: 21177600487963.895\n",
      "epoch: 1077\n",
      "training loss: 31276161561949.707\n",
      "validation loss: 21177298603312.125\n",
      "epoch: 1078\n",
      "training loss: 31275954263358.94\n",
      "validation loss: 21176998717077.14\n",
      "epoch: 1079\n",
      "training loss: 31275747058187.434\n",
      "validation loss: 21176700821215.676\n",
      "epoch: 1080\n",
      "training loss: 31275539945805.918\n",
      "validation loss: 21176404907717.652\n",
      "epoch: 1081\n",
      "training loss: 31275332925590.02\n",
      "validation loss: 21176110968606.02\n",
      "epoch: 1082\n",
      "training loss: 31275125996920.19\n",
      "validation loss: 21175818995936.652\n",
      "epoch: 1083\n",
      "training loss: 31274919159181.7\n",
      "validation loss: 21175528981798.168\n",
      "epoch: 1084\n",
      "training loss: 31274712411764.58\n",
      "validation loss: 21175240918311.836\n",
      "epoch: 1085\n",
      "training loss: 31274505754063.594\n",
      "validation loss: 21174954797631.36\n",
      "epoch: 1086\n",
      "training loss: 31274299185478.207\n",
      "validation loss: 21174670611942.824\n",
      "epoch: 1087\n",
      "training loss: 31274092705412.54\n",
      "validation loss: 21174388353464.473\n",
      "epoch: 1088\n",
      "training loss: 31273886313275.332\n",
      "validation loss: 21174108014446.63\n",
      "epoch: 1089\n",
      "training loss: 31273680008479.91\n",
      "validation loss: 21173829587171.523\n",
      "epoch: 1090\n",
      "training loss: 31273473790444.14\n",
      "validation loss: 21173553063953.184\n",
      "epoch: 1091\n",
      "training loss: 31273267658590.438\n",
      "validation loss: 21173278437137.258\n",
      "epoch: 1092\n",
      "training loss: 31273061612345.652\n",
      "validation loss: 21173005699100.918\n",
      "epoch: 1093\n",
      "training loss: 31272855651141.12\n",
      "validation loss: 21172734842252.703\n",
      "epoch: 1094\n",
      "training loss: 31272649774412.56\n",
      "validation loss: 21172465859032.37\n",
      "epoch: 1095\n",
      "training loss: 31272443981600.086\n",
      "validation loss: 21172198741910.797\n",
      "epoch: 1096\n",
      "training loss: 31272238272148.15\n",
      "validation loss: 21171933483389.816\n",
      "epoch: 1097\n",
      "training loss: 31272032645505.5\n",
      "validation loss: 21171670076002.086\n",
      "epoch: 1098\n",
      "training loss: 31271827101125.176\n",
      "validation loss: 21171408512310.977\n",
      "epoch: 1099\n",
      "training loss: 31271621638464.45\n",
      "validation loss: 21171148784910.406\n",
      "epoch: 1100\n",
      "training loss: 31271416256984.805\n",
      "validation loss: 21170890886424.742\n",
      "epoch: 1101\n",
      "training loss: 31271210956151.906\n",
      "validation loss: 21170634809508.645\n",
      "epoch: 1102\n",
      "training loss: 31271005735435.574\n",
      "validation loss: 21170380546846.953\n",
      "epoch: 1103\n",
      "training loss: 31270800594309.715\n",
      "validation loss: 21170128091154.54\n",
      "epoch: 1104\n",
      "training loss: 31270595532252.324\n",
      "validation loss: 21169877435176.195\n",
      "epoch: 1105\n",
      "training loss: 31270390548745.45\n",
      "validation loss: 21169628571686.492\n",
      "epoch: 1106\n",
      "training loss: 31270185643275.17\n",
      "validation loss: 21169381493489.656\n",
      "epoch: 1107\n",
      "training loss: 31269980815331.535\n",
      "validation loss: 21169136193419.45\n",
      "epoch: 1108\n",
      "training loss: 31269776064408.535\n",
      "validation loss: 21168892664339.02\n",
      "epoch: 1109\n",
      "training loss: 31269571390004.113\n",
      "validation loss: 21168650899140.8\n",
      "epoch: 1110\n",
      "training loss: 31269366791620.086\n",
      "validation loss: 21168410890746.37\n",
      "epoch: 1111\n",
      "training loss: 31269162268762.152\n",
      "validation loss: 21168172632106.316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1112\n",
      "training loss: 31268957820939.836\n",
      "validation loss: 21167936116200.156\n",
      "epoch: 1113\n",
      "training loss: 31268753447666.45\n",
      "validation loss: 21167701336036.156\n",
      "epoch: 1114\n",
      "training loss: 31268549148459.11\n",
      "validation loss: 21167468284651.23\n",
      "epoch: 1115\n",
      "training loss: 31268344922838.64\n",
      "validation loss: 21167236955110.836\n",
      "epoch: 1116\n",
      "training loss: 31268140770329.62\n",
      "validation loss: 21167007340508.832\n",
      "epoch: 1117\n",
      "training loss: 31267936690460.297\n",
      "validation loss: 21166779433967.36\n",
      "epoch: 1118\n",
      "training loss: 31267732682762.57\n",
      "validation loss: 21166553228636.707\n",
      "epoch: 1119\n",
      "training loss: 31267528746771.98\n",
      "validation loss: 21166328717695.227\n",
      "epoch: 1120\n",
      "training loss: 31267324882027.67\n",
      "validation loss: 21166105894349.168\n",
      "epoch: 1121\n",
      "training loss: 31267121088072.355\n",
      "validation loss: 21165884751832.617\n",
      "epoch: 1122\n",
      "training loss: 31266917364452.29\n",
      "validation loss: 21165665283407.3\n",
      "epoch: 1123\n",
      "training loss: 31266713710717.25\n",
      "validation loss: 21165447482362.54\n",
      "epoch: 1124\n",
      "training loss: 31266510126420.52\n",
      "validation loss: 21165231342015.086\n",
      "epoch: 1125\n",
      "training loss: 31266306611118.824\n",
      "validation loss: 21165016855709.03\n",
      "epoch: 1126\n",
      "training loss: 31266103164372.332\n",
      "validation loss: 21164804016815.664\n",
      "epoch: 1127\n",
      "training loss: 31265899785744.633\n",
      "validation loss: 21164592818733.38\n",
      "epoch: 1128\n",
      "training loss: 31265696474802.684\n",
      "validation loss: 21164383254887.555\n",
      "epoch: 1129\n",
      "training loss: 31265493231116.83\n",
      "validation loss: 21164175318730.418\n",
      "epoch: 1130\n",
      "training loss: 31265290054260.703\n",
      "validation loss: 21163969003740.957\n",
      "epoch: 1131\n",
      "training loss: 31265086943811.29\n",
      "validation loss: 21163764303424.793\n",
      "epoch: 1132\n",
      "training loss: 31264883899348.805\n",
      "validation loss: 21163561211314.055\n",
      "epoch: 1133\n",
      "training loss: 31264680920456.777\n",
      "validation loss: 21163359720967.312\n",
      "epoch: 1134\n",
      "training loss: 31264478006721.93\n",
      "validation loss: 21163159825969.406\n",
      "epoch: 1135\n",
      "training loss: 31264275157734.18\n",
      "validation loss: 21162961519931.36\n",
      "epoch: 1136\n",
      "training loss: 31264072373086.65\n",
      "validation loss: 21162764796490.28\n",
      "epoch: 1137\n",
      "training loss: 31263869652375.594\n",
      "validation loss: 21162569649309.24\n",
      "epoch: 1138\n",
      "training loss: 31263666995200.434\n",
      "validation loss: 21162376072077.14\n",
      "epoch: 1139\n",
      "training loss: 31263464401163.66\n",
      "validation loss: 21162184058508.64\n",
      "epoch: 1140\n",
      "training loss: 31263261869870.86\n",
      "validation loss: 21161993602344.03\n",
      "epoch: 1141\n",
      "training loss: 31263059400930.68\n",
      "validation loss: 21161804697349.13\n",
      "epoch: 1142\n",
      "training loss: 31262856993954.805\n",
      "validation loss: 21161617337315.145\n",
      "epoch: 1143\n",
      "training loss: 31262654648557.914\n",
      "validation loss: 21161431516058.617\n",
      "epoch: 1144\n",
      "training loss: 31262452364357.684\n",
      "validation loss: 21161247227421.27\n",
      "epoch: 1145\n",
      "training loss: 31262250140974.77\n",
      "validation loss: 21161064465269.934\n",
      "epoch: 1146\n",
      "training loss: 31262047978032.746\n",
      "validation loss: 21160883223496.41\n",
      "epoch: 1147\n",
      "training loss: 31261845875158.105\n",
      "validation loss: 21160703496017.395\n",
      "epoch: 1148\n",
      "training loss: 31261643831980.26\n",
      "validation loss: 21160525276774.35\n",
      "epoch: 1149\n",
      "training loss: 31261441848131.48\n",
      "validation loss: 21160348559733.414\n",
      "epoch: 1150\n",
      "training loss: 31261239923246.875\n",
      "validation loss: 21160173338885.297\n",
      "epoch: 1151\n",
      "training loss: 31261038056964.406\n",
      "validation loss: 21159999608245.184\n",
      "epoch: 1152\n",
      "training loss: 31260836248924.82\n",
      "validation loss: 21159827361852.6\n",
      "epoch: 1153\n",
      "training loss: 31260634498771.67\n",
      "validation loss: 21159656593771.367\n",
      "epoch: 1154\n",
      "training loss: 31260432806151.246\n",
      "validation loss: 21159487298089.453\n",
      "epoch: 1155\n",
      "training loss: 31260231170712.605\n",
      "validation loss: 21159319468918.91\n",
      "epoch: 1156\n",
      "training loss: 31260029592107.516\n",
      "validation loss: 21159153100395.76\n",
      "epoch: 1157\n",
      "training loss: 31259828069990.43\n",
      "validation loss: 21158988186679.914\n",
      "epoch: 1158\n",
      "training loss: 31259626604018.5\n",
      "validation loss: 21158824721955.098\n",
      "epoch: 1159\n",
      "training loss: 31259425193851.508\n",
      "validation loss: 21158662700428.75\n",
      "epoch: 1160\n",
      "training loss: 31259223839151.906\n",
      "validation loss: 21158502116331.973\n",
      "epoch: 1161\n",
      "training loss: 31259022539584.723\n",
      "validation loss: 21158342963919.508\n",
      "epoch: 1162\n",
      "training loss: 31258821294817.61\n",
      "validation loss: 21158185237469.684\n",
      "epoch: 1163\n",
      "training loss: 31258620104520.742\n",
      "validation loss: 21158028931284.48\n",
      "epoch: 1164\n",
      "training loss: 31258418968366.875\n",
      "validation loss: 21157874039689.688\n",
      "epoch: 1165\n",
      "training loss: 31258217886031.266\n",
      "validation loss: 21157720557035.207\n",
      "epoch: 1166\n",
      "training loss: 31258016857191.62\n",
      "validation loss: 21157568477695.832\n",
      "epoch: 1167\n",
      "training loss: 31257815881528.08\n",
      "validation loss: 21157417796072.855\n",
      "epoch: 1168\n",
      "training loss: 31257614958723.05\n",
      "validation loss: 21157268506597.96\n",
      "epoch: 1169\n",
      "training loss: 31257414088460.99\n",
      "validation loss: 21157120603743.4\n",
      "epoch: 1170\n",
      "training loss: 31257213270427.54\n",
      "validation loss: 21156974082054.05\n",
      "epoch: 1171\n",
      "training loss: 31257012504306.35\n",
      "validation loss: 21156828936278.996\n",
      "epoch: 1172\n",
      "training loss: 31256811789757.965\n",
      "validation loss: 21156685162233.164\n",
      "epoch: 1173\n",
      "training loss: 31256611126006.562\n",
      "validation loss: 21156542773716.723\n",
      "epoch: 1174\n",
      "training loss: 31256408737273.75\n",
      "validation loss: 21156481585395.543\n",
      "epoch: 1175\n",
      "training loss: 31258135382549.1\n",
      "validation loss: 21231866517244.773\n",
      "epoch: 1176\n",
      "training loss: 31260486886657.66\n",
      "validation loss: 21219570628534.367\n",
      "epoch: 1177\n",
      "training loss: 31260279713777.24\n",
      "validation loss: 21219424667247.97\n",
      "epoch: 1178\n",
      "training loss: 31260078722701.2\n",
      "validation loss: 21219280061646.02\n",
      "epoch: 1179\n",
      "training loss: 31259877780773.277\n",
      "validation loss: 21219136778216.926\n",
      "epoch: 1180\n",
      "training loss: 31259676887707.47\n",
      "validation loss: 21218994811685.684\n",
      "epoch: 1181\n",
      "training loss: 31259476043219.965\n",
      "validation loss: 21218854156798.633\n",
      "epoch: 1182\n",
      "training loss: 31259275247029.17\n",
      "validation loss: 21218714808323.367\n",
      "epoch: 1183\n",
      "training loss: 31259074498855.664\n",
      "validation loss: 21218576761048.633\n",
      "epoch: 1184\n",
      "training loss: 31258873798422.17\n",
      "validation loss: 21218440009784.26\n",
      "epoch: 1185\n",
      "training loss: 31258673145453.617\n",
      "validation loss: 21218304549361.066\n",
      "epoch: 1186\n",
      "training loss: 31258472539676.992\n",
      "validation loss: 21218170374630.758\n",
      "epoch: 1187\n",
      "training loss: 31258271980821.45\n",
      "validation loss: 21218037480465.855\n",
      "epoch: 1188\n",
      "training loss: 31258071468618.207\n",
      "validation loss: 21217905861759.6\n",
      "epoch: 1189\n",
      "training loss: 31257871002800.586\n",
      "validation loss: 21217775513425.875\n",
      "epoch: 1190\n",
      "training loss: 31257670583103.973\n",
      "validation loss: 21217646430399.117\n",
      "epoch: 1191\n",
      "training loss: 31257470209265.78\n",
      "validation loss: 21217518607634.203\n",
      "epoch: 1192\n",
      "training loss: 31257269881025.49\n",
      "validation loss: 21217392040106.402\n",
      "epoch: 1193\n",
      "training loss: 31257069598124.56\n",
      "validation loss: 21217266722811.28\n",
      "epoch: 1194\n",
      "training loss: 31256869360306.48\n",
      "validation loss: 21217142650764.613\n",
      "epoch: 1195\n",
      "training loss: 31256669167316.727\n",
      "validation loss: 21217019819002.285\n",
      "epoch: 1196\n",
      "training loss: 31256469018902.723\n",
      "validation loss: 21216898222580.223\n",
      "epoch: 1197\n",
      "training loss: 31256268914813.88\n",
      "validation loss: 21216777856574.31\n",
      "epoch: 1198\n",
      "training loss: 31256068854801.52\n",
      "validation loss: 21216658716080.305\n",
      "epoch: 1199\n",
      "training loss: 31255868838618.906\n",
      "validation loss: 21216540796213.742\n",
      "epoch: 1200\n",
      "training loss: 31255668866021.207\n",
      "validation loss: 21216424092109.88\n",
      "epoch: 1201\n",
      "training loss: 31255468936765.492\n",
      "validation loss: 21216308598923.574\n",
      "epoch: 1202\n",
      "training loss: 31255269050610.707\n",
      "validation loss: 21216194311829.242\n",
      "epoch: 1203\n",
      "training loss: 31255069207317.668\n",
      "validation loss: 21216081226020.76\n",
      "epoch: 1204\n",
      "training loss: 31254869406649.04\n",
      "validation loss: 21215969336711.375\n",
      "epoch: 1205\n",
      "training loss: 31254669648369.324\n",
      "validation loss: 21215858639133.617\n",
      "epoch: 1206\n",
      "training loss: 31254469932244.836\n",
      "validation loss: 21215749128539.26\n",
      "epoch: 1207\n",
      "training loss: 31254270258043.734\n",
      "validation loss: 21215640800199.2\n",
      "epoch: 1208\n",
      "training loss: 31254070625535.918\n",
      "validation loss: 21215533649403.383\n",
      "epoch: 1209\n",
      "training loss: 31253871034493.125\n",
      "validation loss: 21215427671460.742\n",
      "epoch: 1210\n",
      "training loss: 31253671484688.824\n",
      "validation loss: 21215322861699.11\n",
      "epoch: 1211\n",
      "training loss: 31253471975898.242\n",
      "validation loss: 21215219215465.125\n",
      "epoch: 1212\n",
      "training loss: 31253272507898.363\n",
      "validation loss: 21215116728124.184\n",
      "epoch: 1213\n",
      "training loss: 31253073080467.883\n",
      "validation loss: 21215015395060.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1214\n",
      "training loss: 31252873693387.203\n",
      "validation loss: 21214915211676.203\n",
      "epoch: 1215\n",
      "training loss: 31252674346438.434\n",
      "validation loss: 21214816173392.93\n",
      "epoch: 1216\n",
      "training loss: 31252475039405.387\n",
      "validation loss: 21214718275650.1\n",
      "epoch: 1217\n",
      "training loss: 31252275772073.53\n",
      "validation loss: 21214621513905.63\n",
      "epoch: 1218\n",
      "training loss: 31252076544229.984\n",
      "validation loss: 21214525883635.727\n",
      "epoch: 1219\n",
      "training loss: 31251877355663.54\n",
      "validation loss: 21214431380334.785\n",
      "epoch: 1220\n",
      "training loss: 31251678206164.6\n",
      "validation loss: 21214337999515.33\n",
      "epoch: 1221\n",
      "training loss: 31251479095525.22\n",
      "validation loss: 21214245736707.953\n",
      "epoch: 1222\n",
      "training loss: 31251280023539.035\n",
      "validation loss: 21214154587461.2\n",
      "epoch: 1223\n",
      "training loss: 31251080990001.28\n",
      "validation loss: 21214064547341.52\n",
      "epoch: 1224\n",
      "training loss: 31250881994708.797\n",
      "validation loss: 21213975611933.195\n",
      "epoch: 1225\n",
      "training loss: 31250683037459.977\n",
      "validation loss: 21213887776838.27\n",
      "Mean absolute error: $6215120875.88\n",
      "Nodes: 100\n",
      "Learning Rate: 1e-05\n",
      "epoch: 0\n",
      "training loss: 34755935605964.44\n",
      "validation loss: 52955211075083.914\n",
      "epoch: 1\n",
      "training loss: 36396080640667.78\n",
      "validation loss: 61735469410170.35\n",
      "epoch: 2\n",
      "training loss: 35565582205317.35\n",
      "validation loss: 58793332930256.164\n",
      "epoch: 3\n",
      "training loss: 35303462546603.95\n",
      "validation loss: 57534992953518.42\n",
      "epoch: 4\n",
      "training loss: 35210376382732.81\n",
      "validation loss: 56965036311162.46\n",
      "epoch: 5\n",
      "training loss: 35157081857275.223\n",
      "validation loss: 56579210142956.77\n",
      "epoch: 6\n",
      "training loss: 35115673141949.773\n",
      "validation loss: 56246760951730.95\n",
      "epoch: 7\n",
      "training loss: 35080303054501.617\n",
      "validation loss: 55941870494967.31\n",
      "epoch: 8\n",
      "training loss: 35049283224616.8\n",
      "validation loss: 55659042067872.33\n",
      "epoch: 9\n",
      "training loss: 35021791315130.92\n",
      "validation loss: 55396175039641.51\n",
      "epoch: 10\n",
      "training loss: 34997258621675.055\n",
      "validation loss: 55151692742273.16\n",
      "epoch: 11\n",
      "training loss: 34975238223713.617\n",
      "validation loss: 54924116969786.76\n",
      "epoch: 12\n",
      "training loss: 34955364559559.324\n",
      "validation loss: 54712035296715.16\n",
      "epoch: 13\n",
      "training loss: 34937333952389.46\n",
      "validation loss: 54514117462506.23\n",
      "epoch: 14\n",
      "training loss: 34920892031911.45\n",
      "validation loss: 54329128429340.18\n",
      "epoch: 15\n",
      "training loss: 34905824529328.03\n",
      "validation loss: 54155933479647.42\n",
      "epoch: 16\n",
      "training loss: 34891950180660.785\n",
      "validation loss: 53993497466851.77\n",
      "epoch: 17\n",
      "training loss: 34879114942780.11\n",
      "validation loss: 53840880502185.16\n",
      "epoch: 18\n",
      "training loss: 34867179696156.324\n",
      "validation loss: 53697175262517.984\n",
      "epoch: 19\n",
      "training loss: 34856044871443.766\n",
      "validation loss: 53561711249787.31\n",
      "epoch: 20\n",
      "training loss: 34845611110842.84\n",
      "validation loss: 53433757165422.43\n",
      "epoch: 21\n",
      "training loss: 34835795980160.586\n",
      "validation loss: 53312682872391.11\n",
      "epoch: 22\n",
      "training loss: 34826528855192.844\n",
      "validation loss: 53197921489601.77\n",
      "epoch: 23\n",
      "training loss: 34817748908651.94\n",
      "validation loss: 53088961127746.21\n",
      "epoch: 24\n",
      "training loss: 34809403608681.492\n",
      "validation loss: 52985338979955.0\n",
      "epoch: 25\n",
      "training loss: 34801447479956.28\n",
      "validation loss: 52886636165459.36\n",
      "epoch: 26\n",
      "training loss: 34793841062839.78\n",
      "validation loss: 52792473138273.91\n",
      "epoch: 27\n",
      "training loss: 34786550034744.418\n",
      "validation loss: 52702505596008.86\n",
      "epoch: 28\n",
      "training loss: 34779544466833.617\n",
      "validation loss: 52616420838836.26\n",
      "epoch: 29\n",
      "training loss: 34772798194278.547\n",
      "validation loss: 52533934531814.234\n",
      "epoch: 30\n",
      "training loss: 34766288282033.344\n",
      "validation loss: 52454787826557.86\n",
      "epoch: 31\n",
      "training loss: 34759994571094.246\n",
      "validation loss: 52378744801680.52\n",
      "epoch: 32\n",
      "training loss: 34753899292667.188\n",
      "validation loss: 52305590185227.54\n",
      "epoch: 33\n",
      "training loss: 34747986739700.117\n",
      "validation loss: 52235127326173.39\n",
      "epoch: 34\n",
      "training loss: 34742242986923.73\n",
      "validation loss: 52167176385743.984\n",
      "epoch: 35\n",
      "training loss: 34736655651950.65\n",
      "validation loss: 52101572722749.07\n",
      "epoch: 36\n",
      "training loss: 34731213691158.77\n",
      "validation loss: 52038165450217.336\n",
      "epoch: 37\n",
      "training loss: 34725907225068.863\n",
      "validation loss: 51976816143405.414\n",
      "epoch: 38\n",
      "training loss: 34720727388752.836\n",
      "validation loss: 51917397681711.6\n",
      "epoch: 39\n",
      "training loss: 34715666203503.08\n",
      "validation loss: 51859793209189.97\n",
      "epoch: 40\n",
      "training loss: 34710716466577.234\n",
      "validation loss: 51803895200253.195\n",
      "epoch: 41\n",
      "training loss: 34705871656324.36\n",
      "validation loss: 51749604618806.61\n",
      "epoch: 42\n",
      "training loss: 34701125850412.633\n",
      "validation loss: 51696830160495.13\n",
      "epoch: 43\n",
      "training loss: 34696473655228.082\n",
      "validation loss: 51645487568998.984\n",
      "epoch: 44\n",
      "training loss: 34691910144808.523\n",
      "validation loss: 51595499018403.11\n",
      "epoch: 45\n",
      "training loss: 34687430807925.562\n",
      "validation loss: 51546792554616.234\n",
      "epoch: 46\n",
      "training loss: 34683031502137.832\n",
      "validation loss: 51499301589639.91\n",
      "epoch: 47\n",
      "training loss: 34678708413815.484\n",
      "validation loss: 51452964443209.53\n",
      "epoch: 48\n",
      "training loss: 34674680443250.26\n",
      "validation loss: 51408643170632.69\n",
      "epoch: 49\n",
      "training loss: 34670478598042.07\n",
      "validation loss: 51364295793117.13\n",
      "epoch: 50\n",
      "training loss: 34666364472829.59\n",
      "validation loss: 51321045961638.62\n",
      "epoch: 51\n",
      "training loss: 34662317750845.293\n",
      "validation loss: 51278760830933.63\n",
      "epoch: 52\n",
      "training loss: 34656311134376.94\n",
      "validation loss: 51213926213334.52\n",
      "epoch: 53\n",
      "training loss: 34655309584156.195\n",
      "validation loss: 51189271751787.53\n",
      "epoch: 54\n",
      "training loss: 34652045909446.07\n",
      "validation loss: 51154514840864.84\n",
      "epoch: 55\n",
      "training loss: 34648411741645.848\n",
      "validation loss: 51118431684193.24\n",
      "epoch: 56\n",
      "training loss: 34644743705751.6\n",
      "validation loss: 51082610485992.445\n",
      "epoch: 57\n",
      "training loss: 34641100790198.98\n",
      "validation loss: 51047288268213.14\n",
      "epoch: 58\n",
      "training loss: 34637493501740.055\n",
      "validation loss: 51012478219818.81\n",
      "epoch: 59\n",
      "training loss: 34633923642407.35\n",
      "validation loss: 50978163074778.58\n",
      "epoch: 60\n",
      "training loss: 34630391269308.754\n",
      "validation loss: 50944325514524.875\n",
      "epoch: 61\n",
      "training loss: 34626895981330.324\n",
      "validation loss: 50910951539171.17\n",
      "epoch: 62\n",
      "training loss: 34623437202157.17\n",
      "validation loss: 50878029859550.78\n",
      "epoch: 63\n",
      "training loss: 34620014269621.53\n",
      "validation loss: 50845551028178.695\n",
      "epoch: 64\n",
      "training loss: 34616626477470.934\n",
      "validation loss: 50813506797581.41\n",
      "epoch: 65\n",
      "training loss: 34613273099551.72\n",
      "validation loss: 50781889693954.74\n",
      "epoch: 66\n",
      "training loss: 34609953404827.137\n",
      "validation loss: 50750692742020.93\n",
      "epoch: 67\n",
      "training loss: 34606666666819.734\n",
      "validation loss: 50719909289781.9\n",
      "epoch: 68\n",
      "training loss: 34603412169495.434\n",
      "validation loss: 50689532898183.26\n",
      "epoch: 69\n",
      "training loss: 34600189210849.355\n",
      "validation loss: 50659557272681.09\n",
      "epoch: 70\n",
      "training loss: 34596997105004.5\n",
      "validation loss: 50629976221725.59\n",
      "epoch: 71\n",
      "training loss: 34593835183350.715\n",
      "validation loss: 50600783632433.164\n",
      "epoch: 72\n",
      "training loss: 34590702795066.07\n",
      "validation loss: 50571973457143.37\n",
      "epoch: 73\n",
      "training loss: 34587599307243.96\n",
      "validation loss: 50543539706785.234\n",
      "epoch: 74\n",
      "training loss: 34584524104770.55\n",
      "validation loss: 50515476448424.84\n",
      "epoch: 75\n",
      "training loss: 34581476590046.65\n",
      "validation loss: 50487777805306.67\n",
      "epoch: 76\n",
      "training loss: 34578456182615.008\n",
      "validation loss: 50460437958310.42\n",
      "epoch: 77\n",
      "training loss: 34575462318732.582\n",
      "validation loss: 50433451148141.35\n",
      "epoch: 78\n",
      "training loss: 34572494450913.07\n",
      "validation loss: 50406811677826.914\n",
      "epoch: 79\n",
      "training loss: 34569552047456.215\n",
      "validation loss: 50380513915257.875\n",
      "epoch: 80\n",
      "training loss: 34566634591974.25\n",
      "validation loss: 50354552295617.71\n",
      "epoch: 81\n",
      "training loss: 34563741582922.035\n",
      "validation loss: 50328921323612.29\n",
      "epoch: 82\n",
      "training loss: 34560872533135.074\n",
      "validation loss: 50303615575454.17\n",
      "epoch: 83\n",
      "training loss: 34558026969377.85\n",
      "validation loss: 50278629700583.72\n",
      "epoch: 84\n",
      "training loss: 34555204431903.824\n",
      "validation loss: 50253958423124.25\n",
      "epoch: 85\n",
      "training loss: 34552404474028.086\n",
      "validation loss: 50229596543080.234\n",
      "epoch: 86\n",
      "training loss: 34549626661712.7\n",
      "validation loss: 50205538937291.38\n",
      "epoch: 87\n",
      "training loss: 34546870573165.05\n",
      "validation loss: 50181780560159.66\n",
      "epoch: 88\n",
      "training loss: 34544135798448.977\n",
      "validation loss: 50158316444167.05\n",
      "epoch: 89\n",
      "training loss: 34541421939108.344\n",
      "validation loss: 50135141700201.7\n",
      "epoch: 90\n",
      "training loss: 34538728607803.082\n",
      "validation loss: 50112251517710.445\n",
      "epoch: 91\n",
      "training loss: 34536055427956.992\n",
      "validation loss: 50089641164694.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92\n",
      "training loss: 34533402033417.258\n",
      "validation loss: 50067305987560.96\n",
      "epoch: 93\n",
      "training loss: 34530768068125.207\n",
      "validation loss: 50045241410854.84\n",
      "epoch: 94\n",
      "training loss: 34528153185797.87\n",
      "validation loss: 50023442936868.055\n",
      "epoch: 95\n",
      "training loss: 34525557049620.227\n",
      "validation loss: 50001906145155.03\n",
      "epoch: 96\n",
      "training loss: 34522979331947.504\n",
      "validation loss: 49980626691955.945\n",
      "epoch: 97\n",
      "training loss: 34520419714017.492\n",
      "validation loss: 49959600309541.57\n",
      "epoch: 98\n",
      "training loss: 34517877885672.19\n",
      "validation loss: 49938822805488.586\n",
      "epoch: 99\n",
      "training loss: 34515353545088.902\n",
      "validation loss: 49918290061894.27\n",
      "epoch: 100\n",
      "training loss: 34512846398520.023\n",
      "validation loss: 49897998034538.28\n",
      "epoch: 101\n",
      "training loss: 34510356160041.59\n",
      "validation loss: 49877942751998.61\n",
      "epoch: 102\n",
      "training loss: 34507882551310.008\n",
      "validation loss: 49858120314728.445\n",
      "epoch: 103\n",
      "training loss: 34505425301326.94\n",
      "validation loss: 49838526894099.8\n",
      "epoch: 104\n",
      "training loss: 34502984146211.887\n",
      "validation loss: 49819158731418.89\n",
      "epoch: 105\n",
      "training loss: 34500558828982.21\n",
      "validation loss: 49800012136918.52\n",
      "epoch: 106\n",
      "training loss: 34498149099340.56\n",
      "validation loss: 49781083488731.86\n",
      "epoch: 107\n",
      "training loss: 34495754713469.11\n",
      "validation loss: 49762369231850.875\n",
      "epoch: 108\n",
      "training loss: 34493375433830.69\n",
      "validation loss: 49743865877073.875\n",
      "epoch: 109\n",
      "training loss: 34491011028976.438\n",
      "validation loss: 49725569999944.87\n",
      "epoch: 110\n",
      "training loss: 34488661273359.625\n",
      "validation loss: 49707478239687.39\n",
      "epoch: 111\n",
      "training loss: 34486325947155.742\n",
      "validation loss: 49689587298135.97\n",
      "epoch: 112\n",
      "training loss: 34484004836088.44\n",
      "validation loss: 49671893938667.21\n",
      "epoch: 113\n",
      "training loss: 34481697731261.016\n",
      "validation loss: 49654394985132.2\n",
      "epoch: 114\n",
      "training loss: 34479404428993.613\n",
      "validation loss: 49637087320793.01\n",
      "epoch: 115\n",
      "training loss: 34477124730665.53\n",
      "validation loss: 49619967887263.97\n",
      "epoch: 116\n",
      "training loss: 34474858442562.793\n",
      "validation loss: 49603033683459.65\n",
      "epoch: 117\n",
      "training loss: 34472605375730.613\n",
      "validation loss: 49586281764550.93\n",
      "epoch: 118\n",
      "training loss: 34470365345830.645\n",
      "validation loss: 49569709240929.84\n",
      "epoch: 119\n",
      "training loss: 34468138173002.88\n",
      "validation loss: 49553313277184.73\n",
      "epoch: 120\n",
      "training loss: 34465923681732.0\n",
      "validation loss: 49537091091086.16\n",
      "epoch: 121\n",
      "training loss: 34463721700718.03\n",
      "validation loss: 49521039952584.555\n",
      "epoch: 122\n",
      "training loss: 34461532062751.188\n",
      "validation loss: 49505157182820.17\n",
      "epoch: 123\n",
      "training loss: 34459354604590.758\n",
      "validation loss: 49489440153145.89\n",
      "epoch: 124\n",
      "training loss: 34457189166847.83\n",
      "validation loss: 49473886284163.52\n",
      "epoch: 125\n",
      "training loss: 34455035593871.836\n",
      "validation loss: 49458493044773.664\n",
      "epoch: 126\n",
      "training loss: 34452893733640.72\n",
      "validation loss: 49443257951239.83\n",
      "epoch: 127\n",
      "training loss: 34450763437654.586\n",
      "validation loss: 49428178566266.79\n",
      "epoch: 128\n",
      "training loss: 34448644560832.77\n",
      "validation loss: 49413252498093.61\n",
      "epoch: 129\n",
      "training loss: 34446536961414.215\n",
      "validation loss: 49398477399601.29\n",
      "epoch: 130\n",
      "training loss: 34444440500860.95\n",
      "validation loss: 49383850967435.25\n",
      "epoch: 131\n",
      "training loss: 34442355043764.707\n",
      "validation loss: 49369370941142.94\n",
      "epoch: 132\n",
      "training loss: 34440280457756.45\n",
      "validation loss: 49355035102325.945\n",
      "epoch: 133\n",
      "training loss: 34438216613418.81\n",
      "validation loss: 49340841273807.57\n",
      "epoch: 134\n",
      "training loss: 34436163384201.234\n",
      "validation loss: 49326787318814.88\n",
      "epoch: 135\n",
      "training loss: 34434120646337.875\n",
      "validation loss: 49312871140175.97\n",
      "epoch: 136\n",
      "training loss: 34432088278768.023\n",
      "validation loss: 49299090679531.71\n",
      "epoch: 137\n",
      "training loss: 34430066163059.047\n",
      "validation loss: 49285443916562.44\n",
      "epoch: 138\n",
      "training loss: 34428054183331.773\n",
      "validation loss: 49271928868229.08\n",
      "epoch: 139\n",
      "training loss: 34426052226188.184\n",
      "validation loss: 49258543588028.766\n",
      "epoch: 140\n",
      "training loss: 34424060180641.387\n",
      "validation loss: 49245286165264.73\n",
      "epoch: 141\n",
      "training loss: 34422077938047.777\n",
      "validation loss: 49232154724330.37\n",
      "epoch: 142\n",
      "training loss: 34420105392041.273\n",
      "validation loss: 49219147424007.26\n",
      "epoch: 143\n",
      "training loss: 34418142438469.684\n",
      "validation loss: 49206262456777.03\n",
      "epoch: 144\n",
      "training loss: 34416188975332.938\n",
      "validation loss: 49193498048146.68\n",
      "epoch: 145\n",
      "training loss: 34414244902723.363\n",
      "validation loss: 49180852455987.516\n",
      "epoch: 146\n",
      "training loss: 34412310122767.68\n",
      "validation loss: 49168323969887.21\n",
      "epoch: 147\n",
      "training loss: 34410384539570.867\n",
      "validation loss: 49155910910514.9\n",
      "epoch: 148\n",
      "training loss: 34408468059161.773\n",
      "validation loss: 49143611628998.95\n",
      "epoch: 149\n",
      "training loss: 34406560589440.31\n",
      "validation loss: 49131424506317.73\n",
      "epoch: 150\n",
      "training loss: 34404662040126.383\n",
      "validation loss: 49119347952702.234\n",
      "epoch: 151\n",
      "training loss: 34402772322710.297\n",
      "validation loss: 49107380407051.336\n",
      "epoch: 152\n",
      "training loss: 34400891350404.76\n",
      "validation loss: 49095520336358.8\n",
      "epoch: 153\n",
      "training loss: 34399019038098.254\n",
      "validation loss: 49083766235151.984\n",
      "epoch: 154\n",
      "training loss: 34397155302309.914\n",
      "validation loss: 49072116624942.35\n",
      "epoch: 155\n",
      "training loss: 34395300061145.777\n",
      "validation loss: 49060570053686.99\n",
      "epoch: 156\n",
      "training loss: 34393453234256.285\n",
      "validation loss: 49049125095261.516\n",
      "epoch: 157\n",
      "training loss: 34391614742795.164\n",
      "validation loss: 49037780348943.74\n",
      "epoch: 158\n",
      "training loss: 34389784509379.52\n",
      "validation loss: 49026534438907.97\n",
      "epoch: 159\n",
      "training loss: 34387962458051.09\n",
      "validation loss: 49015386013729.984\n",
      "epoch: 160\n",
      "training loss: 34386148514238.727\n",
      "validation loss: 49004333745902.18\n",
      "epoch: 161\n",
      "training loss: 34384342604722.023\n",
      "validation loss: 48993376331358.84\n",
      "epoch: 162\n",
      "training loss: 34382544657595.945\n",
      "validation loss: 48982512489011.2\n",
      "epoch: 163\n",
      "training loss: 34380754602236.62\n",
      "validation loss: 48971740960292.266\n",
      "epoch: 164\n",
      "training loss: 34378972369268.09\n",
      "validation loss: 48961060508711.09\n",
      "epoch: 165\n",
      "training loss: 34377197890530.043\n",
      "validation loss: 48950469919416.516\n",
      "epoch: 166\n",
      "training loss: 34375431099046.617\n",
      "validation loss: 48939967998769.53\n",
      "epoch: 167\n",
      "training loss: 34373671928995.99\n",
      "validation loss: 48929553573925.23\n",
      "epoch: 168\n",
      "training loss: 34371920315680.94\n",
      "validation loss: 48919225492422.836\n",
      "epoch: 169\n",
      "training loss: 34370176195500.35\n",
      "validation loss: 48908982621784.836\n",
      "epoch: 170\n",
      "training loss: 34368439505921.35\n",
      "validation loss: 48898823849124.03\n",
      "epoch: 171\n",
      "training loss: 34366710185452.555\n",
      "validation loss: 48888748080759.055\n",
      "epoch: 172\n",
      "training loss: 34364988173617.84\n",
      "validation loss: 48878754241837.91\n",
      "epoch: 173\n",
      "training loss: 34363273410931.066\n",
      "validation loss: 48868841275969.086\n",
      "epoch: 174\n",
      "training loss: 34361565838871.43\n",
      "validation loss: 48859008144860.82\n",
      "epoch: 175\n",
      "training loss: 34359865399859.62\n",
      "validation loss: 48849253827967.49\n",
      "epoch: 176\n",
      "training loss: 34358172037234.62\n",
      "validation loss: 48839577322143.47\n",
      "epoch: 177\n",
      "training loss: 34356485695231.195\n",
      "validation loss: 48829977641304.336\n",
      "epoch: 178\n",
      "training loss: 34354806318958.03\n",
      "validation loss: 48820453816095.04\n",
      "epoch: 179\n",
      "training loss: 34353133854376.527\n",
      "validation loss: 48811004893564.8\n",
      "epoch: 180\n",
      "training loss: 34351468248280.24\n",
      "validation loss: 48801629936849.22\n",
      "epoch: 181\n",
      "training loss: 34349809448274.777\n",
      "validation loss: 48792328024858.45\n",
      "epoch: 182\n",
      "training loss: 34348157402758.453\n",
      "validation loss: 48783098251972.24\n",
      "epoch: 183\n",
      "training loss: 34346512060903.37\n",
      "validation loss: 48773939727741.15\n",
      "epoch: 184\n",
      "training loss: 34344873372637.113\n",
      "validation loss: 48764851576594.0\n",
      "epoch: 185\n",
      "training loss: 34343241288624.95\n",
      "validation loss: 48755832937551.32\n",
      "epoch: 186\n",
      "training loss: 34341615760252.504\n",
      "validation loss: 48746882963944.875\n",
      "epoch: 187\n",
      "training loss: 34339996739608.977\n",
      "validation loss: 48738000823142.74\n",
      "epoch: 188\n",
      "training loss: 34338384179470.824\n",
      "validation loss: 48729185696280.45\n",
      "epoch: 189\n",
      "training loss: 34336778033285.918\n",
      "validation loss: 48720436777997.305\n",
      "epoch: 190\n",
      "training loss: 34335178255158.066\n",
      "validation loss: 48711753276178.23\n",
      "epoch: 191\n",
      "training loss: 34333584799832.094\n",
      "validation loss: 48703134411701.266\n",
      "epoch: 192\n",
      "training loss: 34331997622679.312\n",
      "validation loss: 48694579418189.76\n",
      "epoch: 193\n",
      "training loss: 34330416679683.3\n",
      "validation loss: 48686087541770.055\n",
      "epoch: 194\n",
      "training loss: 34328841927426.215\n",
      "validation loss: 48677658040833.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 195\n",
      "training loss: 34327273323075.406\n",
      "validation loss: 48669290185806.01\n",
      "epoch: 196\n",
      "training loss: 34325710824370.418\n",
      "validation loss: 48660983258916.02\n",
      "epoch: 197\n",
      "training loss: 34324154389610.383\n",
      "validation loss: 48652736553975.52\n",
      "epoch: 198\n",
      "training loss: 34322603977641.746\n",
      "validation loss: 48644549376159.38\n",
      "epoch: 199\n",
      "training loss: 34321059547846.27\n",
      "validation loss: 48636421041791.65\n",
      "epoch: 200\n",
      "training loss: 34319521060129.516\n",
      "validation loss: 48628350878135.84\n",
      "epoch: 201\n",
      "training loss: 34317988474909.504\n",
      "validation loss: 48620338223189.49\n",
      "epoch: 202\n",
      "training loss: 34316461753105.688\n",
      "validation loss: 48612382425482.76\n",
      "epoch: 203\n",
      "training loss: 34314940856128.38\n",
      "validation loss: 48604482843881.42\n",
      "epoch: 204\n",
      "training loss: 34313425745868.312\n",
      "validation loss: 48596638847393.64\n",
      "epoch: 205\n",
      "training loss: 34311916384686.484\n",
      "validation loss: 48588849814980.71\n",
      "epoch: 206\n",
      "training loss: 34310412735404.44\n",
      "validation loss: 48581115135371.82\n",
      "epoch: 207\n",
      "training loss: 34308914761294.59\n",
      "validation loss: 48573434206882.16\n",
      "epoch: 208\n",
      "training loss: 34307422426071.004\n",
      "validation loss: 48565806437235.19\n",
      "epoch: 209\n",
      "training loss: 34305935693880.297\n",
      "validation loss: 48558231243388.2\n",
      "epoch: 210\n",
      "training loss: 34304454529292.83\n",
      "validation loss: 48550708051361.54\n",
      "epoch: 211\n",
      "training loss: 34302978897294.137\n",
      "validation loss: 48543236296071.15\n",
      "epoch: 212\n",
      "training loss: 34301508763276.605\n",
      "validation loss: 48535815421164.7\n",
      "epoch: 213\n",
      "training loss: 34300044093031.285\n",
      "validation loss: 48528444878860.664\n",
      "epoch: 214\n",
      "training loss: 34298584852740.027\n",
      "validation loss: 48521124129791.086\n",
      "epoch: 215\n",
      "training loss: 34297131008967.805\n",
      "validation loss: 48513852642847.195\n",
      "epoch: 216\n",
      "training loss: 34295682528655.15\n",
      "validation loss: 48506629895028.16\n",
      "epoch: 217\n",
      "training loss: 34294239379110.875\n",
      "validation loss: 48499455371292.875\n",
      "epoch: 218\n",
      "training loss: 34292801528005.01\n",
      "validation loss: 48492328564414.93\n",
      "epoch: 219\n",
      "training loss: 34291368943361.797\n",
      "validation loss: 48485248974840.11\n",
      "epoch: 220\n",
      "training loss: 34289941593553.023\n",
      "validation loss: 48478216110546.97\n",
      "epoch: 221\n",
      "training loss: 34288519447291.38\n",
      "validation loss: 48471229486910.266\n",
      "epoch: 222\n",
      "training loss: 34287102473624.125\n",
      "validation loss: 48464288626566.87\n",
      "epoch: 223\n",
      "training loss: 34285690641926.8\n",
      "validation loss: 48457393059284.6\n",
      "epoch: 224\n",
      "training loss: 34284283921897.16\n",
      "validation loss: 48450542321833.32\n",
      "epoch: 225\n",
      "training loss: 34282882283549.277\n",
      "validation loss: 48443735957859.05\n",
      "epoch: 226\n",
      "training loss: 34281485697207.754\n",
      "validation loss: 48436973517760.195\n",
      "epoch: 227\n",
      "training loss: 34280094133502.09\n",
      "validation loss: 48430254558566.4\n",
      "epoch: 228\n",
      "training loss: 34278707563361.19\n",
      "validation loss: 48423578643819.78\n",
      "epoch: 229\n",
      "training loss: 34277325958008.07\n",
      "validation loss: 48416945343458.43\n",
      "epoch: 230\n",
      "training loss: 34275949288954.582\n",
      "validation loss: 48410354233702.3\n",
      "epoch: 231\n",
      "training loss: 34274577527996.38\n",
      "validation loss: 48403804896941.36\n",
      "epoch: 232\n",
      "training loss: 34273210647207.92\n",
      "validation loss: 48397296921625.79\n",
      "epoch: 233\n",
      "training loss: 34271848618937.7\n",
      "validation loss: 48390829902158.52\n",
      "epoch: 234\n",
      "training loss: 34270491415803.45\n",
      "validation loss: 48384403438789.86\n",
      "epoch: 235\n",
      "training loss: 34269139010687.605\n",
      "validation loss: 48378017137513.96\n",
      "epoch: 236\n",
      "training loss: 34267791376732.8\n",
      "validation loss: 48371670609967.56\n",
      "epoch: 237\n",
      "training loss: 34266448487337.504\n",
      "validation loss: 48365363473330.75\n",
      "epoch: 238\n",
      "training loss: 34265110316151.723\n",
      "validation loss: 48359095350229.36\n",
      "epoch: 239\n",
      "training loss: 34263776837072.844\n",
      "validation loss: 48352865868639.48\n",
      "epoch: 240\n",
      "training loss: 34262448024241.59\n",
      "validation loss: 48346674661793.94\n",
      "epoch: 241\n",
      "training loss: 34261123852038.023\n",
      "validation loss: 48340521368090.32\n",
      "epoch: 242\n",
      "training loss: 34259804295077.684\n",
      "validation loss: 48334405631000.93\n",
      "epoch: 243\n",
      "training loss: 34258489328207.797\n",
      "validation loss: 48328327098984.52\n",
      "epoch: 244\n",
      "training loss: 34257178926503.598\n",
      "validation loss: 48322285425399.68\n",
      "epoch: 245\n",
      "training loss: 34255873065264.695\n",
      "validation loss: 48316280268419.91\n",
      "epoch: 246\n",
      "training loss: 34254571720011.574\n",
      "validation loss: 48310311290950.3\n",
      "epoch: 247\n",
      "training loss: 34253274866482.14\n",
      "validation loss: 48304378160545.945\n",
      "epoch: 248\n",
      "training loss: 34251982480628.36\n",
      "validation loss: 48298480549331.68\n",
      "epoch: 249\n",
      "training loss: 34250694538613.016\n",
      "validation loss: 48292618133923.766\n",
      "epoch: 250\n",
      "training loss: 34249411016806.406\n",
      "validation loss: 48286790595352.53\n",
      "epoch: 251\n",
      "training loss: 34248131891783.31\n",
      "validation loss: 48280997618987.06\n",
      "epoch: 252\n",
      "training loss: 34246857140319.87\n",
      "validation loss: 48275238894460.9\n",
      "epoch: 253\n",
      "training loss: 34245586739390.594\n",
      "validation loss: 48269514115599.4\n",
      "epoch: 254\n",
      "training loss: 34244320666165.453\n",
      "validation loss: 48263822980348.47\n",
      "epoch: 255\n",
      "training loss: 34243058898007.004\n",
      "validation loss: 48258165190704.44\n",
      "epoch: 256\n",
      "training loss: 34241801412467.605\n",
      "validation loss: 48252540452645.59\n",
      "epoch: 257\n",
      "training loss: 34240548187286.66\n",
      "validation loss: 48246948476064.83\n",
      "epoch: 258\n",
      "training loss: 34239299200387.94\n",
      "validation loss: 48241388974703.516\n",
      "epoch: 259\n",
      "training loss: 34238054429877.0\n",
      "validation loss: 48235861666086.81\n",
      "epoch: 260\n",
      "training loss: 34236813854038.598\n",
      "validation loss: 48230366271460.055\n",
      "epoch: 261\n",
      "training loss: 34235577451334.156\n",
      "validation loss: 48224902515726.4\n",
      "epoch: 262\n",
      "training loss: 34234345200399.37\n",
      "validation loss: 48219470127385.695\n",
      "epoch: 263\n",
      "training loss: 34233117080041.766\n",
      "validation loss: 48214068838474.31\n",
      "epoch: 264\n",
      "training loss: 34231893069238.395\n",
      "validation loss: 48208698384506.484\n",
      "epoch: 265\n",
      "training loss: 34230673147133.477\n",
      "validation loss: 48203358504416.266\n",
      "epoch: 266\n",
      "training loss: 34229457293036.19\n",
      "validation loss: 48198048940500.99\n",
      "epoch: 267\n",
      "training loss: 34228245486418.5\n",
      "validation loss: 48192769438365.63\n",
      "epoch: 268\n",
      "training loss: 34227037706912.902\n",
      "validation loss: 48187519746868.14\n",
      "epoch: 269\n",
      "training loss: 34225833934310.434\n",
      "validation loss: 48182299618065.86\n",
      "epoch: 270\n",
      "training loss: 34224634148558.504\n",
      "validation loss: 48177108807163.06\n",
      "epoch: 271\n",
      "training loss: 34223438329758.938\n",
      "validation loss: 48171947072459.266\n",
      "epoch: 272\n",
      "training loss: 34222246458165.95\n",
      "validation loss: 48166814175298.555\n",
      "epoch: 273\n",
      "training loss: 34221058514184.215\n",
      "validation loss: 48161709880020.04\n",
      "epoch: 274\n",
      "training loss: 34219874478366.957\n",
      "validation loss: 48156633953908.98\n",
      "epoch: 275\n",
      "training loss: 34218694331414.12\n",
      "validation loss: 48151586167148.96\n",
      "epoch: 276\n",
      "training loss: 34217518054170.45\n",
      "validation loss: 48146566292774.92\n",
      "epoch: 277\n",
      "training loss: 34216345627623.79\n",
      "validation loss: 48141574106626.945\n",
      "epoch: 278\n",
      "training loss: 34215177032903.277\n",
      "validation loss: 48136609387305.234\n",
      "epoch: 279\n",
      "training loss: 34214012251277.62\n",
      "validation loss: 48131671916125.34\n",
      "epoch: 280\n",
      "training loss: 34212851264153.438\n",
      "validation loss: 48126761477074.95\n",
      "epoch: 281\n",
      "training loss: 34211694053073.555\n",
      "validation loss: 48121877856770.83\n",
      "epoch: 282\n",
      "training loss: 34210540599715.39\n",
      "validation loss: 48117020844416.81\n",
      "epoch: 283\n",
      "training loss: 34209390885889.4\n",
      "validation loss: 48112190231762.695\n",
      "epoch: 284\n",
      "training loss: 34208244893537.438\n",
      "validation loss: 48107385813063.58\n",
      "epoch: 285\n",
      "training loss: 34207102604731.305\n",
      "validation loss: 48102607385040.32\n",
      "epoch: 286\n",
      "training loss: 34205964001671.164\n",
      "validation loss: 48097854746840.164\n",
      "epoch: 287\n",
      "training loss: 34204829066684.11\n",
      "validation loss: 48093127699998.88\n",
      "epoch: 288\n",
      "training loss: 34203697782222.676\n",
      "validation loss: 48088426048402.72\n",
      "epoch: 289\n",
      "training loss: 34202570130863.453\n",
      "validation loss: 48083749598251.79\n",
      "epoch: 290\n",
      "training loss: 34201446095305.664\n",
      "validation loss: 48079098158023.55\n",
      "epoch: 291\n",
      "training loss: 34200325658369.79\n",
      "validation loss: 48074471538437.36\n",
      "epoch: 292\n",
      "training loss: 34199208802996.21\n",
      "validation loss: 48069869552419.445\n",
      "epoch: 293\n",
      "training loss: 34198095512243.914\n",
      "validation loss: 48065292015068.68\n",
      "epoch: 294\n",
      "training loss: 34196985769289.125\n",
      "validation loss: 48060738743622.51\n",
      "epoch: 295\n",
      "training loss: 34195879557424.11\n",
      "validation loss: 48056209557424.35\n",
      "epoch: 296\n",
      "training loss: 34194776860055.86\n",
      "validation loss: 48051704277890.71\n",
      "epoch: 297\n",
      "training loss: 34193677660704.863\n",
      "validation loss: 48047222728479.305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 298\n",
      "training loss: 34192581943003.92\n",
      "validation loss: 48042764734657.84\n",
      "epoch: 299\n",
      "training loss: 34191489690696.92\n",
      "validation loss: 48038330123873.15\n",
      "epoch: 300\n",
      "training loss: 34190400887637.664\n",
      "validation loss: 48033918725520.79\n",
      "epoch: 301\n",
      "training loss: 34189315517788.723\n",
      "validation loss: 48029530370915.51\n",
      "epoch: 302\n",
      "training loss: 34188233565220.32\n",
      "validation loss: 48025164893262.02\n",
      "epoch: 303\n",
      "training loss: 34187155014109.176\n",
      "validation loss: 48020822127626.41\n",
      "epoch: 304\n",
      "training loss: 34186079848737.457\n",
      "validation loss: 48016501910907.7\n",
      "epoch: 305\n",
      "training loss: 34185008053491.64\n",
      "validation loss: 48012204081810.516\n",
      "epoch: 306\n",
      "training loss: 34183939612861.52\n",
      "validation loss: 48007928480817.79\n",
      "epoch: 307\n",
      "training loss: 34182874511439.098\n",
      "validation loss: 48003674950163.99\n",
      "epoch: 308\n",
      "training loss: 34181812733917.58\n",
      "validation loss: 47999443333809.02\n",
      "epoch: 309\n",
      "training loss: 34180754265090.387\n",
      "validation loss: 47995233477412.414\n",
      "epoch: 310\n",
      "training loss: 34179699089850.13\n",
      "validation loss: 47991045228308.016\n",
      "epoch: 311\n",
      "training loss: 34178647193187.65\n",
      "validation loss: 47986878435479.11\n",
      "epoch: 312\n",
      "training loss: 34177598560191.027\n",
      "validation loss: 47982732949534.05\n",
      "epoch: 313\n",
      "training loss: 34176553176044.66\n",
      "validation loss: 47978608622682.22\n",
      "epoch: 314\n",
      "training loss: 34175511026028.312\n",
      "validation loss: 47974505308710.46\n",
      "epoch: 315\n",
      "training loss: 34174472095516.227\n",
      "validation loss: 47970422862959.88\n",
      "epoch: 316\n",
      "training loss: 34173436369976.137\n",
      "validation loss: 47966361142303.016\n",
      "epoch: 317\n",
      "training loss: 34172403834968.477\n",
      "validation loss: 47962320005121.68\n",
      "epoch: 318\n",
      "training loss: 34171374476145.414\n",
      "validation loss: 47958299311284.74\n",
      "epoch: 319\n",
      "training loss: 34170348279250.07\n",
      "validation loss: 47954298922126.63\n",
      "epoch: 320\n",
      "training loss: 34169325230115.57\n",
      "validation loss: 47950318700426.14\n",
      "epoch: 321\n",
      "training loss: 34168305314664.273\n",
      "validation loss: 47946358510385.4\n",
      "epoch: 322\n",
      "training loss: 34167288518906.902\n",
      "validation loss: 47942418217609.56\n",
      "epoch: 323\n",
      "training loss: 34166274828941.777\n",
      "validation loss: 47938497689086.54\n",
      "epoch: 324\n",
      "training loss: 34165264230953.953\n",
      "validation loss: 47934596793167.195\n",
      "epoch: 325\n",
      "training loss: 34164256711214.45\n",
      "validation loss: 47930715399545.9\n",
      "epoch: 326\n",
      "training loss: 34163252256079.484\n",
      "validation loss: 47926853379241.38\n",
      "epoch: 327\n",
      "training loss: 34162250851989.676\n",
      "validation loss: 47923010604577.984\n",
      "epoch: 328\n",
      "training loss: 34161252485469.305\n",
      "validation loss: 47919186949166.984\n",
      "epoch: 329\n",
      "training loss: 34160257143125.535\n",
      "validation loss: 47915382287888.664\n",
      "epoch: 330\n",
      "training loss: 34159264811647.723\n",
      "validation loss: 47911596496874.26\n",
      "epoch: 331\n",
      "training loss: 34158275477806.62\n",
      "validation loss: 47907829453488.445\n",
      "epoch: 332\n",
      "training loss: 34157289128453.71\n",
      "validation loss: 47904081036312.09\n",
      "epoch: 333\n",
      "training loss: 34156305750520.477\n",
      "validation loss: 47900351125125.3\n",
      "epoch: 334\n",
      "training loss: 34155325331017.74\n",
      "validation loss: 47896639600890.69\n",
      "epoch: 335\n",
      "training loss: 34154347857034.87\n",
      "validation loss: 47892946345736.86\n",
      "epoch: 336\n",
      "training loss: 34153373315739.195\n",
      "validation loss: 47889271242942.53\n",
      "epoch: 337\n",
      "training loss: 34152401694375.324\n",
      "validation loss: 47885614176920.445\n",
      "epoch: 338\n",
      "training loss: 34151432980264.445\n",
      "validation loss: 47881975033201.914\n",
      "epoch: 339\n",
      "training loss: 34150467160803.64\n",
      "validation loss: 47878353698421.24\n",
      "epoch: 340\n",
      "training loss: 34149504223465.36\n",
      "validation loss: 47874750060301.07\n",
      "epoch: 341\n",
      "training loss: 34148544155796.64\n",
      "validation loss: 47871164007637.11\n",
      "epoch: 342\n",
      "training loss: 34147586945418.58\n",
      "validation loss: 47867595430283.72\n",
      "epoch: 343\n",
      "training loss: 34146632580025.656\n",
      "validation loss: 47864044219139.695\n",
      "epoch: 344\n",
      "training loss: 34145681047385.14\n",
      "validation loss: 47860510266133.93\n",
      "epoch: 345\n",
      "training loss: 34144732335336.504\n",
      "validation loss: 47856993464211.805\n",
      "epoch: 346\n",
      "training loss: 34143786431790.742\n",
      "validation loss: 47853493707321.21\n",
      "epoch: 347\n",
      "training loss: 34142843324729.89\n",
      "validation loss: 47850010890399.41\n",
      "epoch: 348\n",
      "training loss: 34141903002206.363\n",
      "validation loss: 47846544909359.75\n",
      "epoch: 349\n",
      "training loss: 34140965452342.406\n",
      "validation loss: 47843095661078.62\n",
      "epoch: 350\n",
      "training loss: 34140030663329.504\n",
      "validation loss: 47839663043382.734\n",
      "epoch: 351\n",
      "training loss: 34139098623427.848\n",
      "validation loss: 47836246955036.58\n",
      "epoch: 352\n",
      "training loss: 34138169320965.766\n",
      "validation loss: 47832847295730.06\n",
      "epoch: 353\n",
      "training loss: 34137242744339.164\n",
      "validation loss: 47829463966066.195\n",
      "epoch: 354\n",
      "training loss: 34136318882010.98\n",
      "validation loss: 47826096867549.375\n",
      "epoch: 355\n",
      "training loss: 34135397722510.67\n",
      "validation loss: 47822745902573.52\n",
      "epoch: 356\n",
      "training loss: 34134479254433.652\n",
      "validation loss: 47819410974410.42\n",
      "epoch: 357\n",
      "training loss: 34133563466440.79\n",
      "validation loss: 47816091987198.48\n",
      "epoch: 358\n",
      "training loss: 34132650347257.875\n",
      "validation loss: 47812788845931.305\n",
      "epoch: 359\n",
      "training loss: 34131739885675.14\n",
      "validation loss: 47809501456446.97\n",
      "epoch: 360\n",
      "training loss: 34130832070546.676\n",
      "validation loss: 47806229725416.88\n",
      "epoch: 361\n",
      "training loss: 34129926890790.03\n",
      "validation loss: 47802973560335.37\n",
      "epoch: 362\n",
      "training loss: 34129024335385.637\n",
      "validation loss: 47799732869508.87\n",
      "epoch: 363\n",
      "training loss: 34128124393376.35\n",
      "validation loss: 47796507562045.94\n",
      "epoch: 364\n",
      "training loss: 34127227053866.99\n",
      "validation loss: 47793297547846.766\n",
      "epoch: 365\n",
      "training loss: 34126332306023.777\n",
      "validation loss: 47790102737593.38\n",
      "epoch: 366\n",
      "training loss: 34125440139073.992\n",
      "validation loss: 47786923042739.67\n",
      "epoch: 367\n",
      "training loss: 34124550542305.39\n",
      "validation loss: 47783758375501.8\n",
      "epoch: 368\n",
      "training loss: 34123663505065.766\n",
      "validation loss: 47780608648848.51\n",
      "epoch: 369\n",
      "training loss: 34122779016762.547\n",
      "validation loss: 47777473776491.86\n",
      "epoch: 370\n",
      "training loss: 34121897066862.266\n",
      "validation loss: 47774353672877.91\n",
      "epoch: 371\n",
      "training loss: 34121017644890.195\n",
      "validation loss: 47771248253177.74\n",
      "epoch: 372\n",
      "training loss: 34120140740429.797\n",
      "validation loss: 47768157433278.29\n",
      "epoch: 373\n",
      "training loss: 34119266343122.39\n",
      "validation loss: 47765081129773.73\n",
      "epoch: 374\n",
      "training loss: 34118394442666.668\n",
      "validation loss: 47762019259956.59\n",
      "epoch: 375\n",
      "training loss: 34117525028818.24\n",
      "validation loss: 47758971741809.44\n",
      "epoch: 376\n",
      "training loss: 34116658091389.266\n",
      "validation loss: 47755938493996.27\n",
      "epoch: 377\n",
      "training loss: 34115793620248.0\n",
      "validation loss: 47752919435854.37\n",
      "epoch: 378\n",
      "training loss: 34114931605318.37\n",
      "validation loss: 47749914487386.06\n",
      "epoch: 379\n",
      "training loss: 34114072036579.6\n",
      "validation loss: 47746923569250.73\n",
      "epoch: 380\n",
      "training loss: 34113214904065.758\n",
      "validation loss: 47743946602756.88\n",
      "epoch: 381\n",
      "training loss: 34112360197865.383\n",
      "validation loss: 47740983509854.4\n",
      "epoch: 382\n",
      "training loss: 34111507908121.062\n",
      "validation loss: 47738034213126.81\n",
      "epoch: 383\n",
      "training loss: 34110658025029.082\n",
      "validation loss: 47735098635783.86\n",
      "epoch: 384\n",
      "training loss: 34109810538838.977\n",
      "validation loss: 47732176701653.9\n",
      "epoch: 385\n",
      "training loss: 34108965439853.168\n",
      "validation loss: 47729268335176.805\n",
      "epoch: 386\n",
      "training loss: 34108122718426.613\n",
      "validation loss: 47726373461396.56\n",
      "epoch: 387\n",
      "training loss: 34107282364966.348\n",
      "validation loss: 47723492005954.21\n",
      "epoch: 388\n",
      "training loss: 34106444369931.195\n",
      "validation loss: 47720623895080.91\n",
      "epoch: 389\n",
      "training loss: 34105608723831.35\n",
      "validation loss: 47717769055591.12\n",
      "epoch: 390\n",
      "training loss: 34104775417228.004\n",
      "validation loss: 47714927414875.59\n",
      "epoch: 391\n",
      "training loss: 34103944440732.992\n",
      "validation loss: 47712098900894.88\n",
      "epoch: 392\n",
      "training loss: 34103115785008.43\n",
      "validation loss: 47709283442172.77\n",
      "epoch: 393\n",
      "training loss: 34102289440766.387\n",
      "validation loss: 47706480967789.61\n",
      "epoch: 394\n",
      "training loss: 34101465398768.453\n",
      "validation loss: 47703691407376.19\n",
      "epoch: 395\n",
      "training loss: 34100643649825.492\n",
      "validation loss: 47700914691107.164\n",
      "epoch: 396\n",
      "training loss: 34099824184797.184\n",
      "validation loss: 47698150749695.125\n",
      "epoch: 397\n",
      "training loss: 34099006994591.785\n",
      "validation loss: 47695399514384.35\n",
      "epoch: 398\n",
      "training loss: 34098192070165.723\n",
      "validation loss: 47692660916944.79\n",
      "epoch: 399\n",
      "training loss: 34097379402523.246\n",
      "validation loss: 47689934889666.14\n",
      "epoch: 400\n",
      "training loss: 34096568982716.156\n",
      "validation loss: 47687221365352.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 401\n",
      "training loss: 34095760801843.418\n",
      "validation loss: 47684520277314.62\n",
      "epoch: 402\n",
      "training loss: 34094954851050.85\n",
      "validation loss: 47681831559367.95\n",
      "epoch: 403\n",
      "training loss: 34094151121530.824\n",
      "validation loss: 47679155145823.445\n",
      "epoch: 404\n",
      "training loss: 34093349604521.883\n",
      "validation loss: 47676490971483.88\n",
      "epoch: 405\n",
      "training loss: 34092550291308.484\n",
      "validation loss: 47673838971637.97\n",
      "epoch: 406\n",
      "training loss: 34091753173220.645\n",
      "validation loss: 47671199082055.086\n",
      "epoch: 407\n",
      "training loss: 34090958241633.656\n",
      "validation loss: 47668571238980.1\n",
      "epoch: 408\n",
      "training loss: 34090165487967.746\n",
      "validation loss: 47665955379127.96\n",
      "epoch: 409\n",
      "training loss: 34089374903687.773\n",
      "validation loss: 47663351439678.82\n",
      "epoch: 410\n",
      "training loss: 34088586480302.957\n",
      "validation loss: 47660759358272.78\n",
      "epoch: 411\n",
      "training loss: 34087800209366.53\n",
      "validation loss: 47658179073005.07\n",
      "epoch: 412\n",
      "training loss: 34087016082475.477\n",
      "validation loss: 47655610522421.06\n",
      "epoch: 413\n",
      "training loss: 34086234091270.215\n",
      "validation loss: 47653053645511.5\n",
      "epoch: 414\n",
      "training loss: 34085454227434.3\n",
      "validation loss: 47650508381707.73\n",
      "epoch: 415\n",
      "training loss: 34084676482694.145\n",
      "validation loss: 47647974670876.945\n",
      "epoch: 416\n",
      "training loss: 34083900848818.715\n",
      "validation loss: 47645452453317.625\n",
      "epoch: 417\n",
      "training loss: 34083127317619.293\n",
      "validation loss: 47642941669754.984\n",
      "epoch: 418\n",
      "training loss: 34082355880949.11\n",
      "validation loss: 47640442261336.42\n",
      "epoch: 419\n",
      "training loss: 34081586530703.152\n",
      "validation loss: 47637954169627.16\n",
      "epoch: 420\n",
      "training loss: 34080819258817.805\n",
      "validation loss: 47635477336605.734\n",
      "epoch: 421\n",
      "training loss: 34080054057270.625\n",
      "validation loss: 47633011704659.81\n",
      "epoch: 422\n",
      "training loss: 34079290918080.08\n",
      "validation loss: 47630557216582.03\n",
      "epoch: 423\n",
      "training loss: 34078529833305.223\n",
      "validation loss: 47628113815565.51\n",
      "epoch: 424\n",
      "training loss: 34077770795045.46\n",
      "validation loss: 47625681445199.984\n",
      "epoch: 425\n",
      "training loss: 34077013795440.28\n",
      "validation loss: 47623260049467.6\n",
      "epoch: 426\n",
      "training loss: 34076258826668.99\n",
      "validation loss: 47620849572739.03\n",
      "epoch: 427\n",
      "training loss: 34075505880950.426\n",
      "validation loss: 47618449959769.3\n",
      "epoch: 428\n",
      "training loss: 34074754950542.766\n",
      "validation loss: 47616061155694.09\n",
      "epoch: 429\n",
      "training loss: 34074006027743.17\n",
      "validation loss: 47613683106025.695\n",
      "epoch: 430\n",
      "training loss: 34073259104887.605\n",
      "validation loss: 47611315756649.41\n",
      "epoch: 431\n",
      "training loss: 34072514174350.586\n",
      "validation loss: 47608959053819.586\n",
      "epoch: 432\n",
      "training loss: 34071771228544.85\n",
      "validation loss: 47606612944156.164\n",
      "epoch: 433\n",
      "training loss: 34071030259921.2\n",
      "validation loss: 47604277374640.7\n",
      "epoch: 434\n",
      "training loss: 34070291260968.223\n",
      "validation loss: 47601952292613.17\n",
      "epoch: 435\n",
      "training loss: 34069554224212.035\n",
      "validation loss: 47599637645768.07\n",
      "epoch: 436\n",
      "training loss: 34068819142216.016\n",
      "validation loss: 47597333382151.1\n",
      "epoch: 437\n",
      "training loss: 34068086007580.637\n",
      "validation loss: 47595039450155.59\n",
      "epoch: 438\n",
      "training loss: 34067354812943.188\n",
      "validation loss: 47592755798519.28\n",
      "epoch: 439\n",
      "training loss: 34066625550977.504\n",
      "validation loss: 47590482376320.734\n",
      "epoch: 440\n",
      "training loss: 34065898214393.77\n",
      "validation loss: 47588219132976.195\n",
      "epoch: 441\n",
      "training loss: 34065172795938.3\n",
      "validation loss: 47585966018236.1\n",
      "epoch: 442\n",
      "training loss: 34064449288393.293\n",
      "validation loss: 47583722982182.195\n",
      "epoch: 443\n",
      "training loss: 34063727684576.56\n",
      "validation loss: 47581489975223.945\n",
      "epoch: 444\n",
      "training loss: 34063007977341.375\n",
      "validation loss: 47579266948095.695\n",
      "epoch: 445\n",
      "training loss: 34062290159576.19\n",
      "validation loss: 47577053851853.375\n",
      "epoch: 446\n",
      "training loss: 34061574224204.457\n",
      "validation loss: 47574850637871.59\n",
      "epoch: 447\n",
      "training loss: 34060860164184.33\n",
      "validation loss: 47572657257840.484\n",
      "epoch: 448\n",
      "training loss: 34060147972508.582\n",
      "validation loss: 47570473663762.73\n",
      "epoch: 449\n",
      "training loss: 34059437642204.215\n",
      "validation loss: 47568299807950.67\n",
      "epoch: 450\n",
      "training loss: 34058729166332.387\n",
      "validation loss: 47566135643023.44\n",
      "epoch: 451\n",
      "training loss: 34058022537988.125\n",
      "validation loss: 47563981121903.95\n",
      "epoch: 452\n",
      "training loss: 34057317750300.137\n",
      "validation loss: 47561836197816.16\n",
      "epoch: 453\n",
      "training loss: 34056614796430.586\n",
      "validation loss: 47559700824282.27\n",
      "epoch: 454\n",
      "training loss: 34055913669574.914\n",
      "validation loss: 47557574955120.01\n",
      "epoch: 455\n",
      "training loss: 34055214362961.56\n",
      "validation loss: 47555458544439.695\n",
      "epoch: 456\n",
      "training loss: 34054516869851.855\n",
      "validation loss: 47553351546641.92\n",
      "epoch: 457\n",
      "training loss: 34053821183539.746\n",
      "validation loss: 47551253916414.484\n",
      "epoch: 458\n",
      "training loss: 34053127297351.605\n",
      "validation loss: 47549165608729.99\n",
      "epoch: 459\n",
      "training loss: 34052435204646.06\n",
      "validation loss: 47547086578843.34\n",
      "epoch: 460\n",
      "training loss: 34051744898813.758\n",
      "validation loss: 47545016782288.95\n",
      "epoch: 461\n",
      "training loss: 34051056373277.17\n",
      "validation loss: 47542956174878.37\n",
      "epoch: 462\n",
      "training loss: 34050369621490.47\n",
      "validation loss: 47540904712697.83\n",
      "epoch: 463\n",
      "training loss: 34049684636939.176\n",
      "validation loss: 47538862352105.54\n",
      "epoch: 464\n",
      "training loss: 34049001413140.137\n",
      "validation loss: 47536829049729.586\n",
      "epoch: 465\n",
      "training loss: 34048319943641.258\n",
      "validation loss: 47534804762465.39\n",
      "epoch: 466\n",
      "training loss: 34047640222021.277\n",
      "validation loss: 47532789447473.164\n",
      "epoch: 467\n",
      "training loss: 34046962241889.633\n",
      "validation loss: 47530783062175.84\n",
      "epoch: 468\n",
      "training loss: 34046285996886.277\n",
      "validation loss: 47528785564256.64\n",
      "epoch: 469\n",
      "training loss: 34045611480681.45\n",
      "validation loss: 47526796911656.73\n",
      "epoch: 470\n",
      "training loss: 34044938686975.504\n",
      "validation loss: 47524817062573.04\n",
      "epoch: 471\n",
      "training loss: 34044267609498.79\n",
      "validation loss: 47522845975456.11\n",
      "epoch: 472\n",
      "training loss: 34043598242011.355\n",
      "validation loss: 47520883609007.69\n",
      "epoch: 473\n",
      "training loss: 34042930578302.86\n",
      "validation loss: 47518929922178.74\n",
      "epoch: 474\n",
      "training loss: 34042264612192.375\n",
      "validation loss: 47516984874167.21\n",
      "epoch: 475\n",
      "training loss: 34041600337528.18\n",
      "validation loss: 47515048424415.92\n",
      "epoch: 476\n",
      "training loss: 34040937748187.59\n",
      "validation loss: 47513120532610.414\n",
      "epoch: 477\n",
      "training loss: 34040276838076.836\n",
      "validation loss: 47511201158677.016\n",
      "epoch: 478\n",
      "training loss: 34039617601130.836\n",
      "validation loss: 47509290262780.766\n",
      "epoch: 479\n",
      "training loss: 34038960031313.016\n",
      "validation loss: 47507387805323.164\n",
      "epoch: 480\n",
      "training loss: 34038304122615.17\n",
      "validation loss: 47505493746940.51\n",
      "epoch: 481\n",
      "training loss: 34037649869057.305\n",
      "validation loss: 47503608048501.66\n",
      "epoch: 482\n",
      "training loss: 34036997264687.426\n",
      "validation loss: 47501730671106.27\n",
      "epoch: 483\n",
      "training loss: 34036346303581.387\n",
      "validation loss: 47499861576082.73\n",
      "epoch: 484\n",
      "training loss: 34035696979842.78\n",
      "validation loss: 47498000724986.38\n",
      "epoch: 485\n",
      "training loss: 34035049287602.645\n",
      "validation loss: 47496148079597.39\n",
      "epoch: 486\n",
      "training loss: 34034403221019.44\n",
      "validation loss: 47494303601919.23\n",
      "epoch: 487\n",
      "training loss: 34033758774278.84\n",
      "validation loss: 47492467254176.695\n",
      "epoch: 488\n",
      "training loss: 34033115941593.5\n",
      "validation loss: 47490638998813.875\n",
      "epoch: 489\n",
      "training loss: 34032474717203.0\n",
      "validation loss: 47488818798492.68\n",
      "epoch: 490\n",
      "training loss: 34031835095373.625\n",
      "validation loss: 47487006616090.93\n",
      "epoch: 491\n",
      "training loss: 34031197070398.24\n",
      "validation loss: 47485202414700.48\n",
      "epoch: 492\n",
      "training loss: 34030560636596.11\n",
      "validation loss: 47483406157625.79\n",
      "epoch: 493\n",
      "training loss: 34029925788312.742\n",
      "validation loss: 47481617808381.82\n",
      "epoch: 494\n",
      "training loss: 34029292519919.78\n",
      "validation loss: 47479837330692.68\n",
      "epoch: 495\n",
      "training loss: 34028660825814.793\n",
      "validation loss: 47478064688489.73\n",
      "epoch: 496\n",
      "training loss: 34028030700421.13\n",
      "validation loss: 47476299845909.99\n",
      "epoch: 497\n",
      "training loss: 34027402138187.832\n",
      "validation loss: 47474542767294.57\n",
      "epoch: 498\n",
      "training loss: 34026775133589.41\n",
      "validation loss: 47472793417186.98\n",
      "epoch: 499\n",
      "training loss: 34026149681125.734\n",
      "validation loss: 47471051760331.47\n",
      "epoch: 500\n",
      "training loss: 34025525775321.88\n",
      "validation loss: 47469317761671.61\n",
      "epoch: 501\n",
      "training loss: 34024903410727.965\n",
      "validation loss: 47467591386348.54\n",
      "epoch: 502\n",
      "training loss: 34024282581919.047\n",
      "validation loss: 47465872599699.555\n",
      "epoch: 503\n",
      "training loss: 34023663283494.934\n",
      "validation loss: 47464161367256.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 504\n",
      "training loss: 34023045510080.074\n",
      "validation loss: 47462457654744.44\n",
      "epoch: 505\n",
      "training loss: 34022429256323.39\n",
      "validation loss: 47460761428079.65\n",
      "epoch: 506\n",
      "training loss: 34021814516898.152\n",
      "validation loss: 47459072653368.83\n",
      "epoch: 507\n",
      "training loss: 34021201286501.84\n",
      "validation loss: 47457391296907.16\n",
      "epoch: 508\n",
      "training loss: 34020589559856.008\n",
      "validation loss: 47455717325176.93\n",
      "epoch: 509\n",
      "training loss: 34019979331706.117\n",
      "validation loss: 47454050704846.29\n",
      "epoch: 510\n",
      "training loss: 34019370596821.434\n",
      "validation loss: 47452391402767.59\n",
      "epoch: 511\n",
      "training loss: 34018763349994.887\n",
      "validation loss: 47450739385976.19\n",
      "epoch: 512\n",
      "training loss: 34018157586042.938\n",
      "validation loss: 47449094621688.9\n",
      "epoch: 513\n",
      "training loss: 34017553299805.395\n",
      "validation loss: 47447457077302.72\n",
      "epoch: 514\n",
      "training loss: 34016950486145.375\n",
      "validation loss: 47445826720393.42\n",
      "epoch: 515\n",
      "training loss: 34016349139949.082\n",
      "validation loss: 47444203518714.234\n",
      "epoch: 516\n",
      "training loss: 34015749256125.723\n",
      "validation loss: 47442587440194.51\n",
      "epoch: 517\n",
      "training loss: 34015150829607.37\n",
      "validation loss: 47440978452938.42\n",
      "epoch: 518\n",
      "training loss: 34014553855348.844\n",
      "validation loss: 47439376525223.664\n",
      "epoch: 519\n",
      "training loss: 34013958328327.57\n",
      "validation loss: 47437781625500.1\n",
      "epoch: 520\n",
      "training loss: 34013364243543.414\n",
      "validation loss: 47436193722388.56\n",
      "epoch: 521\n",
      "training loss: 34012771596018.65\n",
      "validation loss: 47434612784679.59\n",
      "epoch: 522\n",
      "training loss: 34012180380797.74\n",
      "validation loss: 47433038781332.1\n",
      "epoch: 523\n",
      "training loss: 34011590592947.273\n",
      "validation loss: 47431471681472.22\n",
      "epoch: 524\n",
      "training loss: 34011002227555.79\n",
      "validation loss: 47429911454392.15\n",
      "epoch: 525\n",
      "training loss: 34010415279733.758\n",
      "validation loss: 47428358069548.84\n",
      "epoch: 526\n",
      "training loss: 34009829744613.27\n",
      "validation loss: 47426811496562.69\n",
      "epoch: 527\n",
      "training loss: 34009245617348.133\n",
      "validation loss: 47425271705216.586\n",
      "epoch: 528\n",
      "training loss: 34008662893113.582\n",
      "validation loss: 47423738665454.65\n",
      "epoch: 529\n",
      "training loss: 34008081567106.29\n",
      "validation loss: 47422212347381.12\n",
      "epoch: 530\n",
      "training loss: 34007501634544.117\n",
      "validation loss: 47420692721259.016\n",
      "epoch: 531\n",
      "training loss: 34006923090666.094\n",
      "validation loss: 47419179757509.16\n",
      "epoch: 532\n",
      "training loss: 34006345930732.297\n",
      "validation loss: 47417673426709.086\n",
      "epoch: 533\n",
      "training loss: 34005770150023.67\n",
      "validation loss: 47416173699591.87\n",
      "epoch: 534\n",
      "training loss: 34005195743841.965\n",
      "validation loss: 47414680547044.99\n",
      "epoch: 535\n",
      "training loss: 34004622707509.6\n",
      "validation loss: 47413193940109.27\n",
      "epoch: 536\n",
      "training loss: 34004051036369.574\n",
      "validation loss: 47411713849977.82\n",
      "epoch: 537\n",
      "training loss: 34003480725785.324\n",
      "validation loss: 47410240247994.92\n",
      "epoch: 538\n",
      "training loss: 34002911771140.613\n",
      "validation loss: 47408773105655.08\n",
      "epoch: 539\n",
      "training loss: 34002344167839.44\n",
      "validation loss: 47407312394601.67\n",
      "epoch: 540\n",
      "training loss: 34001777911305.918\n",
      "validation loss: 47405858086626.4\n",
      "epoch: 541\n",
      "training loss: 34001212996984.188\n",
      "validation loss: 47404410153667.82\n",
      "epoch: 542\n",
      "training loss: 34000649420338.246\n",
      "validation loss: 47402968567810.52\n",
      "epoch: 543\n",
      "training loss: 34000087176851.902\n",
      "validation loss: 47401533301284.13\n",
      "epoch: 544\n",
      "training loss: 33999526262028.637\n",
      "validation loss: 47400104326462.23\n",
      "epoch: 545\n",
      "training loss: 33998966671391.516\n",
      "validation loss: 47398681615861.445\n",
      "epoch: 546\n",
      "training loss: 33998408400483.066\n",
      "validation loss: 47397265142140.48\n",
      "epoch: 547\n",
      "training loss: 33997851444865.156\n",
      "validation loss: 47395854878098.984\n",
      "epoch: 548\n",
      "training loss: 33997295800118.95\n",
      "validation loss: 47394450796676.76\n",
      "epoch: 549\n",
      "training loss: 33996741461844.734\n",
      "validation loss: 47393052870952.836\n",
      "epoch: 550\n",
      "training loss: 33996188425661.855\n",
      "validation loss: 47391661074144.36\n",
      "epoch: 551\n",
      "training loss: 33995636687208.613\n",
      "validation loss: 47390275379605.79\n",
      "epoch: 552\n",
      "training loss: 33995086242142.137\n",
      "validation loss: 47388895760827.99\n",
      "epoch: 553\n",
      "training loss: 33994537086138.32\n",
      "validation loss: 47387522191437.234\n",
      "epoch: 554\n",
      "training loss: 33993989214891.69\n",
      "validation loss: 47386154645194.3\n",
      "epoch: 555\n",
      "training loss: 33993442624115.305\n",
      "validation loss: 47384793095993.61\n",
      "epoch: 556\n",
      "training loss: 33992897309540.67\n",
      "validation loss: 47383437517862.266\n",
      "epoch: 557\n",
      "training loss: 33992353266917.676\n",
      "validation loss: 47382087884959.48\n",
      "epoch: 558\n",
      "training loss: 33991810492014.387\n",
      "validation loss: 47380744171575.14\n",
      "epoch: 559\n",
      "training loss: 33991268980617.1\n",
      "validation loss: 47379406352129.54\n",
      "epoch: 560\n",
      "training loss: 33990728728530.098\n",
      "validation loss: 47378074401172.05\n",
      "epoch: 561\n",
      "training loss: 33990189731575.652\n",
      "validation loss: 47376748293380.555\n",
      "epoch: 562\n",
      "training loss: 33989651985593.91\n",
      "validation loss: 47375428003560.516\n",
      "epoch: 563\n",
      "training loss: 33989115486442.75\n",
      "validation loss: 47374113506644.12\n",
      "epoch: 564\n",
      "training loss: 33988580229997.746\n",
      "validation loss: 47372804777689.49\n",
      "epoch: 565\n",
      "training loss: 33988046212152.05\n",
      "validation loss: 47371501791879.85\n",
      "epoch: 566\n",
      "training loss: 33987513428816.31\n",
      "validation loss: 47370204524522.76\n",
      "epoch: 567\n",
      "training loss: 33986981875918.547\n",
      "validation loss: 47368912951049.16\n",
      "epoch: 568\n",
      "training loss: 33986451549404.098\n",
      "validation loss: 47367627047012.79\n",
      "epoch: 569\n",
      "training loss: 33985922445235.508\n",
      "validation loss: 47366346788089.24\n",
      "epoch: 570\n",
      "training loss: 33985394559392.453\n",
      "validation loss: 47365072150075.25\n",
      "epoch: 571\n",
      "training loss: 33984867887871.63\n",
      "validation loss: 47363803108887.984\n",
      "epoch: 572\n",
      "training loss: 33984342426686.676\n",
      "validation loss: 47362539640563.96\n",
      "epoch: 573\n",
      "training loss: 33983818171868.098\n",
      "validation loss: 47361281721258.76\n",
      "epoch: 574\n",
      "training loss: 33983295119463.14\n",
      "validation loss: 47360029327245.88\n",
      "epoch: 575\n",
      "training loss: 33982773265535.76\n",
      "validation loss: 47358782434916.23\n",
      "epoch: 576\n",
      "training loss: 33982252606166.484\n",
      "validation loss: 47357541020777.22\n",
      "epoch: 577\n",
      "training loss: 33981733137452.348\n",
      "validation loss: 47356305061452.14\n",
      "epoch: 578\n",
      "training loss: 33981214855506.797\n",
      "validation loss: 47355074533679.414\n",
      "epoch: 579\n",
      "training loss: 33980697756459.63\n",
      "validation loss: 47353849414311.75\n",
      "epoch: 580\n",
      "training loss: 33980181836456.86\n",
      "validation loss: 47352629680315.66\n",
      "epoch: 581\n",
      "training loss: 33979667091660.703\n",
      "validation loss: 47351415308770.54\n",
      "epoch: 582\n",
      "training loss: 33979153518249.44\n",
      "validation loss: 47350206276868.125\n",
      "epoch: 583\n",
      "training loss: 33978641112417.33\n",
      "validation loss: 47349002561911.57\n",
      "epoch: 584\n",
      "training loss: 33978129870374.58\n",
      "validation loss: 47347804141315.12\n",
      "epoch: 585\n",
      "training loss: 33977619788347.203\n",
      "validation loss: 47346610992603.016\n",
      "epoch: 586\n",
      "training loss: 33977110862576.98\n",
      "validation loss: 47345423093409.055\n",
      "epoch: 587\n",
      "training loss: 33976603089321.35\n",
      "validation loss: 47344240421475.945\n",
      "epoch: 588\n",
      "training loss: 33976096464853.35\n",
      "validation loss: 47343062954654.51\n",
      "epoch: 589\n",
      "training loss: 33975590985461.516\n",
      "validation loss: 47341890670902.984\n",
      "epoch: 590\n",
      "training loss: 33975086647449.824\n",
      "validation loss: 47340723548286.62\n",
      "epoch: 591\n",
      "training loss: 33974583447137.582\n",
      "validation loss: 47339561564976.734\n",
      "epoch: 592\n",
      "training loss: 33974081380859.418\n",
      "validation loss: 47338404699250.31\n",
      "epoch: 593\n",
      "training loss: 33973580444965.08\n",
      "validation loss: 47337252929489.125\n",
      "epoch: 594\n",
      "training loss: 33973080635819.496\n",
      "validation loss: 47336106234179.27\n",
      "epoch: 595\n",
      "training loss: 33972581949802.59\n",
      "validation loss: 47334964591910.445\n",
      "epoch: 596\n",
      "training loss: 33972084383309.297\n",
      "validation loss: 47333827981375.5\n",
      "epoch: 597\n",
      "training loss: 33971587932749.383\n",
      "validation loss: 47332696381369.51\n",
      "epoch: 598\n",
      "training loss: 33971092594547.465\n",
      "validation loss: 47331569770789.48\n",
      "epoch: 599\n",
      "training loss: 33970598365142.875\n",
      "validation loss: 47330448128633.49\n",
      "epoch: 600\n",
      "training loss: 33970105240989.613\n",
      "validation loss: 47329331434000.23\n",
      "epoch: 601\n",
      "training loss: 33969613218556.277\n",
      "validation loss: 47328219666088.37\n",
      "epoch: 602\n",
      "training loss: 33969122294325.97\n",
      "validation loss: 47327112804195.95\n",
      "epoch: 603\n",
      "training loss: 33968632464796.215\n",
      "validation loss: 47326010827719.78\n",
      "epoch: 604\n",
      "training loss: 33968143726478.918\n",
      "validation loss: 47324913716154.85\n",
      "epoch: 605\n",
      "training loss: 33967656075900.3\n",
      "validation loss: 47323821449093.86\n",
      "epoch: 606\n",
      "training loss: 33967169509600.773\n",
      "validation loss: 47322734006226.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 607\n",
      "training loss: 33966684024134.91\n",
      "validation loss: 47321651367338.75\n",
      "epoch: 608\n",
      "training loss: 33966199616071.367\n",
      "validation loss: 47320573512312.94\n",
      "epoch: 609\n",
      "training loss: 33965716281992.81\n",
      "validation loss: 47319500421126.336\n",
      "epoch: 610\n",
      "training loss: 33965234018495.844\n",
      "validation loss: 47318432073851.164\n",
      "epoch: 611\n",
      "training loss: 33964752822190.945\n",
      "validation loss: 47317368450653.86\n",
      "epoch: 612\n",
      "training loss: 33964272689702.387\n",
      "validation loss: 47316309531794.555\n",
      "epoch: 613\n",
      "training loss: 33963793617668.156\n",
      "validation loss: 47315255297626.445\n",
      "epoch: 614\n",
      "training loss: 33963315602739.94\n",
      "validation loss: 47314205728595.43\n",
      "epoch: 615\n",
      "training loss: 33962838641582.99\n",
      "validation loss: 47313160805239.445\n",
      "epoch: 616\n",
      "training loss: 33962362730876.098\n",
      "validation loss: 47312120508187.91\n",
      "epoch: 617\n",
      "training loss: 33961887867311.516\n",
      "validation loss: 47311084818161.26\n",
      "epoch: 618\n",
      "training loss: 33961414047594.88\n",
      "validation loss: 47310053715970.445\n",
      "epoch: 619\n",
      "training loss: 33960941268445.15\n",
      "validation loss: 47309027182516.375\n",
      "epoch: 620\n",
      "training loss: 33960469526594.55\n",
      "validation loss: 47308005198789.38\n",
      "epoch: 621\n",
      "training loss: 33959998818788.527\n",
      "validation loss: 47306987745868.66\n",
      "epoch: 622\n",
      "training loss: 33959529141785.617\n",
      "validation loss: 47305974804922.0\n",
      "epoch: 623\n",
      "training loss: 33959060492357.426\n",
      "validation loss: 47304966357205.0\n",
      "epoch: 624\n",
      "training loss: 33958592867288.586\n",
      "validation loss: 47303962384060.586\n",
      "epoch: 625\n",
      "training loss: 33958126263376.64\n",
      "validation loss: 47302962866918.84\n",
      "epoch: 626\n",
      "training loss: 33957660677432.016\n",
      "validation loss: 47301967787296.1\n",
      "epoch: 627\n",
      "training loss: 33957196106277.957\n",
      "validation loss: 47300977126794.72\n",
      "epoch: 628\n",
      "training loss: 33956732546750.418\n",
      "validation loss: 47299990867102.41\n",
      "epoch: 629\n",
      "training loss: 33956269995698.086\n",
      "validation loss: 47299008989991.984\n",
      "epoch: 630\n",
      "training loss: 33955808449982.24\n",
      "validation loss: 47298031477320.69\n",
      "epoch: 631\n",
      "training loss: 33955347906476.72\n",
      "validation loss: 47297058311029.83\n",
      "epoch: 632\n",
      "training loss: 33954888362067.88\n",
      "validation loss: 47296089473144.19\n",
      "epoch: 633\n",
      "training loss: 33954429813654.48\n",
      "validation loss: 47295124945771.73\n",
      "epoch: 634\n",
      "training loss: 33953972258147.7\n",
      "validation loss: 47294164711102.96\n",
      "epoch: 635\n",
      "training loss: 33953515692470.992\n",
      "validation loss: 47293208751410.52\n",
      "epoch: 636\n",
      "training loss: 33953060113560.098\n",
      "validation loss: 47292257049048.91\n",
      "epoch: 637\n",
      "training loss: 33952605518362.94\n",
      "validation loss: 47291309586453.68\n",
      "epoch: 638\n",
      "training loss: 33952151903839.58\n",
      "validation loss: 47290366346141.32\n",
      "epoch: 639\n",
      "training loss: 33951699266962.164\n",
      "validation loss: 47289427310708.64\n",
      "epoch: 640\n",
      "training loss: 33951247604714.848\n",
      "validation loss: 47288492462832.32\n",
      "epoch: 641\n",
      "training loss: 33950796914093.754\n",
      "validation loss: 47287561785268.484\n",
      "epoch: 642\n",
      "training loss: 33950347192106.906\n",
      "validation loss: 47286635260852.34\n",
      "epoch: 643\n",
      "training loss: 33949898435774.19\n",
      "validation loss: 47285712872497.71\n",
      "epoch: 644\n",
      "training loss: 33949450642127.242\n",
      "validation loss: 47284794603196.51\n",
      "epoch: 645\n",
      "training loss: 33949003808209.473\n",
      "validation loss: 47283880436018.414\n",
      "epoch: 646\n",
      "training loss: 33948557931075.94\n",
      "validation loss: 47282970354110.43\n",
      "epoch: 647\n",
      "training loss: 33948113007793.344\n",
      "validation loss: 47282064340696.43\n",
      "epoch: 648\n",
      "training loss: 33947669035439.953\n",
      "validation loss: 47281162379076.81\n",
      "epoch: 649\n",
      "training loss: 33947226011105.465\n",
      "validation loss: 47280264452627.86\n",
      "epoch: 650\n",
      "training loss: 33946783931891.145\n",
      "validation loss: 47279370544801.66\n",
      "epoch: 651\n",
      "training loss: 33946342794909.59\n",
      "validation loss: 47278480639125.46\n",
      "epoch: 652\n",
      "training loss: 33945902597284.734\n",
      "validation loss: 47277594719201.375\n",
      "epoch: 653\n",
      "training loss: 33945463336151.844\n",
      "validation loss: 47276712768705.87\n",
      "epoch: 654\n",
      "training loss: 33945025008657.363\n",
      "validation loss: 47275834771389.44\n",
      "epoch: 655\n",
      "training loss: 33944587611958.996\n",
      "validation loss: 47274960711076.28\n",
      "epoch: 656\n",
      "training loss: 33944151143225.473\n",
      "validation loss: 47274090571663.62\n",
      "epoch: 657\n",
      "training loss: 33943715599636.68\n",
      "validation loss: 47273224337121.734\n",
      "epoch: 658\n",
      "training loss: 33943280978383.508\n",
      "validation loss: 47272361991493.17\n",
      "epoch: 659\n",
      "training loss: 33942847276667.777\n",
      "validation loss: 47271503518892.58\n",
      "epoch: 660\n",
      "training loss: 33942414491702.28\n",
      "validation loss: 47270648903506.3\n",
      "epoch: 661\n",
      "training loss: 33941982620710.645\n",
      "validation loss: 47269798129591.85\n",
      "epoch: 662\n",
      "training loss: 33941551660927.32\n",
      "validation loss: 47268951181477.83\n",
      "epoch: 663\n",
      "training loss: 33941121609597.516\n",
      "validation loss: 47268108043563.14\n",
      "epoch: 664\n",
      "training loss: 33940692463977.145\n",
      "validation loss: 47267268700316.98\n",
      "epoch: 665\n",
      "training loss: 33940264221332.797\n",
      "validation loss: 47266433136278.234\n",
      "epoch: 666\n",
      "training loss: 33939836878941.668\n",
      "validation loss: 47265601336055.31\n",
      "epoch: 667\n",
      "training loss: 33939410434091.516\n",
      "validation loss: 47264773284325.53\n",
      "epoch: 668\n",
      "training loss: 33938984884080.594\n",
      "validation loss: 47263948965834.94\n",
      "epoch: 669\n",
      "training loss: 33938560226217.652\n",
      "validation loss: 47263128365397.88\n",
      "epoch: 670\n",
      "training loss: 33938136457821.81\n",
      "validation loss: 47262311467896.625\n",
      "epoch: 671\n",
      "training loss: 33937713576222.57\n",
      "validation loss: 47261498258281.1\n",
      "epoch: 672\n",
      "training loss: 33937291578759.777\n",
      "validation loss: 47260688721568.45\n",
      "epoch: 673\n",
      "training loss: 33936870462783.48\n",
      "validation loss: 47259882842842.625\n",
      "epoch: 674\n",
      "training loss: 33936450225654.01\n",
      "validation loss: 47259080607254.22\n",
      "epoch: 675\n",
      "training loss: 33936030864741.82\n",
      "validation loss: 47258282000019.96\n",
      "epoch: 676\n",
      "training loss: 33935612377427.504\n",
      "validation loss: 47257487006422.39\n",
      "epoch: 677\n",
      "training loss: 33935194761101.742\n",
      "validation loss: 47256695611809.56\n",
      "epoch: 678\n",
      "training loss: 33934778013165.24\n",
      "validation loss: 47255907801594.77\n",
      "epoch: 679\n",
      "training loss: 33934362131028.656\n",
      "validation loss: 47255123561256.0\n",
      "epoch: 680\n",
      "training loss: 33933947112112.62\n",
      "validation loss: 47254342876335.75\n",
      "epoch: 681\n",
      "training loss: 33933532953847.625\n",
      "validation loss: 47253565732440.67\n",
      "epoch: 682\n",
      "training loss: 33933119653674.03\n",
      "validation loss: 47252792115241.32\n",
      "epoch: 683\n",
      "training loss: 33932707209041.973\n",
      "validation loss: 47252022010471.58\n",
      "epoch: 684\n",
      "training loss: 33932295617411.348\n",
      "validation loss: 47251255403928.555\n",
      "epoch: 685\n",
      "training loss: 33931884876251.746\n",
      "validation loss: 47250492281472.22\n",
      "epoch: 686\n",
      "training loss: 33931474983042.453\n",
      "validation loss: 47249732629024.99\n",
      "epoch: 687\n",
      "training loss: 33931065935272.33\n",
      "validation loss: 47248976432571.56\n",
      "epoch: 688\n",
      "training loss: 33930657730439.824\n",
      "validation loss: 47248223678158.336\n",
      "epoch: 689\n",
      "training loss: 33930250366052.91\n",
      "validation loss: 47247474351893.33\n",
      "epoch: 690\n",
      "training loss: 33929843839629.08\n",
      "validation loss: 47246728439945.914\n",
      "epoch: 691\n",
      "training loss: 33929438148695.2\n",
      "validation loss: 47245985928546.13\n",
      "epoch: 692\n",
      "training loss: 33929033290787.58\n",
      "validation loss: 47245246803984.85\n",
      "epoch: 693\n",
      "training loss: 33928629263451.883\n",
      "validation loss: 47244511052613.164\n",
      "epoch: 694\n",
      "training loss: 33928226064243.055\n",
      "validation loss: 47243778660842.08\n",
      "epoch: 695\n",
      "training loss: 33927823690725.32\n",
      "validation loss: 47243049615142.37\n",
      "epoch: 696\n",
      "training loss: 33927422140472.156\n",
      "validation loss: 47242323902044.21\n",
      "epoch: 697\n",
      "training loss: 33927021411066.17\n",
      "validation loss: 47241601508136.7\n",
      "epoch: 698\n",
      "training loss: 33926621500099.164\n",
      "validation loss: 47240882420067.914\n",
      "epoch: 699\n",
      "training loss: 33926222405171.992\n",
      "validation loss: 47240166624544.26\n",
      "epoch: 700\n",
      "training loss: 33925824123894.598\n",
      "validation loss: 47239454108330.39\n",
      "epoch: 701\n",
      "training loss: 33925426653885.93\n",
      "validation loss: 47238744858248.875\n",
      "epoch: 702\n",
      "training loss: 33925029992773.887\n",
      "validation loss: 47238038861179.73\n",
      "epoch: 703\n",
      "training loss: 33924634138195.34\n",
      "validation loss: 47237336104060.45\n",
      "epoch: 704\n",
      "training loss: 33924239087796.04\n",
      "validation loss: 47236636573885.51\n",
      "epoch: 705\n",
      "training loss: 33923844839230.57\n",
      "validation loss: 47235940257706.05\n",
      "epoch: 706\n",
      "training loss: 33923451390162.348\n",
      "validation loss: 47235247142629.67\n",
      "epoch: 707\n",
      "training loss: 33923058738263.555\n",
      "validation loss: 47234557215820.16\n",
      "epoch: 708\n",
      "training loss: 33922666881215.1\n",
      "validation loss: 47233870464497.2\n",
      "epoch: 709\n",
      "training loss: 33922275816706.594\n",
      "validation loss: 47233186875936.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 710\n",
      "training loss: 33921885542436.297\n",
      "validation loss: 47232506437467.25\n",
      "epoch: 711\n",
      "training loss: 33921496056111.08\n",
      "validation loss: 47231829136476.445\n",
      "epoch: 712\n",
      "training loss: 33921107355446.363\n",
      "validation loss: 47231154960404.0\n",
      "epoch: 713\n",
      "training loss: 33920719438166.15\n",
      "validation loss: 47230483896744.77\n",
      "epoch: 714\n",
      "training loss: 33920332302002.934\n",
      "validation loss: 47229815933048.0\n",
      "epoch: 715\n",
      "training loss: 33919945944697.617\n",
      "validation loss: 47229151056916.64\n",
      "epoch: 716\n",
      "training loss: 33919560363999.58\n",
      "validation loss: 47228489256007.46\n",
      "epoch: 717\n",
      "training loss: 33919175557666.543\n",
      "validation loss: 47227830518030.62\n",
      "epoch: 718\n",
      "training loss: 33918791523464.613\n",
      "validation loss: 47227174830749.484\n",
      "epoch: 719\n",
      "training loss: 33918408259168.17\n",
      "validation loss: 47226522181980.24\n",
      "epoch: 720\n",
      "training loss: 33918025762559.87\n",
      "validation loss: 47225872559591.76\n",
      "epoch: 721\n",
      "training loss: 33917644031430.61\n",
      "validation loss: 47225225951505.2\n",
      "epoch: 722\n",
      "training loss: 33917263063579.47\n",
      "validation loss: 47224582345693.92\n",
      "epoch: 723\n",
      "training loss: 33916882856813.727\n",
      "validation loss: 47223941730183.164\n",
      "epoch: 724\n",
      "training loss: 33916503408948.703\n",
      "validation loss: 47223304093049.57\n",
      "epoch: 725\n",
      "training loss: 33916124717807.902\n",
      "validation loss: 47222669422421.38\n",
      "epoch: 726\n",
      "training loss: 33915746781222.805\n",
      "validation loss: 47222037706477.75\n",
      "epoch: 727\n",
      "training loss: 33915369597032.926\n",
      "validation loss: 47221408933448.734\n",
      "epoch: 728\n",
      "training loss: 33914993163085.785\n",
      "validation loss: 47220783091615.0\n",
      "epoch: 729\n",
      "training loss: 33914617477236.805\n",
      "validation loss: 47220160169307.51\n",
      "epoch: 730\n",
      "training loss: 33914242537349.336\n",
      "validation loss: 47219540154907.41\n",
      "epoch: 731\n",
      "training loss: 33913868341294.586\n",
      "validation loss: 47218923036845.5\n",
      "epoch: 732\n",
      "training loss: 33913494886951.63\n",
      "validation loss: 47218308803602.445\n",
      "epoch: 733\n",
      "training loss: 33913122172207.3\n",
      "validation loss: 47217697443708.055\n",
      "epoch: 734\n",
      "training loss: 33912750194956.242\n",
      "validation loss: 47217088945741.45\n",
      "epoch: 735\n",
      "training loss: 33912378953100.793\n",
      "validation loss: 47216483298330.48\n",
      "epoch: 736\n",
      "training loss: 33912008444551.008\n",
      "validation loss: 47215880490151.68\n",
      "epoch: 737\n",
      "training loss: 33911638667224.633\n",
      "validation loss: 47215280509930.13\n",
      "epoch: 738\n",
      "training loss: 33911269619046.97\n",
      "validation loss: 47214683346438.86\n",
      "epoch: 739\n",
      "training loss: 33910901297951.01\n",
      "validation loss: 47214088988499.05\n",
      "epoch: 740\n",
      "training loss: 33910533701877.22\n",
      "validation loss: 47213497424979.43\n",
      "epoch: 741\n",
      "training loss: 33910166828773.645\n",
      "validation loss: 47212908644796.234\n",
      "epoch: 742\n",
      "training loss: 33909800676595.84\n",
      "validation loss: 47212322636913.08\n",
      "epoch: 743\n",
      "training loss: 33909435243306.78\n",
      "validation loss: 47211739390340.43\n",
      "epoch: 744\n",
      "training loss: 33909070526876.887\n",
      "validation loss: 47211158894135.65\n",
      "epoch: 745\n",
      "training loss: 33908706525284.004\n",
      "validation loss: 47210581137402.664\n",
      "epoch: 746\n",
      "training loss: 33908343236513.297\n",
      "validation loss: 47210006109291.625\n",
      "epoch: 747\n",
      "training loss: 33907980658557.297\n",
      "validation loss: 47209433798998.98\n",
      "epoch: 748\n",
      "training loss: 33907618789415.8\n",
      "validation loss: 47208864195766.83\n",
      "epoch: 749\n",
      "training loss: 33907257627095.92\n",
      "validation loss: 47208297288883.266\n",
      "epoch: 750\n",
      "training loss: 33906897169611.977\n",
      "validation loss: 47207733067681.52\n",
      "epoch: 751\n",
      "training loss: 33906537414985.492\n",
      "validation loss: 47207171521540.37\n",
      "epoch: 752\n",
      "training loss: 33906178361245.156\n",
      "validation loss: 47206612639883.26\n",
      "epoch: 753\n",
      "training loss: 33905820006426.824\n",
      "validation loss: 47206056412178.81\n",
      "epoch: 754\n",
      "training loss: 33905462348573.445\n",
      "validation loss: 47205502827939.945\n",
      "epoch: 755\n",
      "training loss: 33905105385735.035\n",
      "validation loss: 47204951876724.055\n",
      "epoch: 756\n",
      "training loss: 33904749115968.707\n",
      "validation loss: 47204403548132.83\n",
      "epoch: 757\n",
      "training loss: 33904393537338.527\n",
      "validation loss: 47203857831811.695\n",
      "epoch: 758\n",
      "training loss: 33904038647915.59\n",
      "validation loss: 47203314717449.98\n",
      "epoch: 759\n",
      "training loss: 33903684445777.92\n",
      "validation loss: 47202774194780.38\n",
      "epoch: 760\n",
      "training loss: 33903330929010.52\n",
      "validation loss: 47202236253579.18\n",
      "epoch: 761\n",
      "training loss: 33902978095705.21\n",
      "validation loss: 47201700883665.516\n",
      "epoch: 762\n",
      "training loss: 33902625943960.734\n",
      "validation loss: 47201168074901.62\n",
      "epoch: 763\n",
      "training loss: 33902274471882.656\n",
      "validation loss: 47200637817192.336\n",
      "epoch: 764\n",
      "training loss: 33901923677583.344\n",
      "validation loss: 47200110100485.1\n",
      "epoch: 765\n",
      "training loss: 33901573559181.934\n",
      "validation loss: 47199584914769.58\n",
      "epoch: 766\n",
      "training loss: 33901224114804.34\n",
      "validation loss: 47199062250077.64\n",
      "epoch: 767\n",
      "training loss: 33900875342583.164\n",
      "validation loss: 47198542096482.94\n",
      "epoch: 768\n",
      "training loss: 33900527240657.703\n",
      "validation loss: 47198024444100.914\n",
      "epoch: 769\n",
      "training loss: 33900179807173.92\n",
      "validation loss: 47197509283088.516\n",
      "epoch: 770\n",
      "training loss: 33899833040284.418\n",
      "validation loss: 47196996603644.07\n",
      "epoch: 771\n",
      "training loss: 33899486938148.375\n",
      "validation loss: 47196486396006.836\n",
      "epoch: 772\n",
      "training loss: 33899141498931.57\n",
      "validation loss: 47195978650457.195\n",
      "epoch: 773\n",
      "training loss: 33898796720806.332\n",
      "validation loss: 47195473357316.2\n",
      "epoch: 774\n",
      "training loss: 33898452601951.473\n",
      "validation loss: 47194970506945.445\n",
      "epoch: 775\n",
      "training loss: 33898109140552.33\n",
      "validation loss: 47194470089746.8\n",
      "epoch: 776\n",
      "training loss: 33897766334800.688\n",
      "validation loss: 47193972096162.44\n",
      "epoch: 777\n",
      "training loss: 33897424182894.754\n",
      "validation loss: 47193476516674.44\n",
      "epoch: 778\n",
      "training loss: 33897082683039.17\n",
      "validation loss: 47192983341804.625\n",
      "epoch: 779\n",
      "training loss: 33896741833444.93\n",
      "validation loss: 47192492562114.52\n",
      "epoch: 780\n",
      "training loss: 33896401632329.4\n",
      "validation loss: 47192004168204.97\n",
      "epoch: 781\n",
      "training loss: 33896062077916.246\n",
      "validation loss: 47191518150716.055\n",
      "epoch: 782\n",
      "training loss: 33895723168435.46\n",
      "validation loss: 47191034500326.99\n",
      "epoch: 783\n",
      "training loss: 33895384902123.273\n",
      "validation loss: 47190553207755.695\n",
      "epoch: 784\n",
      "training loss: 33895047277222.2\n",
      "validation loss: 47190074263759.03\n",
      "epoch: 785\n",
      "training loss: 33894710291980.934\n",
      "validation loss: 47189597659132.13\n",
      "epoch: 786\n",
      "training loss: 33894373944654.387\n",
      "validation loss: 47189123384708.516\n",
      "epoch: 787\n",
      "training loss: 33894038233503.594\n",
      "validation loss: 47188651431359.805\n",
      "epoch: 788\n",
      "training loss: 33893703156795.78\n",
      "validation loss: 47188181789995.71\n",
      "epoch: 789\n",
      "training loss: 33893368712804.246\n",
      "validation loss: 47187714451563.62\n",
      "epoch: 790\n",
      "training loss: 33893034899808.39\n",
      "validation loss: 47187249407048.59\n",
      "epoch: 791\n",
      "training loss: 33892701716093.668\n",
      "validation loss: 47186786647473.11\n",
      "epoch: 792\n",
      "training loss: 33892369159951.57\n",
      "validation loss: 47186326163896.914\n",
      "epoch: 793\n",
      "training loss: 33892037229679.6\n",
      "validation loss: 47185867947416.85\n",
      "epoch: 794\n",
      "training loss: 33891705923581.23\n",
      "validation loss: 47185411989166.64\n",
      "epoch: 795\n",
      "training loss: 33891375239965.906\n",
      "validation loss: 47184958280316.81\n",
      "epoch: 796\n",
      "training loss: 33891045177148.984\n",
      "validation loss: 47184506812074.48\n",
      "epoch: 797\n",
      "training loss: 33890715733451.754\n",
      "validation loss: 47184057575683.08\n",
      "epoch: 798\n",
      "training loss: 33890386907201.375\n",
      "validation loss: 47183610562422.38\n",
      "epoch: 799\n",
      "training loss: 33890058696730.848\n",
      "validation loss: 47183165763608.234\n",
      "epoch: 800\n",
      "training loss: 33889731100379.03\n",
      "validation loss: 47182723170592.25\n",
      "epoch: 801\n",
      "training loss: 33889404116490.574\n",
      "validation loss: 47182282774761.93\n",
      "epoch: 802\n",
      "training loss: 33889077743415.926\n",
      "validation loss: 47181844567540.38\n",
      "epoch: 803\n",
      "training loss: 33888751979511.27\n",
      "validation loss: 47181408540385.95\n",
      "epoch: 804\n",
      "training loss: 33888426823138.543\n",
      "validation loss: 47180974684792.39\n",
      "epoch: 805\n",
      "training loss: 33888102272665.395\n",
      "validation loss: 47180542992288.49\n",
      "epoch: 806\n",
      "training loss: 33887778326465.12\n",
      "validation loss: 47180113454437.85\n",
      "epoch: 807\n",
      "training loss: 33887454982916.754\n",
      "validation loss: 47179686062839.1\n",
      "epoch: 808\n",
      "training loss: 33887132240404.9\n",
      "validation loss: 47179260809125.29\n",
      "epoch: 809\n",
      "training loss: 33886810097319.8\n",
      "validation loss: 47178837684963.88\n",
      "epoch: 810\n",
      "training loss: 33886488552057.285\n",
      "validation loss: 47178416682056.734\n",
      "epoch: 811\n",
      "training loss: 33886167603018.777\n",
      "validation loss: 47177997792139.805\n",
      "epoch: 812\n",
      "training loss: 33885847248611.215\n",
      "validation loss: 47177581006983.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 813\n",
      "training loss: 33885527487247.047\n",
      "validation loss: 47177166318390.14\n",
      "epoch: 814\n",
      "training loss: 33885208317344.25\n",
      "validation loss: 47176753718198.52\n",
      "epoch: 815\n",
      "training loss: 33884889737326.285\n",
      "validation loss: 47176343198279.164\n",
      "epoch: 816\n",
      "training loss: 33884571745622.016\n",
      "validation loss: 47175934750536.34\n",
      "epoch: 817\n",
      "training loss: 33884254340665.785\n",
      "validation loss: 47175528366907.52\n",
      "epoch: 818\n",
      "training loss: 33883937520897.3\n",
      "validation loss: 47175124039363.266\n",
      "epoch: 819\n",
      "training loss: 33883621284761.695\n",
      "validation loss: 47174721759907.02\n",
      "epoch: 820\n",
      "training loss: 33883305630709.445\n",
      "validation loss: 47174321520574.99\n",
      "epoch: 821\n",
      "training loss: 33882990557196.35\n",
      "validation loss: 47173923313435.99\n",
      "epoch: 822\n",
      "training loss: 33882676062683.56\n",
      "validation loss: 47173527130591.29\n",
      "epoch: 823\n",
      "training loss: 33882362145637.477\n",
      "validation loss: 47173132964174.445\n",
      "epoch: 824\n",
      "training loss: 33882048804529.836\n",
      "validation loss: 47172740806351.25\n",
      "epoch: 825\n",
      "training loss: 33881736037837.555\n",
      "validation loss: 47172350649319.43\n",
      "epoch: 826\n",
      "training loss: 33881423844042.836\n",
      "validation loss: 47171962485308.68\n",
      "epoch: 827\n",
      "training loss: 33881112221633.055\n",
      "validation loss: 47171576306580.22\n",
      "epoch: 828\n",
      "training loss: 33880801169100.812\n",
      "validation loss: 47171192105427.14\n",
      "epoch: 829\n",
      "training loss: 33880490684943.8\n",
      "validation loss: 47170809874173.69\n",
      "epoch: 830\n",
      "training loss: 33880180767664.953\n",
      "validation loss: 47170429605175.75\n",
      "epoch: 831\n",
      "training loss: 33879871415772.254\n",
      "validation loss: 47170051290819.98\n",
      "epoch: 832\n",
      "training loss: 33879562627778.797\n",
      "validation loss: 47169674923524.29\n",
      "epoch: 833\n",
      "training loss: 33879254402202.78\n",
      "validation loss: 47169300495737.35\n",
      "epoch: 834\n",
      "training loss: 33878946737567.44\n",
      "validation loss: 47168927999938.67\n",
      "epoch: 835\n",
      "training loss: 33878639632401.08\n",
      "validation loss: 47168557428638.36\n",
      "epoch: 836\n",
      "training loss: 33878333085236.97\n",
      "validation loss: 47168188774376.875\n",
      "epoch: 837\n",
      "training loss: 33878027094613.438\n",
      "validation loss: 47167822029725.164\n",
      "epoch: 838\n",
      "training loss: 33877721659073.727\n",
      "validation loss: 47167457187284.1\n",
      "epoch: 839\n",
      "training loss: 33877416777166.074\n",
      "validation loss: 47167094239684.83\n",
      "epoch: 840\n",
      "training loss: 33877112447443.66\n",
      "validation loss: 47166733179588.42\n",
      "epoch: 841\n",
      "training loss: 33876808668464.527\n",
      "validation loss: 47166373999685.55\n",
      "epoch: 842\n",
      "training loss: 33876505438791.68\n",
      "validation loss: 47166016692696.69\n",
      "epoch: 843\n",
      "training loss: 33876202756992.96\n",
      "validation loss: 47165661251371.78\n",
      "epoch: 844\n",
      "training loss: 33875900621641.055\n",
      "validation loss: 47165307668490.086\n",
      "epoch: 845\n",
      "training loss: 33875599031313.492\n",
      "validation loss: 47164955936860.17\n",
      "epoch: 846\n",
      "training loss: 33875297984592.652\n",
      "validation loss: 47164606049319.734\n",
      "epoch: 847\n",
      "training loss: 33874997480065.66\n",
      "validation loss: 47164257998735.49\n",
      "epoch: 848\n",
      "training loss: 33874697516324.445\n",
      "validation loss: 47163911778002.875\n",
      "epoch: 849\n",
      "training loss: 33874398091965.688\n",
      "validation loss: 47163567380046.19\n",
      "epoch: 850\n",
      "training loss: 33874099205590.8\n",
      "validation loss: 47163224797818.37\n",
      "epoch: 851\n",
      "training loss: 33873800855805.906\n",
      "validation loss: 47162884024300.625\n",
      "epoch: 852\n",
      "training loss: 33873503041221.844\n",
      "validation loss: 47162545052502.664\n",
      "epoch: 853\n",
      "training loss: 33873205760454.117\n",
      "validation loss: 47162207875462.37\n",
      "epoch: 854\n",
      "training loss: 33872909012122.89\n",
      "validation loss: 47161872486245.7\n",
      "epoch: 855\n",
      "training loss: 33872612794852.992\n",
      "validation loss: 47161538877946.71\n",
      "epoch: 856\n",
      "training loss: 33872317107273.836\n",
      "validation loss: 47161207043687.07\n",
      "epoch: 857\n",
      "training loss: 33872021948019.465\n",
      "validation loss: 47160876976616.39\n",
      "epoch: 858\n",
      "training loss: 33871727315728.492\n",
      "validation loss: 47160548669911.69\n",
      "epoch: 859\n",
      "training loss: 33871433209044.105\n",
      "validation loss: 47160222116777.586\n",
      "epoch: 860\n",
      "training loss: 33871139626614.023\n",
      "validation loss: 47159897310445.99\n",
      "epoch: 861\n",
      "training loss: 33870846567090.555\n",
      "validation loss: 47159574244176.13\n",
      "epoch: 862\n",
      "training loss: 33870554029130.41\n",
      "validation loss: 47159252911254.2\n",
      "epoch: 863\n",
      "training loss: 33870262011394.9\n",
      "validation loss: 47158933304993.47\n",
      "epoch: 864\n",
      "training loss: 33869970512549.734\n",
      "validation loss: 47158615418734.04\n",
      "epoch: 865\n",
      "training loss: 33869679531265.13\n",
      "validation loss: 47158299245842.805\n",
      "epoch: 866\n",
      "training loss: 33869389066215.7\n",
      "validation loss: 47157984779713.3\n",
      "epoch: 867\n",
      "training loss: 33869099116080.523\n",
      "validation loss: 47157672013765.49\n",
      "epoch: 868\n",
      "training loss: 33868809679543.02\n",
      "validation loss: 47157360941445.76\n",
      "epoch: 869\n",
      "training loss: 33868520755291.05\n",
      "validation loss: 47157051556226.79\n",
      "epoch: 870\n",
      "training loss: 33868232342016.836\n",
      "validation loss: 47156743851607.54\n",
      "epoch: 871\n",
      "training loss: 33867944438416.914\n",
      "validation loss: 47156437821112.8\n",
      "epoch: 872\n",
      "training loss: 33867657043192.156\n",
      "validation loss: 47156133458293.38\n",
      "epoch: 873\n",
      "training loss: 33867370155047.805\n",
      "validation loss: 47155830756725.97\n",
      "epoch: 874\n",
      "training loss: 33867083772693.324\n",
      "validation loss: 47155529710012.875\n",
      "epoch: 875\n",
      "training loss: 33866797894842.527\n",
      "validation loss: 47155230311782.11\n",
      "epoch: 876\n",
      "training loss: 33866512520213.42\n",
      "validation loss: 47154932555686.95\n",
      "epoch: 877\n",
      "training loss: 33866227647528.312\n",
      "validation loss: 47154636435406.26\n",
      "epoch: 878\n",
      "training loss: 33865943275513.715\n",
      "validation loss: 47154341944644.03\n",
      "epoch: 879\n",
      "training loss: 33865659402900.355\n",
      "validation loss: 47154049077129.49\n",
      "epoch: 880\n",
      "training loss: 33865376028423.15\n",
      "validation loss: 47153757826616.71\n",
      "epoch: 881\n",
      "training loss: 33865093150821.2\n",
      "validation loss: 47153468186884.93\n",
      "epoch: 882\n",
      "training loss: 33864810768837.785\n",
      "validation loss: 47153180151738.016\n",
      "epoch: 883\n",
      "training loss: 33864528881220.28\n",
      "validation loss: 47152893715004.625\n",
      "epoch: 884\n",
      "training loss: 33864247486720.23\n",
      "validation loss: 47152608870537.93\n",
      "epoch: 885\n",
      "training loss: 33863966584093.285\n",
      "validation loss: 47152325612215.67\n",
      "epoch: 886\n",
      "training loss: 33863686172099.184\n",
      "validation loss: 47152043933939.984\n",
      "epoch: 887\n",
      "training loss: 33863406249501.746\n",
      "validation loss: 47151763829637.17\n",
      "epoch: 888\n",
      "training loss: 33863126815068.855\n",
      "validation loss: 47151485293257.84\n",
      "epoch: 889\n",
      "training loss: 33862847867572.426\n",
      "validation loss: 47151208318776.484\n",
      "epoch: 890\n",
      "training loss: 33862569405788.434\n",
      "validation loss: 47150932900191.65\n",
      "epoch: 891\n",
      "training loss: 33862291428496.832\n",
      "validation loss: 47150659031525.766\n",
      "epoch: 892\n",
      "training loss: 33862013934481.598\n",
      "validation loss: 47150386706824.99\n",
      "epoch: 893\n",
      "training loss: 33861736922530.7\n",
      "validation loss: 47150115920159.04\n",
      "epoch: 894\n",
      "training loss: 33861460391436.04\n",
      "validation loss: 47149846665621.28\n",
      "epoch: 895\n",
      "training loss: 33861184339993.49\n",
      "validation loss: 47149578937328.4\n",
      "epoch: 896\n",
      "training loss: 33860908767002.85\n",
      "validation loss: 47149312729420.56\n",
      "epoch: 897\n",
      "training loss: 33860633671267.867\n",
      "validation loss: 47149048036061.016\n",
      "epoch: 898\n",
      "training loss: 33860359051596.14\n",
      "validation loss: 47148784851436.26\n",
      "epoch: 899\n",
      "training loss: 33860084906799.223\n",
      "validation loss: 47148523169755.75\n",
      "epoch: 900\n",
      "training loss: 33859811235692.47\n",
      "validation loss: 47148262985251.81\n",
      "epoch: 901\n",
      "training loss: 33859538037095.14\n",
      "validation loss: 47148004292179.836\n",
      "epoch: 902\n",
      "training loss: 33859265309830.348\n",
      "validation loss: 47147747084817.7\n",
      "epoch: 903\n",
      "training loss: 33858993052724.99\n",
      "validation loss: 47147491357466.0\n",
      "epoch: 904\n",
      "training loss: 33858721264609.793\n",
      "validation loss: 47147237104447.914\n",
      "epoch: 905\n",
      "training loss: 33858449944319.3\n",
      "validation loss: 47146984320108.89\n",
      "epoch: 906\n",
      "training loss: 33858179090691.824\n",
      "validation loss: 47146732998816.97\n",
      "epoch: 907\n",
      "training loss: 33857908702569.43\n",
      "validation loss: 47146483134962.195\n",
      "epoch: 908\n",
      "training loss: 33857638778797.96\n",
      "validation loss: 47146234722956.96\n",
      "epoch: 909\n",
      "training loss: 33857369318226.992\n",
      "validation loss: 47145987757235.47\n",
      "epoch: 910\n",
      "training loss: 33857100319709.797\n",
      "validation loss: 47145742232254.12\n",
      "epoch: 911\n",
      "training loss: 33856831782103.39\n",
      "validation loss: 47145498142491.02\n",
      "epoch: 912\n",
      "training loss: 33856563704268.46\n",
      "validation loss: 47145255482446.05\n",
      "epoch: 913\n",
      "training loss: 33856296085069.39\n",
      "validation loss: 47145014246640.836\n",
      "epoch: 914\n",
      "training loss: 33856028923374.23\n",
      "validation loss: 47144774429618.53\n",
      "epoch: 915\n",
      "training loss: 33855762218054.64\n",
      "validation loss: 47144536025943.766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 916\n",
      "training loss: 33855495967985.98\n",
      "validation loss: 47144299030202.54\n",
      "epoch: 917\n",
      "training loss: 33855230172047.19\n",
      "validation loss: 47144063437002.23\n",
      "epoch: 918\n",
      "training loss: 33854964829120.816\n",
      "validation loss: 47143829240971.31\n",
      "epoch: 919\n",
      "training loss: 33854699938093.035\n",
      "validation loss: 47143596436759.47\n",
      "epoch: 920\n",
      "training loss: 33854435497853.566\n",
      "validation loss: 47143365019037.36\n",
      "epoch: 921\n",
      "training loss: 33854171507295.695\n",
      "validation loss: 47143134982496.57\n",
      "epoch: 922\n",
      "training loss: 33853907965316.3\n",
      "validation loss: 47142906321849.57\n",
      "epoch: 923\n",
      "training loss: 33853644870815.758\n",
      "validation loss: 47142679031829.516\n",
      "epoch: 924\n",
      "training loss: 33853382222697.98\n",
      "validation loss: 47142453107190.27\n",
      "epoch: 925\n",
      "training loss: 33853120019870.39\n",
      "validation loss: 47142228542706.29\n",
      "epoch: 926\n",
      "training loss: 33852858261243.914\n",
      "validation loss: 47142005333172.46\n",
      "epoch: 927\n",
      "training loss: 33852596945732.97\n",
      "validation loss: 47141783473404.11\n",
      "epoch: 928\n",
      "training loss: 33852336072255.4\n",
      "validation loss: 47141562958236.85\n",
      "epoch: 929\n",
      "training loss: 33852075639732.57\n",
      "validation loss: 47141343782526.57\n",
      "epoch: 930\n",
      "training loss: 33851815647089.24\n",
      "validation loss: 47141125941149.164\n",
      "epoch: 931\n",
      "training loss: 33851556093253.62\n",
      "validation loss: 47140909429000.76\n",
      "epoch: 932\n",
      "training loss: 33851296977157.32\n",
      "validation loss: 47140694240997.26\n",
      "epoch: 933\n",
      "training loss: 33851038297735.38\n",
      "validation loss: 47140480372074.59\n",
      "epoch: 934\n",
      "training loss: 33850780053926.215\n",
      "validation loss: 47140267817188.445\n",
      "epoch: 935\n",
      "training loss: 33850522244671.605\n",
      "validation loss: 47140056571314.125\n",
      "epoch: 936\n",
      "training loss: 33850264868916.715\n",
      "validation loss: 47139846629446.695\n",
      "epoch: 937\n",
      "training loss: 33850007925610.05\n",
      "validation loss: 47139637986600.65\n",
      "epoch: 938\n",
      "training loss: 33849751413703.45\n",
      "validation loss: 47139430637809.945\n",
      "epoch: 939\n",
      "training loss: 33849495332152.082\n",
      "validation loss: 47139224578128.04\n",
      "epoch: 940\n",
      "training loss: 33849239679914.445\n",
      "validation loss: 47139019802627.516\n",
      "epoch: 941\n",
      "training loss: 33848984455952.31\n",
      "validation loss: 47138816306400.31\n",
      "epoch: 942\n",
      "training loss: 33848729659230.742\n",
      "validation loss: 47138614084557.37\n",
      "epoch: 943\n",
      "training loss: 33848475288718.094\n",
      "validation loss: 47138413132228.72\n",
      "epoch: 944\n",
      "training loss: 33848221343385.953\n",
      "validation loss: 47138213444563.39\n",
      "epoch: 945\n",
      "training loss: 33847967822209.188\n",
      "validation loss: 47138015016729.23\n",
      "epoch: 946\n",
      "training loss: 33847714724165.89\n",
      "validation loss: 47137817843912.98\n",
      "epoch: 947\n",
      "training loss: 33847462048237.36\n",
      "validation loss: 47137621921320.0\n",
      "epoch: 948\n",
      "training loss: 33847209793408.137\n",
      "validation loss: 47137427244174.36\n",
      "epoch: 949\n",
      "training loss: 33846957958665.934\n",
      "validation loss: 47137233807718.66\n",
      "epoch: 950\n",
      "training loss: 33846706543001.668\n",
      "validation loss: 47137041607213.93\n",
      "epoch: 951\n",
      "training loss: 33846455545409.42\n",
      "validation loss: 47136850637939.82\n",
      "epoch: 952\n",
      "training loss: 33846204964886.457\n",
      "validation loss: 47136660895194.08\n",
      "epoch: 953\n",
      "training loss: 33845954800433.164\n",
      "validation loss: 47136472374292.766\n",
      "epoch: 954\n",
      "training loss: 33845705051053.08\n",
      "validation loss: 47136285070570.17\n",
      "epoch: 955\n",
      "training loss: 33845455715752.87\n",
      "validation loss: 47136098979378.63\n",
      "epoch: 956\n",
      "training loss: 33845206793542.336\n",
      "validation loss: 47135914096088.516\n",
      "epoch: 957\n",
      "training loss: 33844958283434.324\n",
      "validation loss: 47135730416088.06\n",
      "epoch: 958\n",
      "training loss: 33844710184444.84\n",
      "validation loss: 47135547934783.52\n",
      "epoch: 959\n",
      "training loss: 33844462495592.92\n",
      "validation loss: 47135366647598.83\n",
      "epoch: 960\n",
      "training loss: 33844215215900.68\n",
      "validation loss: 47135186549975.68\n",
      "epoch: 961\n",
      "training loss: 33843968344393.312\n",
      "validation loss: 47135007637373.336\n",
      "epoch: 962\n",
      "training loss: 33843721880099.023\n",
      "validation loss: 47134829905268.76\n",
      "epoch: 963\n",
      "training loss: 33843475822049.082\n",
      "validation loss: 47134653349156.305\n",
      "epoch: 964\n",
      "training loss: 33843230169277.754\n",
      "validation loss: 47134477964547.73\n",
      "epoch: 965\n",
      "training loss: 33842984920822.305\n",
      "validation loss: 47134303746972.13\n",
      "epoch: 966\n",
      "training loss: 33842740075723.047\n",
      "validation loss: 47134130691976.07\n",
      "epoch: 967\n",
      "training loss: 33842495633023.234\n",
      "validation loss: 47133958795123.01\n",
      "epoch: 968\n",
      "training loss: 33842251591769.105\n",
      "validation loss: 47133788051993.695\n",
      "epoch: 969\n",
      "training loss: 33842007951009.875\n",
      "validation loss: 47133618458185.94\n",
      "epoch: 970\n",
      "training loss: 33841764709797.7\n",
      "validation loss: 47133450009314.484\n",
      "epoch: 971\n",
      "training loss: 33841521867187.676\n",
      "validation loss: 47133282701010.99\n",
      "epoch: 972\n",
      "training loss: 33841279422237.85\n",
      "validation loss: 47133116528924.04\n",
      "epoch: 973\n",
      "training loss: 33841037374009.16\n",
      "validation loss: 47132951488718.78\n",
      "epoch: 974\n",
      "training loss: 33840795721565.5\n",
      "validation loss: 47132787576077.34\n",
      "epoch: 975\n",
      "training loss: 33840554463973.58\n",
      "validation loss: 47132624786698.23\n",
      "epoch: 976\n",
      "training loss: 33840313600303.07\n",
      "validation loss: 47132463116296.61\n",
      "epoch: 977\n",
      "training loss: 33840073129626.492\n",
      "validation loss: 47132302560604.125\n",
      "epoch: 978\n",
      "training loss: 33839833051019.227\n",
      "validation loss: 47132143115368.84\n",
      "epoch: 979\n",
      "training loss: 33839593363559.51\n",
      "validation loss: 47131984776355.16\n",
      "epoch: 980\n",
      "training loss: 33839354066328.438\n",
      "validation loss: 47131827539343.78\n",
      "epoch: 981\n",
      "training loss: 33839115158409.918\n",
      "validation loss: 47131671400131.66\n",
      "epoch: 982\n",
      "training loss: 33838876638890.68\n",
      "validation loss: 47131516354531.74\n",
      "epoch: 983\n",
      "training loss: 33838638506860.277\n",
      "validation loss: 47131362398373.2\n",
      "epoch: 984\n",
      "training loss: 33838400761411.074\n",
      "validation loss: 47131209527501.22\n",
      "epoch: 985\n",
      "training loss: 33838163401638.176\n",
      "validation loss: 47131057737776.79\n",
      "epoch: 986\n",
      "training loss: 33837926426639.543\n",
      "validation loss: 47130907025076.91\n",
      "epoch: 987\n",
      "training loss: 33837689835515.844\n",
      "validation loss: 47130757385294.375\n",
      "epoch: 988\n",
      "training loss: 33837453627370.52\n",
      "validation loss: 47130608814337.63\n",
      "epoch: 989\n",
      "training loss: 33837217801309.78\n",
      "validation loss: 47130461308130.87\n",
      "epoch: 990\n",
      "training loss: 33836982356442.547\n",
      "validation loss: 47130314862613.84\n",
      "epoch: 991\n",
      "training loss: 33836747291880.484\n",
      "validation loss: 47130169473741.94\n",
      "epoch: 992\n",
      "training loss: 33836512606737.992\n",
      "validation loss: 47130025137485.99\n",
      "epoch: 993\n",
      "training loss: 33836278300132.145\n",
      "validation loss: 47129881849832.18\n",
      "epoch: 994\n",
      "training loss: 33836044371182.72\n",
      "validation loss: 47129739606782.13\n",
      "epoch: 995\n",
      "training loss: 33835810819012.207\n",
      "validation loss: 47129598404352.75\n",
      "epoch: 996\n",
      "training loss: 33835577642745.758\n",
      "validation loss: 47129458238576.055\n",
      "epoch: 997\n",
      "training loss: 33835344841511.18\n",
      "validation loss: 47129319105499.4\n",
      "epoch: 998\n",
      "training loss: 33835112414438.957\n",
      "validation loss: 47129181001185.05\n",
      "epoch: 999\n",
      "training loss: 33834880360662.21\n",
      "validation loss: 47129043921710.5\n",
      "epoch: 1000\n",
      "training loss: 33834648679316.715\n",
      "validation loss: 47128907863168.07\n",
      "epoch: 1001\n",
      "training loss: 33834417369540.832\n",
      "validation loss: 47128772821664.99\n",
      "epoch: 1002\n",
      "training loss: 33834186430475.598\n",
      "validation loss: 47128638793323.516\n",
      "epoch: 1003\n",
      "training loss: 33833955861264.625\n",
      "validation loss: 47128505774280.45\n",
      "epoch: 1004\n",
      "training loss: 33833725661054.14\n",
      "validation loss: 47128373760687.52\n",
      "epoch: 1005\n",
      "training loss: 33833495828992.92\n",
      "validation loss: 47128242748710.95\n",
      "epoch: 1006\n",
      "training loss: 33833266364232.375\n",
      "validation loss: 47128112734531.71\n",
      "epoch: 1007\n",
      "training loss: 33833037265926.45\n",
      "validation loss: 47127983714345.23\n",
      "epoch: 1008\n",
      "training loss: 33832808533231.664\n",
      "validation loss: 47127855684361.41\n",
      "epoch: 1009\n",
      "training loss: 33832580165307.066\n",
      "validation loss: 47127728640804.61\n",
      "epoch: 1010\n",
      "training loss: 33832352161314.32\n",
      "validation loss: 47127602579913.695\n",
      "epoch: 1011\n",
      "training loss: 33832124520417.516\n",
      "validation loss: 47127477497941.51\n",
      "epoch: 1012\n",
      "training loss: 33831897241783.336\n",
      "validation loss: 47127353391155.43\n",
      "epoch: 1013\n",
      "training loss: 33831670324580.953\n",
      "validation loss: 47127230255836.81\n",
      "epoch: 1014\n",
      "training loss: 33831443767982.05\n",
      "validation loss: 47127108088281.26\n",
      "epoch: 1015\n",
      "training loss: 33831217571160.812\n",
      "validation loss: 47126986884798.49\n",
      "epoch: 1016\n",
      "training loss: 33830991733293.902\n",
      "validation loss: 47126866641712.16\n",
      "epoch: 1017\n",
      "training loss: 33830766253560.457\n",
      "validation loss: 47126747355359.81\n",
      "epoch: 1018\n",
      "training loss: 33830541131142.082\n",
      "validation loss: 47126629022092.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1019\n",
      "training loss: 33830316365222.86\n",
      "validation loss: 47126511638277.02\n",
      "epoch: 1020\n",
      "training loss: 33830091954989.277\n",
      "validation loss: 47126395200290.97\n",
      "epoch: 1021\n",
      "training loss: 33829867899630.312\n",
      "validation loss: 47126279704527.766\n",
      "epoch: 1022\n",
      "training loss: 33829644198337.36\n",
      "validation loss: 47126165147393.95\n",
      "epoch: 1023\n",
      "training loss: 33829420850304.22\n",
      "validation loss: 47126051525309.555\n",
      "epoch: 1024\n",
      "training loss: 33829197854727.117\n",
      "validation loss: 47125938834708.32\n",
      "epoch: 1025\n",
      "training loss: 33828975210804.684\n",
      "validation loss: 47125827072037.36\n",
      "epoch: 1026\n",
      "training loss: 33828752917737.945\n",
      "validation loss: 47125716233757.37\n",
      "epoch: 1027\n",
      "training loss: 33828530974730.324\n",
      "validation loss: 47125606316342.3\n",
      "epoch: 1028\n",
      "training loss: 33828309380987.605\n",
      "validation loss: 47125497316279.48\n",
      "epoch: 1029\n",
      "training loss: 33828088135717.94\n",
      "validation loss: 47125389230069.516\n",
      "epoch: 1030\n",
      "training loss: 33827867238131.875\n",
      "validation loss: 47125282054226.336\n",
      "epoch: 1031\n",
      "training loss: 33827646687442.266\n",
      "validation loss: 47125175785276.836\n",
      "epoch: 1032\n",
      "training loss: 33827426482864.34\n",
      "validation loss: 47125070419761.195\n",
      "epoch: 1033\n",
      "training loss: 33827206623615.668\n",
      "validation loss: 47124965954232.62\n",
      "epoch: 1034\n",
      "training loss: 33826987108916.098\n",
      "validation loss: 47124862385257.21\n",
      "epoch: 1035\n",
      "training loss: 33826767937987.86\n",
      "validation loss: 47124759709414.2\n",
      "epoch: 1036\n",
      "training loss: 33826549110055.473\n",
      "validation loss: 47124657923295.66\n",
      "epoch: 1037\n",
      "training loss: 33826330624345.72\n",
      "validation loss: 47124557023506.41\n",
      "epoch: 1038\n",
      "training loss: 33826112480087.72\n",
      "validation loss: 47124457006664.13\n",
      "epoch: 1039\n",
      "training loss: 33825894676512.88\n",
      "validation loss: 47124357869399.305\n",
      "epoch: 1040\n",
      "training loss: 33825677212854.84\n",
      "validation loss: 47124259608355.02\n",
      "epoch: 1041\n",
      "training loss: 33825460088349.555\n",
      "validation loss: 47124162220187.07\n",
      "epoch: 1042\n",
      "training loss: 33825243302235.22\n",
      "validation loss: 47124065701563.82\n",
      "epoch: 1043\n",
      "training loss: 33825026853752.273\n",
      "validation loss: 47123970049166.055\n",
      "epoch: 1044\n",
      "training loss: 33824810742143.42\n",
      "validation loss: 47123875259687.3\n",
      "epoch: 1045\n",
      "training loss: 33824594966653.582\n",
      "validation loss: 47123781329833.21\n",
      "epoch: 1046\n",
      "training loss: 33824379526529.918\n",
      "validation loss: 47123688256322.08\n",
      "epoch: 1047\n",
      "training loss: 33824164421021.805\n",
      "validation loss: 47123596035884.39\n",
      "epoch: 1048\n",
      "training loss: 33823949649380.81\n",
      "validation loss: 47123504665262.91\n",
      "epoch: 1049\n",
      "training loss: 33823735210860.734\n",
      "validation loss: 47123414141212.7\n",
      "epoch: 1050\n",
      "training loss: 33823521104717.58\n",
      "validation loss: 47123324460501.016\n",
      "epoch: 1051\n",
      "training loss: 33823307330209.496\n",
      "validation loss: 47123235619907.13\n",
      "epoch: 1052\n",
      "training loss: 33823093886596.855\n",
      "validation loss: 47123147616222.59\n",
      "epoch: 1053\n",
      "training loss: 33822880773142.17\n",
      "validation loss: 47123060446250.77\n",
      "epoch: 1054\n",
      "training loss: 33822667989110.137\n",
      "validation loss: 47122974106807.12\n",
      "epoch: 1055\n",
      "training loss: 33822455533767.59\n",
      "validation loss: 47122888594719.016\n",
      "epoch: 1056\n",
      "training loss: 33822243406383.53\n",
      "validation loss: 47122803906825.82\n",
      "epoch: 1057\n",
      "training loss: 33822031606229.094\n",
      "validation loss: 47122720039978.6\n",
      "epoch: 1058\n",
      "training loss: 33821820132577.547\n",
      "validation loss: 47122636991040.23\n",
      "epoch: 1059\n",
      "training loss: 33821608984704.277\n",
      "validation loss: 47122554756885.4\n",
      "epoch: 1060\n",
      "training loss: 33821398161886.8\n",
      "validation loss: 47122473334400.44\n",
      "epoch: 1061\n",
      "training loss: 33821187663404.727\n",
      "validation loss: 47122392720483.336\n",
      "epoch: 1062\n",
      "training loss: 33820977488539.79\n",
      "validation loss: 47122312912043.71\n",
      "epoch: 1063\n",
      "training loss: 33820767636575.816\n",
      "validation loss: 47122233906002.68\n",
      "epoch: 1064\n",
      "training loss: 33820558106798.695\n",
      "validation loss: 47122155699292.92\n",
      "epoch: 1065\n",
      "training loss: 33820348898496.418\n",
      "validation loss: 47122078288858.54\n",
      "epoch: 1066\n",
      "training loss: 33820140010959.043\n",
      "validation loss: 47122001671655.055\n",
      "epoch: 1067\n",
      "training loss: 33819931443478.7\n",
      "validation loss: 47121925844649.414\n",
      "epoch: 1068\n",
      "training loss: 33819723195349.562\n",
      "validation loss: 47121850804819.77\n",
      "epoch: 1069\n",
      "training loss: 33819515265867.86\n",
      "validation loss: 47121776549155.66\n",
      "epoch: 1070\n",
      "training loss: 33819307654331.883\n",
      "validation loss: 47121703074657.805\n",
      "epoch: 1071\n",
      "training loss: 33819100360041.926\n",
      "validation loss: 47121630378338.12\n",
      "epoch: 1072\n",
      "training loss: 33818893382300.344\n",
      "validation loss: 47121558457219.66\n",
      "epoch: 1073\n",
      "training loss: 33818686720411.48\n",
      "validation loss: 47121487308336.54\n",
      "epoch: 1074\n",
      "training loss: 33818480373681.72\n",
      "validation loss: 47121416928733.98\n",
      "epoch: 1075\n",
      "training loss: 33818274341419.465\n",
      "validation loss: 47121347315468.24\n",
      "epoch: 1076\n",
      "training loss: 33818068622935.062\n",
      "validation loss: 47121278465606.44\n",
      "epoch: 1077\n",
      "training loss: 33817863217540.91\n",
      "validation loss: 47121210376226.62\n",
      "epoch: 1078\n",
      "training loss: 33817658124551.355\n",
      "validation loss: 47121143044417.79\n",
      "epoch: 1079\n",
      "training loss: 33817453343282.76\n",
      "validation loss: 47121076467279.76\n",
      "epoch: 1080\n",
      "training loss: 33817248873053.402\n",
      "validation loss: 47121010641923.01\n",
      "epoch: 1081\n",
      "training loss: 33817044713183.574\n",
      "validation loss: 47120945565468.89\n",
      "epoch: 1082\n",
      "training loss: 33816840862995.52\n",
      "validation loss: 47120881235049.43\n",
      "epoch: 1083\n",
      "training loss: 33816637321813.41\n",
      "validation loss: 47120817647807.29\n",
      "epoch: 1084\n",
      "training loss: 33816434088963.387\n",
      "validation loss: 47120754800895.77\n",
      "epoch: 1085\n",
      "training loss: 33816231163773.492\n",
      "validation loss: 47120692691478.67\n",
      "epoch: 1086\n",
      "training loss: 33816028545573.75\n",
      "validation loss: 47120631316730.48\n",
      "epoch: 1087\n",
      "training loss: 33815826233696.05\n",
      "validation loss: 47120570673835.93\n",
      "epoch: 1088\n",
      "training loss: 33815624227474.24\n",
      "validation loss: 47120510759990.36\n",
      "epoch: 1089\n",
      "training loss: 33815422526244.066\n",
      "validation loss: 47120451572399.49\n",
      "epoch: 1090\n",
      "training loss: 33815221129343.17\n",
      "validation loss: 47120393108279.46\n",
      "epoch: 1091\n",
      "training loss: 33815020036111.117\n",
      "validation loss: 47120335364856.58\n",
      "epoch: 1092\n",
      "training loss: 33814819245889.312\n",
      "validation loss: 47120278339367.516\n",
      "epoch: 1093\n",
      "training loss: 33814618758021.074\n",
      "validation loss: 47120222029059.23\n",
      "epoch: 1094\n",
      "training loss: 33814418571851.613\n",
      "validation loss: 47120166431188.734\n",
      "epoch: 1095\n",
      "training loss: 33814218686727.98\n",
      "validation loss: 47120111543023.29\n",
      "epoch: 1096\n",
      "training loss: 33814019101999.1\n",
      "validation loss: 47120057361840.36\n",
      "epoch: 1097\n",
      "training loss: 33813819817015.758\n",
      "validation loss: 47120003884927.31\n",
      "epoch: 1098\n",
      "training loss: 33813620831130.59\n",
      "validation loss: 47119951109581.62\n",
      "epoch: 1099\n",
      "training loss: 33813422143698.074\n",
      "validation loss: 47119899033110.766\n",
      "epoch: 1100\n",
      "training loss: 33813223754074.51\n",
      "validation loss: 47119847652832.13\n",
      "epoch: 1101\n",
      "training loss: 33813025661618.06\n",
      "validation loss: 47119796966073.055\n",
      "epoch: 1102\n",
      "training loss: 33812827865688.664\n",
      "validation loss: 47119746970170.8\n",
      "epoch: 1103\n",
      "training loss: 33812630365648.13\n",
      "validation loss: 47119697662472.29\n",
      "epoch: 1104\n",
      "training loss: 33812433160860.055\n",
      "validation loss: 47119649040334.45\n",
      "epoch: 1105\n",
      "training loss: 33812236250689.83\n",
      "validation loss: 47119601101123.85\n",
      "epoch: 1106\n",
      "training loss: 33812039634504.652\n",
      "validation loss: 47119553842216.69\n",
      "epoch: 1107\n",
      "training loss: 33811843311673.547\n",
      "validation loss: 47119507260999.1\n",
      "epoch: 1108\n",
      "training loss: 33811647281567.258\n",
      "validation loss: 47119461354866.62\n",
      "epoch: 1109\n",
      "training loss: 33811451543558.37\n",
      "validation loss: 47119416121224.516\n",
      "epoch: 1110\n",
      "training loss: 33811256097021.195\n",
      "validation loss: 47119371557487.484\n",
      "epoch: 1111\n",
      "training loss: 33811060941331.85\n",
      "validation loss: 47119327661079.94\n",
      "epoch: 1112\n",
      "training loss: 33810866075868.207\n",
      "validation loss: 47119284429435.69\n",
      "epoch: 1113\n",
      "training loss: 33810671500009.85\n",
      "validation loss: 47119241859997.81\n",
      "epoch: 1114\n",
      "training loss: 33810477213138.195\n",
      "validation loss: 47119199950219.17\n",
      "epoch: 1115\n",
      "training loss: 33810283214636.324\n",
      "validation loss: 47119158697561.73\n",
      "epoch: 1116\n",
      "training loss: 33810089503889.08\n",
      "validation loss: 47119118099496.77\n",
      "epoch: 1117\n",
      "training loss: 33809896080283.062\n",
      "validation loss: 47119078153505.07\n",
      "epoch: 1118\n",
      "training loss: 33809702943206.56\n",
      "validation loss: 47119038857076.5\n",
      "epoch: 1119\n",
      "training loss: 33809510092049.6\n",
      "validation loss: 47119000207710.26\n",
      "epoch: 1120\n",
      "training loss: 33809317526203.93\n",
      "validation loss: 47118962202914.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1121\n",
      "training loss: 33809125245062.98\n",
      "validation loss: 47118924840207.234\n",
      "epoch: 1122\n",
      "training loss: 33808933248021.906\n",
      "validation loss: 47118888117114.59\n",
      "epoch: 1123\n",
      "training loss: 33808741534477.55\n",
      "validation loss: 47118852031172.46\n",
      "epoch: 1124\n",
      "training loss: 33808550103828.445\n",
      "validation loss: 47118816579925.586\n",
      "epoch: 1125\n",
      "training loss: 33808358955474.79\n",
      "validation loss: 47118781760927.64\n",
      "epoch: 1126\n",
      "training loss: 33808168088818.484\n",
      "validation loss: 47118747571741.42\n",
      "epoch: 1127\n",
      "training loss: 33807977503263.094\n",
      "validation loss: 47118714009938.64\n",
      "epoch: 1128\n",
      "training loss: 33807787198213.855\n",
      "validation loss: 47118681073099.81\n",
      "epoch: 1129\n",
      "training loss: 33807597173077.652\n",
      "validation loss: 47118648758814.33\n",
      "epoch: 1130\n",
      "training loss: 33807407427263.035\n",
      "validation loss: 47118617064680.56\n",
      "epoch: 1131\n",
      "training loss: 33807217960180.19\n",
      "validation loss: 47118585988305.49\n",
      "epoch: 1132\n",
      "training loss: 33807028771240.977\n",
      "validation loss: 47118555527305.02\n",
      "epoch: 1133\n",
      "training loss: 33806839859858.863\n",
      "validation loss: 47118525679303.71\n",
      "epoch: 1134\n",
      "training loss: 33806651225448.953\n",
      "validation loss: 47118496441934.79\n",
      "epoch: 1135\n",
      "training loss: 33806462867427.98\n",
      "validation loss: 47118467812840.16\n",
      "epoch: 1136\n",
      "training loss: 33806274785214.3\n",
      "validation loss: 47118439789670.32\n",
      "epoch: 1137\n",
      "training loss: 33806086978227.9\n",
      "validation loss: 47118412370084.55\n",
      "epoch: 1138\n",
      "training loss: 33805899445890.35\n",
      "validation loss: 47118385551750.39\n",
      "epoch: 1139\n",
      "training loss: 33805712187624.844\n",
      "validation loss: 47118359332344.14\n",
      "epoch: 1140\n",
      "training loss: 33805525202856.16\n",
      "validation loss: 47118333709550.53\n",
      "epoch: 1141\n",
      "training loss: 33805338491010.69\n",
      "validation loss: 47118308681062.65\n",
      "epoch: 1142\n",
      "training loss: 33805152051516.383\n",
      "validation loss: 47118284244582.086\n",
      "epoch: 1143\n",
      "training loss: 33804965883802.8\n",
      "validation loss: 47118260397818.87\n",
      "epoch: 1144\n",
      "training loss: 33804779987301.08\n",
      "validation loss: 47118237138491.32\n",
      "epoch: 1145\n",
      "training loss: 33804594361443.89\n",
      "validation loss: 47118214464326.055\n",
      "epoch: 1146\n",
      "training loss: 33804409005665.52\n",
      "validation loss: 47118192373058.09\n",
      "epoch: 1147\n",
      "training loss: 33804223919401.8\n",
      "validation loss: 47118170862430.67\n",
      "epoch: 1148\n",
      "training loss: 33804039102090.082\n",
      "validation loss: 47118149930195.14\n",
      "epoch: 1149\n",
      "training loss: 33803854553169.324\n",
      "validation loss: 47118129574111.21\n",
      "epoch: 1150\n",
      "training loss: 33803670272079.992\n",
      "validation loss: 47118109791946.65\n",
      "epoch: 1151\n",
      "training loss: 33803486258264.113\n",
      "validation loss: 47118090581477.42\n",
      "epoch: 1152\n",
      "training loss: 33803302511165.234\n",
      "validation loss: 47118071940487.5\n",
      "epoch: 1153\n",
      "training loss: 33803119030228.44\n",
      "validation loss: 47118053866769.06\n",
      "epoch: 1154\n",
      "training loss: 33802935814900.33\n",
      "validation loss: 47118036358122.18\n",
      "epoch: 1155\n",
      "training loss: 33802752864629.02\n",
      "validation loss: 47118019412355.016\n",
      "epoch: 1156\n",
      "training loss: 33802570178864.16\n",
      "validation loss: 47118003027283.664\n",
      "epoch: 1157\n",
      "training loss: 33802387757056.902\n",
      "validation loss: 47117987200732.21\n",
      "epoch: 1158\n",
      "training loss: 33802205598659.906\n",
      "validation loss: 47117971930532.67\n",
      "epoch: 1159\n",
      "training loss: 33802023703127.285\n",
      "validation loss: 47117957214524.82\n",
      "epoch: 1160\n",
      "training loss: 33801842069914.715\n",
      "validation loss: 47117943050556.34\n",
      "epoch: 1161\n",
      "training loss: 33801660698479.316\n",
      "validation loss: 47117929436482.836\n",
      "epoch: 1162\n",
      "training loss: 33801479588279.695\n",
      "validation loss: 47117916370167.516\n",
      "epoch: 1163\n",
      "training loss: 33801298738775.95\n",
      "validation loss: 47117903849481.49\n",
      "epoch: 1164\n",
      "training loss: 33801118149429.66\n",
      "validation loss: 47117891872303.57\n",
      "epoch: 1165\n",
      "training loss: 33800937819703.848\n",
      "validation loss: 47117880436520.26\n",
      "epoch: 1166\n",
      "training loss: 33800757749063.023\n",
      "validation loss: 47117869540025.66\n",
      "epoch: 1167\n",
      "training loss: 33800577936973.12\n",
      "validation loss: 47117859180721.555\n",
      "epoch: 1168\n",
      "training loss: 33800398382901.574\n",
      "validation loss: 47117849356517.414\n",
      "epoch: 1169\n",
      "training loss: 33800219086317.227\n",
      "validation loss: 47117840065330.19\n",
      "epoch: 1170\n",
      "training loss: 33800040046690.38\n",
      "validation loss: 47117831305084.37\n",
      "epoch: 1171\n",
      "training loss: 33799861263492.8\n",
      "validation loss: 47117823073712.13\n",
      "epoch: 1172\n",
      "training loss: 33799682736197.65\n",
      "validation loss: 47117815369152.875\n",
      "epoch: 1173\n",
      "training loss: 33799504464279.51\n",
      "validation loss: 47117808189353.68\n",
      "epoch: 1174\n",
      "training loss: 33799326447214.445\n",
      "validation loss: 47117801532268.95\n",
      "epoch: 1175\n",
      "training loss: 33799148684479.875\n",
      "validation loss: 47117795395860.5\n",
      "epoch: 1176\n",
      "training loss: 33798971175554.695\n",
      "validation loss: 47117789778097.586\n",
      "epoch: 1177\n",
      "training loss: 33798793919919.168\n",
      "validation loss: 47117784676956.836\n",
      "epoch: 1178\n",
      "training loss: 33798616917054.965\n",
      "validation loss: 47117780090422.03\n",
      "epoch: 1179\n",
      "training loss: 33798440166445.18\n",
      "validation loss: 47117776016484.37\n",
      "epoch: 1180\n",
      "training loss: 33798263667574.273\n",
      "validation loss: 47117772453142.234\n",
      "epoch: 1181\n",
      "training loss: 33798087419928.125\n",
      "validation loss: 47117769398401.36\n",
      "epoch: 1182\n",
      "training loss: 33797911422993.992\n",
      "validation loss: 47117766850274.55\n",
      "epoch: 1183\n",
      "training loss: 33797735676260.51\n",
      "validation loss: 47117764806781.92\n",
      "epoch: 1184\n",
      "training loss: 33797560179217.69\n",
      "validation loss: 47117763265950.59\n",
      "epoch: 1185\n",
      "training loss: 33797384931356.918\n",
      "validation loss: 47117762225814.9\n",
      "epoch: 1186\n",
      "training loss: 33797209932170.973\n",
      "validation loss: 47117761684416.32\n",
      "epoch: 1187\n",
      "training loss: 33797035181153.94\n",
      "validation loss: 47117761639803.25\n",
      "epoch: 1188\n",
      "training loss: 33796860677801.324\n",
      "validation loss: 47117762090031.21\n",
      "epoch: 1189\n",
      "training loss: 33796686421609.945\n",
      "validation loss: 47117763033162.78\n",
      "epoch: 1190\n",
      "training loss: 33796512412077.996\n",
      "validation loss: 47117764467267.44\n",
      "epoch: 1191\n",
      "training loss: 33796338648705.008\n",
      "validation loss: 47117766390421.71\n",
      "epoch: 1192\n",
      "training loss: 33796165130991.84\n",
      "validation loss: 47117768800708.91\n",
      "epoch: 1193\n",
      "training loss: 33795991858440.72\n",
      "validation loss: 47117771696219.4\n",
      "epoch: 1194\n",
      "training loss: 33795818830555.156\n",
      "validation loss: 47117775075050.33\n",
      "epoch: 1195\n",
      "training loss: 33795646046840.047\n",
      "validation loss: 47117778935305.805\n",
      "epoch: 1196\n",
      "training loss: 33795473506801.58\n",
      "validation loss: 47117783275096.66\n",
      "epoch: 1197\n",
      "training loss: 33795301209947.258\n",
      "validation loss: 47117788092540.51\n",
      "epoch: 1198\n",
      "training loss: 33795129155785.902\n",
      "validation loss: 47117793385761.84\n",
      "epoch: 1199\n",
      "training loss: 33794957343827.645\n",
      "validation loss: 47117799152891.805\n",
      "epoch: 1200\n",
      "training loss: 33794785773583.965\n",
      "validation loss: 47117805392068.375\n",
      "epoch: 1201\n",
      "training loss: 33794614444567.562\n",
      "validation loss: 47117812101436.08\n",
      "epoch: 1202\n",
      "training loss: 33794443356292.508\n",
      "validation loss: 47117819279146.25\n",
      "epoch: 1203\n",
      "training loss: 33794272508274.117\n",
      "validation loss: 47117826923356.71\n",
      "epoch: 1204\n",
      "training loss: 33794101900029.008\n",
      "validation loss: 47117835032232.016\n",
      "epoch: 1205\n",
      "training loss: 33793931531075.117\n",
      "validation loss: 47117843603943.33\n",
      "epoch: 1206\n",
      "training loss: 33793761400931.62\n",
      "validation loss: 47117852636668.33\n",
      "epoch: 1207\n",
      "training loss: 33793591509118.977\n",
      "validation loss: 47117862128591.21\n",
      "epoch: 1208\n",
      "training loss: 33793421855158.918\n",
      "validation loss: 47117872077902.76\n",
      "epoch: 1209\n",
      "training loss: 33793252438574.46\n",
      "validation loss: 47117882482800.22\n",
      "epoch: 1210\n",
      "training loss: 33793083258889.89\n",
      "validation loss: 47117893341487.33\n",
      "epoch: 1211\n",
      "training loss: 33792914315630.695\n",
      "validation loss: 47117904652174.18\n",
      "epoch: 1212\n",
      "training loss: 33792745608323.67\n",
      "validation loss: 47117916413077.31\n",
      "epoch: 1213\n",
      "training loss: 33792577136496.867\n",
      "validation loss: 47117928622419.8\n",
      "epoch: 1214\n",
      "training loss: 33792408899679.562\n",
      "validation loss: 47117941278430.94\n",
      "epoch: 1215\n",
      "training loss: 33792240897402.266\n",
      "validation loss: 47117954379346.28\n",
      "epoch: 1216\n",
      "training loss: 33792073129196.75\n",
      "validation loss: 47117967923408.02\n",
      "epoch: 1217\n",
      "training loss: 33791905594596.027\n",
      "validation loss: 47117981908864.33\n",
      "epoch: 1218\n",
      "training loss: 33791738293134.305\n",
      "validation loss: 47117996333969.8\n",
      "epoch: 1219\n",
      "training loss: 33791571224347.04\n",
      "validation loss: 47118011196985.22\n",
      "epoch: 1220\n",
      "training loss: 33791404387770.938\n",
      "validation loss: 47118026496177.69\n",
      "epoch: 1221\n",
      "training loss: 33791237782943.86\n",
      "validation loss: 47118042229820.375\n",
      "epoch: 1222\n",
      "training loss: 33791071409404.957\n",
      "validation loss: 47118058396192.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1223\n",
      "training loss: 33790905266694.516\n",
      "validation loss: 47118074993580.375\n",
      "epoch: 1224\n",
      "training loss: 33790739354354.09\n",
      "validation loss: 47118092020274.86\n",
      "epoch: 1225\n",
      "training loss: 33790573671926.414\n",
      "validation loss: 47118109474574.18\n",
      "epoch: 1226\n",
      "training loss: 33790408218955.41\n",
      "validation loss: 47118127354782.1\n",
      "epoch: 1227\n",
      "training loss: 33790242994986.215\n",
      "validation loss: 47118145659208.62\n",
      "epoch: 1228\n",
      "training loss: 33790077999565.12\n",
      "validation loss: 47118164386169.6\n",
      "epoch: 1229\n",
      "training loss: 33789913232239.676\n",
      "validation loss: 47118183533987.22\n",
      "epoch: 1230\n",
      "training loss: 33789748692558.56\n",
      "validation loss: 47118203100989.42\n",
      "epoch: 1231\n",
      "training loss: 33789584380071.617\n",
      "validation loss: 47118223085510.086\n",
      "epoch: 1232\n",
      "training loss: 33789420294329.906\n",
      "validation loss: 47118243485889.2\n",
      "epoch: 1233\n",
      "training loss: 33789256434885.65\n",
      "validation loss: 47118264300472.516\n",
      "epoch: 1234\n",
      "training loss: 33789092801292.242\n",
      "validation loss: 47118285527611.85\n",
      "epoch: 1235\n",
      "training loss: 33788929393104.215\n",
      "validation loss: 47118307165664.79\n",
      "epoch: 1236\n",
      "training loss: 33788766209877.297\n",
      "validation loss: 47118329212994.74\n",
      "epoch: 1237\n",
      "training loss: 33788603251168.35\n",
      "validation loss: 47118351667971.11\n",
      "epoch: 1238\n",
      "training loss: 33788440516535.387\n",
      "validation loss: 47118374528968.914\n",
      "Mean absolute error: $6676444162.47\n",
      "Nodes: 100\n",
      "Learning Rate: 1e-06\n",
      "epoch: 0\n",
      "training loss: 34151378692213.32\n",
      "validation loss: 32599906301101.164\n",
      "epoch: 1\n",
      "training loss: 32676989049913.55\n",
      "validation loss: 32225561001201.215\n",
      "epoch: 2\n",
      "training loss: 32170754834374.027\n",
      "validation loss: 31491713515595.008\n",
      "epoch: 3\n",
      "training loss: 31996149003953.586\n",
      "validation loss: 31294409650645.21\n",
      "epoch: 4\n",
      "training loss: 31814610034939.547\n",
      "validation loss: 31465075831886.074\n",
      "epoch: 5\n",
      "training loss: 31769309377068.535\n",
      "validation loss: 31515751544882.836\n",
      "epoch: 6\n",
      "training loss: 31745601789646.7\n",
      "validation loss: 31606752149198.19\n",
      "epoch: 7\n",
      "training loss: 31728215857807.312\n",
      "validation loss: 31582018659296.496\n",
      "epoch: 8\n",
      "training loss: 31726206346637.9\n",
      "validation loss: 31601643366598.88\n",
      "epoch: 9\n",
      "training loss: 31724737538347.586\n",
      "validation loss: 31602893946114.164\n",
      "epoch: 10\n",
      "training loss: 31722789541070.688\n",
      "validation loss: 31592645431579.098\n",
      "epoch: 11\n",
      "training loss: 31720276164634.18\n",
      "validation loss: 31575392013318.777\n",
      "epoch: 12\n",
      "training loss: 31717332848740.31\n",
      "validation loss: 31553954331849.45\n",
      "epoch: 13\n",
      "training loss: 31714080026855.277\n",
      "validation loss: 31530064303373.613\n",
      "epoch: 14\n",
      "training loss: 31709611970528.723\n",
      "validation loss: 31462848959416.105\n",
      "epoch: 15\n",
      "training loss: 31706128575702.89\n",
      "validation loss: 31437009900275.37\n",
      "epoch: 16\n",
      "training loss: 31702609245707.188\n",
      "validation loss: 31410735371316.477\n",
      "epoch: 17\n",
      "training loss: 31699068328403.88\n",
      "validation loss: 31384267866307.63\n",
      "epoch: 18\n",
      "training loss: 31695525380248.68\n",
      "validation loss: 31357752565047.277\n",
      "epoch: 19\n",
      "training loss: 31691992117203.285\n",
      "validation loss: 31331275530569.363\n",
      "epoch: 20\n",
      "training loss: 31688475464429.367\n",
      "validation loss: 31304887495077.9\n",
      "epoch: 21\n",
      "training loss: 31684979447828.527\n",
      "validation loss: 31278618098235.934\n",
      "epoch: 22\n",
      "training loss: 31681506349179.914\n",
      "validation loss: 31252484397768.426\n",
      "epoch: 23\n",
      "training loss: 31678057405327.94\n",
      "validation loss: 31226495952018.832\n",
      "epoch: 24\n",
      "training loss: 31674633229193.97\n",
      "validation loss: 31200657853647.543\n",
      "epoch: 25\n",
      "training loss: 31671234062496.594\n",
      "validation loss: 31174972539788.14\n",
      "epoch: 26\n",
      "training loss: 31667859927157.793\n",
      "validation loss: 31149440871883.3\n",
      "epoch: 27\n",
      "training loss: 31664510715893.266\n",
      "validation loss: 31124062779726.734\n",
      "epoch: 28\n",
      "training loss: 31661186246360.36\n",
      "validation loss: 31098837645504.586\n",
      "epoch: 29\n",
      "training loss: 31657886293490.332\n",
      "validation loss: 31073764532733.94\n",
      "epoch: 30\n",
      "training loss: 31654610608770.758\n",
      "validation loss: 31048842322680.836\n",
      "epoch: 31\n",
      "training loss: 31651358931724.406\n",
      "validation loss: 31024069795591.598\n",
      "epoch: 32\n",
      "training loss: 31648130996722.94\n",
      "validation loss: 30999445679008.57\n",
      "epoch: 33\n",
      "training loss: 31644926537013.504\n",
      "validation loss: 30974968676454.195\n",
      "epoch: 34\n",
      "training loss: 31641745287079.58\n",
      "validation loss: 30950637484407.926\n",
      "epoch: 35\n",
      "training loss: 31638586984007.848\n",
      "validation loss: 30926450802302.6\n",
      "epoch: 36\n",
      "training loss: 31635451368262.086\n",
      "validation loss: 30902407338359.5\n",
      "epoch: 37\n",
      "training loss: 31632338184102.965\n",
      "validation loss: 30878505812944.113\n",
      "epoch: 38\n",
      "training loss: 31629247179798.24\n",
      "validation loss: 30854744960445.336\n",
      "epoch: 39\n",
      "training loss: 31626178107708.336\n",
      "validation loss: 30831123530276.7\n",
      "epoch: 40\n",
      "training loss: 31623130724298.227\n",
      "validation loss: 30807640287356.246\n",
      "epoch: 41\n",
      "training loss: 31620104790106.56\n",
      "validation loss: 30784294012277.98\n",
      "epoch: 42\n",
      "training loss: 31617100069690.383\n",
      "validation loss: 30761083501301.62\n",
      "epoch: 43\n",
      "training loss: 31614116331555.715\n",
      "validation loss: 30738007566236.465\n",
      "epoch: 44\n",
      "training loss: 31611153348081.33\n",
      "validation loss: 30715065034264.305\n",
      "epoch: 45\n",
      "training loss: 31608210895438.52\n",
      "validation loss: 30692254747728.215\n",
      "epoch: 46\n",
      "training loss: 31605288753510.145\n",
      "validation loss: 30669575563903.29\n",
      "epoch: 47\n",
      "training loss: 31602386705809.54\n",
      "validation loss: 30647026354758.58\n",
      "epoch: 48\n",
      "training loss: 31599504539400.65\n",
      "validation loss: 30624606006716.055\n",
      "epoch: 49\n",
      "training loss: 31596642044819.062\n",
      "validation loss: 30602313420409.684\n",
      "epoch: 50\n",
      "training loss: 31593799015995.46\n",
      "validation loss: 30580147510446.688\n",
      "epoch: 51\n",
      "training loss: 31590975250179.895\n",
      "validation loss: 30558107205171.977\n",
      "epoch: 52\n",
      "training loss: 31588170547868.57\n",
      "validation loss: 30536191446436.41\n",
      "epoch: 53\n",
      "training loss: 31585384712731.992\n",
      "validation loss: 30514399189369.203\n",
      "epoch: 54\n",
      "training loss: 31582617551545.34\n",
      "validation loss: 30492729402154.605\n",
      "epoch: 55\n",
      "training loss: 31579868874120.1\n",
      "validation loss: 30471181065812.902\n",
      "epoch: 56\n",
      "training loss: 31577138493237.86\n",
      "validation loss: 30449753173985.76\n",
      "epoch: 57\n",
      "training loss: 31574426224584.668\n",
      "validation loss: 30428444732725.824\n",
      "epoch: 58\n",
      "training loss: 31571731886571.99\n",
      "validation loss: 30407254760290.727\n",
      "epoch: 59\n",
      "training loss: 31569109127861.53\n",
      "validation loss: 30386175658795.742\n",
      "epoch: 60\n",
      "training loss: 31566449469253.54\n",
      "validation loss: 30365216815543.6\n",
      "epoch: 61\n",
      "training loss: 31563807414625.914\n",
      "validation loss: 30344374879917.047\n",
      "epoch: 62\n",
      "training loss: 31561182713059.08\n",
      "validation loss: 30323648391646.645\n",
      "epoch: 63\n",
      "training loss: 31558575148445.082\n",
      "validation loss: 30303036113535.312\n",
      "epoch: 64\n",
      "training loss: 31555984526455.18\n",
      "validation loss: 30282536945843.176\n",
      "epoch: 65\n",
      "training loss: 31553410666716.34\n",
      "validation loss: 30262149875225.07\n",
      "epoch: 66\n",
      "training loss: 31545457659540.934\n",
      "validation loss: 30228505425040.16\n",
      "epoch: 67\n",
      "training loss: 31545260859284.184\n",
      "validation loss: 30235703543326.383\n",
      "epoch: 68\n",
      "training loss: 31544689176582.676\n",
      "validation loss: 30232116103119.74\n",
      "epoch: 69\n",
      "training loss: 31543535980349.42\n",
      "validation loss: 30221862067206.156\n",
      "epoch: 70\n",
      "training loss: 31541911542857.39\n",
      "validation loss: 30207576311170.633\n",
      "epoch: 71\n",
      "training loss: 31539965278413.0\n",
      "validation loss: 30190893700315.453\n",
      "epoch: 72\n",
      "training loss: 31537816313375.535\n",
      "validation loss: 30172811957571.137\n",
      "epoch: 73\n",
      "training loss: 31535546478695.754\n",
      "validation loss: 30153934163932.773\n",
      "epoch: 74\n",
      "training loss: 31533208451746.49\n",
      "validation loss: 30134622746431.375\n",
      "epoch: 75\n",
      "training loss: 31530835031017.098\n",
      "validation loss: 30115094684410.793\n",
      "epoch: 76\n",
      "training loss: 31528446255899.594\n",
      "validation loss: 30095479507557.574\n",
      "epoch: 77\n",
      "training loss: 31526054225961.883\n",
      "validation loss: 30075854328865.82\n",
      "epoch: 78\n",
      "training loss: 31523666183144.93\n",
      "validation loss: 30056264901385.082\n",
      "epoch: 79\n",
      "training loss: 31521286426209.72\n",
      "validation loss: 30036738237613.094\n",
      "epoch: 80\n",
      "training loss: 31518917480881.895\n",
      "validation loss: 30017290159264.336\n",
      "epoch: 81\n",
      "training loss: 31516560808533.863\n",
      "validation loss: 29997929809388.81\n",
      "epoch: 82\n",
      "training loss: 31514217233102.45\n",
      "validation loss: 29978662347371.566\n",
      "epoch: 83\n",
      "training loss: 31511887197458.473\n",
      "validation loss: 29959490557998.59\n",
      "epoch: 84\n",
      "training loss: 31509570917070.727\n",
      "validation loss: 29940415811934.355\n",
      "epoch: 85\n",
      "training loss: 31507268472002.97\n",
      "validation loss: 29921438638955.434\n",
      "epoch: 86\n",
      "training loss: 31504979861951.555\n",
      "validation loss: 29902559070025.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87\n",
      "training loss: 31502705039154.387\n",
      "validation loss: 29883776841404.383\n",
      "epoch: 88\n",
      "training loss: 31500443928059.73\n",
      "validation loss: 29865091516408.137\n",
      "epoch: 89\n",
      "training loss: 31498196437075.965\n",
      "validation loss: 29846502558038.434\n",
      "epoch: 90\n",
      "training loss: 31495962465586.035\n",
      "validation loss: 29828009372279.273\n",
      "epoch: 91\n",
      "training loss: 31493741908130.77\n",
      "validation loss: 29809611333896.566\n",
      "epoch: 92\n",
      "training loss: 31491534656900.2\n",
      "validation loss: 29791307801794.785\n",
      "epoch: 93\n",
      "training loss: 31489340603212.996\n",
      "validation loss: 29773098128142.03\n",
      "epoch: 94\n",
      "training loss: 31487159638391.99\n",
      "validation loss: 29754981663777.6\n",
      "epoch: 95\n",
      "training loss: 31484991654278.336\n",
      "validation loss: 29736957761401.05\n",
      "epoch: 96\n",
      "training loss: 31482836543531.02\n",
      "validation loss: 29719025777438.695\n",
      "epoch: 97\n",
      "training loss: 31480694199799.008\n",
      "validation loss: 29701185073121.07\n",
      "epoch: 98\n",
      "training loss: 31478564517823.426\n",
      "validation loss: 29683435015090.94\n",
      "epoch: 99\n",
      "training loss: 31476447393530.5\n",
      "validation loss: 29665774975733.566\n",
      "epoch: 100\n",
      "training loss: 31474342723425.246\n",
      "validation loss: 29648204333365.926\n",
      "epoch: 101\n",
      "training loss: 31471891192050.805\n",
      "validation loss: 29631045173533.098\n",
      "epoch: 102\n",
      "training loss: 31469786566179.668\n",
      "validation loss: 29613633956571.58\n",
      "epoch: 103\n",
      "training loss: 31467698007782.85\n",
      "validation loss: 29596332465902.74\n",
      "epoch: 104\n",
      "training loss: 31465623859894.402\n",
      "validation loss: 29579130825853.75\n",
      "epoch: 105\n",
      "training loss: 31463563095127.594\n",
      "validation loss: 29562022914597.625\n",
      "epoch: 106\n",
      "training loss: 31461515062241.94\n",
      "validation loss: 29545004852442.836\n",
      "epoch: 107\n",
      "training loss: 31459479335031.984\n",
      "validation loss: 29528074099855.82\n",
      "epoch: 108\n",
      "training loss: 31457455622129.113\n",
      "validation loss: 29511228919203.215\n",
      "epoch: 109\n",
      "training loss: 31455443713125.24\n",
      "validation loss: 29494468053507.043\n",
      "epoch: 110\n",
      "training loss: 31453443446377.375\n",
      "validation loss: 29477790534698.07\n",
      "epoch: 111\n",
      "training loss: 31451454689761.69\n",
      "validation loss: 29461195569150.79\n",
      "epoch: 112\n",
      "training loss: 31449477329164.97\n",
      "validation loss: 29444682469341.24\n",
      "epoch: 113\n",
      "training loss: 31447511261599.82\n",
      "validation loss: 29428250613031.88\n",
      "epoch: 114\n",
      "training loss: 31445556391084.05\n",
      "validation loss: 29411899418886.47\n",
      "epoch: 115\n",
      "training loss: 31443612626171.94\n",
      "validation loss: 29395628331891.055\n",
      "epoch: 116\n",
      "training loss: 31441679878473.64\n",
      "validation loss: 29379436814628.625\n",
      "epoch: 117\n",
      "training loss: 31439758061764.848\n",
      "validation loss: 29363324342047.414\n",
      "epoch: 118\n",
      "training loss: 31437847091449.617\n",
      "validation loss: 29347290398314.953\n",
      "epoch: 119\n",
      "training loss: 31435946884234.62\n",
      "validation loss: 29331334474917.645\n",
      "epoch: 120\n",
      "training loss: 31434057357929.543\n",
      "validation loss: 29315456069503.758\n",
      "epoch: 121\n",
      "training loss: 31432178431323.28\n",
      "validation loss: 29299654685170.727\n",
      "epoch: 122\n",
      "training loss: 31430310024105.28\n",
      "validation loss: 29283929830017.812\n",
      "epoch: 123\n",
      "training loss: 31428452056814.195\n",
      "validation loss: 29268281016857.676\n",
      "epoch: 124\n",
      "training loss: 31426604450802.777\n",
      "validation loss: 29252707763022.953\n",
      "epoch: 125\n",
      "training loss: 31424767128212.734\n",
      "validation loss: 29237209590230.133\n",
      "epoch: 126\n",
      "training loss: 31422940011955.312\n",
      "validation loss: 29221786024477.53\n",
      "epoch: 127\n",
      "training loss: 31421123025695.973\n",
      "validation loss: 29206436595964.547\n",
      "epoch: 128\n",
      "training loss: 31419316093840.89\n",
      "validation loss: 29191160839023.285\n",
      "epoch: 129\n",
      "training loss: 31417519141525.05\n",
      "validation loss: 29175958292058.266\n",
      "epoch: 130\n",
      "training loss: 31415732094601.383\n",
      "validation loss: 29160828497491.297\n",
      "epoch: 131\n",
      "training loss: 31413954879630.41\n",
      "validation loss: 29145771001709.43\n",
      "epoch: 132\n",
      "training loss: 31412187423870.35\n",
      "validation loss: 29130785355015.32\n",
      "epoch: 133\n",
      "training loss: 31410429655267.523\n",
      "validation loss: 29115871111579.1\n",
      "epoch: 134\n",
      "training loss: 31408681502447.215\n",
      "validation loss: 29101027829391.688\n",
      "epoch: 135\n",
      "training loss: 31406942894704.48\n",
      "validation loss: 29086255070218.914\n",
      "epoch: 136\n",
      "training loss: 31405213761995.504\n",
      "validation loss: 29071552399556.77\n",
      "epoch: 137\n",
      "training loss: 31403494034928.7\n",
      "validation loss: 29056919386587.117\n",
      "epoch: 138\n",
      "training loss: 31401783644756.473\n",
      "validation loss: 29042355604134.508\n",
      "epoch: 139\n",
      "training loss: 31400082523366.797\n",
      "validation loss: 29027860628623.465\n",
      "epoch: 140\n",
      "training loss: 31398390603275.07\n",
      "validation loss: 29013434040036.453\n",
      "epoch: 141\n",
      "training loss: 31396707817616.117\n",
      "validation loss: 28999075421872.53\n",
      "epoch: 142\n",
      "training loss: 31395034100136.44\n",
      "validation loss: 28984784361106.742\n",
      "epoch: 143\n",
      "training loss: 31393369385186.316\n",
      "validation loss: 28970560448149.77\n",
      "epoch: 144\n",
      "training loss: 31391713607712.375\n",
      "validation loss: 28956403276808.582\n",
      "epoch: 145\n",
      "training loss: 31390066703250.184\n",
      "validation loss: 28942312444247.54\n",
      "epoch: 146\n",
      "training loss: 31388428607916.85\n",
      "validation loss: 28928287550949.91\n",
      "epoch: 147\n",
      "training loss: 31386799258403.96\n",
      "validation loss: 28914328200680.168\n",
      "epoch: 148\n",
      "training loss: 31385178591970.492\n",
      "validation loss: 28900434000446.78\n",
      "epoch: 149\n",
      "training loss: 31383566546435.91\n",
      "validation loss: 28886604560465.473\n",
      "epoch: 150\n",
      "training loss: 31381963060173.434\n",
      "validation loss: 28872839494123.15\n",
      "epoch: 151\n",
      "training loss: 31380368072103.246\n",
      "validation loss: 28859138417942.156\n",
      "epoch: 152\n",
      "training loss: 31378781521686.06\n",
      "validation loss: 28845500951545.305\n",
      "epoch: 153\n",
      "training loss: 31377203348916.562\n",
      "validation loss: 28831926717621.164\n",
      "epoch: 154\n",
      "training loss: 31375633494317.258\n",
      "validation loss: 28818415341890.113\n",
      "epoch: 155\n",
      "training loss: 31374071898932.098\n",
      "validation loss: 28804966453070.598\n",
      "epoch: 156\n",
      "training loss: 31372518504320.363\n",
      "validation loss: 28791579682846.043\n",
      "epoch: 157\n",
      "training loss: 31370973252550.72\n",
      "validation loss: 28778254665832.2\n",
      "epoch: 158\n",
      "training loss: 31369436086195.33\n",
      "validation loss: 28764991039545.03\n",
      "epoch: 159\n",
      "training loss: 31367906948323.99\n",
      "validation loss: 28751788444368.953\n",
      "epoch: 160\n",
      "training loss: 31366385782498.363\n",
      "validation loss: 28738646523525.44\n",
      "epoch: 161\n",
      "training loss: 31364872532766.48\n",
      "validation loss: 28725564923042.383\n",
      "epoch: 162\n",
      "training loss: 31363367143657.113\n",
      "validation loss: 28712543291723.555\n",
      "epoch: 163\n",
      "training loss: 31361869560174.49\n",
      "validation loss: 28699581281118.75\n",
      "epoch: 164\n",
      "training loss: 31360379727792.64\n",
      "validation loss: 28686678545494.043\n",
      "epoch: 165\n",
      "training loss: 31358897592450.49\n",
      "validation loss: 28673834741802.9\n",
      "epoch: 166\n",
      "training loss: 31357423100546.42\n",
      "validation loss: 28661049529657.223\n",
      "epoch: 167\n",
      "training loss: 31355956198933.28\n",
      "validation loss: 28648322571299.11\n",
      "epoch: 168\n",
      "training loss: 31354496834913.43\n",
      "validation loss: 28635653531572.99\n",
      "epoch: 169\n",
      "training loss: 31353044956233.645\n",
      "validation loss: 28623042077897.906\n",
      "epoch: 170\n",
      "training loss: 31351600511080.49\n",
      "validation loss: 28610487880240.547\n",
      "epoch: 171\n",
      "training loss: 31350163448075.203\n",
      "validation loss: 28597990611088.137\n",
      "epoch: 172\n",
      "training loss: 31348733716269.375\n",
      "validation loss: 28585549945422.355\n",
      "epoch: 173\n",
      "training loss: 31347311265140.05\n",
      "validation loss: 28573165560693.02\n",
      "epoch: 174\n",
      "training loss: 31345896044585.16\n",
      "validation loss: 28560837136792.383\n",
      "epoch: 175\n",
      "training loss: 31344488004919.258\n",
      "validation loss: 28548564356029.926\n",
      "epoch: 176\n",
      "training loss: 31343087096868.77\n",
      "validation loss: 28536346903107.086\n",
      "epoch: 177\n",
      "training loss: 31341693271567.965\n",
      "validation loss: 28524184465092.82\n",
      "epoch: 178\n",
      "training loss: 31340306480554.418\n",
      "validation loss: 28512076731399.055\n",
      "epoch: 179\n",
      "training loss: 31338926675764.85\n",
      "validation loss: 28500023393756.734\n",
      "epoch: 180\n",
      "training loss: 31337553809530.996\n",
      "validation loss: 28488024146192.08\n",
      "epoch: 181\n",
      "training loss: 31336187834575.51\n",
      "validation loss: 28476078685003.285\n",
      "epoch: 182\n",
      "training loss: 31334828704007.85\n",
      "validation loss: 28464186708737.297\n",
      "epoch: 183\n",
      "training loss: 31333476371320.293\n",
      "validation loss: 28452347918167.105\n",
      "epoch: 184\n",
      "training loss: 31332130790384.12\n",
      "validation loss: 28440562016269.33\n",
      "epoch: 185\n",
      "training loss: 31330791915445.65\n",
      "validation loss: 28428828708201.98\n",
      "epoch: 186\n",
      "training loss: 31329459701122.367\n",
      "validation loss: 28417147701282.508\n",
      "epoch: 187\n",
      "training loss: 31328134102399.22\n",
      "validation loss: 28405518704966.254\n",
      "epoch: 188\n",
      "training loss: 31326815074625.05\n",
      "validation loss: 28393941430825.29\n",
      "epoch: 189\n",
      "training loss: 31325502573508.594\n",
      "validation loss: 28382415592527.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190\n",
      "training loss: 31324196555115.273\n",
      "validation loss: 28370940905813.863\n",
      "epoch: 191\n",
      "training loss: 31322896975863.312\n",
      "validation loss: 28359517088482.34\n",
      "epoch: 192\n",
      "training loss: 31321603792520.434\n",
      "validation loss: 28348143860363.12\n",
      "epoch: 193\n",
      "training loss: 31320316962200.31\n",
      "validation loss: 28336820943300.95\n",
      "epoch: 194\n",
      "training loss: 31319036442359.17\n",
      "validation loss: 28325548061134.938\n",
      "epoch: 195\n",
      "training loss: 31317762190792.387\n",
      "validation loss: 28314324939679.043\n",
      "epoch: 196\n",
      "training loss: 31316494165631.29\n",
      "validation loss: 28303151306703.062\n",
      "epoch: 197\n",
      "training loss: 31315232325339.695\n",
      "validation loss: 28292026891913.355\n",
      "epoch: 198\n",
      "training loss: 31313976628710.83\n",
      "validation loss: 28280951426934.387\n",
      "epoch: 199\n",
      "training loss: 31312727034864.152\n",
      "validation loss: 28269924645290.188\n",
      "epoch: 200\n",
      "training loss: 31311483503242.004\n",
      "validation loss: 28258946282385.918\n",
      "epoch: 201\n",
      "training loss: 31310245993606.707\n",
      "validation loss: 28248016075490.035\n",
      "epoch: 202\n",
      "training loss: 31309014466037.43\n",
      "validation loss: 28237133763716.438\n",
      "epoch: 203\n",
      "training loss: 31307788880927.234\n",
      "validation loss: 28226299088006.97\n",
      "epoch: 204\n",
      "training loss: 31306569198979.848\n",
      "validation loss: 28215511791113.836\n",
      "epoch: 205\n",
      "training loss: 31305355381207.062\n",
      "validation loss: 28204771617582.75\n",
      "epoch: 206\n",
      "training loss: 31304147388925.535\n",
      "validation loss: 28194078313735.812\n",
      "epoch: 207\n",
      "training loss: 31302945183754.035\n",
      "validation loss: 28183431627654.84\n",
      "epoch: 208\n",
      "training loss: 31301748727610.574\n",
      "validation loss: 28172831309164.89\n",
      "epoch: 209\n",
      "training loss: 31300557982709.656\n",
      "validation loss: 28162277109817.95\n",
      "epoch: 210\n",
      "training loss: 31299372911559.41\n",
      "validation loss: 28151768782876.77\n",
      "epoch: 211\n",
      "training loss: 31298193476959.06\n",
      "validation loss: 28141306083299.137\n",
      "epoch: 212\n",
      "training loss: 31297019641995.957\n",
      "validation loss: 28130888767721.934\n",
      "epoch: 213\n",
      "training loss: 31295851370043.066\n",
      "validation loss: 28120516594445.734\n",
      "epoch: 214\n",
      "training loss: 31294688624756.383\n",
      "validation loss: 28110189323419.53\n",
      "epoch: 215\n",
      "training loss: 31293531370072.164\n",
      "validation loss: 28099906716225.43\n",
      "epoch: 216\n",
      "training loss: 31292379570204.46\n",
      "validation loss: 28089668536063.79\n",
      "epoch: 217\n",
      "training loss: 31291233189642.562\n",
      "validation loss: 28079474547738.43\n",
      "epoch: 218\n",
      "training loss: 31290092193148.55\n",
      "validation loss: 28069324517642.082\n",
      "epoch: 219\n",
      "training loss: 31288956545754.703\n",
      "validation loss: 28059218213741.832\n",
      "epoch: 220\n",
      "training loss: 31287826212761.168\n",
      "validation loss: 28049155405565.047\n",
      "epoch: 221\n",
      "training loss: 31286701159733.41\n",
      "validation loss: 28039135864185.055\n",
      "epoch: 222\n",
      "training loss: 31285581352499.855\n",
      "validation loss: 28029159362207.39\n",
      "epoch: 223\n",
      "training loss: 31284466757149.664\n",
      "validation loss: 28019225673756.05\n",
      "epoch: 224\n",
      "training loss: 31283357340030.18\n",
      "validation loss: 28009334574459.832\n",
      "epoch: 225\n",
      "training loss: 31282253067744.816\n",
      "validation loss: 27999485841438.938\n",
      "epoch: 226\n",
      "training loss: 31281153907150.58\n",
      "validation loss: 27989679253291.69\n",
      "epoch: 227\n",
      "training loss: 31280059825355.926\n",
      "validation loss: 27979914590081.395\n",
      "epoch: 228\n",
      "training loss: 31278970789718.68\n",
      "validation loss: 27970191633323.56\n",
      "epoch: 229\n",
      "training loss: 31277886767843.395\n",
      "validation loss: 27960510165972.715\n",
      "epoch: 230\n",
      "training loss: 31276807727579.613\n",
      "validation loss: 27950869972410.13\n",
      "epoch: 231\n",
      "training loss: 31275733637019.535\n",
      "validation loss: 27941270838431.14\n",
      "epoch: 232\n",
      "training loss: 31274664464495.81\n",
      "validation loss: 27931712551232.707\n",
      "epoch: 233\n",
      "training loss: 31273600178579.47\n",
      "validation loss: 27922194899401.277\n",
      "epoch: 234\n",
      "training loss: 31272540748077.977\n",
      "validation loss: 27912717672900.75\n",
      "epoch: 235\n",
      "training loss: 31271486142033.05\n",
      "validation loss: 27903280663060.492\n",
      "epoch: 236\n",
      "training loss: 31270436329718.555\n",
      "validation loss: 27893883662563.43\n",
      "epoch: 237\n",
      "training loss: 31269391280638.63\n",
      "validation loss: 27884526465434.52\n",
      "epoch: 238\n",
      "training loss: 31268350964525.523\n",
      "validation loss: 27875208867029.086\n",
      "epoch: 239\n",
      "training loss: 31267315351337.86\n",
      "validation loss: 27865930664021.53\n",
      "epoch: 240\n",
      "training loss: 31266284411258.46\n",
      "validation loss: 27856691654394.004\n",
      "epoch: 241\n",
      "training loss: 31265258114692.48\n",
      "validation loss: 27847491637425.188\n",
      "epoch: 242\n",
      "training loss: 31264236432265.543\n",
      "validation loss: 27838330413679.355\n",
      "epoch: 243\n",
      "training loss: 31263219334821.754\n",
      "validation loss: 27829207784995.406\n",
      "epoch: 244\n",
      "training loss: 31262206793421.965\n",
      "validation loss: 27820123554476.17\n",
      "epoch: 245\n",
      "training loss: 31261198779341.848\n",
      "validation loss: 27811077526477.715\n",
      "epoch: 246\n",
      "training loss: 31260195264069.973\n",
      "validation loss: 27802069506598.7\n",
      "epoch: 247\n",
      "training loss: 31259196219306.195\n",
      "validation loss: 27793099301670.14\n",
      "epoch: 248\n",
      "training loss: 31258201616959.617\n",
      "validation loss: 27784166719744.91\n",
      "epoch: 249\n",
      "training loss: 31257211429147.07\n",
      "validation loss: 27775271570087.723\n",
      "epoch: 250\n",
      "training loss: 31256225628191.195\n",
      "validation loss: 27766413663164.98\n",
      "epoch: 251\n",
      "training loss: 31255244186618.77\n",
      "validation loss: 27757592810634.805\n",
      "epoch: 252\n",
      "training loss: 31254267077158.867\n",
      "validation loss: 27748808825337.074\n",
      "epoch: 253\n",
      "training loss: 31253294272741.312\n",
      "validation loss: 27740061521283.855\n",
      "epoch: 254\n",
      "training loss: 31252325746494.867\n",
      "validation loss: 27731350713649.605\n",
      "epoch: 255\n",
      "training loss: 31251361471745.746\n",
      "validation loss: 27722676218761.832\n",
      "epoch: 256\n",
      "training loss: 31250401422015.715\n",
      "validation loss: 27714037854091.4\n",
      "epoch: 257\n",
      "training loss: 31249445571020.562\n",
      "validation loss: 27705435438243.305\n",
      "epoch: 258\n",
      "training loss: 31248493892668.645\n",
      "validation loss: 27696868790947.61\n",
      "epoch: 259\n",
      "training loss: 31247546361058.99\n",
      "validation loss: 27688337733050.02\n",
      "epoch: 260\n",
      "training loss: 31246602950479.85\n",
      "validation loss: 27679842086502.992\n",
      "epoch: 261\n",
      "training loss: 31245663635407.3\n",
      "validation loss: 27671381674356.95\n",
      "epoch: 262\n",
      "training loss: 31244728390503.336\n",
      "validation loss: 27662956320751.098\n",
      "epoch: 263\n",
      "training loss: 31243797190614.625\n",
      "validation loss: 27654565850904.996\n",
      "epoch: 264\n",
      "training loss: 31242870010770.85\n",
      "validation loss: 27646210091109.74\n",
      "epoch: 265\n",
      "training loss: 31241946826183.156\n",
      "validation loss: 27637888868719.367\n",
      "epoch: 266\n",
      "training loss: 31241027612242.785\n",
      "validation loss: 27629602012142.508\n",
      "epoch: 267\n",
      "training loss: 31240112344519.52\n",
      "validation loss: 27621349350833.953\n",
      "epoch: 268\n",
      "training loss: 31239200998760.26\n",
      "validation loss: 27613130715286.367\n",
      "epoch: 269\n",
      "training loss: 31238293550887.438\n",
      "validation loss: 27604945937021.977\n",
      "epoch: 270\n",
      "training loss: 31237389976997.812\n",
      "validation loss: 27596794848584.66\n",
      "epoch: 271\n",
      "training loss: 31236490253360.8\n",
      "validation loss: 27588677283531.703\n",
      "epoch: 272\n",
      "training loss: 31235594356417.14\n",
      "validation loss: 27580593076425.887\n",
      "epoch: 273\n",
      "training loss: 31234702262777.707\n",
      "validation loss: 27572542062827.81\n",
      "epoch: 274\n",
      "training loss: 31233813949221.78\n",
      "validation loss: 27564524079287.83\n",
      "epoch: 275\n",
      "training loss: 31232929392695.934\n",
      "validation loss: 27556538963338.54\n",
      "epoch: 276\n",
      "training loss: 31232048570312.582\n",
      "validation loss: 27548586553487.09\n",
      "epoch: 277\n",
      "training loss: 31231171459348.645\n",
      "validation loss: 27540666689207.617\n",
      "epoch: 278\n",
      "training loss: 31230298037244.14\n",
      "validation loss: 27532779210933.734\n",
      "epoch: 279\n",
      "training loss: 31229428281601.086\n",
      "validation loss: 27524923960051.297\n",
      "epoch: 280\n",
      "training loss: 31228562170181.875\n",
      "validation loss: 27517100778890.863\n",
      "epoch: 281\n",
      "training loss: 31227699680908.17\n",
      "validation loss: 27509309510720.52\n",
      "epoch: 282\n",
      "training loss: 31226840791859.74\n",
      "validation loss: 27501549999738.93\n",
      "epoch: 283\n",
      "training loss: 31225985481272.99\n",
      "validation loss: 27493822091067.945\n",
      "epoch: 284\n",
      "training loss: 31225133727539.637\n",
      "validation loss: 27486125630745.625\n",
      "epoch: 285\n",
      "training loss: 31224285509205.703\n",
      "validation loss: 27478460465719.43\n",
      "epoch: 286\n",
      "training loss: 31223440804970.16\n",
      "validation loss: 27470826443839.2\n",
      "epoch: 287\n",
      "training loss: 31222599593683.668\n",
      "validation loss: 27463223413850.414\n",
      "epoch: 288\n",
      "training loss: 31221761854347.36\n",
      "validation loss: 27455651225387.332\n",
      "epoch: 289\n",
      "training loss: 31220927566111.812\n",
      "validation loss: 27448109728966.504\n",
      "epoch: 290\n",
      "training loss: 31220096708275.574\n",
      "validation loss: 27440598775979.92\n",
      "epoch: 291\n",
      "training loss: 31219269260284.203\n",
      "validation loss: 27433118218688.65\n",
      "epoch: 292\n",
      "training loss: 31218445201729.02\n",
      "validation loss: 27425667910216.242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 293\n",
      "training loss: 31217624512345.883\n",
      "validation loss: 27418247704542.32\n",
      "epoch: 294\n",
      "training loss: 31216807172014.27\n",
      "validation loss: 27410857456496.363\n",
      "epoch: 295\n",
      "training loss: 31215993160755.805\n",
      "validation loss: 27403497021751.17\n",
      "epoch: 296\n",
      "training loss: 31215182458733.492\n",
      "validation loss: 27396166256816.88\n",
      "epoch: 297\n",
      "training loss: 31214375046250.22\n",
      "validation loss: 27388865019034.52\n",
      "epoch: 298\n",
      "training loss: 31213570903747.94\n",
      "validation loss: 27381593166570.11\n",
      "epoch: 299\n",
      "training loss: 31212770011806.387\n",
      "validation loss: 27374350558408.492\n",
      "epoch: 300\n",
      "training loss: 31211972351142.246\n",
      "validation loss: 27367137054347.492\n",
      "epoch: 301\n",
      "training loss: 31211177902607.73\n",
      "validation loss: 27359952514991.742\n",
      "epoch: 302\n",
      "training loss: 31210386647189.85\n",
      "validation loss: 27352796801747.07\n",
      "epoch: 303\n",
      "training loss: 31209598566009.043\n",
      "validation loss: 27345669776814.406\n",
      "epoch: 304\n",
      "training loss: 31208813640318.324\n",
      "validation loss: 27338571303184.18\n",
      "epoch: 305\n",
      "training loss: 31208031851502.23\n",
      "validation loss: 27331501244630.617\n",
      "epoch: 306\n",
      "training loss: 31207253181075.71\n",
      "validation loss: 27324459465706.0\n",
      "epoch: 307\n",
      "training loss: 31206477610683.074\n",
      "validation loss: 27317445831735.03\n",
      "epoch: 308\n",
      "training loss: 31205705122097.227\n",
      "validation loss: 27310460208809.52\n",
      "epoch: 309\n",
      "training loss: 31204935697218.293\n",
      "validation loss: 27303502463782.53\n",
      "epoch: 310\n",
      "training loss: 31204169318073.004\n",
      "validation loss: 27296572464263.293\n",
      "epoch: 311\n",
      "training loss: 31203405966813.344\n",
      "validation loss: 27289670078611.477\n",
      "epoch: 312\n",
      "training loss: 31202645625715.812\n",
      "validation loss: 27282795175932.06\n",
      "epoch: 313\n",
      "training loss: 31201888277180.43\n",
      "validation loss: 27275947626069.996\n",
      "epoch: 314\n",
      "training loss: 31201133903729.61\n",
      "validation loss: 27269127299604.863\n",
      "epoch: 315\n",
      "training loss: 31200382488007.516\n",
      "validation loss: 27262334067845.906\n",
      "epoch: 316\n",
      "training loss: 31199634012778.777\n",
      "validation loss: 27255567802826.594\n",
      "epoch: 317\n",
      "training loss: 31198888460927.855\n",
      "validation loss: 27248828377299.773\n",
      "epoch: 318\n",
      "training loss: 31198145815457.81\n",
      "validation loss: 27242115664732.406\n",
      "epoch: 319\n",
      "training loss: 31197406059489.62\n",
      "validation loss: 27235429539300.69\n",
      "epoch: 320\n",
      "training loss: 31196669176261.184\n",
      "validation loss: 27228769875885.098\n",
      "epoch: 321\n",
      "training loss: 31195935149126.426\n",
      "validation loss: 27222136550065.4\n",
      "epoch: 322\n",
      "training loss: 31195203961554.375\n",
      "validation loss: 27215529438115.83\n",
      "epoch: 323\n",
      "training loss: 31194475597128.31\n",
      "validation loss: 27208948417000.28\n",
      "epoch: 324\n",
      "training loss: 31193750039544.85\n",
      "validation loss: 27202393364367.477\n",
      "epoch: 325\n",
      "training loss: 31193027272613.01\n",
      "validation loss: 27195864158546.176\n",
      "epoch: 326\n",
      "training loss: 31192307280253.586\n",
      "validation loss: 27189360678540.703\n",
      "epoch: 327\n",
      "training loss: 31191590046497.95\n",
      "validation loss: 27182882804026.01\n",
      "epoch: 328\n",
      "training loss: 31190875555487.46\n",
      "validation loss: 27176430415343.27\n",
      "epoch: 329\n",
      "training loss: 31190163791472.516\n",
      "validation loss: 27170003393495.22\n",
      "epoch: 330\n",
      "training loss: 31189454738811.75\n",
      "validation loss: 27163601620141.688\n",
      "epoch: 331\n",
      "training loss: 31188748381971.188\n",
      "validation loss: 27157224977595.035\n",
      "epoch: 332\n",
      "training loss: 31188044705523.387\n",
      "validation loss: 27150873348815.75\n",
      "epoch: 333\n",
      "training loss: 31187343694146.684\n",
      "validation loss: 27144546617408.047\n",
      "epoch: 334\n",
      "training loss: 31186645332624.324\n",
      "validation loss: 27138244667615.453\n",
      "epoch: 335\n",
      "training loss: 31185949605843.613\n",
      "validation loss: 27131967384316.44\n",
      "epoch: 336\n",
      "training loss: 31185256498795.395\n",
      "validation loss: 27125714653020.367\n",
      "epoch: 337\n",
      "training loss: 31184565996572.832\n",
      "validation loss: 27119486359862.855\n",
      "epoch: 338\n",
      "training loss: 31183878084370.977\n",
      "validation loss: 27113282391601.9\n",
      "epoch: 339\n",
      "training loss: 31183192747485.965\n",
      "validation loss: 27107102635613.6\n",
      "epoch: 340\n",
      "training loss: 31182509971313.953\n",
      "validation loss: 27100946979887.832\n",
      "epoch: 341\n",
      "training loss: 31181829741350.664\n",
      "validation loss: 27094815313024.37\n",
      "epoch: 342\n",
      "training loss: 31181152043190.51\n",
      "validation loss: 27088707524228.73\n",
      "epoch: 343\n",
      "training loss: 31180476862525.86\n",
      "validation loss: 27082623503308.094\n",
      "epoch: 344\n",
      "training loss: 31179804185146.19\n",
      "validation loss: 27076563140667.31\n",
      "epoch: 345\n",
      "training loss: 31179133996937.645\n",
      "validation loss: 27070526327305.105\n",
      "epoch: 346\n",
      "training loss: 31178466283881.99\n",
      "validation loss: 27064512954809.92\n",
      "epoch: 347\n",
      "training loss: 31177801032055.926\n",
      "validation loss: 27058522915356.035\n",
      "epoch: 348\n",
      "training loss: 31177138227630.637\n",
      "validation loss: 27052556101699.918\n",
      "epoch: 349\n",
      "training loss: 31176477856870.684\n",
      "validation loss: 27046612407176.066\n",
      "epoch: 350\n",
      "training loss: 31175819906133.668\n",
      "validation loss: 27040691725693.56\n",
      "epoch: 351\n",
      "training loss: 31175164361869.234\n",
      "validation loss: 27034793951731.953\n",
      "epoch: 352\n",
      "training loss: 31174511210618.52\n",
      "validation loss: 27028918980337.76\n",
      "epoch: 353\n",
      "training loss: 31173860439013.49\n",
      "validation loss: 27023066707120.727\n",
      "epoch: 354\n",
      "training loss: 31173212033776.184\n",
      "validation loss: 27017237028250.07\n",
      "epoch: 355\n",
      "training loss: 31172565981718.07\n",
      "validation loss: 27011429840450.938\n",
      "epoch: 356\n",
      "training loss: 31171922269739.36\n",
      "validation loss: 27005645041000.715\n",
      "epoch: 357\n",
      "training loss: 31171280884828.34\n",
      "validation loss: 26999882527725.484\n",
      "epoch: 358\n",
      "training loss: 31170641814060.715\n",
      "validation loss: 26994142198996.465\n",
      "epoch: 359\n",
      "training loss: 31170005044598.848\n",
      "validation loss: 26988423953726.414\n",
      "epoch: 360\n",
      "training loss: 31169370563691.387\n",
      "validation loss: 26982727691366.363\n",
      "epoch: 361\n",
      "training loss: 31168738358672.27\n",
      "validation loss: 26977053311901.832\n",
      "epoch: 362\n",
      "training loss: 31168108416960.426\n",
      "validation loss: 26971400715849.746\n",
      "epoch: 363\n",
      "training loss: 31167480726058.793\n",
      "validation loss: 26965769804254.695\n",
      "epoch: 364\n",
      "training loss: 31166855273554.035\n",
      "validation loss: 26960160478685.83\n",
      "epoch: 365\n",
      "training loss: 31166232047115.56\n",
      "validation loss: 26954572641233.254\n",
      "epoch: 366\n",
      "training loss: 31165611034495.188\n",
      "validation loss: 26949006194504.918\n",
      "epoch: 367\n",
      "training loss: 31164992223526.406\n",
      "validation loss: 26943461041623.23\n",
      "epoch: 368\n",
      "training loss: 31164375602123.797\n",
      "validation loss: 26937937086221.812\n",
      "epoch: 369\n",
      "training loss: 31163761158282.43\n",
      "validation loss: 26932434232442.234\n",
      "epoch: 370\n",
      "training loss: 31163148880077.234\n",
      "validation loss: 26926952384930.797\n",
      "epoch: 371\n",
      "training loss: 31162538755662.414\n",
      "validation loss: 26921491448835.375\n",
      "epoch: 372\n",
      "training loss: 31161930773270.883\n",
      "validation loss: 26916051329802.21\n",
      "epoch: 373\n",
      "training loss: 31161324921213.64\n",
      "validation loss: 26910631933972.81\n",
      "epoch: 374\n",
      "training loss: 31160721187879.22\n",
      "validation loss: 26905233167980.79\n",
      "epoch: 375\n",
      "training loss: 31160119561732.996\n",
      "validation loss: 26899854938948.754\n",
      "epoch: 376\n",
      "training loss: 31159520031316.92\n",
      "validation loss: 26894497154485.43\n",
      "epoch: 377\n",
      "training loss: 31158922585248.65\n",
      "validation loss: 26889159722682.42\n",
      "epoch: 378\n",
      "training loss: 31158327212220.99\n",
      "validation loss: 26883842552111.176\n",
      "epoch: 379\n",
      "training loss: 31157733901001.613\n",
      "validation loss: 26878545551820.21\n",
      "epoch: 380\n",
      "training loss: 31157142640432.168\n",
      "validation loss: 26873268631331.9\n",
      "epoch: 381\n",
      "training loss: 31156553419427.945\n",
      "validation loss: 26868011700639.66\n",
      "epoch: 382\n",
      "training loss: 31155966226977.387\n",
      "validation loss: 26862774670205.13\n",
      "epoch: 383\n",
      "training loss: 31155381052141.246\n",
      "validation loss: 26857557450954.984\n",
      "epoch: 384\n",
      "training loss: 31154797884052.332\n",
      "validation loss: 26852359954278.3\n",
      "epoch: 385\n",
      "training loss: 31154216711914.89\n",
      "validation loss: 26847182092023.63\n",
      "epoch: 386\n",
      "training loss: 31153637525004.066\n",
      "validation loss: 26842023776496.168\n",
      "epoch: 387\n",
      "training loss: 31153060312665.387\n",
      "validation loss: 26836884920454.957\n",
      "epoch: 388\n",
      "training loss: 31152485064314.234\n",
      "validation loss: 26831765437110.08\n",
      "epoch: 389\n",
      "training loss: 31151911769435.35\n",
      "validation loss: 26826665240119.89\n",
      "epoch: 390\n",
      "training loss: 31151340417582.324\n",
      "validation loss: 26821584243588.312\n",
      "epoch: 391\n",
      "training loss: 31150770998377.05\n",
      "validation loss: 26816522362062.074\n",
      "epoch: 392\n",
      "training loss: 31150203501509.254\n",
      "validation loss: 26811479510528.01\n",
      "epoch: 393\n",
      "training loss: 31149637916736.023\n",
      "validation loss: 26806455604410.402\n",
      "epoch: 394\n",
      "training loss: 31149074233881.15\n",
      "validation loss: 26801450559568.223\n",
      "epoch: 395\n",
      "training loss: 31148512442834.973\n",
      "validation loss: 26796464292292.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 396\n",
      "training loss: 31147952533553.598\n",
      "validation loss: 26791496719304.703\n",
      "epoch: 397\n",
      "training loss: 31147394496058.414\n",
      "validation loss: 26786547757751.61\n",
      "epoch: 398\n",
      "training loss: 31146838320435.883\n",
      "validation loss: 26781617325205.523\n",
      "epoch: 399\n",
      "training loss: 31146283996836.707\n",
      "validation loss: 26776705339660.09\n",
      "epoch: 400\n",
      "training loss: 31145731515475.727\n",
      "validation loss: 26771811719528.37\n",
      "epoch: 401\n",
      "training loss: 31145180866631.074\n",
      "validation loss: 26766936383639.98\n",
      "epoch: 402\n",
      "training loss: 31144632040644.008\n",
      "validation loss: 26762079251238.805\n",
      "epoch: 403\n",
      "training loss: 31144085027918.414\n",
      "validation loss: 26757240241980.56\n",
      "epoch: 404\n",
      "training loss: 31143539818920.105\n",
      "validation loss: 26752419275930.05\n",
      "epoch: 405\n",
      "training loss: 31142996404176.645\n",
      "validation loss: 26747616273558.97\n",
      "epoch: 406\n",
      "training loss: 31142454774276.79\n",
      "validation loss: 26742831155743.418\n",
      "epoch: 407\n",
      "training loss: 31141914919870.133\n",
      "validation loss: 26738063843761.53\n",
      "epoch: 408\n",
      "training loss: 31141376831666.4\n",
      "validation loss: 26733314259290.88\n",
      "epoch: 409\n",
      "training loss: 31140840500435.28\n",
      "validation loss: 26728582324406.293\n",
      "epoch: 410\n",
      "training loss: 31140305917005.918\n",
      "validation loss: 26723867961577.5\n",
      "epoch: 411\n",
      "training loss: 31139773072266.434\n",
      "validation loss: 26719171093666.688\n",
      "epoch: 412\n",
      "training loss: 31139241957163.53\n",
      "validation loss: 26714491643926.273\n",
      "epoch: 413\n",
      "training loss: 31138712562702.098\n",
      "validation loss: 26709829535996.527\n",
      "epoch: 414\n",
      "training loss: 31138184879944.605\n",
      "validation loss: 26705184693903.234\n",
      "epoch: 415\n",
      "training loss: 31137658900011.055\n",
      "validation loss: 26700557042055.684\n",
      "epoch: 416\n",
      "training loss: 31137134614078.266\n",
      "validation loss: 26695946505244.117\n",
      "epoch: 417\n",
      "training loss: 31136612013379.496\n",
      "validation loss: 26691353008637.62\n",
      "epoch: 418\n",
      "training loss: 31136091089204.13\n",
      "validation loss: 26686776477781.89\n",
      "epoch: 419\n",
      "training loss: 31135571832897.086\n",
      "validation loss: 26682216838596.918\n",
      "epoch: 420\n",
      "training loss: 31135054235858.758\n",
      "validation loss: 26677674017375.074\n",
      "epoch: 421\n",
      "training loss: 31134538289544.215\n",
      "validation loss: 26673147940778.59\n",
      "epoch: 422\n",
      "training loss: 31134023985463.2\n",
      "validation loss: 26668638535837.76\n",
      "epoch: 423\n",
      "training loss: 31133511315179.438\n",
      "validation loss: 26664145729948.49\n",
      "epoch: 424\n",
      "training loss: 31133000270310.223\n",
      "validation loss: 26659669450870.203\n",
      "epoch: 425\n",
      "training loss: 31132490842526.375\n",
      "validation loss: 26655209626724.0\n",
      "epoch: 426\n",
      "training loss: 31131983023551.438\n",
      "validation loss: 26650766185990.19\n",
      "epoch: 427\n",
      "training loss: 31131476805161.566\n",
      "validation loss: 26646339057506.484\n",
      "epoch: 428\n",
      "training loss: 31130972179185.203\n",
      "validation loss: 26641928170465.895\n",
      "epoch: 429\n",
      "training loss: 31130469137502.363\n",
      "validation loss: 26637533454414.45\n",
      "epoch: 430\n",
      "training loss: 31129967672044.59\n",
      "validation loss: 26633154839249.426\n",
      "epoch: 431\n",
      "training loss: 31129467774794.555\n",
      "validation loss: 26628792255217.293\n",
      "epoch: 432\n",
      "training loss: 31128969437785.406\n",
      "validation loss: 26624445632911.46\n",
      "epoch: 433\n",
      "training loss: 31128472653100.746\n",
      "validation loss: 26620114903270.562\n",
      "epoch: 434\n",
      "training loss: 31127977412874.08\n",
      "validation loss: 26615799997576.32\n",
      "epoch: 435\n",
      "training loss: 31127483709288.56\n",
      "validation loss: 26611500847451.66\n",
      "epoch: 436\n",
      "training loss: 31126991534576.54\n",
      "validation loss: 26607217384858.688\n",
      "epoch: 437\n",
      "training loss: 31126500881019.39\n",
      "validation loss: 26602949542096.895\n",
      "epoch: 438\n",
      "training loss: 31126011740946.82\n",
      "validation loss: 26598697251800.918\n",
      "epoch: 439\n",
      "training loss: 31125524106736.832\n",
      "validation loss: 26594460446938.918\n",
      "epoch: 440\n",
      "training loss: 31125037970815.277\n",
      "validation loss: 26590239060810.59\n",
      "epoch: 441\n",
      "training loss: 31124553325655.516\n",
      "validation loss: 26586033027045.266\n",
      "epoch: 442\n",
      "training loss: 31124070163777.98\n",
      "validation loss: 26581842279599.94\n",
      "epoch: 443\n",
      "training loss: 31123588477750.14\n",
      "validation loss: 26577666752757.715\n",
      "epoch: 444\n",
      "training loss: 31123108260185.88\n",
      "validation loss: 26573506381125.65\n",
      "epoch: 445\n",
      "training loss: 31122629503745.23\n",
      "validation loss: 26569361099633.01\n",
      "epoch: 446\n",
      "training loss: 31122152201134.086\n",
      "validation loss: 26565230843529.49\n",
      "epoch: 447\n",
      "training loss: 31121676345103.85\n",
      "validation loss: 26561115548383.348\n",
      "epoch: 448\n",
      "training loss: 31121201928451.01\n",
      "validation loss: 26557015150079.55\n",
      "epoch: 449\n",
      "training loss: 31120728944017.152\n",
      "validation loss: 26552929584818.246\n",
      "epoch: 450\n",
      "training loss: 31120257384688.3\n",
      "validation loss: 26548858789112.72\n",
      "epoch: 451\n",
      "training loss: 31119787243394.61\n",
      "validation loss: 26544802699787.64\n",
      "epoch: 452\n",
      "training loss: 31119318513110.383\n",
      "validation loss: 26540761253977.566\n",
      "epoch: 453\n",
      "training loss: 31118851186853.42\n",
      "validation loss: 26536734389124.95\n",
      "epoch: 454\n",
      "training loss: 31118385257684.746\n",
      "validation loss: 26532722042978.402\n",
      "epoch: 455\n",
      "training loss: 31117920718708.57\n",
      "validation loss: 26528724153591.254\n",
      "epoch: 456\n",
      "training loss: 31117457563071.6\n",
      "validation loss: 26524740659319.508\n",
      "epoch: 457\n",
      "training loss: 31116995783963.15\n",
      "validation loss: 26520771498820.465\n",
      "epoch: 458\n",
      "training loss: 31116535374614.414\n",
      "validation loss: 26516816611050.723\n",
      "epoch: 459\n",
      "training loss: 31116076328298.6\n",
      "validation loss: 26512875935264.85\n",
      "epoch: 460\n",
      "training loss: 31115618638330.215\n",
      "validation loss: 26508949411013.387\n",
      "epoch: 461\n",
      "training loss: 31115162298065.18\n",
      "validation loss: 26505036978141.52\n",
      "epoch: 462\n",
      "training loss: 31114707300900.133\n",
      "validation loss: 26501138576787.12\n",
      "epoch: 463\n",
      "training loss: 31114253640272.453\n",
      "validation loss: 26497254147379.39\n",
      "epoch: 464\n",
      "training loss: 31113801309659.965\n",
      "validation loss: 26493383630637.215\n",
      "epoch: 465\n",
      "training loss: 31113350302580.355\n",
      "validation loss: 26489526967567.324\n",
      "epoch: 466\n",
      "training loss: 31112900612591.203\n",
      "validation loss: 26485684099462.953\n",
      "epoch: 467\n",
      "training loss: 31112452233289.586\n",
      "validation loss: 26481854967902.176\n",
      "epoch: 468\n",
      "training loss: 31112005158311.914\n",
      "validation loss: 26478039514746.414\n",
      "epoch: 469\n",
      "training loss: 31111559381333.355\n",
      "validation loss: 26474237682138.62\n",
      "epoch: 470\n",
      "training loss: 31111114896067.84\n",
      "validation loss: 26470449412501.98\n",
      "epoch: 471\n",
      "training loss: 31110671696267.72\n",
      "validation loss: 26466674648538.29\n",
      "epoch: 472\n",
      "training loss: 31110229775723.49\n",
      "validation loss: 26462913333226.42\n",
      "epoch: 473\n",
      "training loss: 31109789128263.527\n",
      "validation loss: 26459165409820.82\n",
      "epoch: 474\n",
      "training loss: 31109349747753.91\n",
      "validation loss: 26455430821850.05\n",
      "epoch: 475\n",
      "training loss: 31108911628097.848\n",
      "validation loss: 26451709513115.043\n",
      "epoch: 476\n",
      "training loss: 31108474763235.777\n",
      "validation loss: 26448001427687.883\n",
      "epoch: 477\n",
      "training loss: 31108039147144.918\n",
      "validation loss: 26444306509910.22\n",
      "epoch: 478\n",
      "training loss: 31107604773839.086\n",
      "validation loss: 26440624704391.777\n",
      "epoch: 479\n",
      "training loss: 31107171637368.418\n",
      "validation loss: 26436955956008.953\n",
      "epoch: 480\n",
      "training loss: 31106739731819.11\n",
      "validation loss: 26433300209903.31\n",
      "epoch: 481\n",
      "training loss: 31106309051313.164\n",
      "validation loss: 26429657411480.145\n",
      "epoch: 482\n",
      "training loss: 31105879590008.047\n",
      "validation loss: 26426027506406.97\n",
      "epoch: 483\n",
      "training loss: 31105451342096.77\n",
      "validation loss: 26422410440612.363\n",
      "epoch: 484\n",
      "training loss: 31105024301807.285\n",
      "validation loss: 26418806160284.254\n",
      "epoch: 485\n",
      "training loss: 31104598463402.395\n",
      "validation loss: 26415214611868.652\n",
      "epoch: 486\n",
      "training loss: 31104173821179.496\n",
      "validation loss: 26411635742068.203\n",
      "epoch: 487\n",
      "training loss: 31103750369470.293\n",
      "validation loss: 26408069497840.812\n",
      "epoch: 488\n",
      "training loss: 31103328102640.543\n",
      "validation loss: 26404515826398.2\n",
      "epoch: 489\n",
      "training loss: 31102907015090.074\n",
      "validation loss: 26400974675204.76\n",
      "epoch: 490\n",
      "training loss: 31102487101252.23\n",
      "validation loss: 26397445991975.957\n",
      "epoch: 491\n",
      "training loss: 31102068355593.797\n",
      "validation loss: 26393929724677.04\n",
      "epoch: 492\n",
      "training loss: 31101650772614.61\n",
      "validation loss: 26390425821521.633\n",
      "epoch: 493\n",
      "training loss: 31101234346847.668\n",
      "validation loss: 26386934230970.633\n",
      "epoch: 494\n",
      "training loss: 31100819072858.594\n",
      "validation loss: 26383454901730.637\n",
      "epoch: 495\n",
      "training loss: 31100404945245.39\n",
      "validation loss: 26379987782752.613\n",
      "epoch: 496\n",
      "training loss: 31099991958638.555\n",
      "validation loss: 26376532823230.855\n",
      "epoch: 497\n",
      "training loss: 31099580107700.53\n",
      "validation loss: 26373089972601.426\n",
      "epoch: 498\n",
      "training loss: 31099169387125.473\n",
      "validation loss: 26369659180540.816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 499\n",
      "training loss: 31098759791639.375\n",
      "validation loss: 26366240396964.99\n",
      "epoch: 500\n",
      "training loss: 31098351315999.4\n",
      "validation loss: 26362833572027.676\n",
      "epoch: 501\n",
      "training loss: 31097943954994.113\n",
      "validation loss: 26359438656119.527\n",
      "epoch: 502\n",
      "training loss: 31097537703442.82\n",
      "validation loss: 26356055599866.445\n",
      "epoch: 503\n",
      "training loss: 31097132556195.812\n",
      "validation loss: 26352684354128.703\n",
      "epoch: 504\n",
      "training loss: 31096728508133.684\n",
      "validation loss: 26349324869999.316\n",
      "epoch: 505\n",
      "training loss: 31096325554167.6\n",
      "validation loss: 26345977098803.21\n",
      "epoch: 506\n",
      "training loss: 31095923689238.637\n",
      "validation loss: 26342640992095.56\n",
      "epoch: 507\n",
      "training loss: 31095522908317.977\n",
      "validation loss: 26339316501661.004\n",
      "epoch: 508\n",
      "training loss: 31095123206406.332\n",
      "validation loss: 26336003579511.973\n",
      "epoch: 509\n",
      "training loss: 31094724578534.027\n",
      "validation loss: 26332702177887.85\n",
      "epoch: 510\n",
      "training loss: 31094327019760.81\n",
      "validation loss: 26329412249253.684\n",
      "epoch: 511\n",
      "training loss: 31093930525175.305\n",
      "validation loss: 26326133746298.766\n",
      "epoch: 512\n",
      "training loss: 31093535089895.176\n",
      "validation loss: 26322866621935.72\n",
      "epoch: 513\n",
      "training loss: 31093140709066.92\n",
      "validation loss: 26319610829299.28\n",
      "epoch: 514\n",
      "training loss: 31092747377865.316\n",
      "validation loss: 26316366321744.89\n",
      "epoch: 515\n",
      "training loss: 31092355091493.59\n",
      "validation loss: 26313133052847.777\n",
      "epoch: 516\n",
      "training loss: 31091963845183.09\n",
      "validation loss: 26309910976401.71\n",
      "epoch: 517\n",
      "training loss: 31091573634193.26\n",
      "validation loss: 26306700046417.965\n",
      "epoch: 518\n",
      "training loss: 31091184453811.027\n",
      "validation loss: 26303500217123.867\n",
      "epoch: 519\n",
      "training loss: 31090796299351.008\n",
      "validation loss: 26300311442961.92\n",
      "epoch: 520\n",
      "training loss: 31090409166155.223\n",
      "validation loss: 26297133678588.656\n",
      "epoch: 521\n",
      "training loss: 31090023049592.855\n",
      "validation loss: 26293966878873.418\n",
      "epoch: 522\n",
      "training loss: 31089637945060.246\n",
      "validation loss: 26290810998897.4\n",
      "epoch: 523\n",
      "training loss: 31089253847980.297\n",
      "validation loss: 26287665993952.223\n",
      "epoch: 524\n",
      "training loss: 31088870753802.69\n",
      "validation loss: 26284531819539.133\n",
      "epoch: 525\n",
      "training loss: 31088488658003.582\n",
      "validation loss: 26281408431367.777\n",
      "epoch: 526\n",
      "training loss: 31088107556085.402\n",
      "validation loss: 26278295785355.152\n",
      "epoch: 527\n",
      "training loss: 31087727443576.684\n",
      "validation loss: 26275193837624.49\n",
      "epoch: 528\n",
      "training loss: 31087348316031.92\n",
      "validation loss: 26272102544504.195\n",
      "epoch: 529\n",
      "training loss: 31086970169031.344\n",
      "validation loss: 26269021862526.78\n",
      "epoch: 530\n",
      "training loss: 31086592998180.78\n",
      "validation loss: 26265951748427.773\n",
      "epoch: 531\n",
      "training loss: 31086216799111.46\n",
      "validation loss: 26262892159144.67\n",
      "epoch: 532\n",
      "training loss: 31085841567479.875\n",
      "validation loss: 26259843051815.906\n",
      "epoch: 533\n",
      "training loss: 31085467298967.582\n",
      "validation loss: 26256804383779.79\n",
      "epoch: 534\n",
      "training loss: 31085093989281.035\n",
      "validation loss: 26253776112573.43\n",
      "epoch: 535\n",
      "training loss: 31084721634151.438\n",
      "validation loss: 26250758195931.77\n",
      "epoch: 536\n",
      "training loss: 31084350229334.562\n",
      "validation loss: 26247750591786.516\n",
      "epoch: 537\n",
      "training loss: 31083979770610.586\n",
      "validation loss: 26244753258265.117\n",
      "epoch: 538\n",
      "training loss: 31083610253783.945\n",
      "validation loss: 26241766153689.76\n",
      "epoch: 539\n",
      "training loss: 31083241674683.137\n",
      "validation loss: 26238789236576.355\n",
      "epoch: 540\n",
      "training loss: 31082874029160.6\n",
      "validation loss: 26235822465633.54\n",
      "epoch: 541\n",
      "training loss: 31082507313092.52\n",
      "validation loss: 26232865799761.676\n",
      "epoch: 542\n",
      "training loss: 31082141522378.574\n",
      "validation loss: 26229919198051.74\n",
      "epoch: 543\n",
      "training loss: 31081776652942.13\n",
      "validation loss: 26226982619784.668\n",
      "epoch: 544\n",
      "training loss: 31081412700729.703\n",
      "validation loss: 26224056024430.07\n",
      "epoch: 545\n",
      "training loss: 31081049661710.984\n",
      "validation loss: 26221139371645.316\n",
      "epoch: 546\n",
      "training loss: 31080687531878.6\n",
      "validation loss: 26218232621274.633\n",
      "epoch: 547\n",
      "training loss: 31080326307247.914\n",
      "validation loss: 26215335733347.984\n",
      "epoch: 548\n",
      "training loss: 31079965983857.19\n",
      "validation loss: 26212448668080.43\n",
      "epoch: 549\n",
      "training loss: 31079606557767.11\n",
      "validation loss: 26209571385870.906\n",
      "epoch: 550\n",
      "training loss: 31079248025060.734\n",
      "validation loss: 26206703847301.324\n",
      "epoch: 551\n",
      "training loss: 31078890381843.207\n",
      "validation loss: 26203846013135.566\n",
      "epoch: 552\n",
      "training loss: 31078533624242.004\n",
      "validation loss: 26200997844318.84\n",
      "epoch: 553\n",
      "training loss: 31078177748406.445\n",
      "validation loss: 26198159301976.473\n",
      "epoch: 554\n",
      "training loss: 31077822750507.6\n",
      "validation loss: 26195330347413.01\n",
      "epoch: 555\n",
      "training loss: 31077468626738.06\n",
      "validation loss: 26192510942111.297\n",
      "epoch: 556\n",
      "training loss: 31077115373312.21\n",
      "validation loss: 26189701047731.81\n",
      "epoch: 557\n",
      "training loss: 31076762986465.49\n",
      "validation loss: 26186900626111.336\n",
      "epoch: 558\n",
      "training loss: 31076411462454.83\n",
      "validation loss: 26184109639262.504\n",
      "epoch: 559\n",
      "training loss: 31076060797558.14\n",
      "validation loss: 26181328049372.57\n",
      "epoch: 560\n",
      "training loss: 31075710988074.11\n",
      "validation loss: 26178555818802.52\n",
      "epoch: 561\n",
      "training loss: 31075362030322.492\n",
      "validation loss: 26175792910086.496\n",
      "epoch: 562\n",
      "training loss: 31075013920643.453\n",
      "validation loss: 26173039285930.496\n",
      "epoch: 563\n",
      "training loss: 31074666655397.957\n",
      "validation loss: 26170294909211.906\n",
      "epoch: 564\n",
      "training loss: 31074320230967.22\n",
      "validation loss: 26167559742978.344\n",
      "epoch: 565\n",
      "training loss: 31073974643752.58\n",
      "validation loss: 26164833750446.758\n",
      "epoch: 566\n",
      "training loss: 31073629890175.785\n",
      "validation loss: 26162116895002.86\n",
      "epoch: 567\n",
      "training loss: 31073285966678.31\n",
      "validation loss: 26159409140199.93\n",
      "epoch: 568\n",
      "training loss: 31072942869721.746\n",
      "validation loss: 26156710449758.312\n",
      "epoch: 569\n",
      "training loss: 31072600595787.156\n",
      "validation loss: 26154020787564.15\n",
      "epoch: 570\n",
      "training loss: 31072259141375.5\n",
      "validation loss: 26151340117668.984\n",
      "epoch: 571\n",
      "training loss: 31071918503006.91\n",
      "validation loss: 26148668404288.51\n",
      "epoch: 572\n",
      "training loss: 31071578677221.164\n",
      "validation loss: 26146005611802.137\n",
      "epoch: 573\n",
      "training loss: 31071239660576.992\n",
      "validation loss: 26143351704751.72\n",
      "epoch: 574\n",
      "training loss: 31070901449652.34\n",
      "validation loss: 26140706647841.11\n",
      "epoch: 575\n",
      "training loss: 31070564041044.277\n",
      "validation loss: 26138070405935.285\n",
      "epoch: 576\n",
      "training loss: 31070227431368.45\n",
      "validation loss: 26135442944059.24\n",
      "epoch: 577\n",
      "training loss: 31069891617259.496\n",
      "validation loss: 26132824227397.59\n",
      "epoch: 578\n",
      "training loss: 31069556595370.395\n",
      "validation loss: 26130214221293.332\n",
      "epoch: 579\n",
      "training loss: 31069222362372.75\n",
      "validation loss: 26127612891247.42\n",
      "epoch: 580\n",
      "training loss: 31068888914956.664\n",
      "validation loss: 26125020202917.87\n",
      "epoch: 581\n",
      "training loss: 31068556249830.207\n",
      "validation loss: 26122436122118.75\n",
      "epoch: 582\n",
      "training loss: 31068224363719.7\n",
      "validation loss: 26119860614819.625\n",
      "epoch: 583\n",
      "training loss: 31067893253369.637\n",
      "validation loss: 26117293647144.844\n",
      "epoch: 584\n",
      "training loss: 31067562915542.125\n",
      "validation loss: 26114735185372.383\n",
      "epoch: 585\n",
      "training loss: 31067233347017.164\n",
      "validation loss: 26112185195933.438\n",
      "epoch: 586\n",
      "training loss: 31066904544592.586\n",
      "validation loss: 26109643645411.637\n",
      "epoch: 587\n",
      "training loss: 31066576505083.47\n",
      "validation loss: 26107110500541.934\n",
      "epoch: 588\n",
      "training loss: 31066249225322.477\n",
      "validation loss: 26104585728210.203\n",
      "epoch: 589\n",
      "training loss: 31065922702159.594\n",
      "validation loss: 26102069295452.344\n",
      "epoch: 590\n",
      "training loss: 31065596932462.145\n",
      "validation loss: 26099561169453.664\n",
      "epoch: 591\n",
      "training loss: 31065271913114.29\n",
      "validation loss: 26097061317547.777\n",
      "epoch: 592\n",
      "training loss: 31064947641017.316\n",
      "validation loss: 26094569707216.207\n",
      "epoch: 593\n",
      "training loss: 31064624113089.38\n",
      "validation loss: 26092086306087.527\n",
      "epoch: 594\n",
      "training loss: 31064301326265.55\n",
      "validation loss: 26089611081936.727\n",
      "epoch: 595\n",
      "training loss: 31063979277497.3\n",
      "validation loss: 26087144002684.184\n",
      "epoch: 596\n",
      "training loss: 31063657963752.78\n",
      "validation loss: 26084685036395.21\n",
      "epoch: 597\n",
      "training loss: 31063337382016.598\n",
      "validation loss: 26082234151279.28\n",
      "epoch: 598\n",
      "training loss: 31063017529289.703\n",
      "validation loss: 26079791315689.24\n",
      "epoch: 599\n",
      "training loss: 31062698402589.293\n",
      "validation loss: 26077356498120.668\n",
      "epoch: 600\n",
      "training loss: 31062379998948.83\n",
      "validation loss: 26074929667211.203\n",
      "epoch: 601\n",
      "training loss: 31062062315417.547\n",
      "validation loss: 26072510791739.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 602\n",
      "training loss: 31061745349060.74\n",
      "validation loss: 26070099840625.152\n",
      "epoch: 603\n",
      "training loss: 31061429096959.516\n",
      "validation loss: 26067696782927.2\n",
      "epoch: 604\n",
      "training loss: 31061113556210.746\n",
      "validation loss: 26065301587844.15\n",
      "epoch: 605\n",
      "training loss: 31060798723926.906\n",
      "validation loss: 26062914224712.9\n",
      "epoch: 606\n",
      "training loss: 31060484597235.992\n",
      "validation loss: 26060534663008.16\n",
      "epoch: 607\n",
      "training loss: 31060171173281.46\n",
      "validation loss: 26058162872341.73\n",
      "epoch: 608\n",
      "training loss: 31059858449222.074\n",
      "validation loss: 26055798822461.84\n",
      "epoch: 609\n",
      "training loss: 31059546422231.836\n",
      "validation loss: 26053442483252.47\n",
      "epoch: 610\n",
      "training loss: 31059235089499.89\n",
      "validation loss: 26051093824732.66\n",
      "epoch: 611\n",
      "training loss: 31058924448230.426\n",
      "validation loss: 26048752817055.87\n",
      "epoch: 612\n",
      "training loss: 31058614495642.562\n",
      "validation loss: 26046419430509.27\n",
      "epoch: 613\n",
      "training loss: 31058305228970.28\n",
      "validation loss: 26044093635513.13\n",
      "epoch: 614\n",
      "training loss: 31057996645462.312\n",
      "validation loss: 26041775402620.11\n",
      "epoch: 615\n",
      "training loss: 31057688742382.074\n",
      "validation loss: 26039464702514.645\n",
      "epoch: 616\n",
      "training loss: 31057381517007.523\n",
      "validation loss: 26037161506012.28\n",
      "epoch: 617\n",
      "training loss: 31057074966631.117\n",
      "validation loss: 26034865784058.984\n",
      "epoch: 618\n",
      "training loss: 31056769088559.684\n",
      "validation loss: 26032577507730.566\n",
      "epoch: 619\n",
      "training loss: 31056463880114.37\n",
      "validation loss: 26030296648232.0\n",
      "epoch: 620\n",
      "training loss: 31056159338630.52\n",
      "validation loss: 26028023176896.805\n",
      "epoch: 621\n",
      "training loss: 31055855461457.57\n",
      "validation loss: 26025757065186.35\n",
      "epoch: 622\n",
      "training loss: 31055552245959.027\n",
      "validation loss: 26023498284689.33\n",
      "epoch: 623\n",
      "training loss: 31055249689512.31\n",
      "validation loss: 26021246807121.035\n",
      "epoch: 624\n",
      "training loss: 31054947789508.69\n",
      "validation loss: 26019002604322.79\n",
      "epoch: 625\n",
      "training loss: 31054646543353.23\n",
      "validation loss: 26016765648261.305\n",
      "epoch: 626\n",
      "training loss: 31054345948464.63\n",
      "validation loss: 26014535911028.08\n",
      "epoch: 627\n",
      "training loss: 31054046002275.21\n",
      "validation loss: 26012313364838.742\n",
      "epoch: 628\n",
      "training loss: 31053746702230.668\n",
      "validation loss: 26010097982032.406\n",
      "epoch: 629\n",
      "training loss: 31053448045790.367\n",
      "validation loss: 26007889735071.305\n",
      "epoch: 630\n",
      "training loss: 31053150030426.914\n",
      "validation loss: 26005688596539.934\n",
      "epoch: 631\n",
      "training loss: 31052852653626.15\n",
      "validation loss: 26003494539144.523\n",
      "epoch: 632\n",
      "training loss: 31052555912887.09\n",
      "validation loss: 26001307535712.4\n",
      "epoch: 633\n",
      "training loss: 31052259805721.84\n",
      "validation loss: 25999127559191.492\n",
      "epoch: 634\n",
      "training loss: 31051964329655.36\n",
      "validation loss: 25996954582649.543\n",
      "epoch: 635\n",
      "training loss: 31051669482225.86\n",
      "validation loss: 25994788579273.883\n",
      "epoch: 636\n",
      "training loss: 31051375260984.215\n",
      "validation loss: 25992629522370.508\n",
      "epoch: 637\n",
      "training loss: 31051081663494.098\n",
      "validation loss: 25990477385363.64\n",
      "epoch: 638\n",
      "training loss: 31050788687331.855\n",
      "validation loss: 25988332141795.08\n",
      "epoch: 639\n",
      "training loss: 31050496330086.293\n",
      "validation loss: 25986193765323.574\n",
      "epoch: 640\n",
      "training loss: 31050204589359.047\n",
      "validation loss: 25984062229724.58\n",
      "epoch: 641\n",
      "training loss: 31049913462764.07\n",
      "validation loss: 25981937508889.34\n",
      "epoch: 642\n",
      "training loss: 31049622947927.68\n",
      "validation loss: 25979819576824.414\n",
      "epoch: 643\n",
      "training loss: 31049333042488.355\n",
      "validation loss: 25977708407651.027\n",
      "epoch: 644\n",
      "training loss: 31049043744097.06\n",
      "validation loss: 25975603975604.85\n",
      "epoch: 645\n",
      "training loss: 31048755050416.82\n",
      "validation loss: 25973506255035.1\n",
      "epoch: 646\n",
      "training loss: 31048466959122.523\n",
      "validation loss: 25971415220403.95\n",
      "epoch: 647\n",
      "training loss: 31048179467901.41\n",
      "validation loss: 25969330846286.383\n",
      "epoch: 648\n",
      "training loss: 31047892574452.45\n",
      "validation loss: 25967253107369.234\n",
      "epoch: 649\n",
      "training loss: 31047606276486.332\n",
      "validation loss: 25965181978450.69\n",
      "epoch: 650\n",
      "training loss: 31047320571725.824\n",
      "validation loss: 25963117434440.098\n",
      "epoch: 651\n",
      "training loss: 31047035457905.25\n",
      "validation loss: 25961059450357.03\n",
      "epoch: 652\n",
      "training loss: 31046750932770.426\n",
      "validation loss: 25959008001330.785\n",
      "epoch: 653\n",
      "training loss: 31046466994079.0\n",
      "validation loss: 25956963062600.207\n",
      "epoch: 654\n",
      "training loss: 31046183639600.0\n",
      "validation loss: 25954924609512.758\n",
      "epoch: 655\n",
      "training loss: 31045900867113.7\n",
      "validation loss: 25952892617524.05\n",
      "epoch: 656\n",
      "training loss: 31045618674412.023\n",
      "validation loss: 25950867062197.645\n",
      "epoch: 657\n",
      "training loss: 31045337059298.04\n",
      "validation loss: 25948847919204.152\n",
      "epoch: 658\n",
      "training loss: 31045056019585.855\n",
      "validation loss: 25946835164320.758\n",
      "epoch: 659\n",
      "training loss: 31044775553101.07\n",
      "validation loss: 25944828773431.027\n",
      "epoch: 771\n",
      "training loss: 31016535141784.02\n",
      "validation loss: 25755243872566.133\n",
      "epoch: 772\n",
      "training loss: 31016307773321.793\n",
      "validation loss: 25753824195129.535\n",
      "epoch: 773\n",
      "training loss: 31016080788841.527\n",
      "validation loss: 25752408714942.684\n",
      "epoch: 774\n",
      "training loss: 31015854187093.652\n",
      "validation loss: 25750997417252.723\n",
      "epoch: 775\n",
      "training loss: 31015627966834.266\n",
      "validation loss: 25749590287367.957\n",
      "epoch: 776\n",
      "training loss: 31015402126824.86\n",
      "validation loss: 25748187310657.44\n",
      "epoch: 777\n",
      "training loss: 31015176665832.945\n",
      "validation loss: 25746788472551.133\n",
      "epoch: 778\n",
      "training loss: 31014951582631.383\n",
      "validation loss: 25745393758539.176\n",
      "epoch: 779\n",
      "training loss: 31014726875998.562\n",
      "validation loss: 25744003154171.75\n",
      "epoch: 780\n",
      "training loss: 31014502544718.363\n",
      "validation loss: 25742616645058.824\n",
      "epoch: 781\n",
      "training loss: 31014278587580.14\n",
      "validation loss: 25741234216869.84\n",
      "epoch: 782\n",
      "training loss: 31014055003378.703\n",
      "validation loss: 25739855855333.473\n",
      "epoch: 783\n",
      "training loss: 31013831790914.035\n",
      "validation loss: 25738481546237.203\n",
      "epoch: 784\n",
      "training loss: 31013608948991.918\n",
      "validation loss: 25737111275427.465\n",
      "epoch: 785\n",
      "training loss: 31013386476423.29\n",
      "validation loss: 25735745028808.95\n",
      "epoch: 786\n",
      "training loss: 31013164372024.383\n",
      "validation loss: 25734382792344.473\n",
      "epoch: 787\n",
      "training loss: 31012942634616.73\n",
      "validation loss: 25733024552054.734\n",
      "epoch: 788\n",
      "training loss: 31012721263026.94\n",
      "validation loss: 25731670294017.855\n",
      "epoch: 789\n",
      "training loss: 31012500256087.23\n",
      "validation loss: 25730320004369.62\n",
      "epoch: 790\n",
      "training loss: 31012279612634.83\n",
      "validation loss: 25728973669302.68\n",
      "epoch: 791\n",
      "training loss: 31012059331512.15\n",
      "validation loss: 25727631275066.54\n",
      "epoch: 792\n",
      "training loss: 31011839411566.742\n",
      "validation loss: 25726292807967.2\n",
      "epoch: 793\n",
      "training loss: 31011619851651.062\n",
      "validation loss: 25724958254366.82\n",
      "epoch: 794\n",
      "training loss: 31011400650623.08\n",
      "validation loss: 25723627600683.88\n",
      "epoch: 795\n",
      "training loss: 31011181807345.605\n",
      "validation loss: 25722300833392.45\n",
      "epoch: 796\n",
      "training loss: 31010963320686.508\n",
      "validation loss: 25720977939022.125\n",
      "epoch: 797\n",
      "training loss: 31010745189518.426\n",
      "validation loss: 25719658904157.598\n",
      "epoch: 798\n",
      "training loss: 31010527412719.426\n",
      "validation loss: 25718343715438.85\n",
      "epoch: 799\n",
      "training loss: 31010309989172.285\n",
      "validation loss: 25717032359560.457\n",
      "epoch: 800\n",
      "training loss: 31010092917764.727\n",
      "validation loss: 25715724823271.48\n",
      "epoch: 801\n",
      "training loss: 31009876197389.18\n",
      "validation loss: 25714421093375.11\n",
      "epoch: 802\n",
      "training loss: 31009659826943.312\n",
      "validation loss: 25713121156728.766\n",
      "epoch: 803\n",
      "training loss: 31009443805329.484\n",
      "validation loss: 25711825000243.504\n",
      "epoch: 804\n",
      "training loss: 31009228131454.84\n",
      "validation loss: 25710532610883.867\n",
      "epoch: 805\n",
      "training loss: 31009012804231.125\n",
      "validation loss: 25709243975667.496\n",
      "epoch: 806\n",
      "training loss: 31008797822575.266\n",
      "validation loss: 25707959081665.34\n",
      "epoch: 807\n",
      "training loss: 31008583185408.72\n",
      "validation loss: 25706677916000.96\n",
      "epoch: 808\n",
      "training loss: 31008368891657.664\n",
      "validation loss: 25705400465850.45\n",
      "epoch: 809\n",
      "training loss: 31008154940252.742\n",
      "validation loss: 25704126718442.008\n",
      "epoch: 810\n",
      "training loss: 31007941330129.72\n",
      "validation loss: 25702856661056.207\n",
      "epoch: 811\n",
      "training loss: 31007728060228.746\n",
      "validation loss: 25701590281025.258\n",
      "epoch: 812\n",
      "training loss: 31007515129494.418\n",
      "validation loss: 25700327565732.766\n",
      "epoch: 813\n",
      "training loss: 31007302536876.3\n",
      "validation loss: 25699068502614.023\n",
      "epoch: 814\n",
      "training loss: 31007090281328.344\n",
      "validation loss: 25697813079155.207\n",
      "epoch: 815\n",
      "training loss: 31006878361808.836\n",
      "validation loss: 25696561282893.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 816\n",
      "training loss: 31006666777280.977\n",
      "validation loss: 25695313101415.95\n",
      "epoch: 817\n",
      "training loss: 31006455526712.254\n",
      "validation loss: 25694068522361.44\n",
      "epoch: 818\n",
      "training loss: 31006244609074.434\n",
      "validation loss: 25692827533417.81\n",
      "epoch: 819\n",
      "training loss: 31006034023344.133\n",
      "validation loss: 25691590122323.406\n",
      "epoch: 820\n",
      "training loss: 31005823768502.168\n",
      "validation loss: 25690356276866.145\n",
      "epoch: 821\n",
      "training loss: 31005613843533.56\n",
      "validation loss: 25689125984883.246\n",
      "epoch: 822\n",
      "training loss: 31005404247428.117\n",
      "validation loss: 25687899234261.51\n",
      "epoch: 823\n",
      "training loss: 31005194979179.797\n",
      "validation loss: 25686676012936.62\n",
      "epoch: 824\n",
      "training loss: 31004986037786.664\n",
      "validation loss: 25685456308892.918\n",
      "epoch: 825\n",
      "training loss: 31004777422251.496\n",
      "validation loss: 25684240110163.613\n",
      "epoch: 826\n",
      "training loss: 31004569131581.188\n",
      "validation loss: 25683027404830.15\n",
      "epoch: 827\n",
      "training loss: 31004361164786.613\n",
      "validation loss: 25681818181021.945\n",
      "epoch: 828\n",
      "training loss: 31004153520883.375\n",
      "validation loss: 25680612426916.7\n",
      "epoch: 829\n",
      "training loss: 31003946198890.82\n",
      "validation loss: 25679410130739.44\n",
      "epoch: 830\n",
      "training loss: 31003739197832.91\n",
      "validation loss: 25678211280763.05\n",
      "epoch: 831\n",
      "training loss: 31003532516737.594\n",
      "validation loss: 25677015865307.516\n",
      "epoch: 832\n",
      "training loss: 31003326154636.74\n",
      "validation loss: 25675823872739.71\n",
      "epoch: 833\n",
      "training loss: 31003120110566.75\n",
      "validation loss: 25674635291473.688\n",
      "epoch: 834\n",
      "training loss: 31002914383567.95\n",
      "validation loss: 25673450109969.965\n",
      "epoch: 835\n",
      "training loss: 31002708972684.5\n",
      "validation loss: 25672268316735.336\n",
      "epoch: 836\n",
      "training loss: 31002503876965.074\n",
      "validation loss: 25671089900323.098\n",
      "epoch: 837\n",
      "training loss: 31002299095461.95\n",
      "validation loss: 25669914849332.27\n",
      "epoch: 838\n",
      "training loss: 31002094627231.836\n",
      "validation loss: 25668743152407.94\n",
      "epoch: 839\n",
      "training loss: 31001890471335.027\n",
      "validation loss: 25667574798240.484\n",
      "epoch: 840\n",
      "training loss: 31001686626836.207\n",
      "validation loss: 25666409775565.977\n",
      "epoch: 841\n",
      "training loss: 31001483092803.824\n",
      "validation loss: 25665248073165.5\n",
      "epoch: 842\n",
      "training loss: 31001279868310.02\n",
      "validation loss: 25664089679864.934\n",
      "epoch: 843\n",
      "training loss: 31001076952431.297\n",
      "validation loss: 25662934584535.26\n",
      "epoch: 844\n",
      "training loss: 31000874344247.582\n",
      "validation loss: 25661782776091.69\n",
      "epoch: 845\n",
      "training loss: 31000672042843.176\n",
      "validation loss: 25660634243494.125\n",
      "epoch: 846\n",
      "training loss: 31000470047305.74\n",
      "validation loss: 25659488975746.29\n",
      "epoch: 847\n",
      "training loss: 31000268356727.215\n",
      "validation loss: 25658346961896.19\n",
      "epoch: 848\n",
      "training loss: 31000066970202.92\n",
      "validation loss: 25657208191035.293\n",
      "epoch: 849\n",
      "training loss: 30999865886832.426\n",
      "validation loss: 25656072652298.934\n",
      "epoch: 850\n",
      "training loss: 30999665105718.832\n",
      "validation loss: 25654940334865.645\n",
      "epoch: 851\n",
      "training loss: 30999464625968.76\n",
      "validation loss: 25653811227956.99\n",
      "epoch: 852\n",
      "training loss: 30999264446693.043\n",
      "validation loss: 25652685320837.797\n",
      "epoch: 853\n",
      "training loss: 30999064567005.742\n",
      "validation loss: 25651562602815.387\n",
      "epoch: 854\n",
      "training loss: 30998864986025.098\n",
      "validation loss: 25650443063239.98\n",
      "epoch: 855\n",
      "training loss: 30998665702872.547\n",
      "validation loss: 25649326691503.86\n",
      "epoch: 856\n",
      "training loss: 30998466716673.656\n",
      "validation loss: 25648213477041.84\n",
      "epoch: 857\n",
      "training loss: 30998268026557.16\n",
      "validation loss: 25647103409330.46\n",
      "epoch: 858\n",
      "training loss: 30998069631655.87\n",
      "validation loss: 25645996477888.363\n",
      "epoch: 859\n",
      "training loss: 30997871531105.76\n",
      "validation loss: 25644892672275.52\n",
      "epoch: 860\n",
      "training loss: 30997673724046.81\n",
      "validation loss: 25643791982093.684\n",
      "epoch: 861\n",
      "training loss: 30997476209622.13\n",
      "validation loss: 25642694396985.547\n",
      "epoch: 862\n",
      "training loss: 30997278986978.824\n",
      "validation loss: 25641599906635.195\n",
      "epoch: 863\n",
      "training loss: 30997082055267.047\n",
      "validation loss: 25640508500767.297\n",
      "epoch: 864\n",
      "training loss: 30996885413640.973\n",
      "validation loss: 25639420169147.535\n",
      "epoch: 865\n",
      "training loss: 30996689061257.75\n",
      "validation loss: 25638334901581.81\n",
      "epoch: 866\n",
      "training loss: 30996492997278.477\n",
      "validation loss: 25637252687916.68\n",
      "epoch: 867\n",
      "training loss: 30996297220867.26\n",
      "validation loss: 25636173518038.55\n",
      "epoch: 868\n",
      "training loss: 30996101731191.848\n",
      "validation loss: 25635097381873.98\n",
      "epoch: 869\n",
      "training loss: 30995906527423.668\n",
      "validation loss: 25634024269389.477\n",
      "epoch: 870\n",
      "training loss: 30995711608737.07\n",
      "validation loss: 25632954170590.855\n",
      "epoch: 871\n",
      "training loss: 30995516974310.24\n",
      "validation loss: 25631887075523.703\n",
      "epoch: 872\n",
      "training loss: 30995322623324.293\n",
      "validation loss: 25630822974272.56\n",
      "epoch: 873\n",
      "training loss: 30995128554964.14\n",
      "validation loss: 25629761856961.375\n",
      "epoch: 874\n",
      "training loss: 30994934768417.555\n",
      "validation loss: 25628703713752.69\n",
      "epoch: 875\n",
      "training loss: 30994741262876.12\n",
      "validation loss: 25627648534848.133\n",
      "epoch: 876\n",
      "training loss: 30994548037534.22\n",
      "validation loss: 25626596310487.562\n",
      "epoch: 877\n",
      "training loss: 30994355091589.773\n",
      "validation loss: 25625547030949.418\n",
      "epoch: 878\n",
      "training loss: 30994162424244.19\n",
      "validation loss: 25624500686550.5\n",
      "epoch: 879\n",
      "training loss: 30993970034701.66\n",
      "validation loss: 25623457267645.37\n",
      "epoch: 880\n",
      "training loss: 30993777922170.07\n",
      "validation loss: 25622416764626.8\n",
      "epoch: 881\n",
      "training loss: 30993586085860.062\n",
      "validation loss: 25621379167924.926\n",
      "epoch: 882\n",
      "training loss: 30993394524985.957\n",
      "validation loss: 25620344468007.777\n",
      "epoch: 883\n",
      "training loss: 30993203238764.754\n",
      "validation loss: 25619312655380.457\n",
      "epoch: 884\n",
      "training loss: 30993012226416.895\n",
      "validation loss: 25618283720585.41\n",
      "epoch: 885\n",
      "training loss: 30992821487166.18\n",
      "validation loss: 25617257654202.324\n",
      "epoch: 886\n",
      "training loss: 30992631020239.082\n",
      "validation loss: 25616234446847.414\n",
      "epoch: 887\n",
      "training loss: 30992440824865.676\n",
      "validation loss: 25615214089173.973\n",
      "epoch: 888\n",
      "training loss: 30992250900278.65\n",
      "validation loss: 25614196571871.504\n",
      "epoch: 889\n",
      "training loss: 30992061245714.027\n",
      "validation loss: 25613181885666.06\n",
      "epoch: 890\n",
      "training loss: 30991871860411.145\n",
      "validation loss: 25612170021320.105\n",
      "epoch: 891\n",
      "training loss: 30991682743611.902\n",
      "validation loss: 25611160969631.85\n",
      "epoch: 892\n",
      "training loss: 30991493894561.746\n",
      "validation loss: 25610154721435.715\n",
      "epoch: 893\n",
      "training loss: 30991305312508.65\n",
      "validation loss: 25609151267601.566\n",
      "epoch: 894\n",
      "training loss: 30991116996703.836\n",
      "validation loss: 25608150599035.004\n",
      "epoch: 895\n",
      "training loss: 30990928946401.773\n",
      "validation loss: 25607152706677.246\n",
      "epoch: 896\n",
      "training loss: 30990741160859.445\n",
      "validation loss: 25606157581504.44\n",
      "epoch: 897\n",
      "training loss: 30990553639337.008\n",
      "validation loss: 25605165214528.035\n",
      "epoch: 898\n",
      "training loss: 30990366381097.844\n",
      "validation loss: 25604175596794.582\n",
      "epoch: 899\n",
      "training loss: 30990179385407.75\n",
      "validation loss: 25603188719385.12\n",
      "epoch: 900\n",
      "training loss: 30989992651535.74\n",
      "validation loss: 25602204573415.516\n",
      "epoch: 901\n",
      "training loss: 30989806178753.93\n",
      "validation loss: 25601223150036.297\n",
      "epoch: 902\n",
      "training loss: 30989619966336.863\n",
      "validation loss: 25600244440431.996\n",
      "epoch: 903\n",
      "training loss: 30989434013562.22\n",
      "validation loss: 25599268435821.504\n",
      "epoch: 904\n",
      "training loss: 30989248319710.75\n",
      "validation loss: 25598295127457.934\n",
      "epoch: 905\n",
      "training loss: 30989062884065.6\n",
      "validation loss: 25597324506627.945\n",
      "epoch: 906\n",
      "training loss: 30988877705912.99\n",
      "validation loss: 25596356564652.125\n",
      "epoch: 907\n",
      "training loss: 30988692784542.2\n",
      "validation loss: 25595391292884.8\n",
      "epoch: 908\n",
      "training loss: 30988508119244.816\n",
      "validation loss: 25594428682713.406\n",
      "epoch: 909\n",
      "training loss: 30988323709315.496\n",
      "validation loss: 25593468725558.848\n",
      "epoch: 910\n",
      "training loss: 30988139554051.89\n",
      "validation loss: 25592511412875.36\n",
      "epoch: 911\n",
      "training loss: 30987955652753.96\n",
      "validation loss: 25591556736149.832\n",
      "epoch: 912\n",
      "training loss: 30987772004724.62\n",
      "validation loss: 25590604686902.152\n",
      "epoch: 913\n",
      "training loss: 30987588609269.805\n",
      "validation loss: 25589655256685.152\n",
      "epoch: 914\n",
      "training loss: 30987405465697.688\n",
      "validation loss: 25588708437083.832\n",
      "epoch: 915\n",
      "training loss: 30987222573319.355\n",
      "validation loss: 25587764219715.805\n",
      "epoch: 916\n",
      "training loss: 30987039931448.9\n",
      "validation loss: 25586822596231.156\n",
      "epoch: 917\n",
      "training loss: 30986857539402.56\n",
      "validation loss: 25585883558311.73\n",
      "epoch: 918\n",
      "training loss: 30986675396499.516\n",
      "validation loss: 25584947097671.562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 919\n",
      "training loss: 30986493502061.6\n",
      "validation loss: 25584013206056.543\n",
      "epoch: 920\n",
      "training loss: 30986311855413.547\n",
      "validation loss: 25583081875244.46\n",
      "epoch: 921\n",
      "training loss: 30986130455882.22\n",
      "validation loss: 25582153097044.305\n",
      "epoch: 922\n",
      "training loss: 30985949302797.35\n",
      "validation loss: 25581226863296.695\n",
      "epoch: 923\n",
      "training loss: 30985768395491.53\n",
      "validation loss: 25580303165873.73\n",
      "epoch: 924\n",
      "training loss: 30985587733299.46\n",
      "validation loss: 25579381996678.31\n",
      "epoch: 925\n",
      "training loss: 30985407315558.66\n",
      "validation loss: 25578463347644.516\n",
      "epoch: 926\n",
      "training loss: 30985227141609.21\n",
      "validation loss: 25577547210737.35\n",
      "epoch: 927\n",
      "training loss: 30985047210794.01\n",
      "validation loss: 25576633577952.734\n",
      "epoch: 928\n",
      "training loss: 30984867522458.01\n",
      "validation loss: 25575722441316.836\n",
      "epoch: 929\n",
      "training loss: 30984688075948.94\n",
      "validation loss: 25574813792886.508\n",
      "epoch: 930\n",
      "training loss: 30984508870617.03\n",
      "validation loss: 25573907624748.938\n",
      "epoch: 931\n",
      "training loss: 30984329905815.31\n",
      "validation loss: 25573003929021.707\n",
      "epoch: 932\n",
      "training loss: 30984151180898.754\n",
      "validation loss: 25572102697852.133\n",
      "epoch: 933\n",
      "training loss: 30983972695225.11\n",
      "validation loss: 25571203923417.65\n",
      "epoch: 934\n",
      "training loss: 30983794448154.582\n",
      "validation loss: 25570307597925.523\n",
      "epoch: 935\n",
      "training loss: 30983616439050.098\n",
      "validation loss: 25569413713612.89\n",
      "epoch: 936\n",
      "training loss: 30983438667276.5\n",
      "validation loss: 25568522262746.08\n",
      "epoch: 937\n",
      "training loss: 30983261132201.355\n",
      "validation loss: 25567633237621.043\n",
      "epoch: 938\n",
      "training loss: 30983083833194.62\n",
      "validation loss: 25566746630563.02\n",
      "epoch: 939\n",
      "training loss: 30982906769628.97\n",
      "validation loss: 25565862433926.61\n",
      "epoch: 940\n",
      "training loss: 30982729940878.902\n",
      "validation loss: 25564980640095.117\n",
      "epoch: 941\n",
      "training loss: 30982553346321.61\n",
      "validation loss: 25564101241480.902\n",
      "epoch: 942\n",
      "training loss: 30982376985336.65\n",
      "validation loss: 25563224230525.168\n",
      "epoch: 943\n",
      "training loss: 30982200857305.94\n",
      "validation loss: 25562349599697.77\n",
      "epoch: 944\n",
      "training loss: 30982024961614.027\n",
      "validation loss: 25561477341497.266\n",
      "epoch: 945\n",
      "training loss: 30981849297647.266\n",
      "validation loss: 25560607448450.29\n",
      "epoch: 946\n",
      "training loss: 30981673864794.617\n",
      "validation loss: 25559739913111.945\n",
      "epoch: 947\n",
      "training loss: 30981498662447.348\n",
      "validation loss: 25558874728065.508\n",
      "epoch: 948\n",
      "training loss: 30981323689999.312\n",
      "validation loss: 25558011885922.496\n",
      "epoch: 949\n",
      "training loss: 30981148946846.133\n",
      "validation loss: 25557151379321.97\n",
      "epoch: 950\n",
      "training loss: 30980974432385.992\n",
      "validation loss: 25556293200930.984\n",
      "epoch: 951\n",
      "training loss: 30980800146019.336\n",
      "validation loss: 25555437343444.29\n",
      "epoch: 952\n",
      "training loss: 30980626087148.887\n",
      "validation loss: 25554583799584.176\n",
      "epoch: 953\n",
      "training loss: 30980452255179.613\n",
      "validation loss: 25553732562100.414\n",
      "epoch: 954\n",
      "training loss: 30980278649519.0\n",
      "validation loss: 25552883623770.242\n",
      "epoch: 955\n",
      "training loss: 30980105269576.254\n",
      "validation loss: 25552036977397.76\n",
      "epoch: 956\n",
      "training loss: 30979932114763.035\n",
      "validation loss: 25551192615814.336\n",
      "epoch: 957\n",
      "training loss: 30979759184493.25\n",
      "validation loss: 25550350531878.297\n",
      "epoch: 958\n",
      "training loss: 30979586478182.984\n",
      "validation loss: 25549510718474.844\n",
      "epoch: 959\n",
      "training loss: 30979413995250.793\n",
      "validation loss: 25548673168516.086\n",
      "epoch: 960\n",
      "training loss: 30979241735116.88\n",
      "validation loss: 25547837874940.383\n",
      "epoch: 961\n",
      "training loss: 30979069697203.89\n",
      "validation loss: 25547004830712.78\n",
      "epoch: 962\n",
      "training loss: 30978897880936.625\n",
      "validation loss: 25546174028824.727\n",
      "epoch: 963\n",
      "training loss: 30978726285742.035\n",
      "validation loss: 25545345462293.934\n",
      "epoch: 964\n",
      "training loss: 30978554911049.227\n",
      "validation loss: 25544519124164.293\n",
      "epoch: 965\n",
      "training loss: 30978383756289.418\n",
      "validation loss: 25543695007505.734\n",
      "epoch: 966\n",
      "training loss: 30978212820896.22\n",
      "validation loss: 25542873105414.312\n",
      "epoch: 967\n",
      "training loss: 30978042104304.832\n",
      "validation loss: 25542053411011.53\n",
      "epoch: 968\n",
      "training loss: 30977871605952.805\n",
      "validation loss: 25541235917444.78\n",
      "epoch: 969\n",
      "training loss: 30977701325279.812\n",
      "validation loss: 25540420617887.094\n",
      "epoch: 970\n",
      "training loss: 30977531261727.56\n",
      "validation loss: 25539607505536.945\n",
      "epoch: 971\n",
      "training loss: 30977361414739.863\n",
      "validation loss: 25538796573618.23\n",
      "epoch: 972\n",
      "training loss: 30977191783762.58\n",
      "validation loss: 25537987815380.086\n",
      "epoch: 973\n",
      "training loss: 30977022368243.914\n",
      "validation loss: 25537181224097.03\n",
      "epoch: 974\n",
      "training loss: 30976853167633.547\n",
      "validation loss: 25536376793068.24\n",
      "epoch: 975\n",
      "training loss: 30976684181383.48\n",
      "validation loss: 25535574515618.02\n",
      "epoch: 976\n",
      "training loss: 30976515408947.727\n",
      "validation loss: 25534774385095.523\n",
      "epoch: 977\n",
      "training loss: 30976346849782.32\n",
      "validation loss: 25533976394874.645\n",
      "epoch: 978\n",
      "training loss: 30976178503345.293\n",
      "validation loss: 25533180538353.9\n",
      "epoch: 979\n",
      "training loss: 30976010369096.69\n",
      "validation loss: 25532386808956.3\n",
      "epoch: 980\n",
      "training loss: 30975842446498.53\n",
      "validation loss: 25531595200129.297\n",
      "epoch: 981\n",
      "training loss: 30975674735015.066\n",
      "validation loss: 25530805705344.86\n",
      "epoch: 982\n",
      "training loss: 30975507234112.008\n",
      "validation loss: 25530018318098.723\n",
      "epoch: 983\n",
      "training loss: 30975339943257.273\n",
      "validation loss: 25529233031910.97\n",
      "epoch: 984\n",
      "training loss: 30975172861920.73\n",
      "validation loss: 25528449840325.64\n",
      "epoch: 985\n",
      "training loss: 30975005989574.195\n",
      "validation loss: 25527668736910.68\n",
      "epoch: 986\n",
      "training loss: 30974839325691.42\n",
      "validation loss: 25526889715257.863\n",
      "epoch: 987\n",
      "training loss: 30974672869748.066\n",
      "validation loss: 25526112768982.59\n",
      "epoch: 988\n",
      "training loss: 30974506621221.72\n",
      "validation loss: 25525337891723.895\n",
      "epoch: 989\n",
      "training loss: 30974340579591.85\n",
      "validation loss: 25524565077144.29\n",
      "epoch: 990\n",
      "training loss: 30974174744339.87\n",
      "validation loss: 25523794318929.664\n",
      "epoch: 991\n",
      "training loss: 30974009114949.31\n",
      "validation loss: 25523025610789.402\n",
      "epoch: 992\n",
      "training loss: 30973843690905.047\n",
      "validation loss: 25522258946455.676\n",
      "epoch: 993\n",
      "training loss: 30973678471694.098\n",
      "validation loss: 25521494319683.914\n",
      "epoch: 994\n",
      "training loss: 30973513456805.35\n",
      "validation loss: 25520731724252.59\n",
      "epoch: 995\n",
      "training loss: 30973348645729.523\n",
      "validation loss: 25519971153963.027\n",
      "epoch: 996\n",
      "training loss: 30973184037959.223\n",
      "validation loss: 25519212602639.355\n",
      "epoch: 997\n",
      "training loss: 30973019632988.844\n",
      "validation loss: 25518456064128.39\n",
      "epoch: 998\n",
      "training loss: 30972855430314.625\n",
      "validation loss: 25517701532299.57\n",
      "epoch: 999\n",
      "training loss: 30972691429434.652\n",
      "validation loss: 25516949001044.81\n",
      "epoch: 1000\n",
      "training loss: 30972527629848.78\n",
      "validation loss: 25516198464278.434\n",
      "epoch: 1001\n",
      "training loss: 30972364031058.69\n",
      "validation loss: 25515449915937.098\n",
      "epoch: 1002\n",
      "training loss: 30972200632567.85\n",
      "validation loss: 25514703349979.65\n",
      "epoch: 1003\n",
      "training loss: 30972037433881.523\n",
      "validation loss: 25513958760387.066\n",
      "epoch: 1004\n",
      "training loss: 30971874434506.74\n",
      "validation loss: 25513216141162.355\n",
      "epoch: 1005\n",
      "training loss: 30971711633952.297\n",
      "validation loss: 25512475486330.453\n",
      "epoch: 1006\n",
      "training loss: 30971549031728.785\n",
      "validation loss: 25511736789938.145\n",
      "epoch: 1007\n",
      "training loss: 30971386627348.508\n",
      "validation loss: 25511000046053.94\n",
      "epoch: 1008\n",
      "training loss: 30971224420325.816\n",
      "validation loss: 25510265248768.227\n",
      "epoch: 1009\n",
      "training loss: 30971062410176.242\n",
      "validation loss: 25509532392192.54\n",
      "epoch: 1010\n",
      "training loss: 30970900596417.324\n",
      "validation loss: 25508801470460.098\n",
      "epoch: 1011\n",
      "training loss: 30970738978568.31\n",
      "validation loss: 25508072477725.49\n",
      "epoch: 1012\n",
      "training loss: 30970577556150.17\n",
      "validation loss: 25507345408164.58\n",
      "epoch: 1013\n",
      "training loss: 30970416328685.61\n",
      "validation loss: 25506620255974.457\n",
      "epoch: 1014\n",
      "training loss: 30970255295698.98\n",
      "validation loss: 25505897015373.312\n",
      "epoch: 1015\n",
      "training loss: 30970094456716.363\n",
      "validation loss: 25505175680600.348\n",
      "epoch: 1016\n",
      "training loss: 30969933811265.523\n",
      "validation loss: 25504456245915.695\n",
      "epoch: 1017\n",
      "training loss: 30969773358875.87\n",
      "validation loss: 25503738705600.35\n",
      "epoch: 1018\n",
      "training loss: 30969613099078.523\n",
      "validation loss: 25503023053956.035\n",
      "epoch: 1019\n",
      "training loss: 30969453031406.24\n",
      "validation loss: 25502309285305.152\n",
      "epoch: 1020\n",
      "training loss: 30969293155393.426\n",
      "validation loss: 25501597393990.664\n",
      "epoch: 1021\n",
      "training loss: 30969133470576.156\n",
      "validation loss: 25500887374376.043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1022\n",
      "training loss: 30968973976492.125\n",
      "validation loss: 25500179220845.168\n",
      "epoch: 1023\n",
      "training loss: 30968814672680.66\n",
      "validation loss: 25499472927802.223\n",
      "epoch: 1024\n",
      "training loss: 30968655558682.73\n",
      "validation loss: 25498768489671.637\n",
      "epoch: 1025\n",
      "training loss: 30968496634040.902\n",
      "validation loss: 25498065900897.977\n",
      "epoch: 1026\n",
      "training loss: 30968337898299.355\n",
      "validation loss: 25497365155945.863\n",
      "epoch: 1027\n",
      "training loss: 30968179351003.883\n",
      "validation loss: 25496666249299.926\n",
      "epoch: 1028\n",
      "training loss: 30968020991701.87\n",
      "validation loss: 25495969175464.652\n",
      "epoch: 1029\n",
      "training loss: 30967862819942.273\n",
      "validation loss: 25495273928964.355\n",
      "epoch: 1030\n",
      "training loss: 30967704835275.664\n",
      "validation loss: 25494580504343.09\n",
      "epoch: 1031\n",
      "training loss: 30967547037254.152\n",
      "validation loss: 25493888896164.52\n",
      "epoch: 1032\n",
      "training loss: 30967389425431.445\n",
      "validation loss: 25493199099011.9\n",
      "epoch: 1033\n",
      "training loss: 30967231999362.78\n",
      "validation loss: 25492511107487.945\n",
      "epoch: 1034\n",
      "training loss: 30967074758604.98\n",
      "validation loss: 25491824916214.777\n",
      "epoch: 1035\n",
      "training loss: 30966917702716.395\n",
      "validation loss: 25491140519833.83\n",
      "epoch: 1036\n",
      "training loss: 30966760831256.914\n",
      "validation loss: 25490457913005.75\n",
      "epoch: 1037\n",
      "training loss: 30966604143787.97\n",
      "validation loss: 25489777090410.383\n",
      "epoch: 1038\n",
      "training loss: 30966447639872.508\n",
      "validation loss: 25489098046746.61\n",
      "epoch: 1039\n",
      "training loss: 30966291319074.992\n",
      "validation loss: 25488420776732.316\n",
      "epoch: 1040\n",
      "training loss: 30966135180961.43\n",
      "validation loss: 25487745275104.297\n",
      "epoch: 1041\n",
      "training loss: 30965979225099.3\n",
      "validation loss: 25487071536618.184\n",
      "epoch: 1042\n",
      "training loss: 30965823451057.293\n",
      "validation loss: 25486399556048.16\n",
      "epoch: 1043\n",
      "training loss: 30965667858406.184\n",
      "validation loss: 25485729328187.5\n",
      "epoch: 1044\n",
      "training loss: 30965512446717.957\n",
      "validation loss: 25485060847847.855\n",
      "epoch: 1045\n",
      "training loss: 30965357215566.043\n",
      "validation loss: 25484394109859.42\n",
      "epoch: 1046\n",
      "training loss: 30965202164525.367\n",
      "validation loss: 25483729109070.816\n",
      "epoch: 1047\n",
      "training loss: 30965047293172.332\n",
      "validation loss: 25483065840349.027\n",
      "epoch: 1048\n",
      "training loss: 30964892601084.773\n",
      "validation loss: 25482404298579.344\n",
      "epoch: 1049\n",
      "training loss: 30964738087841.99\n",
      "validation loss: 25481744478665.254\n",
      "epoch: 1050\n",
      "training loss: 30964583753024.74\n",
      "validation loss: 25481086375528.387\n",
      "epoch: 1051\n",
      "training loss: 30964429596215.203\n",
      "validation loss: 25480429984108.434\n",
      "epoch: 1052\n",
      "training loss: 30964275616997.016\n",
      "validation loss: 25479775299363.066\n",
      "epoch: 1053\n",
      "training loss: 30964121814955.23\n",
      "validation loss: 25479122316267.875\n",
      "epoch: 1054\n",
      "training loss: 30963968189676.32\n",
      "validation loss: 25478471029816.28\n",
      "epoch: 1055\n",
      "training loss: 30963814740748.176\n",
      "validation loss: 25477821435019.477\n",
      "epoch: 1056\n",
      "training loss: 30963661467760.1\n",
      "validation loss: 25477173526906.33\n",
      "epoch: 1057\n",
      "training loss: 30963508370302.805\n",
      "validation loss: 25476527300523.32\n",
      "epoch: 1058\n",
      "training loss: 30963355447968.387\n",
      "validation loss: 25475882750934.477\n",
      "epoch: 1059\n",
      "training loss: 30963202700350.04\n",
      "validation loss: 25475239873221.094\n",
      "epoch: 1060\n",
      "training loss: 30963050127042.95\n",
      "validation loss: 25474598662482.285\n",
      "epoch: 1061\n",
      "training loss: 30962897727643.38\n",
      "validation loss: 25473959113834.24\n",
      "epoch: 1062\n",
      "training loss: 30962745501748.95\n",
      "validation loss: 25473321222410.46\n",
      "epoch: 1063\n",
      "training loss: 30962593448958.67\n",
      "validation loss: 25472684983361.566\n",
      "epoch: 1064\n",
      "training loss: 30962441568872.906\n",
      "validation loss: 25472050391855.336\n",
      "epoch: 1065\n",
      "training loss: 30962289861093.38\n",
      "validation loss: 25471417443076.55\n",
      "epoch: 1066\n",
      "training loss: 30962138325223.16\n",
      "validation loss: 25470786132226.977\n",
      "epoch: 1067\n",
      "training loss: 30961986960866.656\n",
      "validation loss: 25470156454525.26\n",
      "epoch: 1068\n",
      "training loss: 30961835767629.617\n",
      "validation loss: 25469528405206.895\n",
      "epoch: 1069\n",
      "training loss: 30961684745119.125\n",
      "validation loss: 25468901979524.125\n",
      "epoch: 1070\n",
      "training loss: 30961533892943.297\n",
      "validation loss: 25468277172745.652\n",
      "epoch: 1071\n",
      "training loss: 30961383210712.164\n",
      "validation loss: 25467653980157.258\n",
      "epoch: 1072\n",
      "training loss: 30961232698036.754\n",
      "validation loss: 25467032397061.02\n",
      "epoch: 1073\n",
      "training loss: 30961082354529.46\n",
      "validation loss: 25466412418775.555\n",
      "epoch: 1074\n",
      "training loss: 30960932179803.926\n",
      "validation loss: 25465794040635.83\n",
      "epoch: 1075\n",
      "training loss: 30960782173475.117\n",
      "validation loss: 25465177257993.19\n",
      "epoch: 1076\n",
      "training loss: 30960632335159.277\n",
      "validation loss: 25464562066215.27\n",
      "epoch: 1077\n",
      "training loss: 30960482664473.953\n",
      "validation loss: 25463948460685.88\n",
      "epoch: 1078\n",
      "training loss: 30960333161037.97\n",
      "validation loss: 25463336436805.008\n",
      "epoch: 1079\n",
      "training loss: 30960183824471.43\n",
      "validation loss: 25462725989988.7\n",
      "epoch: 1080\n",
      "training loss: 30960034654395.375\n",
      "validation loss: 25462117115668.81\n",
      "epoch: 1081\n",
      "training loss: 30959885650432.76\n",
      "validation loss: 25461509809293.566\n",
      "epoch: 1082\n",
      "training loss: 30959736812207.492\n",
      "validation loss: 25460904066326.883\n",
      "epoch: 1083\n",
      "training loss: 30959588139344.715\n",
      "validation loss: 25460299882248.44\n",
      "epoch: 1084\n",
      "training loss: 30959439631470.824\n",
      "validation loss: 25459697252553.715\n",
      "epoch: 1085\n",
      "training loss: 30959291288213.477\n",
      "validation loss: 25459096172753.84\n",
      "epoch: 1086\n",
      "training loss: 30959143109201.543\n",
      "validation loss: 25458496638375.6\n",
      "epoch: 1087\n",
      "training loss: 30958995094064.824\n",
      "validation loss: 25457898644961.074\n",
      "epoch: 1088\n",
      "training loss: 30958847242435.0\n",
      "validation loss: 25457302188068.312\n",
      "epoch: 1089\n",
      "training loss: 30958699553944.61\n",
      "validation loss: 25456707263270.562\n",
      "epoch: 1090\n",
      "training loss: 30958552028227.473\n",
      "validation loss: 25456113866156.43\n",
      "epoch: 1091\n",
      "training loss: 30958404664918.566\n",
      "validation loss: 25455521992329.844\n",
      "epoch: 1092\n",
      "training loss: 30958257463654.11\n",
      "validation loss: 25454931637409.957\n",
      "epoch: 1093\n",
      "training loss: 30958110424071.504\n",
      "validation loss: 25454342797031.105\n",
      "epoch: 1094\n",
      "training loss: 30957963545809.055\n",
      "validation loss: 25453755466842.504\n",
      "epoch: 1095\n",
      "training loss: 30957816828506.867\n",
      "validation loss: 25453169642508.9\n",
      "epoch: 1096\n",
      "training loss: 30957670271805.938\n",
      "validation loss: 25452585319709.797\n",
      "epoch: 1097\n",
      "training loss: 30957523875348.418\n",
      "validation loss: 25452002494139.617\n",
      "epoch: 1098\n",
      "training loss: 30957377638777.68\n",
      "validation loss: 25451421161507.67\n",
      "epoch: 1099\n",
      "training loss: 30957231561738.23\n",
      "validation loss: 25450841317538.074\n",
      "epoch: 1100\n",
      "training loss: 30957085643875.773\n",
      "validation loss: 25450262957969.684\n",
      "epoch: 1101\n",
      "training loss: 30956939884836.824\n",
      "validation loss: 25449686078555.83\n",
      "epoch: 1102\n",
      "training loss: 30956794284269.71\n",
      "validation loss: 25449110675064.926\n",
      "epoch: 1103\n",
      "training loss: 30956648841823.605\n",
      "validation loss: 25448536743279.72\n",
      "epoch: 1104\n",
      "training loss: 30956503557148.82\n",
      "validation loss: 25447964278997.49\n",
      "epoch: 1105\n",
      "training loss: 30956358429896.8\n",
      "validation loss: 25447393278029.93\n",
      "epoch: 1106\n",
      "training loss: 30956213459720.14\n",
      "validation loss: 25446823736203.184\n",
      "epoch: 1107\n",
      "training loss: 30956068646272.254\n",
      "validation loss: 25446255649357.44\n",
      "epoch: 1108\n",
      "training loss: 30955923989208.31\n",
      "validation loss: 25445689013347.67\n",
      "epoch: 1109\n",
      "training loss: 30955779488184.29\n",
      "validation loss: 25445123824042.8\n",
      "epoch: 1110\n",
      "training loss: 30955635142857.293\n",
      "validation loss: 25444560077325.94\n",
      "epoch: 1111\n",
      "training loss: 30955490952885.535\n",
      "validation loss: 25443997769094.293\n",
      "epoch: 1112\n",
      "training loss: 30955346917928.35\n",
      "validation loss: 25443436895259.105\n",
      "epoch: 1113\n",
      "training loss: 30955203037645.836\n",
      "validation loss: 25442877451745.395\n",
      "epoch: 1114\n",
      "training loss: 30955059311699.85\n",
      "validation loss: 25442319434492.566\n",
      "epoch: 1115\n",
      "training loss: 30954915739753.023\n",
      "validation loss: 25441762839453.66\n",
      "epoch: 1116\n",
      "training loss: 30954772321469.086\n",
      "validation loss: 25441207662595.56\n",
      "epoch: 1117\n",
      "training loss: 30954629056512.836\n",
      "validation loss: 25440653899898.89\n",
      "epoch: 1118\n",
      "training loss: 30954485944549.848\n",
      "validation loss: 25440101547357.758\n",
      "epoch: 1119\n",
      "training loss: 30954342985247.41\n",
      "validation loss: 25439550600980.4\n",
      "epoch: 1120\n",
      "training loss: 30954200178273.582\n",
      "validation loss: 25439001056788.402\n",
      "epoch: 1121\n",
      "training loss: 30954057523297.47\n",
      "validation loss: 25438452910816.918\n",
      "epoch: 1122\n",
      "training loss: 30953915019989.246\n",
      "validation loss: 25437906159114.56\n",
      "epoch: 1123\n",
      "training loss: 30953772668019.844\n",
      "validation loss: 25437360797743.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1124\n",
      "training loss: 30953630467061.883\n",
      "validation loss: 25436816822778.36\n",
      "epoch: 1125\n",
      "training loss: 30953488416788.703\n",
      "validation loss: 25436274230308.97\n",
      "epoch: 1126\n",
      "training loss: 30953346516874.72\n",
      "validation loss: 25435733016437.008\n",
      "epoch: 1127\n",
      "training loss: 30953204766995.367\n",
      "validation loss: 25435193177277.727\n",
      "epoch: 1128\n",
      "training loss: 30953063166826.79\n",
      "validation loss: 25434654708959.324\n",
      "epoch: 1129\n",
      "training loss: 30952921716046.844\n",
      "validation loss: 25434117607623.57\n",
      "epoch: 1130\n",
      "training loss: 30952780414334.086\n",
      "validation loss: 25433581869425.043\n",
      "epoch: 1131\n",
      "training loss: 30952639261368.074\n",
      "validation loss: 25433047490531.37\n",
      "epoch: 1132\n",
      "training loss: 30952498256829.41\n",
      "validation loss: 25432514467123.094\n",
      "epoch: 1133\n",
      "training loss: 30952357400399.387\n",
      "validation loss: 25431982795393.445\n",
      "epoch: 1134\n",
      "training loss: 30952216691760.953\n",
      "validation loss: 25431452471548.97\n",
      "epoch: 1135\n",
      "training loss: 30952076130597.76\n",
      "validation loss: 25430923491808.785\n",
      "epoch: 1136\n",
      "training loss: 30951935716594.438\n",
      "validation loss: 25430395852404.758\n",
      "epoch: 1137\n",
      "training loss: 30951795449436.312\n",
      "validation loss: 25429869549581.2\n",
      "epoch: 1138\n",
      "training loss: 30951655328810.363\n",
      "validation loss: 25429344579595.574\n",
      "epoch: 1139\n",
      "training loss: 30951515354404.223\n",
      "validation loss: 25428820938717.67\n",
      "epoch: 1140\n",
      "training loss: 30951375525906.527\n",
      "validation loss: 25428298623229.844\n",
      "epoch: 1141\n",
      "training loss: 30951235843006.88\n",
      "validation loss: 25427777629426.934\n",
      "epoch: 1142\n",
      "training loss: 30951096305395.55\n",
      "validation loss: 25427257953616.0\n",
      "epoch: 1143\n",
      "training loss: 30950956912764.43\n",
      "validation loss: 25426739592116.97\n",
      "epoch: 1144\n",
      "training loss: 30950817664806.094\n",
      "validation loss: 25426222541261.88\n",
      "epoch: 1145\n",
      "training loss: 30950678561214.035\n",
      "validation loss: 25425706797395.06\n",
      "epoch: 1146\n",
      "training loss: 30950539601682.414\n",
      "validation loss: 25425192356872.844\n",
      "epoch: 1147\n",
      "training loss: 30950400785907.004\n",
      "validation loss: 25424679216064.266\n",
      "epoch: 1148\n",
      "training loss: 30950262113584.203\n",
      "validation loss: 25424167371350.254\n",
      "epoch: 1149\n",
      "training loss: 30950123584411.383\n",
      "validation loss: 25423656819123.844\n",
      "epoch: 1150\n",
      "training loss: 30949985198086.496\n",
      "validation loss: 25423147555789.875\n",
      "epoch: 1151\n",
      "training loss: 30949846954309.145\n",
      "validation loss: 25422639577765.69\n",
      "epoch: 1152\n",
      "training loss: 30949708852779.52\n",
      "validation loss: 25422132881480.355\n",
      "epoch: 1153\n",
      "training loss: 30949570893198.766\n",
      "validation loss: 25421627463374.82\n",
      "epoch: 1154\n",
      "training loss: 30949433075268.598\n",
      "validation loss: 25421123319901.684\n",
      "epoch: 1155\n",
      "training loss: 30949295398692.367\n",
      "validation loss: 25420620447525.836\n",
      "epoch: 1156\n",
      "training loss: 30949157863174.004\n",
      "validation loss: 25420118842723.703\n",
      "epoch: 1157\n",
      "training loss: 30949020468418.36\n",
      "validation loss: 25419618501983.414\n",
      "epoch: 1158\n",
      "training loss: 30948883214130.855\n",
      "validation loss: 25419119421804.547\n",
      "epoch: 1159\n",
      "training loss: 30948746100018.53\n",
      "validation loss: 25418621598698.79\n",
      "epoch: 1160\n",
      "training loss: 30948609125789.004\n",
      "validation loss: 25418125029189.207\n",
      "epoch: 1161\n",
      "training loss: 30948472291150.773\n",
      "validation loss: 25417629709810.34\n",
      "epoch: 1162\n",
      "training loss: 30948335595812.91\n",
      "validation loss: 25417135637108.023\n",
      "epoch: 1163\n",
      "training loss: 30948199039486.086\n",
      "validation loss: 25416642807640.02\n",
      "epoch: 1164\n",
      "training loss: 30948062621881.508\n",
      "validation loss: 25416151217975.242\n",
      "epoch: 1165\n",
      "training loss: 30947926342710.957\n",
      "validation loss: 25415660864693.723\n",
      "epoch: 1166\n",
      "training loss: 30947790201687.773\n",
      "validation loss: 25415171744387.305\n",
      "epoch: 1167\n",
      "training loss: 30947654198525.86\n",
      "validation loss: 25414683853658.836\n",
      "epoch: 1168\n",
      "training loss: 30947518332939.977\n",
      "validation loss: 25414197189122.395\n",
      "epoch: 1169\n",
      "training loss: 30947382604645.45\n",
      "validation loss: 25413711747403.0\n",
      "epoch: 1170\n",
      "training loss: 30947247013359.156\n",
      "validation loss: 25413227525137.297\n",
      "epoch: 1171\n",
      "training loss: 30947111558798.492\n",
      "validation loss: 25412744518972.777\n",
      "epoch: 1172\n",
      "training loss: 30946976240681.41\n",
      "validation loss: 25412262725567.727\n",
      "epoch: 1173\n",
      "training loss: 30946841058727.383\n",
      "validation loss: 25411782141591.934\n",
      "epoch: 1174\n",
      "training loss: 30946706012656.434\n",
      "validation loss: 25411302763725.91\n",
      "epoch: 1175\n",
      "training loss: 30946571102189.42\n",
      "validation loss: 25410824588661.066\n",
      "epoch: 1176\n",
      "training loss: 30946436327047.73\n",
      "validation loss: 25410347613099.45\n",
      "epoch: 1177\n",
      "training loss: 30946301686954.285\n",
      "validation loss: 25409871833754.45\n",
      "epoch: 1178\n",
      "training loss: 30946167181632.516\n",
      "validation loss: 25409397247349.977\n",
      "epoch: 1179\n",
      "training loss: 30946032810806.34\n",
      "validation loss: 25408923850620.457\n",
      "epoch: 1180\n",
      "training loss: 30945898574201.227\n",
      "validation loss: 25408451640311.516\n",
      "epoch: 1181\n",
      "training loss: 30945764471543.15\n",
      "validation loss: 25407980613179.2\n",
      "epoch: 1182\n",
      "training loss: 30945630502558.89\n",
      "validation loss: 25407510765990.16\n",
      "epoch: 1183\n",
      "training loss: 30945496666975.734\n",
      "validation loss: 25407042095521.395\n",
      "epoch: 1184\n",
      "training loss: 30945362964522.484\n",
      "validation loss: 25406574598560.93\n",
      "epoch: 1185\n",
      "training loss: 30945229394928.426\n",
      "validation loss: 25406108271907.055\n",
      "epoch: 1186\n",
      "training loss: 30945095957923.32\n",
      "validation loss: 25405643112368.25\n",
      "epoch: 1187\n",
      "training loss: 30944962653238.44\n",
      "validation loss: 25405179116763.93\n",
      "epoch: 1188\n",
      "training loss: 30944829480605.54\n",
      "validation loss: 25404716281923.62\n",
      "epoch: 1189\n",
      "training loss: 30944696439756.824\n",
      "validation loss: 25404254604686.94\n",
      "epoch: 1190\n",
      "training loss: 30944563530426.02\n",
      "validation loss: 25403794081904.305\n",
      "epoch: 1191\n",
      "training loss: 30944430752347.293\n",
      "validation loss: 25403334710436.137\n",
      "epoch: 1192\n",
      "training loss: 30944298105255.65\n",
      "validation loss: 25402876487153.055\n",
      "epoch: 1193\n",
      "training loss: 30944165588886.5\n",
      "validation loss: 25402419408935.625\n",
      "epoch: 1194\n",
      "training loss: 30944033202976.793\n",
      "validation loss: 25401963472675.035\n",
      "epoch: 1195\n",
      "training loss: 30943900947263.9\n",
      "validation loss: 25401508675272.344\n",
      "epoch: 1196\n",
      "training loss: 30943768821485.64\n",
      "validation loss: 25401055013638.387\n",
      "epoch: 1197\n",
      "training loss: 30943636825381.324\n",
      "validation loss: 25400602484694.53\n",
      "epoch: 1198\n",
      "training loss: 30943504958690.7\n",
      "validation loss: 25400151085371.88\n",
      "epoch: 1199\n",
      "training loss: 30943373221153.938\n",
      "validation loss: 25399700812611.22\n",
      "epoch: 1200\n",
      "training loss: 30943241612512.695\n",
      "validation loss: 25399251663363.74\n",
      "epoch: 1201\n",
      "training loss: 30943110132509.074\n",
      "validation loss: 25398803634590.242\n",
      "epoch: 1202\n",
      "training loss: 30942978780885.56\n",
      "validation loss: 25398356723261.098\n",
      "epoch: 1203\n",
      "training loss: 30942847557386.14\n",
      "validation loss: 25397910926357.004\n",
      "epoch: 1204\n",
      "training loss: 30942716461755.223\n",
      "validation loss: 25397466240868.1\n",
      "epoch: 1205\n",
      "training loss: 30942585493737.6\n",
      "validation loss: 25397022663794.035\n",
      "epoch: 1206\n",
      "training loss: 30942454653079.566\n",
      "validation loss: 25396580192144.598\n",
      "epoch: 1207\n",
      "training loss: 30942323939527.79\n",
      "validation loss: 25396138822939.0\n",
      "epoch: 1208\n",
      "training loss: 30942193352829.367\n",
      "validation loss: 25395698553205.766\n",
      "epoch: 1209\n",
      "training loss: 30942062892732.83\n",
      "validation loss: 25395259379983.52\n",
      "epoch: 1210\n",
      "training loss: 30941932558987.14\n",
      "validation loss: 25394821300320.133\n",
      "epoch: 1211\n",
      "training loss: 30941802351341.617\n",
      "validation loss: 25394384311272.73\n",
      "epoch: 1212\n",
      "training loss: 30941672269547.062\n",
      "validation loss: 25393948409908.39\n",
      "epoch: 1213\n",
      "training loss: 30941542313354.652\n",
      "validation loss: 25393513593303.35\n",
      "epoch: 1214\n",
      "training loss: 30941412482515.934\n",
      "validation loss: 25393079858542.99\n",
      "epoch: 1215\n",
      "training loss: 30941282776783.93\n",
      "validation loss: 25392647202722.523\n",
      "epoch: 1216\n",
      "training loss: 30941153195912.008\n",
      "validation loss: 25392215622946.227\n",
      "epoch: 1217\n",
      "training loss: 30941023739653.945\n",
      "validation loss: 25391785116327.395\n",
      "epoch: 1218\n",
      "training loss: 30940894407764.926\n",
      "validation loss: 25391355679989.074\n",
      "epoch: 1219\n",
      "training loss: 30940765200000.53\n",
      "validation loss: 25390927311063.254\n",
      "epoch: 1220\n",
      "training loss: 30940636116116.69\n",
      "validation loss: 25390500006690.82\n",
      "epoch: 1221\n",
      "training loss: 30940507155870.76\n",
      "validation loss: 25390073764022.34\n",
      "epoch: 1222\n",
      "training loss: 30940378319020.117\n",
      "validation loss: 25389648580216.91\n",
      "epoch: 1223\n",
      "training loss: 30940249605323.562\n",
      "validation loss: 25389224452443.266\n",
      "epoch: 1224\n",
      "training loss: 30940121014540.242\n",
      "validation loss: 25388801377878.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1225\n",
      "training loss: 30939992546429.656\n",
      "validation loss: 25388379353709.914\n",
      "epoch: 1226\n",
      "training loss: 30939864200752.74\n",
      "validation loss: 25387958377132.1\n",
      "epoch: 1227\n",
      "training loss: 30939735977270.746\n",
      "validation loss: 25387538445349.77\n",
      "epoch: 1228\n",
      "training loss: 30939607875745.29\n",
      "validation loss: 25387119555575.902\n",
      "epoch: 1229\n",
      "training loss: 30939479895939.406\n",
      "validation loss: 25386701705032.85\n",
      "epoch: 1230\n",
      "training loss: 30939352037616.465\n",
      "validation loss: 25386284890951.535\n",
      "epoch: 1231\n",
      "training loss: 30939224300540.137\n",
      "validation loss: 25385869110571.395\n",
      "epoch: 1232\n",
      "training loss: 30939096684475.54\n",
      "validation loss: 25385454361141.152\n",
      "epoch: 1233\n",
      "training loss: 30938969189187.74\n",
      "validation loss: 25385040639917.734\n",
      "epoch: 1234\n",
      "training loss: 30938841814443.223\n",
      "validation loss: 25384627944167.277\n",
      "epoch: 1235\n",
      "training loss: 30938714560008.793\n",
      "validation loss: 25384216271164.305\n",
      "epoch: 1236\n",
      "training loss: 30938587425651.56\n",
      "validation loss: 25383805618191.703\n",
      "epoch: 1237\n",
      "training loss: 30938460411140.043\n",
      "validation loss: 25383395982541.48\n",
      "epoch: 1238\n",
      "training loss: 30938333516243.07\n",
      "validation loss: 25382987361513.938\n",
      "epoch: 1239\n",
      "training loss: 30938206740729.785\n",
      "validation loss: 25382579752417.63\n",
      "epoch: 1240\n",
      "training loss: 30938080084370.727\n",
      "validation loss: 25382173152570.14\n",
      "epoch: 1241\n",
      "training loss: 30937953546936.363\n",
      "validation loss: 25381767559296.992\n",
      "epoch: 1242\n",
      "training loss: 30937827128198.566\n",
      "validation loss: 25381362969932.645\n",
      "epoch: 1243\n",
      "training loss: 30937700827929.504\n",
      "validation loss: 25380959381819.707\n",
      "epoch: 1244\n",
      "training loss: 30937574645901.637\n",
      "validation loss: 25380556792308.883\n",
      "epoch: 1245\n",
      "training loss: 30937448581888.812\n",
      "validation loss: 25380155198759.742\n",
      "epoch: 1246\n",
      "training loss: 30937322635665.168\n",
      "validation loss: 25379754598539.895\n",
      "epoch: 1247\n",
      "training loss: 30937196807005.12\n",
      "validation loss: 25379354989024.945\n",
      "epoch: 1248\n",
      "training loss: 30937071095684.49\n",
      "validation loss: 25378956367599.266\n",
      "epoch: 1249\n",
      "training loss: 30936945501478.977\n",
      "validation loss: 25378558731654.918\n",
      "epoch: 1250\n",
      "training loss: 30936820024165.7\n",
      "validation loss: 25378162078592.68\n",
      "epoch: 1251\n",
      "training loss: 30936694663522.027\n",
      "validation loss: 25377766405821.195\n",
      "epoch: 1252\n",
      "training loss: 30936569419325.594\n",
      "validation loss: 25377371710756.945\n",
      "epoch: 1253\n",
      "training loss: 30936444291355.434\n",
      "validation loss: 25376977990825.074\n",
      "epoch: 1254\n",
      "training loss: 30936319279390.46\n",
      "validation loss: 25376585243458.195\n",
      "epoch: 1255\n",
      "training loss: 30936194383210.965\n",
      "validation loss: 25376193466097.523\n",
      "epoch: 1256\n",
      "training loss: 30936069602597.496\n",
      "validation loss: 25375802656191.977\n",
      "epoch: 1257\n",
      "training loss: 30935944937330.87\n",
      "validation loss: 25375412811198.18\n",
      "epoch: 1258\n",
      "training loss: 30935820387193.258\n",
      "validation loss: 25375023928581.21\n",
      "epoch: 1259\n",
      "training loss: 30935695951966.71\n",
      "validation loss: 25374636005813.54\n",
      "epoch: 1260\n",
      "training loss: 30935571631434.668\n",
      "validation loss: 25374249040376.047\n",
      "epoch: 1261\n",
      "training loss: 30935447425380.793\n",
      "validation loss: 25373863029757.184\n",
      "epoch: 1262\n",
      "training loss: 30935323333588.992\n",
      "validation loss: 25373477971452.938\n",
      "epoch: 1263\n",
      "training loss: 30935199355844.555\n",
      "validation loss: 25373093862967.65\n",
      "epoch: 1264\n",
      "training loss: 30935075491932.61\n",
      "validation loss: 25372710701812.836\n",
      "epoch: 1265\n",
      "training loss: 30934951741639.66\n",
      "validation loss: 25372328485508.336\n",
      "epoch: 1266\n",
      "training loss: 30934828104752.44\n",
      "validation loss: 25371947211581.39\n",
      "epoch: 1267\n",
      "training loss: 30934704581057.906\n",
      "validation loss: 25371566877566.625\n",
      "epoch: 1268\n",
      "training loss: 30934581170344.383\n",
      "validation loss: 25371187481006.895\n",
      "epoch: 1269\n",
      "training loss: 30934457872400.027\n",
      "validation loss: 25370809019452.082\n",
      "epoch: 1270\n",
      "training loss: 30934334687014.37\n",
      "validation loss: 25370431490460.23\n",
      "epoch: 1271\n",
      "training loss: 30934211613976.754\n",
      "validation loss: 25370054891596.38\n",
      "epoch: 1272\n",
      "training loss: 30934088653077.906\n",
      "validation loss: 25369679220433.656\n",
      "epoch: 1273\n",
      "training loss: 30933965804108.746\n",
      "validation loss: 25369304474552.41\n",
      "epoch: 1274\n",
      "training loss: 30933843066860.41\n",
      "validation loss: 25368930651540.18\n",
      "epoch: 1275\n",
      "training loss: 30933720441125.383\n",
      "validation loss: 25368557748992.53\n",
      "epoch: 1276\n",
      "training loss: 30933597926695.965\n",
      "validation loss: 25368185764511.863\n",
      "epoch: 1277\n",
      "training loss: 30933475523365.83\n",
      "validation loss: 25367814695708.53\n",
      "epoch: 1278\n",
      "training loss: 30933353230928.426\n",
      "validation loss: 25367444540199.71\n",
      "epoch: 1279\n",
      "training loss: 30933231049178.594\n",
      "validation loss: 25367075295610.465\n",
      "epoch: 1280\n",
      "training loss: 30933108977911.336\n",
      "validation loss: 25366706959572.87\n",
      "epoch: 1281\n",
      "training loss: 30932987016921.863\n",
      "validation loss: 25366339529726.03\n",
      "epoch: 1282\n",
      "training loss: 30932865166006.703\n",
      "validation loss: 25365973003716.85\n",
      "epoch: 1283\n",
      "training loss: 30932743424962.215\n",
      "validation loss: 25365607379198.914\n",
      "epoch: 1284\n",
      "training loss: 30932621793586.09\n",
      "validation loss: 25365242653833.547\n",
      "epoch: 1285\n",
      "training loss: 30932500271675.805\n",
      "validation loss: 25364878825288.715\n",
      "epoch: 1286\n",
      "training loss: 30932378859030.184\n",
      "validation loss: 25364515891240.082\n",
      "epoch: 1287\n",
      "training loss: 30932257555448.22\n",
      "validation loss: 25364153849370.15\n",
      "epoch: 1288\n",
      "training loss: 30932136360729.094\n",
      "validation loss: 25363792697368.23\n",
      "epoch: 1289\n",
      "training loss: 30932015274673.29\n",
      "validation loss: 25363432432931.293\n",
      "epoch: 1290\n",
      "training loss: 30931894297081.113\n",
      "validation loss: 25363073053762.77\n",
      "epoch: 1291\n",
      "training loss: 30931773427754.168\n",
      "validation loss: 25362714557573.67\n",
      "epoch: 1292\n",
      "training loss: 30931652666493.863\n",
      "validation loss: 25362356942081.453\n",
      "epoch: 1293\n",
      "training loss: 30931532013102.902\n",
      "validation loss: 25362000205011.07\n",
      "epoch: 1294\n",
      "training loss: 30931411467383.773\n",
      "validation loss: 25361644344093.902\n",
      "epoch: 1295\n",
      "training loss: 30931291029140.32\n",
      "validation loss: 25361289357068.76\n",
      "epoch: 1296\n",
      "training loss: 30931170698176.5\n",
      "validation loss: 25360935241681.1\n",
      "epoch: 1297\n",
      "training loss: 30931050474296.445\n",
      "validation loss: 25360581995682.934\n",
      "epoch: 1298\n",
      "training loss: 30930930357305.6\n",
      "validation loss: 25360229616833.69\n",
      "epoch: 1299\n",
      "training loss: 30930810347009.168\n",
      "validation loss: 25359878102899.05\n",
      "epoch: 1300\n",
      "training loss: 30930690443213.68\n",
      "validation loss: 25359527451652.06\n",
      "epoch: 1301\n",
      "training loss: 30930570645725.418\n",
      "validation loss: 25359177660871.93\n",
      "epoch: 1302\n",
      "training loss: 30930450954351.953\n",
      "validation loss: 25358828728345.21\n",
      "epoch: 1303\n",
      "training loss: 30930331368900.664\n",
      "validation loss: 25358480651864.598\n",
      "epoch: 1304\n",
      "training loss: 30930211889180.184\n",
      "validation loss: 25358133429230.023\n",
      "epoch: 1305\n",
      "training loss: 30930092514999.312\n",
      "validation loss: 25357787058247.832\n",
      "epoch: 1306\n",
      "training loss: 30929973246166.977\n",
      "validation loss: 25357441536730.7\n",
      "epoch: 1307\n",
      "training loss: 30929854082493.402\n",
      "validation loss: 25357096862498.51\n",
      "epoch: 1308\n",
      "training loss: 30929735023788.562\n",
      "validation loss: 25356753033377.15\n",
      "epoch: 1309\n",
      "training loss: 30929616069863.758\n",
      "validation loss: 25356410047199.68\n",
      "epoch: 1310\n",
      "training loss: 30929497220529.96\n",
      "validation loss: 25356067901805.105\n",
      "epoch: 1311\n",
      "training loss: 30929378475599.51\n",
      "validation loss: 25355726595039.543\n",
      "epoch: 1312\n",
      "training loss: 30929259834884.434\n",
      "validation loss: 25355386124755.01\n",
      "epoch: 1313\n",
      "training loss: 30929141298198.07\n",
      "validation loss: 25355046488810.617\n",
      "epoch: 1314\n",
      "training loss: 30929022865353.504\n",
      "validation loss: 25354707685071.32\n",
      "epoch: 1315\n",
      "training loss: 30928904536165.074\n",
      "validation loss: 25354369711409.094\n",
      "epoch: 1316\n",
      "training loss: 30928786310446.875\n",
      "validation loss: 25354032565701.715\n",
      "epoch: 1317\n",
      "training loss: 30928668188014.277\n",
      "validation loss: 25353696245833.957\n",
      "epoch: 1318\n",
      "training loss: 30928550168682.367\n",
      "validation loss: 25353360749696.34\n",
      "epoch: 1319\n",
      "training loss: 30928432252267.535\n",
      "validation loss: 25353026075186.336\n",
      "epoch: 1320\n",
      "training loss: 30928314438586.266\n",
      "validation loss: 25352692220207.4\n",
      "epoch: 1321\n",
      "training loss: 30928196727455.145\n",
      "validation loss: 25352359182669.004\n",
      "epoch: 1322\n",
      "training loss: 30928079118692.06\n",
      "validation loss: 25352026960487.484\n",
      "epoch: 1323\n",
      "training loss: 30927961612114.582\n",
      "validation loss: 25351695551584.812\n",
      "epoch: 1324\n",
      "training loss: 30927844207541.586\n",
      "validation loss: 25351364953889.785\n",
      "epoch: 1325\n",
      "training loss: 30927726904791.637\n",
      "validation loss: 25351035165336.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1326\n",
      "training loss: 30927609703684.582\n",
      "validation loss: 25350706183867.055\n",
      "epoch: 1327\n",
      "training loss: 30927492604039.97\n",
      "validation loss: 25350378007427.246\n",
      "epoch: 1328\n",
      "training loss: 30927375605678.62\n",
      "validation loss: 25350050633970.863\n",
      "epoch: 1329\n",
      "training loss: 30927258708421.05\n",
      "validation loss: 25349724061456.88\n",
      "epoch: 1330\n",
      "training loss: 30927141912089.06\n",
      "validation loss: 25349398287850.96\n",
      "epoch: 1331\n",
      "training loss: 30927025216504.105\n",
      "validation loss: 25349073311124.227\n",
      "epoch: 1332\n",
      "training loss: 30926908621488.96\n",
      "validation loss: 25348749129254.465\n",
      "epoch: 1333\n",
      "training loss: 30926792126866.07\n",
      "validation loss: 25348425740224.88\n",
      "epoch: 1334\n",
      "training loss: 30926675732459.125\n",
      "validation loss: 25348103142025.27\n",
      "epoch: 1335\n",
      "training loss: 30926559438091.523\n",
      "validation loss: 25347781332650.832\n",
      "epoch: 1336\n",
      "training loss: 30926443243587.91\n",
      "validation loss: 25347460310103.3\n",
      "epoch: 1337\n",
      "training loss: 30926327148772.63\n",
      "validation loss: 25347140072389.754\n",
      "epoch: 1338\n",
      "training loss: 30926211153471.285\n",
      "validation loss: 25346820617523.797\n",
      "epoch: 1339\n",
      "training loss: 30926095257509.125\n",
      "validation loss: 25346501943524.336\n",
      "epoch: 1340\n",
      "training loss: 30925979460712.707\n",
      "validation loss: 25346184048416.73\n",
      "epoch: 1341\n",
      "training loss: 30925863762908.215\n",
      "validation loss: 25345866930231.598\n",
      "epoch: 1342\n",
      "training loss: 30925748163923.117\n",
      "validation loss: 25345550587006.008\n",
      "epoch: 1343\n",
      "training loss: 30925632663584.53\n",
      "validation loss: 25345235016782.22\n",
      "epoch: 1344\n",
      "training loss: 30925517261720.855\n",
      "validation loss: 25344920217608.9\n",
      "epoch: 1345\n",
      "training loss: 30925401958160.13\n",
      "validation loss: 25344606187539.87\n",
      "epoch: 1346\n",
      "training loss: 30925286752731.645\n",
      "validation loss: 25344292924635.33\n",
      "epoch: 1347\n",
      "training loss: 30925171645264.363\n",
      "validation loss: 25343980426960.574\n",
      "epoch: 1348\n",
      "training loss: 30925056635588.492\n",
      "validation loss: 25343668692587.234\n",
      "epoch: 1349\n",
      "training loss: 30924941723533.9\n",
      "validation loss: 25343357719592.008\n",
      "epoch: 1350\n",
      "training loss: 30924826908931.695\n",
      "validation loss: 25343047506057.88\n",
      "epoch: 1351\n",
      "training loss: 30924712191612.625\n",
      "validation loss: 25342738050072.88\n",
      "epoch: 1352\n",
      "training loss: 30924597571408.723\n",
      "validation loss: 25342429349731.254\n",
      "epoch: 1353\n",
      "training loss: 30924483048151.625\n",
      "validation loss: 25342121403132.28\n",
      "epoch: 1354\n",
      "training loss: 30924368621674.242\n",
      "validation loss: 25341814208381.42\n",
      "epoch: 1355\n",
      "training loss: 30924254291809.113\n",
      "validation loss: 25341507763589.066\n",
      "epoch: 1356\n",
      "training loss: 30924140058390.01\n",
      "validation loss: 25341202066871.812\n",
      "epoch: 1357\n",
      "training loss: 30924025921250.367\n",
      "validation loss: 25340897116351.168\n",
      "epoch: 1358\n",
      "training loss: 30923911880224.863\n",
      "validation loss: 25340592910154.746\n",
      "epoch: 1359\n",
      "training loss: 30923797935147.76\n",
      "validation loss: 25340289446415.035\n",
      "epoch: 1360\n",
      "training loss: 30923684085854.62\n",
      "validation loss: 25339986723270.625\n",
      "epoch: 1361\n",
      "training loss: 30923570332180.61\n",
      "validation loss: 25339684738864.94\n",
      "epoch: 1362\n",
      "training loss: 30923456673962.117\n",
      "validation loss: 25339383491347.45\n",
      "epoch: 1363\n",
      "training loss: 30923343111035.184\n",
      "validation loss: 25339082978872.418\n",
      "epoch: 1364\n",
      "training loss: 30923229643237.066\n",
      "validation loss: 25338783199600.13\n",
      "epoch: 1365\n",
      "training loss: 30923116270404.65\n",
      "validation loss: 25338484151695.62\n",
      "epoch: 1366\n",
      "training loss: 30923002992376.066\n",
      "validation loss: 25338185833329.914\n",
      "epoch: 1367\n",
      "training loss: 30922889808989.03\n",
      "validation loss: 25337888242678.73\n",
      "epoch: 1368\n",
      "training loss: 30922776720082.54\n",
      "validation loss: 25337591377923.754\n",
      "epoch: 1369\n",
      "training loss: 30922663725495.15\n",
      "validation loss: 25337295237251.324\n",
      "epoch: 1370\n",
      "training loss: 30922550825066.69\n",
      "validation loss: 25336999818853.707\n",
      "epoch: 1371\n",
      "training loss: 30922438018636.58\n",
      "validation loss: 25336705120927.805\n",
      "epoch: 1372\n",
      "training loss: 30922325306045.047\n",
      "validation loss: 25336411141676.09\n",
      "epoch: 1373\n",
      "training loss: 30922212687133.176\n",
      "validation loss: 25336117879306.527\n",
      "epoch: 1374\n",
      "training loss: 30922100161741.633\n",
      "validation loss: 25335825332031.715\n",
      "epoch: 1375\n",
      "training loss: 30921987729712.32\n",
      "validation loss: 25335533498070.05\n",
      "epoch: 1376\n",
      "training loss: 30921875390886.727\n",
      "validation loss: 25335242375644.47\n",
      "epoch: 1377\n",
      "training loss: 30921763145107.582\n",
      "validation loss: 25334951962983.703\n",
      "epoch: 1378\n",
      "training loss: 30921650992217.188\n",
      "validation loss: 25334662258320.977\n",
      "epoch: 1379\n",
      "training loss: 30921538932059.105\n",
      "validation loss: 25334373259895.277\n",
      "epoch: 1380\n",
      "training loss: 30921426964476.434\n",
      "validation loss: 25334084965950.043\n",
      "epoch: 1381\n",
      "training loss: 30921315089313.555\n",
      "validation loss: 25333797374734.46\n",
      "epoch: 1382\n",
      "training loss: 30921203306414.38\n",
      "validation loss: 25333510484502.12\n",
      "epoch: 1383\n",
      "training loss: 30921091615624.098\n",
      "validation loss: 25333224293512.29\n",
      "epoch: 1384\n",
      "training loss: 30920980016787.438\n",
      "validation loss: 25332938800028.66\n",
      "epoch: 1385\n",
      "training loss: 30920868509750.35\n",
      "validation loss: 25332654002320.547\n",
      "epoch: 1386\n",
      "training loss: 30920757094358.4\n",
      "validation loss: 25332369898661.645\n",
      "epoch: 1387\n",
      "training loss: 30920645770458.316\n",
      "validation loss: 25332086487331.24\n",
      "epoch: 1388\n",
      "training loss: 30920534537896.45\n",
      "validation loss: 25331803766612.965\n",
      "epoch: 1389\n",
      "training loss: 30920423396519.926\n",
      "validation loss: 25331521734795.707\n",
      "epoch: 1390\n",
      "training loss: 30920312346176.703\n",
      "validation loss: 25331240390173.59\n",
      "epoch: 1391\n",
      "training loss: 30920201386714.28\n",
      "validation loss: 25330959731045.023\n",
      "epoch: 1392\n",
      "training loss: 30920090517981.4\n",
      "validation loss: 25330679755713.926\n",
      "epoch: 1393\n",
      "training loss: 30919979739826.33\n",
      "validation loss: 25330400462488.492\n",
      "epoch: 1394\n",
      "training loss: 30919869052098.6\n",
      "validation loss: 25330121849682.37\n",
      "epoch: 1395\n",
      "training loss: 30919758454647.26\n",
      "validation loss: 25329843915613.44\n",
      "epoch: 1396\n",
      "training loss: 30919647947322.598\n",
      "validation loss: 25329566658605.043\n",
      "epoch: 1397\n",
      "training loss: 30919537529974.434\n",
      "validation loss: 25329290076984.67\n",
      "epoch: 1398\n",
      "training loss: 30919427202453.832\n",
      "validation loss: 25329014169085.273\n",
      "epoch: 1399\n",
      "training loss: 30919316964611.37\n",
      "validation loss: 25328738933243.926\n",
      "epoch: 1400\n",
      "training loss: 30919206816298.445\n",
      "validation loss: 25328464367802.773\n",
      "epoch: 1401\n",
      "training loss: 30919096757367.26\n",
      "validation loss: 25328190471109.03\n",
      "epoch: 1402\n",
      "training loss: 30918986787669.54\n",
      "validation loss: 25327917241514.023\n",
      "epoch: 1403\n",
      "training loss: 30918876907058.23\n",
      "validation loss: 25327644677374.383\n",
      "epoch: 1404\n",
      "training loss: 30918767115385.812\n",
      "validation loss: 25327372777050.824\n",
      "epoch: 1405\n",
      "training loss: 30918657412505.992\n",
      "validation loss: 25327101538909.38\n",
      "epoch: 1406\n",
      "training loss: 30918547798271.984\n",
      "validation loss: 25326830961320.09\n",
      "epoch: 1407\n",
      "training loss: 30918438272538.25\n",
      "validation loss: 25326561042658.3\n",
      "epoch: 1408\n",
      "training loss: 30918328835158.74\n",
      "validation loss: 25326291781303.324\n",
      "epoch: 1409\n",
      "training loss: 30918219485988.22\n",
      "validation loss: 25326023175639.438\n",
      "epoch: 1410\n",
      "training loss: 30918110224882.25\n",
      "validation loss: 25325755224055.812\n",
      "epoch: 1411\n",
      "training loss: 30918001051695.89\n",
      "validation loss: 25325487924945.58\n",
      "epoch: 1412\n",
      "training loss: 30917891966285.445\n",
      "validation loss: 25325221276707.055\n",
      "epoch: 1413\n",
      "training loss: 30917782968506.695\n",
      "validation loss: 25324955277742.496\n",
      "epoch: 1414\n",
      "training loss: 30917674058216.676\n",
      "validation loss: 25324689926459.316\n",
      "epoch: 1415\n",
      "training loss: 30917565235271.895\n",
      "validation loss: 25324425221268.816\n",
      "epoch: 1416\n",
      "training loss: 30917456499529.664\n",
      "validation loss: 25324161160587.125\n",
      "epoch: 1417\n",
      "training loss: 30917347850848.082\n",
      "validation loss: 25323897742835.184\n",
      "epoch: 1418\n",
      "training loss: 30917239289084.766\n",
      "validation loss: 25323634966437.754\n",
      "epoch: 1419\n",
      "training loss: 30917130814098.516\n",
      "validation loss: 25323372829824.71\n",
      "epoch: 1420\n",
      "training loss: 30917022425747.66\n",
      "validation loss: 25323111331429.742\n",
      "epoch: 1421\n",
      "training loss: 30916914123891.715\n",
      "validation loss: 25322850469691.582\n",
      "epoch: 1422\n",
      "training loss: 30916805908389.7\n",
      "validation loss: 25322590243052.75\n",
      "epoch: 1423\n",
      "training loss: 30916697779101.87\n",
      "validation loss: 25322330649960.816\n",
      "epoch: 1424\n",
      "training loss: 30916589735887.926\n",
      "validation loss: 25322071688867.08\n",
      "epoch: 1425\n",
      "training loss: 30916481778608.387\n",
      "validation loss: 25321813358227.523\n",
      "epoch: 1426\n",
      "training loss: 30916373907124.535\n",
      "validation loss: 25321555656502.844\n",
      "epoch: 1427\n",
      "training loss: 30916266121297.145\n",
      "validation loss: 25321298582157.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1428\n",
      "training loss: 30916158420988.203\n",
      "validation loss: 25321042133660.527\n",
      "epoch: 1429\n",
      "training loss: 30916050806059.16\n",
      "validation loss: 25320786309485.242\n",
      "epoch: 1430\n",
      "training loss: 30915943276372.71\n",
      "validation loss: 25320531108109.484\n",
      "epoch: 1431\n",
      "training loss: 30915835831790.984\n",
      "validation loss: 25320276528014.848\n",
      "epoch: 1432\n",
      "training loss: 30915728472176.918\n",
      "validation loss: 25320022567687.504\n",
      "epoch: 1433\n",
      "training loss: 30915621197394.227\n",
      "validation loss: 25319769225618.207\n",
      "epoch: 1434\n",
      "training loss: 30915514007306.055\n",
      "validation loss: 25319516500301.3\n",
      "epoch: 1435\n",
      "training loss: 30915406901776.81\n",
      "validation loss: 25319264390236.0\n",
      "epoch: 1436\n",
      "training loss: 30915299880670.332\n",
      "validation loss: 25319012893925.086\n",
      "epoch: 1437\n",
      "training loss: 30915192943851.24\n",
      "validation loss: 25318762009875.87\n",
      "epoch: 1438\n",
      "training loss: 30915086091184.934\n",
      "validation loss: 25318511736600.156\n",
      "epoch: 1439\n",
      "training loss: 30914979322536.273\n",
      "validation loss: 25318262072613.27\n",
      "epoch: 1440\n",
      "training loss: 30914872637771.316\n",
      "validation loss: 25318013016435.344\n",
      "epoch: 1441\n",
      "training loss: 30914766036755.586\n",
      "validation loss: 25317764566589.996\n",
      "epoch: 1442\n",
      "training loss: 30914659519355.816\n",
      "validation loss: 25317516721605.645\n",
      "epoch: 1443\n",
      "training loss: 30914553085438.188\n",
      "validation loss: 25317269480014.12\n",
      "epoch: 1444\n",
      "training loss: 30914446734869.64\n",
      "validation loss: 25317022840351.74\n",
      "epoch: 1445\n",
      "training loss: 30914340467517.9\n",
      "validation loss: 25316776801159.184\n",
      "epoch: 1446\n",
      "training loss: 30914234283250.133\n",
      "validation loss: 25316531360980.574\n",
      "epoch: 1447\n",
      "training loss: 30914128181934.715\n",
      "validation loss: 25316286518364.75\n",
      "epoch: 1448\n",
      "training loss: 30914022163439.457\n",
      "validation loss: 25316042271863.92\n",
      "epoch: 1449\n",
      "training loss: 30913916227632.934\n",
      "validation loss: 25315798620034.67\n",
      "epoch: 1450\n",
      "training loss: 30913810374384.516\n",
      "validation loss: 25315555561437.934\n",
      "epoch: 1451\n",
      "training loss: 30913704603562.965\n",
      "validation loss: 25315313094637.973\n",
      "epoch: 1452\n",
      "training loss: 30913598915038.273\n",
      "validation loss: 25315071218203.7\n",
      "epoch: 1453\n",
      "training loss: 30913493308679.88\n",
      "validation loss: 25314829930707.387\n",
      "epoch: 1454\n",
      "training loss: 30913387784357.965\n",
      "validation loss: 25314589230725.547\n",
      "epoch: 1455\n",
      "training loss: 30913282341943.47\n",
      "validation loss: 25314349116839.008\n",
      "epoch: 1456\n",
      "training loss: 30913176981306.77\n",
      "validation loss: 25314109587631.867\n",
      "epoch: 1457\n",
      "training loss: 30913071702319.47\n",
      "validation loss: 25313870641692.8\n",
      "epoch: 1458\n",
      "training loss: 30912966504852.55\n",
      "validation loss: 25313632277613.75\n",
      "epoch: 1459\n",
      "training loss: 30912861388777.79\n",
      "validation loss: 25313394493990.883\n",
      "epoch: 1460\n",
      "training loss: 30912756353967.715\n",
      "validation loss: 25313157289424.574\n",
      "epoch: 1461\n",
      "training loss: 30912651400294.258\n",
      "validation loss: 25312920662518.418\n",
      "epoch: 1462\n",
      "training loss: 30912546527630.58\n",
      "validation loss: 25312684611880.53\n",
      "epoch: 1463\n",
      "training loss: 30912441735849.246\n",
      "validation loss: 25312449136122.223\n",
      "epoch: 1464\n",
      "training loss: 30912337024823.566\n",
      "validation loss: 25312214233858.965\n",
      "epoch: 1465\n",
      "training loss: 30912232394427.633\n",
      "validation loss: 25311979903710.38\n",
      "epoch: 1466\n",
      "training loss: 30912127844534.926\n",
      "validation loss: 25311746144299.246\n",
      "epoch: 1467\n",
      "training loss: 30912023375020.137\n",
      "validation loss: 25311512954252.79\n",
      "epoch: 1468\n",
      "training loss: 30911918985757.35\n",
      "validation loss: 25311280332201.363\n",
      "epoch: 1469\n",
      "training loss: 30911814676621.43\n",
      "validation loss: 25311048276779.434\n",
      "epoch: 1470\n",
      "training loss: 30911710447487.984\n",
      "validation loss: 25310816786625.56\n",
      "epoch: 1471\n",
      "training loss: 30911606298232.008\n",
      "validation loss: 25310585860381.363\n",
      "epoch: 1472\n",
      "training loss: 30911502228729.723\n",
      "validation loss: 25310355496692.895\n",
      "epoch: 1473\n",
      "training loss: 30911398238856.723\n",
      "validation loss: 25310125694209.258\n",
      "epoch: 1474\n",
      "training loss: 30911294328489.363\n",
      "validation loss: 25309896451583.56\n",
      "epoch: 1475\n",
      "training loss: 30911190497504.766\n",
      "validation loss: 25309667767473.01\n",
      "epoch: 1476\n",
      "training loss: 30911086745779.42\n",
      "validation loss: 25309439640537.79\n",
      "epoch: 1477\n",
      "training loss: 30910983073191.027\n",
      "validation loss: 25309212069442.438\n",
      "epoch: 1478\n",
      "training loss: 30910879479616.69\n",
      "validation loss: 25308985052854.477\n",
      "epoch: 1479\n",
      "training loss: 30910775964934.234\n",
      "validation loss: 25308758589445.43\n",
      "epoch: 1480\n",
      "training loss: 30910672529022.25\n",
      "validation loss: 25308532677890.78\n",
      "epoch: 1481\n",
      "training loss: 30910569171758.72\n",
      "validation loss: 25308307316868.992\n",
      "epoch: 1482\n",
      "training loss: 30910465893022.805\n",
      "validation loss: 25308082505062.793\n",
      "epoch: 1483\n",
      "training loss: 30910362692693.066\n",
      "validation loss: 25307858241157.84\n",
      "epoch: 1484\n",
      "training loss: 30910259570648.793\n",
      "validation loss: 25307634523843.742\n",
      "epoch: 1485\n",
      "training loss: 30910156526770.04\n",
      "validation loss: 25307411351814.016\n",
      "epoch: 1486\n",
      "training loss: 30910053560936.215\n",
      "validation loss: 25307188723765.055\n",
      "epoch: 1487\n",
      "training loss: 30909950673027.496\n",
      "validation loss: 25306966638397.168\n",
      "epoch: 1488\n",
      "training loss: 30909847862924.773\n",
      "validation loss: 25306745094414.543\n",
      "epoch: 1489\n",
      "training loss: 30909745130508.34\n",
      "validation loss: 25306524090524.215\n",
      "epoch: 1490\n",
      "training loss: 30909642475659.68\n",
      "validation loss: 25306303625437.44\n",
      "epoch: 1491\n",
      "training loss: 30909539898259.617\n",
      "validation loss: 25306083697868.293\n",
      "epoch: 1492\n",
      "training loss: 30909437398189.746\n",
      "validation loss: 25305864306534.68\n",
      "epoch: 1493\n",
      "training loss: 30909334975332.402\n",
      "validation loss: 25305645450158.36\n",
      "epoch: 1494\n",
      "training loss: 30909232629569.273\n",
      "validation loss: 25305427127463.86\n",
      "epoch: 1495\n",
      "training loss: 30909130360782.78\n",
      "validation loss: 25305209337179.53\n",
      "epoch: 1496\n",
      "training loss: 30909028168856.105\n",
      "validation loss: 25304992078037.527\n",
      "epoch: 1497\n",
      "training loss: 30908926053671.77\n",
      "validation loss: 25304775348772.74\n",
      "epoch: 1498\n",
      "training loss: 30908824015113.5\n",
      "validation loss: 25304559148124.203\n",
      "epoch: 1499\n",
      "training loss: 30908722053064.395\n",
      "validation loss: 25304343474833.66\n",
      "epoch: 1500\n",
      "training loss: 30908620167408.246\n",
      "validation loss: 25304128327646.64\n",
      "epoch: 1501\n",
      "training loss: 30908518358029.61\n",
      "validation loss: 25303913705312.383\n",
      "epoch: 1502\n",
      "training loss: 30908416624812.418\n",
      "validation loss: 25303699606582.85\n",
      "epoch: 1503\n",
      "training loss: 30908314967641.285\n",
      "validation loss: 25303486030213.723\n",
      "epoch: 1504\n",
      "training loss: 30908213386401.6\n",
      "validation loss: 25303272974964.4\n",
      "epoch: 1505\n",
      "training loss: 30908111880978.098\n",
      "validation loss: 25303060439596.914\n",
      "epoch: 1506\n",
      "training loss: 30908010451256.668\n",
      "validation loss: 25302848422877.37\n",
      "epoch: 1507\n",
      "training loss: 30907909097122.598\n",
      "validation loss: 25302636923574.5\n",
      "epoch: 1508\n",
      "training loss: 30907807818461.87\n",
      "validation loss: 25302425940460.695\n",
      "epoch: 1509\n",
      "training loss: 30907706615161.23\n",
      "validation loss: 25302215472312.047\n",
      "epoch: 1510\n",
      "training loss: 30907605487106.715\n",
      "validation loss: 25302005517907.2\n",
      "epoch: 1511\n",
      "training loss: 30907504434185.15\n",
      "validation loss: 25301796076028.5\n",
      "epoch: 1512\n",
      "training loss: 30907403456284.055\n",
      "validation loss: 25301587145461.863\n",
      "epoch: 1513\n",
      "training loss: 30907302553290.285\n",
      "validation loss: 25301378724995.812\n",
      "epoch: 1514\n",
      "training loss: 30907201725091.445\n",
      "validation loss: 25301170813422.473\n",
      "epoch: 1515\n",
      "training loss: 30907100971575.855\n",
      "validation loss: 25300963409537.574\n",
      "epoch: 1516\n",
      "training loss: 30907000292631.168\n",
      "validation loss: 25300756512139.39\n",
      "epoch: 1517\n",
      "training loss: 30906899688146.23\n",
      "validation loss: 25300550120030.105\n",
      "epoch: 1518\n",
      "training loss: 30906799158009.22\n",
      "validation loss: 25300344232014.438\n",
      "epoch: 1519\n",
      "training loss: 30906698702109.043\n",
      "validation loss: 25300138846900.668\n",
      "epoch: 1520\n",
      "training loss: 30906598320335.33\n",
      "validation loss: 25299933963500.625\n",
      "epoch: 1521\n",
      "training loss: 30906498012577.008\n",
      "validation loss: 25299729580628.59\n",
      "epoch: 1522\n",
      "training loss: 30906397778723.79\n",
      "validation loss: 25299525697102.426\n",
      "epoch: 1523\n",
      "training loss: 30906297618666.07\n",
      "validation loss: 25299322311743.504\n",
      "epoch: 1524\n",
      "training loss: 30906197532293.562\n",
      "validation loss: 25299119423375.62\n",
      "epoch: 1525\n",
      "training loss: 30906097519496.72\n",
      "validation loss: 25298917030826.105\n",
      "epoch: 1526\n",
      "training loss: 30905997580166.715\n",
      "validation loss: 25298715132925.758\n",
      "epoch: 1527\n",
      "training loss: 30905897714194.03\n",
      "validation loss: 25298513728507.81\n",
      "epoch: 1528\n",
      "training loss: 30905797921469.88\n",
      "validation loss: 25298312816408.97\n",
      "epoch: 1529\n",
      "training loss: 30905698201886.2\n",
      "validation loss: 25298112395469.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1530\n",
      "training loss: 30905598555334.227\n",
      "validation loss: 25297912464531.66\n",
      "epoch: 1531\n",
      "training loss: 30905498981706.402\n",
      "validation loss: 25297713022442.094\n",
      "epoch: 1532\n",
      "training loss: 30905399480894.465\n",
      "validation loss: 25297514068049.43\n",
      "epoch: 1533\n",
      "training loss: 30905300052790.867\n",
      "validation loss: 25297315600205.812\n",
      "epoch: 1534\n",
      "training loss: 30905200697288.83\n",
      "validation loss: 25297117617766.816\n",
      "epoch: 1535\n",
      "training loss: 30905101414280.797\n",
      "validation loss: 25296920119590.34\n",
      "epoch: 1536\n",
      "training loss: 30905002203660.004\n",
      "validation loss: 25296723104537.68\n",
      "epoch: 1537\n",
      "training loss: 30904903065320.37\n",
      "validation loss: 25296526571473.516\n",
      "epoch: 1538\n",
      "training loss: 30904803999155.125\n",
      "validation loss: 25296330519264.87\n",
      "epoch: 1539\n",
      "training loss: 30904705005058.207\n",
      "validation loss: 25296134946782.09\n",
      "epoch: 1540\n",
      "training loss: 30904606082924.29\n",
      "validation loss: 25295939852898.906\n",
      "epoch: 1541\n",
      "training loss: 30904507232647.316\n",
      "validation loss: 25295745236491.324\n",
      "epoch: 1542\n",
      "training loss: 30904408454121.97\n",
      "validation loss: 25295551096438.69\n",
      "epoch: 1543\n",
      "training loss: 30904309747243.64\n",
      "validation loss: 25295357431623.68\n",
      "epoch: 1544\n",
      "training loss: 30904211111907.016\n",
      "validation loss: 25295164240931.227\n",
      "epoch: 1545\n",
      "training loss: 30904112548007.484\n",
      "validation loss: 25294971523249.562\n",
      "epoch: 1546\n",
      "training loss: 30904014055441.184\n",
      "validation loss: 25294779277470.26\n",
      "epoch: 1547\n",
      "training loss: 30903915634103.484\n",
      "validation loss: 25294587502487.062\n",
      "epoch: 1548\n",
      "training loss: 30903817283890.52\n",
      "validation loss: 25294396197197.035\n",
      "epoch: 1549\n",
      "training loss: 30903719004699.117\n",
      "validation loss: 25294205360500.51\n",
      "epoch: 1550\n",
      "training loss: 30903620796425.395\n",
      "validation loss: 25294014991300.027\n",
      "epoch: 1551\n",
      "training loss: 30903522658966.16\n",
      "validation loss: 25293825088501.344\n",
      "epoch: 1552\n",
      "training loss: 30903424592218.977\n",
      "validation loss: 25293635651013.535\n",
      "epoch: 1553\n",
      "training loss: 30903326596080.664\n",
      "validation loss: 25293446677747.79\n",
      "epoch: 1554\n",
      "training loss: 30903228670448.73\n",
      "validation loss: 25293258167618.527\n",
      "epoch: 1555\n",
      "training loss: 30903130815221.434\n",
      "validation loss: 25293070119543.438\n",
      "epoch: 1556\n",
      "training loss: 30903033030296.297\n",
      "validation loss: 25292882532442.293\n",
      "epoch: 1557\n",
      "training loss: 30902935315571.535\n",
      "validation loss: 25292695405238.12\n",
      "epoch: 1558\n",
      "training loss: 30902837670946.105\n",
      "validation loss: 25292508736857.117\n",
      "epoch: 1559\n",
      "training loss: 30902740096318.203\n",
      "validation loss: 25292322526227.6\n",
      "epoch: 1560\n",
      "training loss: 30902642591586.76\n",
      "validation loss: 25292136772281.05\n",
      "epoch: 1561\n",
      "training loss: 30902545156651.41\n",
      "validation loss: 25291951473952.164\n",
      "epoch: 1562\n",
      "training loss: 30902447791411.03\n",
      "validation loss: 25291766630177.67\n",
      "epoch: 1563\n",
      "training loss: 30902350495765.25\n",
      "validation loss: 25291582239897.492\n",
      "epoch: 1564\n",
      "training loss: 30902253269614.387\n",
      "validation loss: 25291398302054.676\n",
      "epoch: 1565\n",
      "training loss: 30902156112858.004\n",
      "validation loss: 25291214815594.33\n",
      "epoch: 1566\n",
      "training loss: 30902059025396.4\n",
      "validation loss: 25291031779464.69\n",
      "epoch: 1567\n",
      "training loss: 30901962007130.555\n",
      "validation loss: 25290849192617.14\n",
      "epoch: 1568\n",
      "training loss: 30901865057960.734\n",
      "validation loss: 25290667054005.055\n",
      "epoch: 1569\n",
      "training loss: 30901768177787.902\n",
      "validation loss: 25290485362584.945\n",
      "epoch: 1570\n",
      "training loss: 30901671366513.723\n",
      "validation loss: 25290304117316.402\n",
      "epoch: 1571\n",
      "training loss: 30901574624039.113\n",
      "validation loss: 25290123317161.008\n",
      "epoch: 1572\n",
      "training loss: 30901477950265.695\n",
      "validation loss: 25289942961083.465\n",
      "epoch: 1573\n",
      "training loss: 30901381345095.83\n",
      "validation loss: 25289763048051.51\n",
      "epoch: 1574\n",
      "training loss: 30901284808431.06\n",
      "validation loss: 25289583577034.88\n",
      "epoch: 1575\n",
      "training loss: 30901188340173.71\n",
      "validation loss: 25289404547006.336\n",
      "epoch: 1576\n",
      "training loss: 30901091940226.777\n",
      "validation loss: 25289225956941.746\n",
      "epoch: 1577\n",
      "training loss: 30900995608492.49\n",
      "validation loss: 25289047805818.88\n",
      "epoch: 1578\n",
      "training loss: 30900899344873.773\n",
      "validation loss: 25288870092618.53\n",
      "epoch: 1579\n",
      "training loss: 30900803149274.32\n",
      "validation loss: 25288692816324.582\n",
      "epoch: 1580\n",
      "training loss: 30900707021597.004\n",
      "validation loss: 25288515975922.76\n",
      "epoch: 1581\n",
      "training loss: 30900610961745.41\n",
      "validation loss: 25288339570401.86\n",
      "epoch: 1582\n",
      "training loss: 30900514969623.86\n",
      "validation loss: 25288163598753.664\n",
      "epoch: 1583\n",
      "training loss: 30900419045135.875\n",
      "validation loss: 25287988059971.812\n",
      "epoch: 1584\n",
      "training loss: 30900323188185.7\n",
      "validation loss: 25287812953053.0\n",
      "epoch: 1585\n",
      "training loss: 30900227398678.29\n",
      "validation loss: 25287638276996.85\n",
      "epoch: 1586\n",
      "training loss: 30900131676517.812\n",
      "validation loss: 25287464030804.87\n",
      "epoch: 1587\n",
      "training loss: 30900036021609.14\n",
      "validation loss: 25287290213481.535\n",
      "epoch: 1588\n",
      "training loss: 30899940433857.867\n",
      "validation loss: 25287116824034.277\n",
      "epoch: 1589\n",
      "training loss: 30899844913168.805\n",
      "validation loss: 25286943861472.355\n",
      "epoch: 1590\n",
      "training loss: 30899749459447.457\n",
      "validation loss: 25286771324808.0\n",
      "epoch: 1591\n",
      "training loss: 30899654072600.055\n",
      "validation loss: 25286599213056.355\n",
      "epoch: 1592\n",
      "training loss: 30899558752532.023\n",
      "validation loss: 25286427525234.395\n",
      "epoch: 1593\n",
      "training loss: 30899463499149.508\n",
      "validation loss: 25286256260361.99\n",
      "epoch: 1594\n",
      "training loss: 30899368312358.855\n",
      "validation loss: 25286085417461.586\n",
      "epoch: 1595\n",
      "training loss: 30899273192067.117\n",
      "validation loss: 25285914995558.52\n",
      "epoch: 1596\n",
      "training loss: 30899178138180.566\n",
      "validation loss: 25285744993679.906\n",
      "epoch: 1597\n",
      "training loss: 30899083150606.17\n",
      "validation loss: 25285575410855.746\n",
      "epoch: 1598\n",
      "training loss: 30898988229251.6\n",
      "validation loss: 25285406246118.94\n",
      "epoch: 1599\n",
      "training loss: 30898893374023.74\n",
      "validation loss: 25285237498504.13\n",
      "epoch: 1600\n",
      "training loss: 30898798584830.168\n",
      "validation loss: 25285069167048.848\n",
      "epoch: 1601\n",
      "training loss: 30898703861579.184\n",
      "validation loss: 25284901250793.48\n",
      "epoch: 1602\n",
      "training loss: 30898609204178.273\n",
      "validation loss: 25284733748780.125\n",
      "epoch: 1603\n",
      "training loss: 30898514612535.633\n",
      "validation loss: 25284566660053.754\n",
      "epoch: 1604\n",
      "training loss: 30898420086560.16\n",
      "validation loss: 25284399983662.17\n",
      "epoch: 1605\n",
      "training loss: 30898325626159.957\n",
      "validation loss: 25284233718654.89\n",
      "epoch: 1606\n",
      "training loss: 30898231231243.81\n",
      "validation loss: 25284067864084.246\n",
      "epoch: 1607\n",
      "training loss: 30898136901721.223\n",
      "validation loss: 25283902419005.402\n",
      "epoch: 1608\n",
      "training loss: 30898042637500.91\n",
      "validation loss: 25283737382475.195\n",
      "epoch: 1609\n",
      "training loss: 30897948438492.25\n",
      "validation loss: 25283572753553.28\n",
      "epoch: 1610\n",
      "training loss: 30897854304604.84\n",
      "validation loss: 25283408531301.73\n",
      "epoch: 1611\n",
      "training loss: 30897760235749.0\n",
      "validation loss: 25283244714785.41\n",
      "epoch: 1612\n",
      "training loss: 30897666231834.203\n",
      "validation loss: 25283081303070.832\n",
      "epoch: 1613\n",
      "training loss: 30897572292770.633\n",
      "validation loss: 25282918295227.3\n",
      "epoch: 1614\n",
      "training loss: 30897478418469.195\n",
      "validation loss: 25282755690326.87\n",
      "epoch: 1615\n",
      "training loss: 30897384608839.96\n",
      "validation loss: 25282593487443.25\n",
      "epoch: 1616\n",
      "training loss: 30897290863793.71\n",
      "validation loss: 25282431685652.875\n",
      "epoch: 1617\n",
      "training loss: 30897197183241.914\n",
      "validation loss: 25282270284034.96\n",
      "epoch: 1618\n",
      "training loss: 30897103567095.242\n",
      "validation loss: 25282109281670.316\n",
      "epoch: 1619\n",
      "training loss: 30897010015265.04\n",
      "validation loss: 25281948677642.477\n",
      "epoch: 1620\n",
      "training loss: 30896916527662.87\n",
      "validation loss: 25281788471037.36\n",
      "epoch: 1621\n",
      "training loss: 30896823104200.99\n",
      "validation loss: 25281628660943.58\n",
      "epoch: 1622\n",
      "training loss: 30896729744790.812\n",
      "validation loss: 25281469246451.336\n",
      "epoch: 1623\n",
      "training loss: 30896636449344.477\n",
      "validation loss: 25281310226653.523\n",
      "epoch: 1624\n",
      "training loss: 30896543217774.812\n",
      "validation loss: 25281151600645.78\n",
      "epoch: 1625\n",
      "training loss: 30896450049993.812\n",
      "validation loss: 25280993367525.242\n",
      "epoch: 1626\n",
      "training loss: 30896356945914.188\n",
      "validation loss: 25280835526391.78\n",
      "epoch: 1627\n",
      "training loss: 30896263905449.324\n",
      "validation loss: 25280678076347.94\n",
      "epoch: 1628\n",
      "training loss: 30896170928511.797\n",
      "validation loss: 25280521016497.785\n",
      "epoch: 1629\n",
      "training loss: 30896078015014.87\n",
      "validation loss: 25280364345948.043\n",
      "epoch: 1630\n",
      "training loss: 30895985164871.992\n",
      "validation loss: 25280208063807.754\n",
      "epoch: 1631\n",
      "training loss: 30895892377997.316\n",
      "validation loss: 25280052169188.605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1632\n",
      "training loss: 30895799654304.156\n",
      "validation loss: 25279896661203.773\n",
      "epoch: 1633\n",
      "training loss: 30895706993706.523\n",
      "validation loss: 25279741538969.086\n",
      "epoch: 1634\n",
      "training loss: 30895614396119.14\n",
      "validation loss: 25279586801603.02\n",
      "epoch: 1635\n",
      "training loss: 30895521861455.86\n",
      "validation loss: 25279432448225.508\n",
      "epoch: 1636\n",
      "training loss: 30895429389631.258\n",
      "validation loss: 25279278477959.133\n",
      "epoch: 1637\n",
      "training loss: 30895336980560.598\n",
      "validation loss: 25279124889929.086\n",
      "epoch: 1638\n",
      "training loss: 30895244634158.31\n",
      "validation loss: 25278971683262.035\n",
      "epoch: 1639\n",
      "training loss: 30895152350339.484\n",
      "validation loss: 25278818857087.207\n",
      "epoch: 1640\n",
      "training loss: 30895060129019.453\n",
      "validation loss: 25278666410536.094\n",
      "epoch: 1641\n",
      "training loss: 30894967970114.19\n",
      "validation loss: 25278514342742.773\n",
      "epoch: 1642\n",
      "training loss: 30894875873538.85\n",
      "validation loss: 25278362652842.742\n",
      "epoch: 1643\n",
      "training loss: 30894783839209.285\n",
      "validation loss: 25278211339974.062\n",
      "epoch: 1644\n",
      "training loss: 30894691867042.023\n",
      "validation loss: 25278060403277.43\n",
      "epoch: 1645\n",
      "training loss: 30894599956952.754\n",
      "validation loss: 25277909841894.88\n",
      "epoch: 1646\n",
      "training loss: 30894508108857.87\n",
      "validation loss: 25277759654971.055\n",
      "epoch: 1647\n",
      "training loss: 30894416322673.926\n",
      "validation loss: 25277609841652.754\n",
      "epoch: 1648\n",
      "training loss: 30894324598318.17\n",
      "validation loss: 25277460401089.344\n",
      "epoch: 1649\n",
      "training loss: 30894232935707.023\n",
      "validation loss: 25277311332431.527\n",
      "epoch: 1650\n",
      "training loss: 30894141334757.555\n",
      "validation loss: 25277162634832.617\n",
      "epoch: 1651\n",
      "training loss: 30894049795387.586\n",
      "validation loss: 25277014307448.395\n",
      "epoch: 1652\n",
      "training loss: 30893958317514.02\n",
      "validation loss: 25276866349436.04\n",
      "epoch: 1653\n",
      "training loss: 30893866901054.49\n",
      "validation loss: 25276718759955.24\n",
      "epoch: 1654\n",
      "training loss: 30893775545926.805\n",
      "validation loss: 25276571538167.812\n",
      "epoch: 1655\n",
      "training loss: 30893684252049.445\n",
      "validation loss: 25276424683238.082\n",
      "epoch: 1656\n",
      "training loss: 30893593019340.043\n",
      "validation loss: 25276278194331.715\n",
      "epoch: 1657\n",
      "training loss: 30893501847716.92\n",
      "validation loss: 25276132070616.86\n",
      "epoch: 1658\n",
      "training loss: 30893410737099.105\n",
      "validation loss: 25275986311264.18\n",
      "epoch: 1659\n",
      "training loss: 30893319687404.727\n",
      "validation loss: 25275840915445.63\n",
      "epoch: 1660\n",
      "training loss: 30893228698552.64\n",
      "validation loss: 25275695882335.656\n",
      "epoch: 1661\n",
      "training loss: 30893137770461.848\n",
      "validation loss: 25275551211110.78\n",
      "epoch: 1662\n",
      "training loss: 30893046903052.066\n",
      "validation loss: 25275406900950.02\n",
      "epoch: 1663\n",
      "training loss: 30892956096242.113\n",
      "validation loss: 25275262951033.645\n",
      "epoch: 1664\n",
      "training loss: 30892865349951.535\n",
      "validation loss: 25275119360544.4\n",
      "epoch: 1665\n",
      "training loss: 30892774664100.01\n",
      "validation loss: 25274976128667.094\n",
      "epoch: 1666\n",
      "training loss: 30892684038607.94\n",
      "validation loss: 25274833254588.97\n",
      "epoch: 1667\n",
      "training loss: 30892593473394.83\n",
      "validation loss: 25274690737498.53\n",
      "epoch: 1668\n",
      "training loss: 30892502968380.875\n",
      "validation loss: 25274548576586.73\n",
      "epoch: 1669\n",
      "training loss: 30892412523486.984\n",
      "validation loss: 25274406771046.934\n",
      "epoch: 1670\n",
      "training loss: 30892322138633.156\n",
      "validation loss: 25274265320073.758\n",
      "epoch: 1671\n",
      "training loss: 30892231813740.117\n",
      "validation loss: 25274124222864.246\n",
      "epoch: 1672\n",
      "training loss: 30892141548728.727\n",
      "validation loss: 25273983478617.426\n",
      "epoch: 1673\n",
      "training loss: 30892051343520.55\n",
      "validation loss: 25273843086534.777\n",
      "epoch: 1674\n",
      "training loss: 30891961198036.27\n",
      "validation loss: 25273703045818.973\n",
      "epoch: 1675\n",
      "training loss: 30891871112197.246\n",
      "validation loss: 25273563355675.062\n",
      "epoch: 1676\n",
      "training loss: 30891781085925.55\n",
      "validation loss: 25273424015310.523\n",
      "epoch: 1677\n",
      "training loss: 30891691119142.36\n",
      "validation loss: 25273285023934.02\n",
      "epoch: 1678\n",
      "training loss: 30891601211769.535\n",
      "validation loss: 25273146380756.562\n",
      "epoch: 1679\n",
      "training loss: 30891511363729.09\n",
      "validation loss: 25273008084991.19\n",
      "epoch: 1680\n",
      "training loss: 30891421574943.758\n",
      "validation loss: 25272870135853.28\n",
      "epoch: 1681\n",
      "training loss: 30891331845335.35\n",
      "validation loss: 25272732532559.39\n",
      "epoch: 1682\n",
      "training loss: 30891242174826.4\n",
      "validation loss: 25272595274328.43\n",
      "epoch: 1683\n",
      "training loss: 30891152563339.566\n",
      "validation loss: 25272458360381.266\n",
      "epoch: 1684\n",
      "training loss: 30891063010798.223\n",
      "validation loss: 25272321789941.125\n",
      "epoch: 1685\n",
      "training loss: 30890973517124.832\n",
      "validation loss: 25272185562232.375\n",
      "epoch: 1686\n",
      "training loss: 30890884082242.574\n",
      "validation loss: 25272049676481.707\n",
      "epoch: 1687\n",
      "training loss: 30890794706075.277\n",
      "validation loss: 25271914131918.156\n",
      "epoch: 1688\n",
      "training loss: 30890705388545.9\n",
      "validation loss: 25271778927771.887\n",
      "epoch: 1689\n",
      "training loss: 30890616129578.082\n",
      "validation loss: 25271644063275.38\n",
      "epoch: 1690\n",
      "training loss: 30890526929095.61\n",
      "validation loss: 25271509537663.023\n",
      "epoch: 1691\n",
      "training loss: 30890437787022.977\n",
      "validation loss: 25271375350171.55\n",
      "epoch: 1692\n",
      "training loss: 30890348703283.76\n",
      "validation loss: 25271241500038.76\n",
      "epoch: 1693\n",
      "training loss: 30890259677802.234\n",
      "validation loss: 25271107986504.77\n",
      "epoch: 1694\n",
      "training loss: 30890170710502.83\n",
      "validation loss: 25270974808811.594\n",
      "epoch: 1695\n",
      "training loss: 30890081801310.65\n",
      "validation loss: 25270841966203.55\n",
      "epoch: 1696\n",
      "training loss: 30889992950149.914\n",
      "validation loss: 25270709457925.99\n",
      "epoch: 1697\n",
      "training loss: 30889904156945.508\n",
      "validation loss: 25270577283226.586\n",
      "epoch: 1698\n",
      "training loss: 30889815421622.484\n",
      "validation loss: 25270445441354.883\n",
      "epoch: 1699\n",
      "training loss: 30889726744106.582\n",
      "validation loss: 25270313931562.68\n",
      "epoch: 1700\n",
      "training loss: 30889638124322.64\n",
      "validation loss: 25270182753102.836\n",
      "epoch: 1701\n",
      "training loss: 30889549562196.152\n",
      "validation loss: 25270051905230.477\n",
      "epoch: 1702\n",
      "training loss: 30889461057652.793\n",
      "validation loss: 25269921387202.58\n",
      "epoch: 1703\n",
      "training loss: 30889372610618.93\n",
      "validation loss: 25269791198278.375\n",
      "epoch: 1704\n",
      "training loss: 30889284221019.98\n",
      "validation loss: 25269661337718.117\n",
      "epoch: 1705\n",
      "training loss: 30889195888782.074\n",
      "validation loss: 25269531804784.297\n",
      "epoch: 1706\n",
      "training loss: 30889107613832.04\n",
      "validation loss: 25269402598741.664\n",
      "epoch: 1707\n",
      "training loss: 30889019396095.76\n",
      "validation loss: 25269273718855.945\n",
      "epoch: 1708\n",
      "training loss: 30888931235499.816\n",
      "validation loss: 25269145164395.137\n",
      "epoch: 1709\n",
      "training loss: 30888843131970.934\n",
      "validation loss: 25269016934629.035\n",
      "epoch: 1710\n",
      "training loss: 30888755085436.535\n",
      "validation loss: 25268889028829.64\n",
      "epoch: 1711\n",
      "training loss: 30888667095823.113\n",
      "validation loss: 25268761446269.953\n",
      "epoch: 1712\n",
      "training loss: 30888579163057.855\n",
      "validation loss: 25268634186225.19\n",
      "epoch: 1713\n",
      "training loss: 30888491287068.07\n",
      "validation loss: 25268507247972.348\n",
      "epoch: 1714\n",
      "training loss: 30888403467781.797\n",
      "validation loss: 25268380630790.645\n",
      "epoch: 1715\n",
      "training loss: 30888315705126.11\n",
      "validation loss: 25268254333960.23\n",
      "epoch: 1716\n",
      "training loss: 30888227999028.79\n",
      "validation loss: 25268128356763.477\n",
      "epoch: 1717\n",
      "training loss: 30888140349417.75\n",
      "validation loss: 25268002698484.52\n",
      "epoch: 1718\n",
      "training loss: 30888052756221.6\n",
      "validation loss: 25267877358409.68\n",
      "epoch: 1719\n",
      "training loss: 30887965219368.023\n",
      "validation loss: 25267752335826.207\n",
      "epoch: 1720\n",
      "training loss: 30887877738785.375\n",
      "validation loss: 25267627630023.543\n",
      "epoch: 1721\n",
      "training loss: 30887790314402.164\n",
      "validation loss: 25267503240292.86\n",
      "epoch: 1722\n",
      "training loss: 30887702946147.586\n",
      "validation loss: 25267379165927.523\n",
      "epoch: 1723\n",
      "training loss: 30887615633949.902\n",
      "validation loss: 25267255406221.797\n",
      "epoch: 1724\n",
      "training loss: 30887528377738.043\n",
      "validation loss: 25267131960472.098\n",
      "epoch: 1725\n",
      "training loss: 30887441177441.094\n",
      "validation loss: 25267008827976.59\n",
      "epoch: 1726\n",
      "training loss: 30887354032988.84\n",
      "validation loss: 25266886008035.59\n",
      "epoch: 1727\n",
      "training loss: 30887266944310.11\n",
      "validation loss: 25266763499950.28\n",
      "epoch: 1728\n",
      "training loss: 30887179911334.418\n",
      "validation loss: 25266641303024.008\n",
      "epoch: 1729\n",
      "training loss: 30887092933991.42\n",
      "validation loss: 25266519416561.824\n",
      "epoch: 1730\n",
      "training loss: 30887006012211.48\n",
      "validation loss: 25266397839870.92\n",
      "epoch: 1731\n",
      "training loss: 30886919145923.98\n",
      "validation loss: 25266276572259.344\n",
      "epoch: 1732\n",
      "training loss: 30886832335059.023\n",
      "validation loss: 25266155613037.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1733\n",
      "training loss: 30886745579546.816\n",
      "validation loss: 25266034961516.562\n",
      "epoch: 1734\n",
      "training loss: 30886658879318.293\n",
      "validation loss: 25265914617011.24\n",
      "epoch: 1735\n",
      "training loss: 30886572234303.402\n",
      "validation loss: 25265794578836.09\n",
      "epoch: 1736\n",
      "training loss: 30886485644432.797\n",
      "validation loss: 25265674846308.08\n",
      "epoch: 1737\n",
      "training loss: 30886399109637.273\n",
      "validation loss: 25265555418745.816\n",
      "epoch: 1738\n",
      "training loss: 30886312629848.3\n",
      "validation loss: 25265436295470.027\n",
      "epoch: 1739\n",
      "training loss: 30886226204996.4\n",
      "validation loss: 25265317475802.215\n",
      "epoch: 1740\n",
      "training loss: 30886139835012.777\n",
      "validation loss: 25265198959066.027\n",
      "epoch: 1741\n",
      "training loss: 30886053519828.777\n",
      "validation loss: 25265080744586.754\n",
      "epoch: 1742\n",
      "training loss: 30885967259376.438\n",
      "validation loss: 25264962831691.742\n",
      "epoch: 1743\n",
      "training loss: 30885881053586.816\n",
      "validation loss: 25264845219709.16\n",
      "epoch: 1744\n",
      "training loss: 30885794902391.668\n",
      "validation loss: 25264727907969.258\n",
      "epoch: 1745\n",
      "training loss: 30885708805722.914\n",
      "validation loss: 25264610895803.906\n",
      "epoch: 1746\n",
      "training loss: 30885622763513.1\n",
      "validation loss: 25264494182547.04\n",
      "epoch: 1747\n",
      "training loss: 30885536775693.863\n",
      "validation loss: 25264377767533.39\n",
      "epoch: 1748\n",
      "training loss: 30885450842197.508\n",
      "validation loss: 25264261650099.746\n",
      "epoch: 1749\n",
      "training loss: 30885364962956.45\n",
      "validation loss: 25264145829584.492\n",
      "epoch: 1750\n",
      "training loss: 30885279137903.84\n",
      "validation loss: 25264030305328.082\n",
      "epoch: 1751\n",
      "training loss: 30885193366971.812\n",
      "validation loss: 25263915076671.727\n",
      "epoch: 1752\n",
      "training loss: 30885107650093.23\n",
      "validation loss: 25263800142958.67\n",
      "epoch: 1753\n",
      "training loss: 30885021987201.043\n",
      "validation loss: 25263685503533.754\n",
      "epoch: 1754\n",
      "training loss: 30884936378228.938\n",
      "validation loss: 25263571157743.883\n",
      "epoch: 1755\n",
      "training loss: 30884850823109.586\n",
      "validation loss: 25263457104936.66\n",
      "epoch: 1756\n",
      "training loss: 30884765321776.37\n",
      "validation loss: 25263343344461.74\n",
      "epoch: 1757\n",
      "training loss: 30884679874162.793\n",
      "validation loss: 25263229875670.35\n",
      "epoch: 1758\n",
      "training loss: 30884594480202.516\n",
      "validation loss: 25263116697915.32\n",
      "epoch: 1759\n",
      "training loss: 30884509139829.83\n",
      "validation loss: 25263003810551.465\n",
      "epoch: 1760\n",
      "training loss: 30884423852978.105\n",
      "validation loss: 25262891212934.33\n",
      "epoch: 1761\n",
      "training loss: 30884338619581.38\n",
      "validation loss: 25262778904421.457\n",
      "epoch: 1762\n",
      "training loss: 30884253439573.816\n",
      "validation loss: 25262666884371.96\n",
      "epoch: 1763\n",
      "training loss: 30884168312890.254\n",
      "validation loss: 25262555152146.938\n",
      "epoch: 1764\n",
      "training loss: 30884083239464.586\n",
      "validation loss: 25262443707108.195\n",
      "epoch: 1765\n",
      "training loss: 30883998219231.344\n",
      "validation loss: 25262332548619.516\n",
      "epoch: 1766\n",
      "training loss: 30883913252125.227\n",
      "validation loss: 25262221676046.23\n",
      "epoch: 1767\n",
      "training loss: 30883828338081.61\n",
      "validation loss: 25262111088755.652\n",
      "epoch: 1768\n",
      "training loss: 30883743477034.867\n",
      "validation loss: 25262000786115.785\n",
      "epoch: 1769\n",
      "training loss: 30883658668920.082\n",
      "validation loss: 25261890767496.555\n",
      "epoch: 1770\n",
      "training loss: 30883573913672.44\n",
      "validation loss: 25261781032269.484\n",
      "epoch: 1771\n",
      "training loss: 30883489211227.85\n",
      "validation loss: 25261671579808.023\n",
      "epoch: 1772\n",
      "training loss: 30883404561521.195\n",
      "validation loss: 25261562409486.266\n",
      "epoch: 1773\n",
      "training loss: 30883319964488.047\n",
      "validation loss: 25261453520680.285\n",
      "epoch: 1774\n",
      "training loss: 30883235420064.137\n",
      "validation loss: 25261344912767.67\n",
      "epoch: 1775\n",
      "training loss: 30883150928185.848\n",
      "validation loss: 25261236585127.953\n",
      "epoch: 1776\n",
      "training loss: 30883066488788.582\n",
      "validation loss: 25261128537141.285\n",
      "epoch: 1777\n",
      "training loss: 30882982101808.43\n",
      "validation loss: 25261020768189.78\n",
      "epoch: 1778\n",
      "training loss: 30882897767181.598\n",
      "validation loss: 25260913277657.047\n",
      "epoch: 1779\n",
      "training loss: 30882813484844.44\n",
      "validation loss: 25260806064928.2\n",
      "epoch: 1780\n",
      "training loss: 30882729254733.96\n",
      "validation loss: 25260699129390.25\n",
      "epoch: 1781\n",
      "training loss: 30882645076786.203\n",
      "validation loss: 25260592470430.85\n",
      "epoch: 1782\n",
      "training loss: 30882560950937.863\n",
      "validation loss: 25260486087439.57\n",
      "epoch: 1783\n",
      "training loss: 30882476877125.793\n",
      "validation loss: 25260379979807.45\n",
      "epoch: 1784\n",
      "training loss: 30882392855287.51\n",
      "validation loss: 25260274146927.453\n",
      "epoch: 1785\n",
      "training loss: 30882308885359.523\n",
      "validation loss: 25260168588193.13\n",
      "epoch: 1786\n",
      "training loss: 30882224967279.05\n",
      "validation loss: 25260063302999.934\n",
      "epoch: 1787\n",
      "training loss: 30882141100983.414\n",
      "validation loss: 25259958290744.816\n",
      "epoch: 1788\n",
      "training loss: 30882057286410.63\n",
      "validation loss: 25259853550826.57\n",
      "epoch: 1789\n",
      "training loss: 30881973523497.703\n",
      "validation loss: 25259749082644.613\n",
      "epoch: 1790\n",
      "training loss: 30881889812182.332\n",
      "validation loss: 25259644885600.227\n",
      "epoch: 1791\n",
      "training loss: 30881806152402.336\n",
      "validation loss: 25259540959096.168\n",
      "epoch: 1792\n",
      "training loss: 30881722544095.63\n",
      "validation loss: 25259437302536.61\n",
      "epoch: 1793\n",
      "training loss: 30881638987200.863\n",
      "validation loss: 25259333915327.625\n",
      "epoch: 1794\n",
      "training loss: 30881555481655.633\n",
      "validation loss: 25259230796875.836\n",
      "epoch: 1795\n",
      "training loss: 30881472027398.24\n",
      "validation loss: 25259127946589.72\n",
      "epoch: 1796\n",
      "training loss: 30881388624367.105\n",
      "validation loss: 25259025363879.22\n",
      "epoch: 1797\n",
      "training loss: 30881305272501.33\n",
      "validation loss: 25258923048156.12\n",
      "epoch: 1798\n",
      "training loss: 30881221971738.996\n",
      "validation loss: 25258820998832.74\n",
      "epoch: 1799\n",
      "training loss: 30881138722018.88\n",
      "validation loss: 25258719215323.27\n",
      "epoch: 1800\n",
      "training loss: 30881055523279.887\n",
      "validation loss: 25258617697043.32\n",
      "epoch: 1801\n",
      "training loss: 30880972375461.023\n",
      "validation loss: 25258516443409.895\n",
      "epoch: 1802\n",
      "training loss: 30880889278501.98\n",
      "validation loss: 25258415453841.844\n",
      "epoch: 1803\n",
      "training loss: 30880806232341.438\n",
      "validation loss: 25258314727758.547\n",
      "epoch: 1804\n",
      "training loss: 30880723236918.77\n",
      "validation loss: 25258214264581.215\n",
      "epoch: 1805\n",
      "training loss: 30880640292173.45\n",
      "validation loss: 25258114063732.477\n",
      "epoch: 1806\n",
      "training loss: 30880557398045.64\n",
      "validation loss: 25258014124636.75\n",
      "epoch: 1807\n",
      "training loss: 30880474554474.508\n",
      "validation loss: 25257914446719.01\n",
      "epoch: 1808\n",
      "training loss: 30880391761399.867\n",
      "validation loss: 25257815029406.008\n",
      "epoch: 1809\n",
      "training loss: 30880309018761.66\n",
      "validation loss: 25257715872125.887\n",
      "epoch: 1810\n",
      "training loss: 30880226326499.957\n",
      "validation loss: 25257616974308.207\n",
      "epoch: 1811\n",
      "training loss: 30880143684555.51\n",
      "validation loss: 25257518335384.3\n",
      "epoch: 1812\n",
      "training loss: 30880061092868.043\n",
      "validation loss: 25257419954785.992\n",
      "epoch: 1813\n",
      "training loss: 30879978551377.953\n",
      "validation loss: 25257321831946.91\n",
      "epoch: 1814\n",
      "training loss: 30879896060025.77\n",
      "validation loss: 25257223966302.055\n",
      "epoch: 1815\n",
      "training loss: 30879813618752.695\n",
      "validation loss: 25257126357288.215\n",
      "epoch: 1816\n",
      "training loss: 30879731227498.91\n",
      "validation loss: 25257029004342.67\n",
      "epoch: 1817\n",
      "training loss: 30879648886205.28\n",
      "validation loss: 25256931906904.48\n",
      "epoch: 1818\n",
      "training loss: 30879566594812.773\n",
      "validation loss: 25256835064414.043\n",
      "epoch: 1819\n",
      "training loss: 30879484353262.496\n",
      "validation loss: 25256738476313.113\n",
      "epoch: 1820\n",
      "training loss: 30879402161496.203\n",
      "validation loss: 25256642142045.223\n",
      "epoch: 1821\n",
      "training loss: 30879320019454.656\n",
      "validation loss: 25256546061054.355\n",
      "epoch: 1822\n",
      "training loss: 30879237927079.266\n",
      "validation loss: 25256450232786.277\n",
      "epoch: 1823\n",
      "training loss: 30879155884311.58\n",
      "validation loss: 25256354656688.062\n",
      "epoch: 1824\n",
      "training loss: 30879073891093.234\n",
      "validation loss: 25256259332208.137\n",
      "epoch: 1825\n",
      "training loss: 30878991947366.586\n",
      "validation loss: 25256164258796.695\n",
      "epoch: 1826\n",
      "training loss: 30878910053072.914\n",
      "validation loss: 25256069435904.33\n",
      "epoch: 1827\n",
      "training loss: 30878828208154.22\n",
      "validation loss: 25255974862983.402\n",
      "epoch: 1828\n",
      "training loss: 30878746412552.582\n",
      "validation loss: 25255880539487.62\n",
      "epoch: 1829\n",
      "training loss: 30878664666210.785\n",
      "validation loss: 25255786464872.418\n",
      "epoch: 1830\n",
      "training loss: 30878582969070.562\n",
      "validation loss: 25255692638593.62\n",
      "epoch: 1831\n",
      "training loss: 30878501321074.35\n",
      "validation loss: 25255599060108.86\n",
      "epoch: 1832\n",
      "training loss: 30878419722164.66\n",
      "validation loss: 25255505728877.008\n",
      "epoch: 1833\n",
      "training loss: 30878338172284.15\n",
      "validation loss: 25255412644358.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1834\n",
      "training loss: 30878256671376.145\n",
      "validation loss: 25255319806014.566\n",
      "epoch: 1835\n",
      "training loss: 30878175219382.906\n",
      "validation loss: 25255227213308.223\n",
      "epoch: 1836\n",
      "training loss: 30878093816247.438\n",
      "validation loss: 25255134865703.297\n",
      "epoch: 1837\n",
      "training loss: 30878012461912.793\n",
      "validation loss: 25255042762665.13\n",
      "epoch: 1838\n",
      "training loss: 30877931156322.14\n",
      "validation loss: 25254950903660.32\n",
      "epoch: 1839\n",
      "training loss: 30877849899419.37\n",
      "validation loss: 25254859288157.223\n",
      "epoch: 1840\n",
      "training loss: 30877768691147.3\n",
      "validation loss: 25254767915624.543\n",
      "epoch: 1841\n",
      "training loss: 30877687531449.42\n",
      "validation loss: 25254676785532.7\n",
      "epoch: 1842\n",
      "training loss: 30877606420269.348\n",
      "validation loss: 25254585897353.38\n",
      "epoch: 1843\n",
      "training loss: 30877525357551.383\n",
      "validation loss: 25254495250560.0\n",
      "epoch: 1844\n",
      "training loss: 30877444343238.773\n",
      "validation loss: 25254404844626.31\n",
      "epoch: 1845\n",
      "training loss: 30877363377275.434\n",
      "validation loss: 25254314679027.76\n",
      "epoch: 1846\n",
      "training loss: 30877282459605.402\n",
      "validation loss: 25254224753241.09\n",
      "epoch: 1847\n",
      "training loss: 30877201590172.83\n",
      "validation loss: 25254135066744.25\n",
      "epoch: 1848\n",
      "training loss: 30877120768922.523\n",
      "validation loss: 25254045619016.914\n",
      "epoch: 1849\n",
      "training loss: 30877039995798.26\n",
      "validation loss: 25253956409539.07\n",
      "epoch: 1850\n",
      "training loss: 30876959270744.5\n",
      "validation loss: 25253867437792.39\n",
      "epoch: 1851\n",
      "training loss: 30876878593705.79\n",
      "validation loss: 25253778703259.824\n",
      "epoch: 1852\n",
      "training loss: 30876797964626.8\n",
      "validation loss: 25253690205425.516\n",
      "epoch: 1853\n",
      "training loss: 30876717383452.875\n",
      "validation loss: 25253601943775.293\n",
      "epoch: 1854\n",
      "training loss: 30876636850128.3\n",
      "validation loss: 25253513917795.33\n",
      "epoch: 1855\n",
      "training loss: 30876556364598.055\n",
      "validation loss: 25253426126973.44\n",
      "epoch: 1856\n",
      "training loss: 30876475926807.2\n",
      "validation loss: 25253338570798.664\n",
      "epoch: 1857\n",
      "training loss: 30876395536700.918\n",
      "validation loss: 25253251248761.285\n",
      "epoch: 1858\n",
      "training loss: 30876315194225.086\n",
      "validation loss: 25253164160353.242\n",
      "epoch: 1859\n",
      "training loss: 30876234899324.5\n",
      "validation loss: 25253077305066.75\n",
      "epoch: 1860\n",
      "training loss: 30876154651944.63\n",
      "validation loss: 25252990682395.69\n",
      "epoch: 1861\n",
      "training loss: 30876074452031.08\n",
      "validation loss: 25252904291835.184\n",
      "epoch: 1862\n",
      "training loss: 30875994299529.52\n",
      "validation loss: 25252818132881.52\n",
      "epoch: 1863\n",
      "training loss: 30875914194386.332\n",
      "validation loss: 25252732205032.65\n",
      "epoch: 1864\n",
      "training loss: 30875834136546.816\n",
      "validation loss: 25252646507786.785\n",
      "epoch: 1865\n",
      "training loss: 30875754125956.977\n",
      "validation loss: 25252561040643.83\n",
      "epoch: 1866\n",
      "training loss: 30875674162562.887\n",
      "validation loss: 25252475803104.824\n",
      "epoch: 1867\n",
      "training loss: 30875594246310.75\n",
      "validation loss: 25252390794672.03\n",
      "epoch: 1868\n",
      "training loss: 30875514377147.44\n",
      "validation loss: 25252306014849.35\n",
      "epoch: 1869\n",
      "training loss: 30875434555018.758\n",
      "validation loss: 25252221463140.914\n",
      "epoch: 1870\n",
      "training loss: 30875354779871.195\n",
      "validation loss: 25252137139052.516\n",
      "epoch: 1871\n",
      "training loss: 30875275051651.344\n",
      "validation loss: 25252053042091.094\n",
      "epoch: 1872\n",
      "training loss: 30875195370305.887\n",
      "validation loss: 25251969171764.805\n",
      "epoch: 1873\n",
      "training loss: 30875115735782.207\n",
      "validation loss: 25251885527583.39\n",
      "epoch: 1874\n",
      "training loss: 30875036148026.59\n",
      "validation loss: 25251802109056.832\n",
      "epoch: 1875\n",
      "training loss: 30874956606986.03\n",
      "validation loss: 25251718915696.76\n",
      "epoch: 1876\n",
      "training loss: 30874877112607.605\n",
      "validation loss: 25251635947015.938\n",
      "epoch: 1877\n",
      "training loss: 30874797664838.49\n",
      "validation loss: 25251553202528.297\n",
      "epoch: 1878\n",
      "training loss: 30874718263626.555\n",
      "validation loss: 25251470681749.387\n",
      "epoch: 1879\n",
      "training loss: 30874638908918.59\n",
      "validation loss: 25251388384194.96\n",
      "epoch: 1880\n",
      "training loss: 30874559600662.06\n",
      "validation loss: 25251306309382.39\n",
      "epoch: 1881\n",
      "training loss: 30874480338804.527\n",
      "validation loss: 25251224456830.195\n",
      "epoch: 1882\n",
      "training loss: 30874401123293.67\n",
      "validation loss: 25251142826058.035\n",
      "epoch: 1883\n",
      "training loss: 30874321954077.832\n",
      "validation loss: 25251061416587.152\n",
      "epoch: 1884\n",
      "training loss: 30874242831104.28\n",
      "validation loss: 25250980227939.008\n",
      "epoch: 1885\n",
      "training loss: 30874163754320.96\n",
      "validation loss: 25250899259636.66\n",
      "epoch: 1886\n",
      "training loss: 30874084723675.93\n",
      "validation loss: 25250818511204.297\n",
      "epoch: 1887\n",
      "training loss: 30874005739117.332\n",
      "validation loss: 25250737982167.215\n",
      "epoch: 1888\n",
      "training loss: 30873926800593.992\n",
      "validation loss: 25250657672052.32\n",
      "epoch: 1889\n",
      "training loss: 30873847908053.66\n",
      "validation loss: 25250577580386.695\n",
      "epoch: 1890\n",
      "training loss: 30873769061444.754\n",
      "validation loss: 25250497706698.996\n",
      "epoch: 1891\n",
      "training loss: 30873690260715.812\n",
      "validation loss: 25250418050518.996\n",
      "epoch: 1892\n",
      "training loss: 30873611505815.42\n",
      "validation loss: 25250338611377.6\n",
      "epoch: 1893\n",
      "training loss: 30873532796692.918\n",
      "validation loss: 25250259388807.277\n",
      "epoch: 1894\n",
      "training loss: 30873454133296.504\n",
      "validation loss: 25250180382340.637\n",
      "epoch: 1895\n",
      "training loss: 30873375515575.08\n",
      "validation loss: 25250101591511.89\n",
      "epoch: 1896\n",
      "training loss: 30873296943477.62\n",
      "validation loss: 25250023015856.348\n",
      "epoch: 1897\n",
      "training loss: 30873218416953.23\n",
      "validation loss: 25249944654910.402\n",
      "epoch: 1898\n",
      "training loss: 30873139935951.684\n",
      "validation loss: 25249866508212.027\n",
      "epoch: 1899\n",
      "training loss: 30873061500421.633\n",
      "validation loss: 25249788575299.316\n",
      "epoch: 1900\n",
      "training loss: 30872983110312.465\n",
      "validation loss: 25249710855711.953\n",
      "epoch: 1901\n",
      "training loss: 30872904765573.613\n",
      "validation loss: 25249633348990.688\n",
      "epoch: 1902\n",
      "training loss: 30872826466154.645\n",
      "validation loss: 25249556054677.37\n",
      "epoch: 1903\n",
      "training loss: 30872748212005.785\n",
      "validation loss: 25249478972315.383\n",
      "epoch: 1904\n",
      "training loss: 30872670003076.17\n",
      "validation loss: 25249402101448.254\n",
      "epoch: 1905\n",
      "training loss: 30872591839315.617\n",
      "validation loss: 25249325441621.05\n",
      "epoch: 1906\n",
      "training loss: 30872513720674.043\n",
      "validation loss: 25249248992379.91\n",
      "epoch: 1907\n",
      "training loss: 30872435647101.44\n",
      "validation loss: 25249172753272.047\n",
      "epoch: 1908\n",
      "training loss: 30872357618547.918\n",
      "validation loss: 25249096723845.742\n",
      "epoch: 1909\n",
      "training loss: 30872279634964.258\n",
      "validation loss: 25249020903650.816\n",
      "epoch: 1910\n",
      "training loss: 30872201696300.133\n",
      "validation loss: 25248945292237.18\n",
      "epoch: 1911\n",
      "training loss: 30872123802505.902\n",
      "validation loss: 25248869889156.3\n",
      "epoch: 1912\n",
      "training loss: 30872045953532.023\n",
      "validation loss: 25248794693960.684\n",
      "epoch: 1913\n",
      "training loss: 30871968149329.035\n",
      "validation loss: 25248719706203.906\n",
      "epoch: 1914\n",
      "training loss: 30871890389848.188\n",
      "validation loss: 25248644925441.05\n",
      "epoch: 1915\n",
      "training loss: 30871812675039.586\n",
      "validation loss: 25248570351227.31\n",
      "epoch: 1916\n",
      "training loss: 30871735004854.06\n",
      "validation loss: 25248495983119.39\n",
      "epoch: 1917\n",
      "training loss: 30871657379242.484\n",
      "validation loss: 25248421820675.04\n",
      "epoch: 1918\n",
      "training loss: 30871579798155.863\n",
      "validation loss: 25248347863453.05\n",
      "epoch: 1919\n",
      "training loss: 30871502261545.88\n",
      "validation loss: 25248274111013.727\n",
      "epoch: 1920\n",
      "training loss: 30871424769363.09\n",
      "validation loss: 25248200562917.44\n",
      "epoch: 1921\n",
      "training loss: 30871347321558.742\n",
      "validation loss: 25248127218726.098\n",
      "epoch: 1922\n",
      "training loss: 30871269918084.188\n",
      "validation loss: 25248054078002.598\n",
      "epoch: 1923\n",
      "training loss: 30871192558890.84\n",
      "validation loss: 25247981140310.914\n",
      "epoch: 1924\n",
      "training loss: 30871115243930.227\n",
      "validation loss: 25247908405216.0\n",
      "epoch: 1925\n",
      "training loss: 30871037973154.55\n",
      "validation loss: 25247835872284.34\n",
      "epoch: 1926\n",
      "training loss: 30870960746514.887\n",
      "validation loss: 25247763541082.44\n",
      "epoch: 1927\n",
      "training loss: 30870883563963.035\n",
      "validation loss: 25247691411178.348\n",
      "epoch: 1928\n",
      "training loss: 30870806425450.836\n",
      "validation loss: 25247619482141.094\n",
      "epoch: 1929\n",
      "training loss: 30870729330930.23\n",
      "validation loss: 25247547753540.723\n",
      "epoch: 1930\n",
      "training loss: 30870652280353.887\n",
      "validation loss: 25247476224948.79\n",
      "epoch: 1931\n",
      "training loss: 30870575273673.305\n",
      "validation loss: 25247404895936.855\n",
      "epoch: 1932\n",
      "training loss: 30870498310840.684\n",
      "validation loss: 25247333766078.0\n",
      "epoch: 1933\n",
      "training loss: 30870421391808.32\n",
      "validation loss: 25247262834946.28\n",
      "epoch: 1934\n",
      "training loss: 30870344516528.586\n",
      "validation loss: 25247192102116.773\n",
      "epoch: 1935\n",
      "training loss: 30870267684954.55\n",
      "validation loss: 25247121567166.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1936\n",
      "training loss: 30870190897038.16\n",
      "validation loss: 25247051229670.594\n",
      "epoch: 1937\n",
      "training loss: 30870114152732.043\n",
      "validation loss: 25246981089208.523\n",
      "epoch: 1938\n",
      "training loss: 30870037451988.9\n",
      "validation loss: 25246911145358.855\n",
      "epoch: 1939\n",
      "training loss: 30869960794761.547\n",
      "validation loss: 25246841397701.605\n",
      "epoch: 1940\n",
      "training loss: 30869884181002.848\n",
      "validation loss: 25246771845817.773\n",
      "epoch: 1941\n",
      "training loss: 30869807610666.402\n",
      "validation loss: 25246702489289.844\n",
      "epoch: 1942\n",
      "training loss: 30869731083704.633\n",
      "validation loss: 25246633327700.273\n",
      "epoch: 1943\n",
      "training loss: 30869654600070.684\n",
      "validation loss: 25246564360632.992\n",
      "epoch: 1944\n",
      "training loss: 30869578159717.758\n",
      "validation loss: 25246495587672.93\n",
      "epoch: 1945\n",
      "training loss: 30869501762599.17\n",
      "validation loss: 25246427008405.957\n",
      "epoch: 1946\n",
      "training loss: 30869425408668.91\n",
      "validation loss: 25246358622419.418\n",
      "epoch: 1947\n",
      "training loss: 30869349097879.83\n",
      "validation loss: 25246290429300.633\n",
      "epoch: 1948\n",
      "training loss: 30869272830185.47\n",
      "validation loss: 25246222428638.375\n",
      "epoch: 1949\n",
      "training loss: 30869196605539.465\n",
      "validation loss: 25246154620022.383\n",
      "epoch: 1950\n",
      "training loss: 30869120423895.53\n",
      "validation loss: 25246087003043.383\n",
      "epoch: 1951\n",
      "training loss: 30869044285207.453\n",
      "validation loss: 25246019577293.01\n",
      "epoch: 1952\n",
      "training loss: 30868968189429.727\n",
      "validation loss: 25245952342364.367\n",
      "epoch: 1953\n",
      "training loss: 30868892136515.688\n",
      "validation loss: 25245885297850.516\n",
      "epoch: 1954\n",
      "training loss: 30868816126419.387\n",
      "validation loss: 25245818443345.973\n",
      "epoch: 1955\n",
      "training loss: 30868740159094.93\n",
      "validation loss: 25245751778446.2\n",
      "epoch: 1956\n",
      "training loss: 30868664234496.516\n",
      "validation loss: 25245685302747.6\n",
      "epoch: 1957\n",
      "training loss: 30868588352578.434\n",
      "validation loss: 25245619015847.523\n",
      "epoch: 1958\n",
      "training loss: 30868512513295.652\n",
      "validation loss: 25245552917344.734\n",
      "epoch: 1959\n",
      "training loss: 30868436716602.008\n",
      "validation loss: 25245487006837.965\n",
      "epoch: 1960\n",
      "training loss: 30868360962452.004\n",
      "validation loss: 25245421283927.37\n",
      "epoch: 1961\n",
      "training loss: 30868285250800.25\n",
      "validation loss: 25245355748214.04\n",
      "epoch: 1962\n",
      "training loss: 30868209581601.44\n",
      "validation loss: 25245290399300.008\n",
      "epoch: 1963\n",
      "training loss: 30868133954810.926\n",
      "validation loss: 25245225236788.7\n",
      "epoch: 1964\n",
      "training loss: 30868058370382.95\n",
      "validation loss: 25245160260283.508\n",
      "epoch: 1965\n",
      "training loss: 30867982828272.43\n",
      "validation loss: 25245095469389.207\n",
      "epoch: 1966\n",
      "training loss: 30867907328434.344\n",
      "validation loss: 25245030863711.535\n",
      "epoch: 1967\n",
      "training loss: 30867831870823.793\n",
      "validation loss: 25244966442857.13\n",
      "epoch: 1968\n",
      "training loss: 30867756455395.918\n",
      "validation loss: 25244902206433.547\n",
      "epoch: 1969\n",
      "training loss: 30867681082106.574\n",
      "validation loss: 25244838154049.758\n",
      "epoch: 1970\n",
      "training loss: 30867605750910.457\n",
      "validation loss: 25244774285314.64\n",
      "epoch: 1971\n",
      "training loss: 30867530461762.95\n",
      "validation loss: 25244710599838.484\n",
      "epoch: 1972\n",
      "training loss: 30867455214619.53\n",
      "validation loss: 25244647097232.49\n",
      "epoch: 1973\n",
      "training loss: 30867380009435.734\n",
      "validation loss: 25244583777108.76\n",
      "epoch: 1974\n",
      "training loss: 30867304846167.184\n",
      "validation loss: 25244520639080.32\n",
      "epoch: 1975\n",
      "training loss: 30867229724770.207\n",
      "validation loss: 25244457682761.562\n",
      "epoch: 1976\n",
      "training loss: 30867154645199.957\n",
      "validation loss: 25244394907766.797\n",
      "epoch: 1977\n",
      "training loss: 30867079607412.293\n",
      "validation loss: 25244332313711.727\n",
      "epoch: 1978\n",
      "training loss: 30867004611363.145\n",
      "validation loss: 25244269900212.953\n",
      "epoch: 1979\n",
      "training loss: 30866929657008.516\n",
      "validation loss: 25244207666887.977\n",
      "epoch: 1980\n",
      "training loss: 30866854744304.48\n",
      "validation loss: 25244145613355.168\n",
      "epoch: 1981\n",
      "training loss: 30866779873207.84\n",
      "validation loss: 25244083739234.31\n",
      "epoch: 1982\n",
      "training loss: 30866705043674.207\n",
      "validation loss: 25244022044145.035\n",
      "epoch: 1983\n",
      "training loss: 30866630255659.887\n",
      "validation loss: 25243960527708.387\n",
      "epoch: 1984\n",
      "training loss: 30866555509121.26\n",
      "validation loss: 25243899189546.28\n",
      "epoch: 1985\n",
      "training loss: 30866480804014.81\n",
      "validation loss: 25243838029281.527\n",
      "epoch: 1986\n",
      "training loss: 30866406140297.062\n",
      "validation loss: 25243777046537.797\n",
      "epoch: 1987\n",
      "training loss: 30866331517925.25\n",
      "validation loss: 25243716240940.13\n",
      "epoch: 1988\n",
      "training loss: 30866256936855.46\n",
      "validation loss: 25243655612113.465\n",
      "epoch: 1989\n",
      "training loss: 30866182397044.445\n",
      "validation loss: 25243595159684.08\n",
      "epoch: 1990\n",
      "training loss: 30866107898449.03\n",
      "validation loss: 25243534883279.133\n",
      "epoch: 1991\n",
      "training loss: 30866033441026.15\n",
      "validation loss: 25243474782526.652\n",
      "epoch: 1992\n",
      "training loss: 30865959024733.41\n",
      "validation loss: 25243414857056.047\n",
      "epoch: 1993\n",
      "training loss: 30865884649527.242\n",
      "validation loss: 25243355106496.535\n",
      "epoch: 1994\n",
      "training loss: 30865810315364.78\n",
      "validation loss: 25243295530478.75\n",
      "epoch: 1995\n",
      "training loss: 30865736022203.25\n",
      "validation loss: 25243236128634.152\n",
      "epoch: 1996\n",
      "training loss: 30865661769999.92\n",
      "validation loss: 25243176900595.05\n",
      "epoch: 1997\n",
      "training loss: 30865587558712.15\n",
      "validation loss: 25243117845994.66\n",
      "epoch: 1998\n",
      "training loss: 30865513388298.01\n",
      "validation loss: 25243058964467.496\n",
      "epoch: 1999\n",
      "training loss: 30865439258714.355\n",
      "validation loss: 25243000255647.926\n",
      "Mean absolute error: $6168218017.11\n",
      "Nodes: 100\n",
      "Learning Rate: 1e-07\n",
      "epoch: 0\n",
      "training loss: 36773977929075.69\n",
      "validation loss: 33744331043449.547\n",
      "epoch: 1\n",
      "training loss: 36352160254929.73\n",
      "validation loss: 32375240433339.06\n",
      "epoch: 2\n",
      "training loss: 35929451103780.07\n",
      "validation loss: 31823515034449.38\n",
      "epoch: 3\n",
      "training loss: 35640004301281.4\n",
      "validation loss: 31442478780837.387\n",
      "epoch: 4\n",
      "training loss: 35411824756386.055\n",
      "validation loss: 30795464574828.992\n",
      "epoch: 5\n",
      "training loss: 35101040856876.062\n",
      "validation loss: 30227951639752.812\n",
      "epoch: 6\n",
      "training loss: 34899290247849.945\n",
      "validation loss: 29924973656489.164\n",
      "epoch: 7\n",
      "training loss: 34692486615912.516\n",
      "validation loss: 29788004873346.344\n",
      "epoch: 8\n",
      "training loss: 34392401767738.402\n",
      "validation loss: 29160030787079.43\n",
      "epoch: 9\n",
      "training loss: 34160116772573.87\n",
      "validation loss: 28746426231367.19\n",
      "epoch: 10\n",
      "training loss: 33881725940252.094\n",
      "validation loss: 28246543836368.832\n",
      "epoch: 11\n",
      "training loss: 33777859053883.703\n",
      "validation loss: 28073328720865.97\n",
      "epoch: 12\n",
      "training loss: 33589154910039.15\n",
      "validation loss: 27681178891121.387\n",
      "epoch: 13\n",
      "training loss: 33592013975667.637\n",
      "validation loss: 27737784368966.15\n",
      "epoch: 14\n",
      "training loss: 33537490266834.094\n",
      "validation loss: 27582031834430.777\n",
      "epoch: 15\n",
      "training loss: 33338088440944.832\n",
      "validation loss: 27246128948018.484\n",
      "epoch: 16\n",
      "training loss: 33189841597433.758\n",
      "validation loss: 26898298188819.64\n",
      "epoch: 17\n",
      "training loss: 33152579413849.19\n",
      "validation loss: 27042431041345.453\n",
      "epoch: 18\n",
      "training loss: 32997708972456.15\n",
      "validation loss: 26604930809116.207\n",
      "epoch: 19\n",
      "training loss: 32913387298122.16\n",
      "validation loss: 26273992991267.195\n",
      "epoch: 20\n",
      "training loss: 32763860978255.92\n",
      "validation loss: 25913769689461.26\n",
      "epoch: 21\n",
      "training loss: 32731545213443.92\n",
      "validation loss: 26234402237156.562\n",
      "epoch: 22\n",
      "training loss: 32635814751231.344\n",
      "validation loss: 26074580146777.062\n",
      "epoch: 23\n",
      "training loss: 32545404732739.848\n",
      "validation loss: 25614177754703.07\n",
      "epoch: 24\n",
      "training loss: 32425786590797.016\n",
      "validation loss: 25320710456570.105\n",
      "epoch: 25\n",
      "training loss: 32376129793482.242\n",
      "validation loss: 25128076528391.01\n",
      "epoch: 26\n",
      "training loss: 32320391307924.805\n",
      "validation loss: 25112680812403.637\n",
      "epoch: 27\n",
      "training loss: 32338905795058.957\n",
      "validation loss: 25253823557285.89\n",
      "epoch: 28\n",
      "training loss: 32321243537039.64\n",
      "validation loss: 25039958678453.293\n",
      "epoch: 29\n",
      "training loss: 32232429841307.527\n",
      "validation loss: 24728022935037.402\n",
      "epoch: 30\n",
      "training loss: 32175490021731.043\n",
      "validation loss: 24599469204872.39\n",
      "epoch: 31\n",
      "training loss: 32127138046741.227\n",
      "validation loss: 24462861028939.19\n",
      "epoch: 32\n",
      "training loss: 32081082955113.477\n",
      "validation loss: 24329515192890.29\n",
      "epoch: 33\n",
      "training loss: 32039229070156.65\n",
      "validation loss: 24209062040507.734\n",
      "epoch: 34\n",
      "training loss: 32000196660996.426\n",
      "validation loss: 24094538057653.38\n",
      "epoch: 35\n",
      "training loss: 31963761170610.094\n",
      "validation loss: 23985607753550.58\n",
      "epoch: 36\n",
      "training loss: 31929686749712.137\n",
      "validation loss: 23881966697999.727\n",
      "epoch: 37\n",
      "training loss: 31893479527302.246\n",
      "validation loss: 23783891100966.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38\n",
      "training loss: 31863679527713.883\n",
      "validation loss: 23689996327802.43\n",
      "epoch: 39\n",
      "training loss: 31835771440192.785\n",
      "validation loss: 23600684030653.445\n",
      "epoch: 40\n",
      "training loss: 31810375252321.22\n",
      "validation loss: 23499622817868.984\n",
      "epoch: 41\n",
      "training loss: 31786066551021.418\n",
      "validation loss: 23419954129247.395\n",
      "epoch: 42\n",
      "training loss: 31763478095941.574\n",
      "validation loss: 23344060683518.504\n",
      "epoch: 43\n",
      "training loss: 31742185446811.312\n",
      "validation loss: 23271746729568.1\n",
      "epoch: 44\n",
      "training loss: 31722084853826.043\n",
      "validation loss: 23202826184379.51\n",
      "epoch: 45\n",
      "training loss: 31703080969172.54\n",
      "validation loss: 23137123752008.105\n",
      "epoch: 46\n",
      "training loss: 31685086166680.33\n",
      "validation loss: 23074474227130.566\n",
      "epoch: 47\n",
      "training loss: 31668019916548.984\n",
      "validation loss: 23014721847465.39\n",
      "epoch: 48\n",
      "training loss: 31651808210693.477\n",
      "validation loss: 22957719691344.63\n",
      "epoch: 49\n",
      "training loss: 31636383034610.35\n",
      "validation loss: 22903329116964.625\n",
      "epoch: 50\n",
      "training loss: 31621681881999.58\n",
      "validation loss: 22851419240045.434\n",
      "epoch: 51\n",
      "training loss: 31607647308680.996\n",
      "validation loss: 22801866446761.535\n",
      "epoch: 52\n",
      "training loss: 31594226522625.06\n",
      "validation loss: 22754553938829.71\n",
      "epoch: 53\n",
      "training loss: 31581371007174.426\n",
      "validation loss: 22709371307457.434\n",
      "epoch: 54\n",
      "training loss: 31569036174769.953\n",
      "validation loss: 22666214132248.93\n",
      "epoch: 55\n",
      "training loss: 31557181048711.88\n",
      "validation loss: 22624983599594.914\n",
      "epoch: 56\n",
      "training loss: 31545767970686.66\n",
      "validation loss: 22585586131097.496\n",
      "epoch: 57\n",
      "training loss: 31534762331973.64\n",
      "validation loss: 22547933001978.914\n",
      "epoch: 58\n",
      "training loss: 31524132326410.332\n",
      "validation loss: 22511939898375.387\n",
      "epoch: 59\n",
      "training loss: 31513848722727.83\n",
      "validation loss: 22477526260411.117\n",
      "epoch: 60\n",
      "training loss: 31503827565388.977\n",
      "validation loss: 22444613316577.305\n",
      "epoch: 61\n",
      "training loss: 31485452028009.01\n",
      "validation loss: 22484936576292.07\n",
      "epoch: 62\n",
      "training loss: 31471123807213.453\n",
      "validation loss: 22430754501082.68\n",
      "epoch: 63\n",
      "training loss: 31451963061243.844\n",
      "validation loss: 22338538685464.914\n",
      "epoch: 64\n",
      "training loss: 31447452254681.246\n",
      "validation loss: 22306954322534.21\n",
      "epoch: 65\n",
      "training loss: 31438806014673.75\n",
      "validation loss: 22281041618558.65\n",
      "epoch: 66\n",
      "training loss: 31430346552627.54\n",
      "validation loss: 22256221499700.164\n",
      "epoch: 67\n",
      "training loss: 31422059947547.44\n",
      "validation loss: 22232444010559.79\n",
      "epoch: 68\n",
      "training loss: 31413933400882.29\n",
      "validation loss: 22209661670505.18\n",
      "epoch: 69\n",
      "training loss: 31405955145572.336\n",
      "validation loss: 22187829338184.56\n",
      "epoch: 70\n",
      "training loss: 31398114362770.684\n",
      "validation loss: 22166904084315.35\n",
      "epoch: 71\n",
      "training loss: 31390401105283.707\n",
      "validation loss: 22146845072191.98\n",
      "epoch: 72\n",
      "training loss: 31382806227188.793\n",
      "validation loss: 22127613445395.09\n",
      "epoch: 73\n",
      "training loss: 31375321319131.016\n",
      "validation loss: 22109172222222.56\n",
      "epoch: 74\n",
      "training loss: 31367938648840.887\n",
      "validation loss: 22091486196396.492\n",
      "epoch: 75\n",
      "training loss: 31360651106451.938\n",
      "validation loss: 22074521843632.715\n",
      "epoch: 76\n",
      "training loss: 31353452154231.125\n",
      "validation loss: 22058247233688.84\n",
      "epoch: 77\n",
      "training loss: 31346335780365.996\n",
      "validation loss: 22042631947533.965\n",
      "epoch: 78\n",
      "training loss: 31339296456481.562\n",
      "validation loss: 22027646999308.582\n",
      "epoch: 79\n",
      "training loss: 31332329098585.855\n",
      "validation loss: 22013264762766.61\n",
      "epoch: 80\n",
      "training loss: 31325429031167.777\n",
      "validation loss: 21999458901912.945\n",
      "epoch: 81\n",
      "training loss: 31318591954192.8\n",
      "validation loss: 21986204305569.984\n",
      "epoch: 82\n",
      "training loss: 31311813912762.92\n",
      "validation loss: 21973477025625.18\n",
      "epoch: 83\n",
      "training loss: 31305091269225.7\n",
      "validation loss: 21961254218728.77\n",
      "epoch: 84\n",
      "training loss: 31298420677535.13\n",
      "validation loss: 21949514091226.668\n",
      "epoch: 85\n",
      "training loss: 31291799059682.414\n",
      "validation loss: 21938235847128.316\n",
      "epoch: 86\n",
      "training loss: 31285223584029.703\n",
      "validation loss: 21927399638922.793\n",
      "epoch: 87\n",
      "training loss: 31278691645393.426\n",
      "validation loss: 21916986521069.41\n",
      "epoch: 88\n",
      "training loss: 31272200846735.79\n",
      "validation loss: 21906978406000.266\n",
      "epoch: 89\n",
      "training loss: 31265748982334.848\n",
      "validation loss: 21897358022483.637\n",
      "epoch: 90\n",
      "training loss: 31259334022313.76\n",
      "validation loss: 21888108876206.88\n",
      "epoch: 91\n",
      "training loss: 31252954098418.836\n",
      "validation loss: 21879215212446.703\n",
      "epoch: 92\n",
      "training loss: 31246607490937.594\n",
      "validation loss: 21870661980704.055\n",
      "epoch: 93\n",
      "training loss: 31240292613390.207\n",
      "validation loss: 21862434801188.742\n",
      "epoch: 94\n",
      "training loss: 31233513417376.316\n",
      "validation loss: 21855512367650.355\n",
      "epoch: 95\n",
      "training loss: 31233379062260.68\n",
      "validation loss: 21851985018156.668\n",
      "epoch: 96\n",
      "training loss: 31227308986922.75\n",
      "validation loss: 21844845459114.234\n",
      "epoch: 97\n",
      "training loss: 31221263944705.895\n",
      "validation loss: 21837970174287.93\n",
      "epoch: 98\n",
      "training loss: 31215242874288.94\n",
      "validation loss: 21831348200772.086\n",
      "epoch: 99\n",
      "training loss: 31209245080767.6\n",
      "validation loss: 21824968974108.227\n",
      "epoch: 100\n",
      "training loss: 31203269633652.555\n",
      "validation loss: 21818822542127.594\n",
      "epoch: 101\n",
      "training loss: 31197315771970.65\n",
      "validation loss: 21812899320335.6\n",
      "epoch: 102\n",
      "training loss: 31191382794420.473\n",
      "validation loss: 21807190132108.58\n",
      "epoch: 103\n",
      "training loss: 31185470053829.67\n",
      "validation loss: 21801686197670.88\n",
      "epoch: 104\n",
      "training loss: 31179576952778.004\n",
      "validation loss: 21796379116118.242\n",
      "epoch: 105\n",
      "training loss: 31173702939574.164\n",
      "validation loss: 21791260848311.742\n",
      "epoch: 106\n",
      "training loss: 31167847504557.54\n",
      "validation loss: 21786323700596.81\n",
      "epoch: 107\n",
      "training loss: 31162010176698.727\n",
      "validation loss: 21781560309303.863\n",
      "epoch: 108\n",
      "training loss: 31156190520474.75\n",
      "validation loss: 21776963625989.367\n",
      "epoch: 109\n",
      "training loss: 31150388132996.69\n",
      "validation loss: 21772526903378.668\n",
      "epoch: 110\n",
      "training loss: 31144602641369.582\n",
      "validation loss: 21768243681974.164\n",
      "epoch: 111\n",
      "training loss: 31138833700265.637\n",
      "validation loss: 21764107777294.51\n",
      "epoch: 112\n",
      "training loss: 31133080989693.7\n",
      "validation loss: 21760113267712.48\n",
      "epoch: 113\n",
      "training loss: 31127344212949.13\n",
      "validation loss: 21756254482861.004\n",
      "epoch: 114\n",
      "training loss: 31121623094729.4\n",
      "validation loss: 21752525992578.566\n",
      "epoch: 115\n",
      "training loss: 31115917379402.258\n",
      "validation loss: 21748922596366.785\n",
      "epoch: 116\n",
      "training loss: 31110226829413.902\n",
      "validation loss: 21745439313334.605\n",
      "epoch: 117\n",
      "training loss: 31104551223826.113\n",
      "validation loss: 21742071372604.76\n",
      "epoch: 118\n",
      "training loss: 31098890356971.684\n",
      "validation loss: 21738814204159.707\n",
      "epoch: 119\n",
      "training loss: 31093244037218.797\n",
      "validation loss: 21735663430105.293\n",
      "epoch: 120\n",
      "training loss: 31087612085835.406\n",
      "validation loss: 21732614856331.84\n",
      "epoch: 121\n",
      "training loss: 31081994335945.402\n",
      "validation loss: 21729664464553.082\n",
      "epoch: 122\n",
      "training loss: 31076390631568.086\n",
      "validation loss: 21726808404705.004\n",
      "epoch: 123\n",
      "training loss: 31070800826707.004\n",
      "validation loss: 21724042987687.492\n",
      "epoch: 124\n",
      "training loss: 31065224627668.63\n",
      "validation loss: 21721364678447.523\n",
      "epoch: 125\n",
      "training loss: 31059563206077.727\n",
      "validation loss: 21718789333266.668\n",
      "epoch: 126\n",
      "training loss: 31053879180475.273\n",
      "validation loss: 21716228486449.65\n",
      "epoch: 127\n",
      "training loss: 31046607886742.688\n",
      "validation loss: 21716182929387.215\n",
      "epoch: 128\n",
      "training loss: 31041056082044.527\n",
      "validation loss: 21714127218835.188\n",
      "epoch: 129\n",
      "training loss: 31035517594406.027\n",
      "validation loss: 21711901995281.605\n",
      "epoch: 130\n",
      "training loss: 31029992330047.586\n",
      "validation loss: 21709744125460.465\n",
      "epoch: 131\n",
      "training loss: 31024480200335.316\n",
      "validation loss: 21707651031033.918\n",
      "epoch: 132\n",
      "training loss: 31018981121179.83\n",
      "validation loss: 21705620240104.62\n",
      "epoch: 133\n",
      "training loss: 31013495012669.66\n",
      "validation loss: 21703649382769.74\n",
      "epoch: 134\n",
      "training loss: 31008021798673.973\n",
      "validation loss: 21701736186800.99\n",
      "epoch: 135\n",
      "training loss: 31001329015900.484\n",
      "validation loss: 21700521488077.496\n",
      "epoch: 136\n",
      "training loss: 30995745558945.902\n",
      "validation loss: 21699121405125.977\n",
      "epoch: 137\n",
      "training loss: 30990195399987.94\n",
      "validation loss: 21697766165449.03\n",
      "epoch: 138\n",
      "training loss: 30987364288482.77\n",
      "validation loss: 21700586239776.14\n",
      "epoch: 139\n",
      "training loss: 30982020475418.363\n",
      "validation loss: 21699055953918.73\n",
      "epoch: 140\n",
      "training loss: 30976688618964.113\n",
      "validation loss: 21697572415194.297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 141\n",
      "training loss: 30971368663573.57\n",
      "validation loss: 21696133918986.16\n",
      "epoch: 142\n",
      "training loss: 30966049810965.8\n",
      "validation loss: 21694738830349.457\n",
      "epoch: 143\n",
      "training loss: 30960753506092.867\n",
      "validation loss: 21693385455856.492\n",
      "epoch: 144\n",
      "training loss: 30955468951381.504\n",
      "validation loss: 21692072378248.9\n",
      "epoch: 145\n",
      "training loss: 30950196094439.523\n",
      "validation loss: 21690798189505.125\n",
      "epoch: 146\n",
      "training loss: 30944934887905.555\n",
      "validation loss: 21689561502589.137\n",
      "epoch: 147\n",
      "training loss: 30939845402521.43\n",
      "validation loss: 21688360985983.84\n",
      "epoch: 148\n",
      "training loss: 30934623131792.45\n",
      "validation loss: 21687196785926.973\n",
      "epoch: 149\n",
      "training loss: 30929396971067.574\n",
      "validation loss: 21686066039602.65\n",
      "epoch: 150\n",
      "training loss: 30924123119877.496\n",
      "validation loss: 21684967726530.67\n",
      "epoch: 151\n",
      "training loss: 30918919851778.926\n",
      "validation loss: 21683899992902.723\n",
      "epoch: 152\n",
      "training loss: 30913727976853.73\n",
      "validation loss: 21682862502367.844\n",
      "epoch: 153\n",
      "training loss: 30908547456776.227\n",
      "validation loss: 21681854232947.258\n",
      "epoch: 154\n",
      "training loss: 30903378253058.46\n",
      "validation loss: 21680874184364.664\n",
      "epoch: 155\n",
      "training loss: 30898220327934.285\n",
      "validation loss: 21679921396058.242\n",
      "epoch: 156\n",
      "training loss: 30893073644306.324\n",
      "validation loss: 21678994945545.613\n",
      "epoch: 157\n",
      "training loss: 30887938165697.32\n",
      "validation loss: 21678093946857.207\n",
      "epoch: 158\n",
      "training loss: 30882813856205.492\n",
      "validation loss: 21677217549035.027\n",
      "epoch: 159\n",
      "training loss: 30877700680463.5\n",
      "validation loss: 21676364934694.234\n",
      "epoch: 160\n",
      "training loss: 30872598603600.742\n",
      "validation loss: 21675535318644.55\n",
      "epoch: 161\n",
      "training loss: 30867507591208.832\n",
      "validation loss: 21674727946569.277\n",
      "epoch: 162\n",
      "training loss: 30862427609309.832\n",
      "validation loss: 21673942093759.227\n",
      "epoch: 163\n",
      "training loss: 30857358624327.1\n",
      "validation loss: 21673177063899.375\n",
      "epoch: 164\n",
      "training loss: 30852300603058.586\n",
      "validation loss: 21672432187905.92\n",
      "epoch: 165\n",
      "training loss: 30847253512652.234\n",
      "validation loss: 21671706822811.72\n",
      "epoch: 166\n",
      "training loss: 30842217320583.504\n",
      "validation loss: 21671000350697.906\n",
      "epoch: 167\n",
      "training loss: 30837191994634.65\n",
      "validation loss: 21670312177669.914\n",
      "epoch: 168\n",
      "training loss: 30832177502875.77\n",
      "validation loss: 21669641732875.875\n",
      "epoch: 169\n",
      "training loss: 30827173813647.4\n",
      "validation loss: 21668988467565.742\n",
      "epoch: 170\n",
      "training loss: 30822180895544.547\n",
      "validation loss: 21668351854189.332\n",
      "epoch: 171\n",
      "training loss: 30817198717402.027\n",
      "validation loss: 21667731385531.707\n",
      "epoch: 172\n",
      "training loss: 30812227248281.008\n",
      "validation loss: 21667126573884.277\n",
      "epoch: 173\n",
      "training loss: 30807266457456.695\n",
      "validation loss: 21666536950250.17\n",
      "epoch: 174\n",
      "training loss: 30802316314407.035\n",
      "validation loss: 21665962063582.477\n",
      "epoch: 175\n",
      "training loss: 30797376788802.28\n",
      "validation loss: 21665401480053.87\n",
      "epoch: 176\n",
      "training loss: 30792447850495.523\n",
      "validation loss: 21664854782356.375\n",
      "epoch: 177\n",
      "training loss: 30787529469513.95\n",
      "validation loss: 21664321569030.06\n",
      "epoch: 178\n",
      "training loss: 30782621616050.812\n",
      "validation loss: 21663801453819.348\n",
      "epoch: 179\n",
      "training loss: 30777724260458.133\n",
      "validation loss: 21663294065055.875\n",
      "epoch: 180\n",
      "training loss: 30772837373239.887\n",
      "validation loss: 21662799045066.734\n",
      "epoch: 181\n",
      "training loss: 30767960925045.91\n",
      "validation loss: 21662316049607.145\n",
      "epoch: 182\n",
      "training loss: 30763094886666.168\n",
      "validation loss: 21661844747316.4\n",
      "epoch: 183\n",
      "training loss: 30758239229025.582\n",
      "validation loss: 21661384819196.168\n",
      "epoch: 184\n",
      "training loss: 30753393923179.24\n",
      "validation loss: 21660935958110.36\n",
      "epoch: 185\n",
      "training loss: 30748558940308.07\n",
      "validation loss: 21660497868305.402\n",
      "epoch: 186\n",
      "training loss: 30743734251714.758\n",
      "validation loss: 21660070264950.336\n",
      "epoch: 187\n",
      "training loss: 30738919828820.1\n",
      "validation loss: 21659652873695.707\n",
      "epoch: 188\n",
      "training loss: 30734115643159.617\n",
      "validation loss: 21659245430250.652\n",
      "epoch: 189\n",
      "training loss: 30729321666380.45\n",
      "validation loss: 21658847679977.273\n",
      "epoch: 190\n",
      "training loss: 30724537870238.48\n",
      "validation loss: 21658459377501.66\n",
      "epoch: 191\n",
      "training loss: 30719764226595.754\n",
      "validation loss: 21658080286340.895\n",
      "epoch: 192\n",
      "training loss: 30715000707418.066\n",
      "validation loss: 21657710178545.32\n",
      "epoch: 193\n",
      "training loss: 30710247284772.71\n",
      "validation loss: 21657348834355.45\n",
      "epoch: 194\n",
      "training loss: 30705503930826.496\n",
      "validation loss: 21656996041873.0\n",
      "epoch: 195\n",
      "training loss: 30700770617843.844\n",
      "validation loss: 21656651596745.312\n",
      "epoch: 196\n",
      "training loss: 30696047318185.066\n",
      "validation loss: 21656315301862.74\n",
      "epoch: 197\n",
      "training loss: 30691334004304.81\n",
      "validation loss: 21655986967068.414\n",
      "epoch: 198\n",
      "training loss: 30686630648750.54\n",
      "validation loss: 21655666408879.9\n",
      "epoch: 199\n",
      "training loss: 30681937224161.26\n",
      "validation loss: 21655353450222.176\n",
      "epoch: 200\n",
      "training loss: 30677253703266.195\n",
      "validation loss: 21655047920171.65\n",
      "epoch: 201\n",
      "training loss: 30672580058883.715\n",
      "validation loss: 21654749653710.473\n",
      "epoch: 202\n",
      "training loss: 30667916263920.188\n",
      "validation loss: 21654458491491.062\n",
      "epoch: 203\n",
      "training loss: 30663262291369.07\n",
      "validation loss: 21654174279610.137\n",
      "epoch: 204\n",
      "training loss: 30658618114309.96\n",
      "validation loss: 21653896869392.02\n",
      "epoch: 205\n",
      "training loss: 30653983705907.742\n",
      "validation loss: 21653626117180.766\n",
      "epoch: 206\n",
      "training loss: 30649359039411.797\n",
      "validation loss: 21653361884140.83\n",
      "epoch: 207\n",
      "training loss: 30644744088155.32\n",
      "validation loss: 21653104036065.8\n",
      "epoch: 208\n",
      "training loss: 30640138825554.535\n",
      "validation loss: 21652852443194.98\n",
      "epoch: 209\n",
      "training loss: 30635543225108.176\n",
      "validation loss: 21652606980037.43\n",
      "epoch: 210\n",
      "training loss: 30630957260396.777\n",
      "validation loss: 21652367525203.2\n",
      "epoch: 211\n",
      "training loss: 30626380905082.195\n",
      "validation loss: 21652133961241.44\n",
      "epoch: 212\n",
      "training loss: 30621814132907.035\n",
      "validation loss: 21651906174485.07\n",
      "epoch: 213\n",
      "training loss: 30617256917694.164\n",
      "validation loss: 21651684054901.85\n",
      "epoch: 214\n",
      "training loss: 30612709233346.25\n",
      "validation loss: 21651467495951.453\n",
      "epoch: 215\n",
      "training loss: 30608171053845.312\n",
      "validation loss: 21651256394448.387\n",
      "epoch: 216\n",
      "training loss: 30603642353252.293\n",
      "validation loss: 21651050650430.465\n",
      "epoch: 217\n",
      "training loss: 30599123105706.668\n",
      "validation loss: 21650850167032.71\n",
      "epoch: 218\n",
      "training loss: 30594613285426.066\n",
      "validation loss: 21650654850366.26\n",
      "epoch: 219\n",
      "training loss: 30590112866705.895\n",
      "validation loss: 21650464609402.336\n",
      "epoch: 220\n",
      "training loss: 30585621823918.996\n",
      "validation loss: 21650279355860.816\n",
      "epoch: 221\n",
      "training loss: 30581140131515.305\n",
      "validation loss: 21650099004103.402\n",
      "epoch: 222\n",
      "training loss: 30576667764021.55\n",
      "validation loss: 21649923471031.113\n",
      "epoch: 223\n",
      "training loss: 30572204696040.906\n",
      "validation loss: 21649752675985.906\n",
      "epoch: 224\n",
      "training loss: 30567750902252.71\n",
      "validation loss: 21649586540656.38\n",
      "epoch: 225\n",
      "training loss: 30563306357412.184\n",
      "validation loss: 21649424988987.2\n",
      "epoch: 226\n",
      "training loss: 30558871036350.125\n",
      "validation loss: 21649267947092.266\n",
      "epoch: 227\n",
      "training loss: 30554444913972.637\n",
      "validation loss: 21649115343171.402\n",
      "epoch: 228\n",
      "training loss: 30550027965260.87\n",
      "validation loss: 21648967107430.367\n",
      "epoch: 229\n",
      "training loss: 30545620165270.74\n",
      "validation loss: 21648823172004.176\n",
      "epoch: 230\n",
      "training loss: 30541221489132.67\n",
      "validation loss: 21648683470883.516\n",
      "epoch: 231\n",
      "training loss: 30536831912051.387\n",
      "validation loss: 21648547939844.062\n",
      "epoch: 232\n",
      "training loss: 30532451409305.57\n",
      "validation loss: 21648416516378.805\n",
      "epoch: 233\n",
      "training loss: 30528079956247.688\n",
      "validation loss: 21648289139632.965\n",
      "epoch: 234\n",
      "training loss: 30523717528303.746\n",
      "validation loss: 21648165750341.625\n",
      "epoch: 235\n",
      "training loss: 30519364100973.01\n",
      "validation loss: 21648046290769.887\n",
      "epoch: 236\n",
      "training loss: 30515019649827.793\n",
      "validation loss: 21647930704655.4\n",
      "epoch: 237\n",
      "training loss: 30510684150513.215\n",
      "validation loss: 21647818937153.23\n",
      "epoch: 238\n",
      "training loss: 30506357578746.977\n",
      "validation loss: 21647710934783.004\n",
      "epoch: 239\n",
      "training loss: 30502039910319.117\n",
      "validation loss: 21647606645378.1\n",
      "epoch: 240\n",
      "training loss: 30497731121091.805\n",
      "validation loss: 21647506018036.957\n",
      "epoch: 241\n",
      "training loss: 30493431186999.074\n",
      "validation loss: 21647409003076.332\n",
      "epoch: 242\n",
      "training loss: 30489140084046.645\n",
      "validation loss: 21647315551986.395\n",
      "epoch: 243\n",
      "training loss: 30484857788311.637\n",
      "validation loss: 21647225617387.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 244\n",
      "training loss: 30480584275942.418\n",
      "validation loss: 21647139152989.91\n",
      "epoch: 245\n",
      "training loss: 30476319523158.33\n",
      "validation loss: 21647056113551.977\n",
      "epoch: 246\n",
      "training loss: 30472063506249.453\n",
      "validation loss: 21646976454844.227\n",
      "epoch: 247\n",
      "training loss: 30467816201576.453\n",
      "validation loss: 21646900133611.79\n",
      "epoch: 248\n",
      "training loss: 30463577585570.273\n",
      "validation loss: 21646827107539.46\n",
      "epoch: 249\n",
      "training loss: 30459347634731.992\n",
      "validation loss: 21646757335218.105\n",
      "epoch: 250\n",
      "training loss: 30455126325632.53\n",
      "validation loss: 21646690776112.33\n",
      "epoch: 251\n",
      "training loss: 30450913634912.5\n",
      "validation loss: 21646627390529.434\n",
      "epoch: 252\n",
      "training loss: 30446709539281.945\n",
      "validation loss: 21646567139589.664\n",
      "epoch: 253\n",
      "training loss: 30442514015520.117\n",
      "validation loss: 21646509985197.625\n",
      "epoch: 254\n",
      "training loss: 30438327040475.266\n",
      "validation loss: 21646455890014.82\n",
      "epoch: 255\n",
      "training loss: 30434148591064.44\n",
      "validation loss: 21646404817433.34\n",
      "epoch: 256\n",
      "training loss: 30429978644273.242\n",
      "validation loss: 21646356731550.527\n",
      "epoch: 257\n",
      "training loss: 30425817177155.617\n",
      "validation loss: 21646311597144.74\n",
      "epoch: 258\n",
      "training loss: 30421664166833.645\n",
      "validation loss: 21646269379651.98\n",
      "epoch: 259\n",
      "training loss: 30417519590497.32\n",
      "validation loss: 21646230045143.56\n",
      "epoch: 260\n",
      "training loss: 30413383425404.316\n",
      "validation loss: 21646193560304.555\n",
      "epoch: 261\n",
      "training loss: 30409255648879.812\n",
      "validation loss: 21646159892413.215\n",
      "epoch: 262\n",
      "training loss: 30405136238316.2\n",
      "validation loss: 21646129009321.09\n",
      "epoch: 263\n",
      "training loss: 30401025171172.957\n",
      "validation loss: 21646100879434.027\n",
      "epoch: 264\n",
      "training loss: 30396922424976.383\n",
      "validation loss: 21646075471693.87\n",
      "epoch: 265\n",
      "training loss: 30392827977319.363\n",
      "validation loss: 21646052755560.91\n",
      "epoch: 266\n",
      "training loss: 30388741805861.21\n",
      "validation loss: 21646032700997.02\n",
      "epoch: 267\n",
      "training loss: 30384663888327.414\n",
      "validation loss: 21646015278449.438\n",
      "epoch: 268\n",
      "training loss: 30380594202509.41\n",
      "validation loss: 21646000458835.24\n",
      "epoch: 269\n",
      "training loss: 30376532726264.414\n",
      "validation loss: 21645988213526.36\n",
      "epoch: 270\n",
      "training loss: 30372479437515.156\n",
      "validation loss: 21645978514335.258\n",
      "epoch: 271\n",
      "training loss: 30368434314249.703\n",
      "validation loss: 21645971333501.11\n",
      "epoch: 272\n",
      "training loss: 30364397334521.23\n",
      "validation loss: 21645966643676.543\n",
      "epoch: 273\n",
      "training loss: 30360368476447.793\n",
      "validation loss: 21645964417914.92\n",
      "epoch: 274\n",
      "training loss: 30356347718212.12\n",
      "validation loss: 21645964629658.105\n",
      "epoch: 275\n",
      "training loss: 30352335038061.375\n",
      "validation loss: 21645967252724.676\n",
      "epoch: 276\n",
      "training loss: 30348330414306.785\n",
      "validation loss: 21645972261298.652\n",
      "epoch: 277\n",
      "training loss: 30344333825322.887\n",
      "validation loss: 21645979629918.62\n",
      "epoch: 278\n",
      "training loss: 30340345249541.582\n",
      "validation loss: 21645989333467.285\n",
      "epoch: 279\n",
      "training loss: 30336364664958.64\n",
      "validation loss: 21646001347161.285\n",
      "epoch: 280\n",
      "training loss: 30525341685433.062\n",
      "validation loss: 21269413743710.938\n",
      "epoch: 281\n",
      "training loss: 30520339428152.145\n",
      "validation loss: 21281469368548.72\n",
      "epoch: 282\n",
      "training loss: 30515422050819.47\n",
      "validation loss: 21292981100191.17\n",
      "epoch: 283\n",
      "training loss: 30510583349253.133\n",
      "validation loss: 21303969365756.87\n",
      "epoch: 284\n",
      "training loss: 30505817614958.715\n",
      "validation loss: 21314453926362.32\n",
      "epoch: 285\n",
      "training loss: 30501119595338.367\n",
      "validation loss: 21324453890281.56\n",
      "epoch: 286\n",
      "training loss: 30496484457097.617\n",
      "validation loss: 21333987726719.332\n",
      "epoch: 287\n",
      "training loss: 30491907752592.98\n",
      "validation loss: 21343073280077.32\n",
      "epoch: 288\n",
      "training loss: 30487385388883.793\n",
      "validation loss: 21351727784607.266\n",
      "epoch: 289\n",
      "training loss: 30482913599270.91\n",
      "validation loss: 21359967879356.695\n",
      "epoch: 290\n",
      "training loss: 30478488917122.297\n",
      "validation loss: 21367809623322.85\n",
      "epoch: 291\n",
      "training loss: 30474108151801.555\n",
      "validation loss: 21375268510739.77\n",
      "epoch: 292\n",
      "training loss: 30469768366530.47\n",
      "validation loss: 21382359486431.883\n",
      "epoch: 293\n",
      "training loss: 30465466858029.918\n",
      "validation loss: 21389096961175.367\n",
      "epoch: 294\n",
      "training loss: 30461201137796.26\n",
      "validation loss: 21395494827015.06\n",
      "epoch: 295\n",
      "training loss: 30456968914881.723\n",
      "validation loss: 21401566472491.477\n",
      "epoch: 296\n",
      "training loss: 30452768080057.867\n",
      "validation loss: 21407324797737.887\n",
      "epoch: 297\n",
      "training loss: 30448596691250.92\n",
      "validation loss: 21412782229412.64\n",
      "epoch: 298\n",
      "training loss: 30444452960146.754\n",
      "validation loss: 21417950735436.6\n",
      "epoch: 299\n",
      "training loss: 30440335239871.566\n",
      "validation loss: 21422841839509.91\n",
      "epoch: 300\n",
      "training loss: 30436242013661.67\n",
      "validation loss: 21427466635385.83\n",
      "epoch: 301\n",
      "training loss: 30432171884442.973\n",
      "validation loss: 21431835800883.06\n",
      "epoch: 302\n",
      "training loss: 30428123565247.027\n",
      "validation loss: 21435959611620.91\n",
      "epoch: 303\n",
      "training loss: 30424095870396.445\n",
      "validation loss: 21439847954464.535\n",
      "epoch: 304\n",
      "training loss: 30420087707397.742\n",
      "validation loss: 21443510340669.812\n",
      "epoch: 305\n",
      "training loss: 30416098069484.89\n",
      "validation loss: 21446955918719.83\n",
      "epoch: 306\n",
      "training loss: 30412126028761.246\n",
      "validation loss: 21450193486846.773\n",
      "epoch: 307\n",
      "training loss: 30408170729891.684\n",
      "validation loss: 21453231505234.953\n",
      "epoch: 308\n",
      "training loss: 30404231384300.92\n",
      "validation loss: 21456078107902.03\n",
      "epoch: 309\n",
      "training loss: 30400307264837.16\n",
      "validation loss: 21458741114257.195\n",
      "epoch: 310\n",
      "training loss: 30396397700863.832\n",
      "validation loss: 21461228040336.117\n",
      "epoch: 311\n",
      "training loss: 30392502073744.9\n",
      "validation loss: 21463546109713.55\n",
      "epoch: 312\n",
      "training loss: 30388619812692.293\n",
      "validation loss: 21465702264095.6\n",
      "epoch: 313\n",
      "training loss: 30384750390946.16\n",
      "validation loss: 21467703173594.25\n",
      "epoch: 314\n",
      "training loss: 30380893322261.473\n",
      "validation loss: 21469555246687.84\n",
      "epoch: 315\n",
      "training loss: 30377048157676.062\n",
      "validation loss: 21471264639871.31\n",
      "epoch: 316\n",
      "training loss: 30373214482537.867\n",
      "validation loss: 21472837267001.22\n",
      "epoch: 317\n",
      "training loss: 30369391913770.184\n",
      "validation loss: 21474278808340.3\n",
      "epoch: 318\n",
      "training loss: 30365580097356.15\n",
      "validation loss: 21475594719307.31\n",
      "epoch: 319\n",
      "training loss: 30361778706024.594\n",
      "validation loss: 21476790238937.906\n",
      "epoch: 320\n",
      "training loss: 30357987437121.324\n",
      "validation loss: 21477870398062.78\n",
      "epoch: 321\n",
      "training loss: 30354206010650.668\n",
      "validation loss: 21478840027209.37\n",
      "epoch: 322\n",
      "training loss: 30350434167473.81\n",
      "validation loss: 21479703764234.137\n",
      "epoch: 323\n",
      "training loss: 30346671667650.42\n",
      "validation loss: 21480466061694.87\n",
      "epoch: 324\n",
      "training loss: 30342918288901.37\n",
      "validation loss: 21481131194002.113\n",
      "epoch: 325\n",
      "training loss: 30339173814754.04\n",
      "validation loss: 21481703270382.715\n",
      "epoch: 326\n",
      "training loss: 30335784977050.29\n",
      "validation loss: 21483194244995.2\n",
      "epoch: 327\n",
      "training loss: 30332060812442.977\n",
      "validation loss: 21483573496848.676\n",
      "epoch: 328\n",
      "training loss: 30328344969775.83\n",
      "validation loss: 21483872090451.45\n",
      "epoch: 329\n",
      "training loss: 30324249138874.195\n",
      "validation loss: 21482488180143.746\n",
      "epoch: 330\n",
      "training loss: 30320546151790.566\n",
      "validation loss: 21482646882340.926\n",
      "epoch: 331\n",
      "training loss: 30316851119703.445\n",
      "validation loss: 21482733914105.24\n",
      "Mean absolute error: $6106309475.29\n",
      "Nodes: 100\n",
      "Learning Rate: 1e-08\n",
      "epoch: 0\n",
      "training loss: 37140252900165.1\n",
      "validation loss: 33501402157968.57\n",
      "epoch: 1\n",
      "training loss: 37105165371080.39\n",
      "validation loss: 33455815538286.797\n",
      "epoch: 2\n",
      "training loss: 37084287597175.94\n",
      "validation loss: 33408353384603.574\n",
      "epoch: 3\n",
      "training loss: 37025280603251.83\n",
      "validation loss: 33337493561233.926\n",
      "epoch: 4\n",
      "training loss: 37005289720417.55\n",
      "validation loss: 33347945649603.742\n",
      "epoch: 5\n",
      "training loss: 36962529719719.3\n",
      "validation loss: 33260056682663.93\n",
      "epoch: 6\n",
      "training loss: 36911779629759.85\n",
      "validation loss: 33141614347990.637\n",
      "epoch: 7\n",
      "training loss: 36824628508310.05\n",
      "validation loss: 32959139569644.258\n",
      "epoch: 8\n",
      "training loss: 36796784911825.48\n",
      "validation loss: 33005469095567.016\n",
      "epoch: 9\n",
      "training loss: 36750310705155.555\n",
      "validation loss: 32919733143124.914\n",
      "epoch: 10\n",
      "training loss: 36708717926189.17\n",
      "validation loss: 32913707241656.25\n",
      "epoch: 11\n",
      "training loss: 36642498883935.445\n",
      "validation loss: 32757107004382.633\n",
      "epoch: 12\n",
      "training loss: 36626755642191.445\n",
      "validation loss: 32783268257314.324\n",
      "epoch: 13\n",
      "training loss: 36634069230155.695\n",
      "validation loss: 32702667911588.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "training loss: 36567329597677.5\n",
      "validation loss: 32559069197861.77\n",
      "epoch: 15\n",
      "training loss: 36524298858638.87\n",
      "validation loss: 32473171754310.312\n",
      "epoch: 16\n",
      "training loss: 36480452653588.47\n",
      "validation loss: 32381537802371.867\n",
      "epoch: 17\n",
      "training loss: 36380891056900.81\n",
      "validation loss: 32258885383143.312\n",
      "epoch: 18\n",
      "training loss: 36352474184665.88\n",
      "validation loss: 32274984759784.734\n",
      "epoch: 19\n",
      "training loss: 36303593926635.97\n",
      "validation loss: 32172729446048.48\n",
      "epoch: 20\n",
      "training loss: 36254020616267.69\n",
      "validation loss: 32045693449755.52\n",
      "epoch: 21\n",
      "training loss: 36212378880314.18\n",
      "validation loss: 31942194672361.64\n",
      "epoch: 22\n",
      "training loss: 36163026346805.66\n",
      "validation loss: 32204391530961.547\n",
      "epoch: 23\n",
      "training loss: 36162710300194.55\n",
      "validation loss: 32081771248708.492\n",
      "epoch: 24\n",
      "training loss: 36119821702010.01\n",
      "validation loss: 31934137705852.586\n",
      "epoch: 25\n",
      "training loss: 36116181235817.234\n",
      "validation loss: 32062301993271.8\n",
      "epoch: 26\n",
      "training loss: 36161736599285.74\n",
      "validation loss: 31891689295606.03\n",
      "epoch: 27\n",
      "training loss: 36053771092664.914\n",
      "validation loss: 31857585561275.277\n",
      "epoch: 28\n",
      "training loss: 36088016503462.35\n",
      "validation loss: 31758346928907.33\n",
      "epoch: 29\n",
      "training loss: 36020915169560.414\n",
      "validation loss: 31890216756756.53\n",
      "epoch: 30\n",
      "training loss: 36014096295828.41\n",
      "validation loss: 31610322895019.863\n",
      "epoch: 31\n",
      "training loss: 35999806748089.06\n",
      "validation loss: 31519505278644.258\n",
      "epoch: 32\n",
      "training loss: 36065721946630.0\n",
      "validation loss: 31766083262450.664\n",
      "epoch: 33\n",
      "training loss: 36019247464834.8\n",
      "validation loss: 31646098528052.105\n",
      "epoch: 34\n",
      "training loss: 35947653266451.58\n",
      "validation loss: 31447344074958.293\n",
      "epoch: 35\n",
      "training loss: 35947409267304.3\n",
      "validation loss: 31447560096410.934\n",
      "epoch: 36\n",
      "training loss: 35838539386955.73\n",
      "validation loss: 31195806823652.56\n",
      "epoch: 37\n",
      "training loss: 35782078276611.695\n",
      "validation loss: 31036744155531.906\n",
      "epoch: 38\n",
      "training loss: 35697878354324.3\n",
      "validation loss: 30902413620792.438\n",
      "epoch: 39\n",
      "training loss: 35676817999134.82\n",
      "validation loss: 30971012939183.28\n",
      "epoch: 40\n",
      "training loss: 35600497756182.664\n",
      "validation loss: 30903151543919.918\n",
      "epoch: 41\n",
      "training loss: 35550961616958.26\n",
      "validation loss: 30735429088202.402\n",
      "epoch: 42\n",
      "training loss: 35523577144726.89\n",
      "validation loss: 30665042535549.918\n",
      "epoch: 43\n",
      "training loss: 35496221313808.6\n",
      "validation loss: 30586715466555.895\n",
      "epoch: 44\n",
      "training loss: 35491599255206.33\n",
      "validation loss: 30523407446274.2\n",
      "epoch: 45\n",
      "training loss: 35425791445941.125\n",
      "validation loss: 30480445590003.617\n",
      "epoch: 46\n",
      "training loss: 35284678706206.914\n",
      "validation loss: 30295574844837.95\n",
      "epoch: 47\n",
      "training loss: 35265353703491.6\n",
      "validation loss: 30363861298268.35\n",
      "epoch: 48\n",
      "training loss: 35284269780845.03\n",
      "validation loss: 30435254180218.656\n",
      "epoch: 49\n",
      "training loss: 35281678439278.04\n",
      "validation loss: 30273824510128.816\n",
      "epoch: 50\n",
      "training loss: 35299485653025.016\n",
      "validation loss: 30259415987723.74\n",
      "epoch: 51\n",
      "training loss: 35265293383155.805\n",
      "validation loss: 30118218394610.18\n",
      "epoch: 52\n",
      "training loss: 35203220825145.875\n",
      "validation loss: 30085837721854.234\n",
      "epoch: 53\n",
      "training loss: 35196355135279.64\n",
      "validation loss: 30069496155365.832\n",
      "epoch: 54\n",
      "training loss: 35158274367063.215\n",
      "validation loss: 30001634810447.695\n",
      "epoch: 55\n",
      "training loss: 35143472613470.613\n",
      "validation loss: 29926293772171.293\n",
      "epoch: 56\n",
      "training loss: 35103140427792.49\n",
      "validation loss: 29865919478569.383\n",
      "epoch: 57\n",
      "training loss: 35068226767009.305\n",
      "validation loss: 29808228182367.438\n",
      "epoch: 58\n",
      "training loss: 35020462721011.81\n",
      "validation loss: 29747051777913.504\n",
      "epoch: 59\n",
      "training loss: 34985711520530.207\n",
      "validation loss: 29689795647975.77\n",
      "epoch: 60\n",
      "training loss: 34969168810655.668\n",
      "validation loss: 29634411894802.05\n",
      "epoch: 61\n",
      "training loss: 34958216863561.97\n",
      "validation loss: 29584293507926.71\n",
      "epoch: 62\n",
      "training loss: 34925077935395.684\n",
      "validation loss: 29531408752340.76\n",
      "epoch: 63\n",
      "training loss: 34884849811329.414\n",
      "validation loss: 29496609276110.297\n",
      "epoch: 64\n",
      "training loss: 34851959987333.355\n",
      "validation loss: 29442141868216.18\n",
      "epoch: 65\n",
      "training loss: 34819261691997.676\n",
      "validation loss: 29387917294601.312\n",
      "epoch: 66\n",
      "training loss: 34786753732002.586\n",
      "validation loss: 29333939170975.875\n",
      "epoch: 67\n",
      "training loss: 34754347423197.113\n",
      "validation loss: 29279085477768.13\n",
      "epoch: 68\n",
      "training loss: 34723149013094.04\n",
      "validation loss: 29236891834382.9\n",
      "epoch: 69\n",
      "training loss: 34691223233534.54\n",
      "validation loss: 29183755345882.797\n",
      "epoch: 70\n",
      "training loss: 34659334730771.527\n",
      "validation loss: 29131135830262.223\n",
      "epoch: 71\n",
      "training loss: 34627774804059.234\n",
      "validation loss: 29078508803765.293\n",
      "epoch: 72\n",
      "training loss: 34596397669252.625\n",
      "validation loss: 29026119120643.336\n",
      "epoch: 73\n",
      "training loss: 34565202085767.812\n",
      "validation loss: 28973965544388.875\n",
      "epoch: 74\n",
      "training loss: 34522660408388.23\n",
      "validation loss: 28912011091665.63\n",
      "epoch: 75\n",
      "training loss: 34491522954989.59\n",
      "validation loss: 28859969992013.727\n",
      "epoch: 76\n",
      "training loss: 34461985262833.06\n",
      "validation loss: 28839473887230.09\n",
      "epoch: 77\n",
      "training loss: 34431707709381.504\n",
      "validation loss: 28788815046520.965\n",
      "epoch: 78\n",
      "training loss: 34397039620818.996\n",
      "validation loss: 28727655761402.875\n",
      "epoch: 79\n",
      "training loss: 34366776062833.344\n",
      "validation loss: 28676801430847.855\n",
      "epoch: 80\n",
      "training loss: 34336689501616.97\n",
      "validation loss: 28626178396096.332\n",
      "epoch: 81\n",
      "training loss: 34306779223552.44\n",
      "validation loss: 28575785455288.3\n",
      "epoch: 82\n",
      "training loss: 34277043820377.453\n",
      "validation loss: 28525621381983.066\n",
      "epoch: 83\n",
      "training loss: 34247482122830.08\n",
      "validation loss: 28475684972621.08\n",
      "epoch: 84\n",
      "training loss: 34218049479490.062\n",
      "validation loss: 28425984592605.906\n",
      "epoch: 85\n",
      "training loss: 34187719886047.098\n",
      "validation loss: 28365720490541.277\n",
      "epoch: 86\n",
      "training loss: 34158599225492.184\n",
      "validation loss: 28315973173356.02\n",
      "epoch: 87\n",
      "training loss: 34129699821327.61\n",
      "validation loss: 28266812209802.805\n",
      "epoch: 88\n",
      "training loss: 34100968653795.426\n",
      "validation loss: 28217873915698.41\n",
      "epoch: 89\n",
      "training loss: 34072404601568.92\n",
      "validation loss: 28169157149627.723\n",
      "epoch: 90\n",
      "training loss: 34044006546592.445\n",
      "validation loss: 28120660847255.33\n",
      "epoch: 91\n",
      "training loss: 34015199985160.973\n",
      "validation loss: 28072246029758.223\n",
      "epoch: 92\n",
      "training loss: 33987122676689.33\n",
      "validation loss: 28024173326864.06\n",
      "epoch: 93\n",
      "training loss: 33959208087065.59\n",
      "validation loss: 27976317581161.594\n",
      "epoch: 94\n",
      "training loss: 33931455134132.39\n",
      "validation loss: 27928677634766.03\n",
      "epoch: 95\n",
      "training loss: 33903862743359.94\n",
      "validation loss: 27881252314191.023\n",
      "epoch: 96\n",
      "training loss: 33876429847791.24\n",
      "validation loss: 27834040390899.723\n",
      "epoch: 97\n",
      "training loss: 33849155387985.797\n",
      "validation loss: 27787040442508.723\n",
      "epoch: 98\n",
      "training loss: 33822038311955.695\n",
      "validation loss: 27740250125376.098\n",
      "epoch: 99\n",
      "training loss: 33795077574980.965\n",
      "validation loss: 27693659091429.832\n",
      "epoch: 100\n",
      "training loss: 33768272016377.06\n",
      "validation loss: 27647228863247.63\n",
      "epoch: 101\n",
      "training loss: 33737902465520.695\n",
      "validation loss: 27590889717767.492\n",
      "epoch: 102\n",
      "training loss: 33706550177113.88\n",
      "validation loss: 27506264275136.926\n",
      "epoch: 103\n",
      "training loss: 33713884931907.023\n",
      "validation loss: 27478689202396.477\n",
      "epoch: 104\n",
      "training loss: 33687622614330.703\n",
      "validation loss: 27433111431400.504\n",
      "epoch: 105\n",
      "training loss: 33660639691142.54\n",
      "validation loss: 27387111636578.715\n",
      "epoch: 106\n",
      "training loss: 33634711758332.02\n",
      "validation loss: 27341803244401.242\n",
      "epoch: 107\n",
      "training loss: 33602114980438.082\n",
      "validation loss: 27290880328067.652\n",
      "epoch: 108\n",
      "training loss: 33576440624620.523\n",
      "validation loss: 27246022983638.465\n",
      "epoch: 109\n",
      "training loss: 33550911136184.875\n",
      "validation loss: 27201364190912.285\n",
      "epoch: 110\n",
      "training loss: 33525525571628.516\n",
      "validation loss: 27156902839640.44\n",
      "epoch: 111\n",
      "training loss: 33500282994644.63\n",
      "validation loss: 27112637126780.58\n",
      "epoch: 112\n",
      "training loss: 33475223715589.043\n",
      "validation loss: 27069077989896.223\n",
      "epoch: 113\n",
      "training loss: 33450264916702.367\n",
      "validation loss: 27025207514085.51\n",
      "epoch: 114\n",
      "training loss: 33425446330790.426\n",
      "validation loss: 26981530606357.094\n",
      "epoch: 115\n",
      "training loss: 33400767047380.348\n",
      "validation loss: 26938046283250.402\n",
      "epoch: 116\n",
      "training loss: 33376226162297.71\n",
      "validation loss: 26894753566862.203\n",
      "epoch: 117\n",
      "training loss: 33351822777464.17\n",
      "validation loss: 26851651484811.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 118\n",
      "training loss: 33327555832310.586\n",
      "validation loss: 26808739070195.223\n",
      "epoch: 119\n",
      "training loss: 33304284936068.83\n",
      "validation loss: 26768378690111.594\n",
      "epoch: 120\n",
      "training loss: 33280174535918.207\n",
      "validation loss: 26725738368764.258\n",
      "epoch: 121\n",
      "training loss: 33256319636533.13\n",
      "validation loss: 26683401287000.31\n",
      "epoch: 122\n",
      "training loss: 33232977805840.965\n",
      "validation loss: 26642801710207.918\n",
      "epoch: 123\n",
      "training loss: 33208571313206.46\n",
      "validation loss: 26600743895630.887\n",
      "epoch: 124\n",
      "training loss: 33174894213141.277\n",
      "validation loss: 26531616550806.668\n",
      "epoch: 125\n",
      "training loss: 33149864621647.754\n",
      "validation loss: 26481430768715.08\n",
      "epoch: 126\n",
      "training loss: 33127398576013.65\n",
      "validation loss: 26444712356650.895\n",
      "epoch: 127\n",
      "training loss: 33104197750098.297\n",
      "validation loss: 26403250777706.258\n",
      "epoch: 128\n",
      "training loss: 33081126259072.473\n",
      "validation loss: 26361972435285.656\n",
      "epoch: 129\n",
      "training loss: 33058183265835.535\n",
      "validation loss: 26320875804065.473\n",
      "epoch: 130\n",
      "training loss: 33035367939178.37\n",
      "validation loss: 26279959963258.89\n",
      "epoch: 131\n",
      "training loss: 33012679453647.945\n",
      "validation loss: 26239223997263.46\n",
      "epoch: 132\n",
      "training loss: 32990116989501.957\n",
      "validation loss: 26198666995628.84\n",
      "epoch: 133\n",
      "training loss: 32967679732627.133\n",
      "validation loss: 26158288053024.63\n",
      "epoch: 134\n",
      "training loss: 32945366862742.14\n",
      "validation loss: 26118086269206.273\n",
      "epoch: 135\n",
      "training loss: 32923025705789.05\n",
      "validation loss: 26078058338893.83\n",
      "epoch: 136\n",
      "training loss: 32884667864784.234\n",
      "validation loss: 26028553387295.46\n",
      "epoch: 137\n",
      "training loss: 32862645518425.297\n",
      "validation loss: 25988587960843.97\n",
      "epoch: 138\n",
      "training loss: 32840746996011.15\n",
      "validation loss: 25948800142111.434\n",
      "epoch: 139\n",
      "training loss: 32818971485188.625\n",
      "validation loss: 25909189017796.855\n",
      "epoch: 140\n",
      "training loss: 32797318179306.75\n",
      "validation loss: 25869753679812.418\n",
      "epoch: 141\n",
      "training loss: 32775786277375.598\n",
      "validation loss: 25830493225249.707\n",
      "epoch: 142\n",
      "training loss: 32754374984025.62\n",
      "validation loss: 25791406756346.0\n",
      "epoch: 143\n",
      "training loss: 32733083509467.094\n",
      "validation loss: 25752493380450.594\n",
      "epoch: 144\n",
      "training loss: 32711911069449.973\n",
      "validation loss: 25713752209991.066\n",
      "epoch: 145\n",
      "training loss: 32690856885224.0\n",
      "validation loss: 25675182362439.547\n",
      "epoch: 146\n",
      "training loss: 32669920183499.09\n",
      "validation loss: 25636782960278.887\n",
      "epoch: 147\n",
      "training loss: 32649100196406.035\n",
      "validation loss: 25598553130968.695\n",
      "epoch: 148\n",
      "training loss: 32628396161457.43\n",
      "validation loss: 25560492006911.22\n",
      "epoch: 149\n",
      "training loss: 32607807321508.934\n",
      "validation loss: 25522598725417.043\n",
      "epoch: 150\n",
      "training loss: 32587332924720.69\n",
      "validation loss: 25484872428670.414\n",
      "epoch: 151\n",
      "training loss: 32566972224518.94\n",
      "validation loss: 25447312263694.285\n",
      "epoch: 152\n",
      "training loss: 32546724479557.375\n",
      "validation loss: 25409917382314.95\n",
      "epoch: 153\n",
      "training loss: 32526588953676.164\n",
      "validation loss: 25372686941126.08\n",
      "epoch: 154\n",
      "training loss: 32506564915838.93\n",
      "validation loss: 25335620101452.223\n",
      "epoch: 155\n",
      "training loss: 32486651638910.387\n",
      "validation loss: 25298716029311.266\n",
      "epoch: 156\n",
      "training loss: 32466805995025.223\n",
      "validation loss: 25261973770624.08\n",
      "epoch: 157\n",
      "training loss: 32447111615988.71\n",
      "validation loss: 25225392315069.26\n",
      "epoch: 158\n",
      "training loss: 32427525854537.547\n",
      "validation loss: 25188971155465.785\n",
      "epoch: 159\n",
      "training loss: 32408048004872.85\n",
      "validation loss: 25152709476209.203\n",
      "epoch: 160\n",
      "training loss: 32388677366130.99\n",
      "validation loss: 25116606466196.17\n",
      "epoch: 161\n",
      "training loss: 32369413242348.0\n",
      "validation loss: 25080661318788.67\n",
      "epoch: 162\n",
      "training loss: 32350254942424.43\n",
      "validation loss: 25044873231784.406\n",
      "epoch: 163\n",
      "training loss: 32331201780090.293\n",
      "validation loss: 25009241407400.945\n",
      "epoch: 164\n",
      "training loss: 32312253073870.344\n",
      "validation loss: 24973765052289.438\n",
      "epoch: 165\n",
      "training loss: 32293408147049.58\n",
      "validation loss: 24938443377611.9\n",
      "epoch: 166\n",
      "training loss: 32274666327638.96\n",
      "validation loss: 24903275599261.293\n",
      "epoch: 167\n",
      "training loss: 32256026948341.22\n",
      "validation loss: 24868260938420.258\n",
      "epoch: 168\n",
      "training loss: 32237489346516.7\n",
      "validation loss: 24833398622997.3\n",
      "epoch: 169\n",
      "training loss: 32219052864147.75\n",
      "validation loss: 24798687891620.867\n",
      "epoch: 170\n",
      "training loss: 32200716847793.125\n",
      "validation loss: 24764128006294.082\n",
      "epoch: 171\n",
      "training loss: 32182480648353.438\n",
      "validation loss: 24729718297513.516\n",
      "epoch: 172\n",
      "training loss: 32197520158546.242\n",
      "validation loss: 24717374395931.59\n",
      "epoch: 173\n",
      "training loss: 32179102485352.035\n",
      "validation loss: 24682774147968.793\n",
      "epoch: 174\n",
      "training loss: 32161019633922.477\n",
      "validation loss: 24648900950607.082\n",
      "epoch: 175\n",
      "training loss: 32143033735829.48\n",
      "validation loss: 24615172925124.582\n",
      "epoch: 176\n",
      "training loss: 32125144175693.316\n",
      "validation loss: 24581589178112.004\n",
      "epoch: 177\n",
      "training loss: 32107350342416.523\n",
      "validation loss: 24548148992325.484\n",
      "epoch: 178\n",
      "training loss: 32089651629102.21\n",
      "validation loss: 24514851658202.31\n",
      "epoch: 179\n",
      "training loss: 32072047433023.07\n",
      "validation loss: 24481696473146.56\n",
      "epoch: 180\n",
      "training loss: 32054537155588.793\n",
      "validation loss: 24448682744613.523\n",
      "epoch: 181\n",
      "training loss: 32037120202302.832\n",
      "validation loss: 24415809802130.914\n",
      "epoch: 182\n",
      "training loss: 32019795982770.027\n",
      "validation loss: 24383076877290.742\n",
      "epoch: 183\n",
      "training loss: 32002563910953.957\n",
      "validation loss: 24350483039385.55\n",
      "epoch: 184\n",
      "training loss: 31985423404104.48\n",
      "validation loss: 24318028112874.17\n",
      "epoch: 185\n",
      "training loss: 31968373884079.34\n",
      "validation loss: 24285711185636.7\n",
      "epoch: 186\n",
      "training loss: 31951414776529.562\n",
      "validation loss: 24253531583141.49\n",
      "epoch: 187\n",
      "training loss: 31934545511045.613\n",
      "validation loss: 24221488636101.023\n",
      "epoch: 188\n",
      "training loss: 31917765521129.547\n",
      "validation loss: 24189581681127.535\n",
      "epoch: 189\n",
      "training loss: 31901074244167.188\n",
      "validation loss: 24157810061798.71\n",
      "epoch: 190\n",
      "training loss: 31884471121400.598\n",
      "validation loss: 24126173130434.24\n",
      "epoch: 191\n",
      "training loss: 31867955597900.633\n",
      "validation loss: 24094670251145.773\n",
      "epoch: 192\n",
      "training loss: 31851527122539.785\n",
      "validation loss: 24063300805221.87\n",
      "epoch: 193\n",
      "training loss: 31835185147964.895\n",
      "validation loss: 24032064200747.54\n",
      "epoch: 194\n",
      "training loss: 31818929130569.53\n",
      "validation loss: 24000959888774.09\n",
      "epoch: 195\n",
      "training loss: 31802758530463.598\n",
      "validation loss: 23969987379184.645\n",
      "epoch: 196\n",
      "training loss: 31786672811415.527\n",
      "validation loss: 23939146148621.074\n",
      "epoch: 197\n",
      "training loss: 31770671439584.77\n",
      "validation loss: 23908434201665.324\n",
      "epoch: 198\n",
      "training loss: 31754708527048.887\n",
      "validation loss: 23877284141129.38\n",
      "epoch: 199\n",
      "training loss: 31764399747761.555\n",
      "validation loss: 23828300322628.336\n",
      "epoch: 200\n",
      "training loss: 31748493300056.695\n",
      "validation loss: 23797898551023.633\n",
      "epoch: 201\n",
      "training loss: 31732669183518.105\n",
      "validation loss: 23767623549185.14\n",
      "epoch: 202\n",
      "training loss: 31716897561359.152\n",
      "validation loss: 23737474708891.188\n",
      "epoch: 203\n",
      "training loss: 31701236516351.28\n",
      "validation loss: 23707451214789.457\n",
      "epoch: 204\n",
      "training loss: 31685656290764.277\n",
      "validation loss: 23677552676429.367\n",
      "epoch: 205\n",
      "training loss: 31670156387009.625\n",
      "validation loss: 23647778495323.86\n",
      "epoch: 206\n",
      "training loss: 31654736310850.33\n",
      "validation loss: 23618128076184.67\n",
      "epoch: 207\n",
      "training loss: 31639395571377.527\n",
      "validation loss: 23588600826902.133\n",
      "epoch: 208\n",
      "training loss: 31624133680987.297\n",
      "validation loss: 23559196158524.113\n",
      "epoch: 209\n",
      "training loss: 31608950155357.453\n",
      "validation loss: 23529913485234.746\n",
      "epoch: 210\n",
      "training loss: 31593844513424.605\n",
      "validation loss: 23500752224332.816\n",
      "epoch: 211\n",
      "training loss: 31578816277361.375\n",
      "validation loss: 23471711796209.85\n",
      "epoch: 212\n",
      "training loss: 31563864972553.734\n",
      "validation loss: 23442791624327.785\n",
      "epoch: 213\n",
      "training loss: 31548990127578.547\n",
      "validation loss: 23413991135196.04\n",
      "epoch: 214\n",
      "training loss: 31534191274181.246\n",
      "validation loss: 23385309758347.95\n",
      "epoch: 215\n",
      "training loss: 31519467947253.64\n",
      "validation loss: 23356746926316.35\n",
      "epoch: 216\n",
      "training loss: 31504819684811.977\n",
      "validation loss: 23328302074608.09\n",
      "epoch: 217\n",
      "training loss: 31490246027975.06\n",
      "validation loss: 23299974641677.266\n",
      "epoch: 218\n",
      "training loss: 31475746520942.527\n",
      "validation loss: 23271764068896.793\n",
      "epoch: 219\n",
      "training loss: 31461320710973.38\n",
      "validation loss: 23243669800528.05\n",
      "epoch: 220\n",
      "training loss: 31446968148364.55\n",
      "validation loss: 23215691283687.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 221\n",
      "training loss: 31432688386429.69\n",
      "validation loss: 23187827968312.445\n",
      "epoch: 222\n",
      "training loss: 31418480981478.094\n",
      "validation loss: 23160079307117.15\n",
      "epoch: 223\n",
      "training loss: 31404345492793.734\n",
      "validation loss: 23132444755551.27\n",
      "epoch: 224\n",
      "training loss: 31390281482614.523\n",
      "validation loss: 23104923771746.203\n",
      "epoch: 225\n",
      "training loss: 31376288516111.66\n",
      "validation loss: 23077515816455.168\n",
      "epoch: 226\n",
      "training loss: 31362366161369.113\n",
      "validation loss: 23050220352981.992\n",
      "epoch: 227\n",
      "training loss: 31348513989363.33\n",
      "validation loss: 23023036847095.504\n",
      "epoch: 228\n",
      "training loss: 31334731573942.97\n",
      "validation loss: 22995964766924.93\n",
      "epoch: 229\n",
      "training loss: 31321018491808.9\n",
      "validation loss: 22969003582830.066\n",
      "epoch: 230\n",
      "training loss: 31307374322494.25\n",
      "validation loss: 22942152767237.367\n",
      "epoch: 231\n",
      "training loss: 31293798648344.633\n",
      "validation loss: 22915411794430.21\n",
      "epoch: 232\n",
      "training loss: 31280291054498.438\n",
      "validation loss: 22888780140277.09\n",
      "epoch: 233\n",
      "training loss: 31266851128867.29\n",
      "validation loss: 22862257281877.055\n",
      "epoch: 234\n",
      "training loss: 31253478462116.47\n",
      "validation loss: 22835842697101.926\n",
      "epoch: 235\n",
      "training loss: 31240172647644.92\n",
      "validation loss: 22809535864045.17\n",
      "epoch: 236\n",
      "training loss: 31226933281562.938\n",
      "validation loss: 22783336260620.312\n",
      "epoch: 237\n",
      "training loss: 31213759962649.47\n",
      "validation loss: 22757243366764.555\n",
      "epoch: 238\n",
      "training loss: 31200652291670.71\n",
      "validation loss: 22731256731048.36\n",
      "epoch: 239\n",
      "training loss: 31187609729910.242\n",
      "validation loss: 22705375712189.266\n",
      "epoch: 240\n",
      "training loss: 31171459023427.125\n",
      "validation loss: 22674338814422.61\n",
      "epoch: 241\n",
      "training loss: 31158463588316.434\n",
      "validation loss: 22648541983462.254\n",
      "epoch: 242\n",
      "training loss: 31145616277799.355\n",
      "validation loss: 22623000687670.39\n",
      "epoch: 243\n",
      "training loss: 31132832829348.137\n",
      "validation loss: 22597557185510.574\n",
      "epoch: 244\n",
      "training loss: 31120112847139.797\n",
      "validation loss: 22572218069175.426\n",
      "epoch: 245\n",
      "training loss: 31106082417195.348\n",
      "validation loss: 22547140514853.926\n",
      "epoch: 246\n",
      "training loss: 31093531355974.98\n",
      "validation loss: 22522275868396.98\n",
      "epoch: 247\n",
      "training loss: 31071173346882.08\n",
      "validation loss: 22407154974872.03\n",
      "epoch: 248\n",
      "training loss: 31093561386394.13\n",
      "validation loss: 22395658705575.53\n",
      "epoch: 249\n",
      "training loss: 31060910747568.79\n",
      "validation loss: 22356046164574.742\n",
      "epoch: 250\n",
      "training loss: 31061598042548.82\n",
      "validation loss: 22345099824724.547\n",
      "epoch: 251\n",
      "training loss: 31048514077424.074\n",
      "validation loss: 22320273485095.543\n",
      "epoch: 252\n",
      "training loss: 31035966996386.21\n",
      "validation loss: 22295531175104.895\n",
      "epoch: 253\n",
      "training loss: 31023481356586.67\n",
      "validation loss: 22270888956155.77\n",
      "epoch: 254\n",
      "training loss: 31011056790365.97\n",
      "validation loss: 22246346368304.113\n",
      "epoch: 255\n",
      "training loss: 30998801065906.535\n",
      "validation loss: 22222014495165.707\n",
      "epoch: 256\n",
      "training loss: 30986632736526.773\n",
      "validation loss: 22194996717914.137\n",
      "epoch: 257\n",
      "training loss: 30974393443488.086\n",
      "validation loss: 22170783409288.36\n",
      "epoch: 258\n",
      "training loss: 30987799964841.465\n",
      "validation loss: 22169863018796.594\n",
      "epoch: 259\n",
      "training loss: 30972860513447.316\n",
      "validation loss: 22142693337395.96\n",
      "epoch: 260\n",
      "training loss: 30960781635737.094\n",
      "validation loss: 22118779830156.3\n",
      "epoch: 261\n",
      "training loss: 30948761625370.61\n",
      "validation loss: 22094962447944.094\n",
      "epoch: 262\n",
      "training loss: 30936800136566.785\n",
      "validation loss: 22071240750593.58\n",
      "epoch: 263\n",
      "training loss: 30924896825801.258\n",
      "validation loss: 22047614300202.867\n",
      "epoch: 264\n",
      "training loss: 30913051351790.855\n",
      "validation loss: 22024082661121.19\n",
      "epoch: 265\n",
      "training loss: 30901263375478.223\n",
      "validation loss: 22000645399936.215\n",
      "epoch: 266\n",
      "training loss: 30889532560016.465\n",
      "validation loss: 21977302085461.473\n",
      "epoch: 267\n",
      "training loss: 30877858570753.81\n",
      "validation loss: 21954052288723.84\n",
      "epoch: 268\n",
      "training loss: 30866241075217.85\n",
      "validation loss: 21930895582951.066\n",
      "epoch: 269\n",
      "training loss: 30854679743097.453\n",
      "validation loss: 21907831543559.47\n",
      "epoch: 270\n",
      "training loss: 30843174246205.344\n",
      "validation loss: 21884859748141.586\n",
      "epoch: 271\n",
      "training loss: 30831724257835.55\n",
      "validation loss: 21861979776453.996\n",
      "epoch: 272\n",
      "training loss: 30820322708209.73\n",
      "validation loss: 21839191210405.29\n",
      "epoch: 273\n",
      "training loss: 30808982790426.4\n",
      "validation loss: 21816493635923.246\n",
      "epoch: 274\n",
      "training loss: 30797697416551.117\n",
      "validation loss: 21793886637327.758\n",
      "epoch: 275\n",
      "training loss: 30786466268791.78\n",
      "validation loss: 21771369802910.91\n",
      "epoch: 276\n",
      "training loss: 30775289031593.86\n",
      "validation loss: 21748942723069.04\n",
      "epoch: 277\n",
      "training loss: 30764165391452.52\n",
      "validation loss: 21726604990290.918\n",
      "epoch: 278\n",
      "training loss: 30753095036898.625\n",
      "validation loss: 21704356199146.273\n",
      "epoch: 279\n",
      "training loss: 30742077658484.777\n",
      "validation loss: 21682195946274.414\n",
      "epoch: 280\n",
      "training loss: 30731112948771.47\n",
      "validation loss: 21660123830373.188\n",
      "epoch: 281\n",
      "training loss: 30720200602313.3\n",
      "validation loss: 21638139452188.426\n",
      "epoch: 282\n",
      "training loss: 30709340315645.2\n",
      "validation loss: 21616242414504.184\n",
      "epoch: 283\n",
      "training loss: 30698531787268.715\n",
      "validation loss: 21594432322134.695\n",
      "epoch: 284\n",
      "training loss: 30687774717638.125\n",
      "validation loss: 21572708781920.14\n",
      "epoch: 285\n",
      "training loss: 30677068809145.926\n",
      "validation loss: 21551071402732.883\n",
      "epoch: 286\n",
      "training loss: 30666413766106.246\n",
      "validation loss: 21529519795519.42\n",
      "epoch: 287\n",
      "training loss: 30655809294727.58\n",
      "validation loss: 21508053573513.84\n",
      "epoch: 288\n",
      "training loss: 30645255102990.996\n",
      "validation loss: 21486672353986.395\n",
      "epoch: 289\n",
      "training loss: 30634732521574.168\n",
      "validation loss: 21465375859793.855\n",
      "epoch: 290\n",
      "training loss: 30624277901961.355\n",
      "validation loss: 21444163380757.914\n",
      "epoch: 291\n",
      "training loss: 30613872697953.953\n",
      "validation loss: 21423034756386.89\n",
      "epoch: 292\n",
      "training loss: 30603516625400.574\n",
      "validation loss: 21401989610154.36\n",
      "epoch: 293\n",
      "training loss: 30593209401749.637\n",
      "validation loss: 21381027566475.777\n",
      "epoch: 294\n",
      "training loss: 30582950746273.82\n",
      "validation loss: 21360148251668.902\n",
      "epoch: 295\n",
      "training loss: 30572740380057.605\n",
      "validation loss: 21339351293943.23\n",
      "epoch: 296\n",
      "training loss: 30562578025984.973\n",
      "validation loss: 21318636323389.47\n",
      "epoch: 297\n",
      "training loss: 30552463408727.055\n",
      "validation loss: 21298002971969.13\n",
      "epoch: 298\n",
      "training loss: 30542396254730.01\n",
      "validation loss: 21277450873504.055\n",
      "epoch: 299\n",
      "training loss: 30532376292202.92\n",
      "validation loss: 21256979663666.164\n",
      "epoch: 300\n",
      "training loss: 30522403251105.715\n",
      "validation loss: 21236588979967.12\n",
      "epoch: 301\n",
      "training loss: 30512476863137.316\n",
      "validation loss: 21216278461748.184\n",
      "epoch: 302\n",
      "training loss: 30502596861723.734\n",
      "validation loss: 21196047750170.0\n",
      "epoch: 303\n",
      "training loss: 30492762982006.34\n",
      "validation loss: 21175896488202.55\n",
      "epoch: 304\n",
      "training loss: 30482974960830.117\n",
      "validation loss: 21155824320615.137\n",
      "epoch: 305\n",
      "training loss: 30473232536732.16\n",
      "validation loss: 21135830893966.367\n",
      "epoch: 306\n",
      "training loss: 30463535449930.062\n",
      "validation loss: 21115915856594.285\n",
      "epoch: 307\n",
      "training loss: 30453883442310.5\n",
      "validation loss: 21096078858606.504\n",
      "epoch: 308\n",
      "training loss: 30444276257417.9\n",
      "validation loss: 21076319551870.414\n",
      "epoch: 309\n",
      "training loss: 30434713640443.117\n",
      "validation loss: 21056637590003.45\n",
      "epoch: 310\n",
      "training loss: 30425195338212.26\n",
      "validation loss: 21037032628363.43\n",
      "epoch: 311\n",
      "training loss: 30415721099175.55\n",
      "validation loss: 21017504324038.91\n",
      "epoch: 312\n",
      "training loss: 30406290673396.277\n",
      "validation loss: 20998052335839.64\n",
      "epoch: 313\n",
      "training loss: 30396903812539.82\n",
      "validation loss: 20978676324287.047\n",
      "epoch: 314\n",
      "training loss: 30387560269862.766\n",
      "validation loss: 20959375951604.812\n",
      "epoch: 315\n",
      "training loss: 30378259800202.1\n",
      "validation loss: 20940150881709.434\n",
      "epoch: 316\n",
      "training loss: 30369002159964.42\n",
      "validation loss: 20921000780200.95\n",
      "epoch: 317\n",
      "training loss: 30359787107115.316\n",
      "validation loss: 20901925314353.566\n",
      "epoch: 318\n",
      "training loss: 30350614401168.754\n",
      "validation loss: 20882924153106.535\n",
      "epoch: 319\n",
      "training loss: 30341483803176.55\n",
      "validation loss: 20863996967054.914\n",
      "epoch: 320\n",
      "training loss: 30332395075717.95\n",
      "validation loss: 20845143428440.457\n",
      "epoch: 321\n",
      "training loss: 30323347982889.22\n",
      "validation loss: 20826363211142.574\n",
      "epoch: 322\n",
      "training loss: 30314342290293.37\n",
      "validation loss: 20807655990669.29\n",
      "epoch: 323\n",
      "training loss: 30305377765029.91\n",
      "validation loss: 20789021444148.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 324\n",
      "training loss: 30296454175684.695\n",
      "validation loss: 20770459250318.074\n",
      "epoch: 325\n",
      "training loss: 30287571292319.863\n",
      "validation loss: 20751969089519.004\n",
      "epoch: 326\n",
      "training loss: 30278728886463.773\n",
      "validation loss: 20733550643684.566\n",
      "epoch: 327\n",
      "training loss: 30269926731101.08\n",
      "validation loss: 20715203596332.625\n",
      "epoch: 328\n",
      "training loss: 30261164600662.848\n",
      "validation loss: 20696927632556.71\n",
      "epoch: 329\n",
      "training loss: 30252442271016.766\n",
      "validation loss: 20678722439017.37\n",
      "epoch: 330\n",
      "training loss: 30243759519457.367\n",
      "validation loss: 20660587703933.594\n",
      "epoch: 331\n",
      "training loss: 30235116124696.375\n",
      "validation loss: 20642523117074.227\n",
      "epoch: 332\n",
      "training loss: 30226511866853.098\n",
      "validation loss: 20624528369749.543\n",
      "epoch: 333\n",
      "training loss: 30217946527444.887\n",
      "validation loss: 20606603154802.746\n",
      "epoch: 334\n",
      "training loss: 30209419889377.613\n",
      "validation loss: 20588747166601.58\n",
      "epoch: 335\n",
      "training loss: 30200931736936.32\n",
      "validation loss: 20570960101030.02\n",
      "epoch: 336\n",
      "training loss: 30192481855775.816\n",
      "validation loss: 20553241655479.96\n",
      "epoch: 337\n",
      "training loss: 30184070032911.344\n",
      "validation loss: 20535591528842.93\n",
      "epoch: 338\n",
      "training loss: 30175696056709.312\n",
      "validation loss: 20518009421501.957\n",
      "epoch: 339\n",
      "training loss: 30167359716877.85\n",
      "validation loss: 20500495035323.367\n",
      "epoch: 340\n",
      "training loss: 30159060804457.062\n",
      "validation loss: 20483048073648.71\n",
      "epoch: 341\n",
      "training loss: 30150799111807.234\n",
      "validation loss: 20465668241286.67\n",
      "epoch: 449\n",
      "training loss: 29472582213456.73\n",
      "validation loss: 19013348936971.63\n",
      "epoch: 450\n",
      "training loss: 29467404546954.85\n",
      "validation loss: 19001641603886.01\n",
      "epoch: 451\n",
      "training loss: 29462247294957.418\n",
      "validation loss: 18989976306973.008\n",
      "epoch: 452\n",
      "training loss: 29457110355782.223\n",
      "validation loss: 18978352875032.85\n",
      "epoch: 453\n",
      "training loss: 29451993628348.625\n",
      "validation loss: 18966771137657.055\n",
      "epoch: 454\n",
      "training loss: 29446897012173.668\n",
      "validation loss: 18955230925224.324\n",
      "epoch: 455\n",
      "training loss: 29441820407368.14\n",
      "validation loss: 18943732068896.523\n",
      "epoch: 456\n",
      "training loss: 29436763714632.73\n",
      "validation loss: 18932274400614.605\n",
      "epoch: 457\n",
      "training loss: 29431726835254.19\n",
      "validation loss: 18920857753094.63\n",
      "epoch: 458\n",
      "training loss: 29426709671101.52\n",
      "validation loss: 18909481959823.77\n",
      "epoch: 459\n",
      "training loss: 29421712124622.125\n",
      "validation loss: 18898146855056.355\n",
      "epoch: 460\n",
      "training loss: 29416734098838.152\n",
      "validation loss: 18886852273809.94\n",
      "epoch: 461\n",
      "training loss: 29411775497342.63\n",
      "validation loss: 18875598051861.336\n",
      "epoch: 462\n",
      "training loss: 29406836224295.883\n",
      "validation loss: 18864384025742.77\n",
      "epoch: 463\n",
      "training loss: 29401916184421.727\n",
      "validation loss: 18853210032737.977\n",
      "epoch: 464\n",
      "training loss: 29397015283003.887\n",
      "validation loss: 18842075910878.36\n",
      "epoch: 465\n",
      "training loss: 29392133425882.312\n",
      "validation loss: 18830981498939.14\n",
      "epoch: 466\n",
      "training loss: 29387270519449.598\n",
      "validation loss: 18819926636435.566\n",
      "epoch: 467\n",
      "training loss: 29382426470647.38\n",
      "validation loss: 18808911163619.11\n",
      "epoch: 468\n",
      "training loss: 29377601186962.742\n",
      "validation loss: 18797934921473.703\n",
      "epoch: 469\n",
      "training loss: 29372794576424.742\n",
      "validation loss: 18786997751711.99\n",
      "epoch: 470\n",
      "training loss: 29368006547600.836\n",
      "validation loss: 18776099496771.582\n",
      "epoch: 471\n",
      "training loss: 29363237009593.42\n",
      "validation loss: 18765239999811.4\n",
      "epoch: 472\n",
      "training loss: 29358485872036.336\n",
      "validation loss: 18754419104707.926\n",
      "epoch: 473\n",
      "training loss: 29353753045091.453\n",
      "validation loss: 18743636656051.58\n",
      "epoch: 474\n",
      "training loss: 29349038439445.23\n",
      "validation loss: 18732892499143.066\n",
      "epoch: 475\n",
      "training loss: 29344341966305.332\n",
      "validation loss: 18722186479989.742\n",
      "epoch: 476\n",
      "training loss: 29339663537397.234\n",
      "validation loss: 18711518445302.016\n",
      "epoch: 477\n",
      "training loss: 29335003064960.89\n",
      "validation loss: 18700888242489.773\n",
      "epoch: 478\n",
      "training loss: 29330360461747.383\n",
      "validation loss: 18690295719658.79\n",
      "epoch: 479\n",
      "training loss: 29325735641015.65\n",
      "validation loss: 18679740725607.215\n",
      "epoch: 480\n",
      "training loss: 29321128516529.16\n",
      "validation loss: 18669223109822.027\n",
      "epoch: 481\n",
      "training loss: 29316539002552.68\n",
      "validation loss: 18658742722475.53\n",
      "epoch: 482\n",
      "training loss: 29311967013849.02\n",
      "validation loss: 18648299414421.883\n",
      "epoch: 483\n",
      "training loss: 29307412465675.832\n",
      "validation loss: 18637893037193.613\n",
      "epoch: 484\n",
      "training loss: 29302875273782.402\n",
      "validation loss: 18627523442998.17\n",
      "epoch: 485\n",
      "training loss: 29298355354406.465\n",
      "validation loss: 18617190484714.54\n",
      "epoch: 486\n",
      "training loss: 29293852624271.074\n",
      "validation loss: 18606894015889.754\n",
      "epoch: 487\n",
      "training loss: 29289367000581.477\n",
      "validation loss: 18596633890735.586\n",
      "epoch: 488\n",
      "training loss: 29284898401021.957\n",
      "validation loss: 18586409964125.117\n",
      "epoch: 489\n",
      "training loss: 29280446743752.812\n",
      "validation loss: 18576222091589.43\n",
      "epoch: 490\n",
      "training loss: 29276011947407.22\n",
      "validation loss: 18566070129314.227\n",
      "epoch: 491\n",
      "training loss: 29271593931088.254\n",
      "validation loss: 18555953934136.555\n",
      "epoch: 492\n",
      "training loss: 29267192614365.81\n",
      "validation loss: 18545873363541.492\n",
      "epoch: 493\n",
      "training loss: 29262807917273.617\n",
      "validation loss: 18535828275658.875\n",
      "epoch: 494\n",
      "training loss: 29258439760306.254\n",
      "validation loss: 18525818529260.008\n",
      "epoch: 495\n",
      "training loss: 29254088064416.188\n",
      "validation loss: 18515843983754.47\n",
      "epoch: 496\n",
      "training loss: 29249752751010.82\n",
      "validation loss: 18505904499186.86\n",
      "epoch: 497\n",
      "training loss: 29245433741949.562\n",
      "validation loss: 18495999936233.586\n",
      "epoch: 498\n",
      "training loss: 29241130959540.95\n",
      "validation loss: 18486130156199.676\n",
      "epoch: 499\n",
      "training loss: 29236844326539.707\n",
      "validation loss: 18476295021015.637\n",
      "epoch: 500\n",
      "training loss: 29232573766143.938\n",
      "validation loss: 18466494393234.254\n",
      "epoch: 501\n",
      "training loss: 29228319201992.27\n",
      "validation loss: 18456728136027.492\n",
      "epoch: 502\n",
      "training loss: 29224080558160.973\n",
      "validation loss: 18446996113183.36\n",
      "epoch: 503\n",
      "training loss: 29219857759161.227\n",
      "validation loss: 18437298189102.793\n",
      "epoch: 504\n",
      "training loss: 29215650729936.273\n",
      "validation loss: 18427634228796.598\n",
      "epoch: 505\n",
      "training loss: 29211459395858.684\n",
      "validation loss: 18418004097882.367\n",
      "epoch: 506\n",
      "training loss: 29207283682727.582\n",
      "validation loss: 18408407662581.43\n",
      "epoch: 507\n",
      "training loss: 29203123516765.92\n",
      "validation loss: 18398844789715.812\n",
      "epoch: 508\n",
      "training loss: 29198978824617.79\n",
      "validation loss: 18389315346705.227\n",
      "epoch: 509\n",
      "training loss: 29194849533345.66\n",
      "validation loss: 18379819201564.086\n",
      "epoch: 510\n",
      "training loss: 29190735570427.773\n",
      "validation loss: 18370356222898.477\n",
      "epoch: 511\n",
      "training loss: 29186636863755.445\n",
      "validation loss: 18360926279903.227\n",
      "epoch: 512\n",
      "training loss: 29182553341630.426\n",
      "validation loss: 18351529242358.95\n",
      "epoch: 513\n",
      "training loss: 29178484932762.28\n",
      "validation loss: 18342164980629.074\n",
      "epoch: 514\n",
      "training loss: 29174431566265.785\n",
      "validation loss: 18332833365656.98\n",
      "epoch: 515\n",
      "training loss: 29170393171658.32\n",
      "validation loss: 18323534268963.06\n",
      "epoch: 516\n",
      "training loss: 29166369678857.32\n",
      "validation loss: 18314267562641.82\n",
      "epoch: 517\n",
      "training loss: 29162361018177.695\n",
      "validation loss: 18305033119359.047\n",
      "epoch: 518\n",
      "training loss: 29158367120329.285\n",
      "validation loss: 18295830812348.92\n",
      "epoch: 519\n",
      "training loss: 29154387916414.34\n",
      "validation loss: 18286660515411.188\n",
      "epoch: 520\n",
      "training loss: 29150423337924.926\n",
      "validation loss: 18277522102908.33\n",
      "epoch: 521\n",
      "training loss: 29146473316740.336\n",
      "validation loss: 18268415449762.746\n",
      "epoch: 522\n",
      "training loss: 29142537785124.312\n",
      "validation loss: 18259340431453.99\n",
      "epoch: 523\n",
      "training loss: 29138616675721.223\n",
      "validation loss: 18250296924015.95\n",
      "epoch: 524\n",
      "training loss: 29134709921546.914\n",
      "validation loss: 18241284804034.113\n",
      "epoch: 525\n",
      "training loss: 29130817455908.89\n",
      "validation loss: 18232303948642.758\n",
      "epoch: 526\n",
      "training loss: 29126939185602.754\n",
      "validation loss: 18223354235521.953\n",
      "epoch: 527\n",
      "training loss: 29123481253386.184\n",
      "validation loss: 18214997084659.355\n",
      "epoch: 528\n",
      "training loss: 29119631674865.91\n",
      "validation loss: 18206110278893.895\n",
      "epoch: 529\n",
      "training loss: 29115796117699.242\n",
      "validation loss: 18197254244950.113\n",
      "epoch: 530\n",
      "training loss: 29111974517178.15\n",
      "validation loss: 18188428862714.094\n",
      "epoch: 531\n",
      "training loss: 29108166808957.79\n",
      "validation loss: 18179634012595.234\n",
      "epoch: 532\n",
      "training loss: 29104372929054.332\n",
      "validation loss: 18170869575532.734\n",
      "epoch: 533\n",
      "training loss: 29100592813842.598\n",
      "validation loss: 18162135432992.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 534\n",
      "training loss: 29096826400053.805\n",
      "validation loss: 18153431466966.887\n",
      "epoch: 535\n",
      "training loss: 29093073624773.336\n",
      "validation loss: 18144757559967.38\n",
      "epoch: 536\n",
      "training loss: 29089334425438.457\n",
      "validation loss: 18136113595026.73\n",
      "epoch: 537\n",
      "training loss: 29085608739836.12\n",
      "validation loss: 18127499455694.016\n",
      "epoch: 538\n",
      "training loss: 29081896506100.72\n",
      "validation loss: 18118915026032.566\n",
      "epoch: 539\n",
      "training loss: 29078197662711.92\n",
      "validation loss: 18110360190617.406\n",
      "epoch: 540\n",
      "training loss: 29074512148492.45\n",
      "validation loss: 18101834834532.74\n",
      "epoch: 541\n",
      "training loss: 29070839902605.96\n",
      "validation loss: 18093338843369.43\n",
      "epoch: 542\n",
      "training loss: 29067180864554.824\n",
      "validation loss: 18084872103222.465\n",
      "epoch: 543\n",
      "training loss: 29063534974178.04\n",
      "validation loss: 18076434500688.54\n",
      "epoch: 544\n",
      "training loss: 29059902171649.113\n",
      "validation loss: 18068025922863.504\n",
      "epoch: 545\n",
      "training loss: 29056282397473.863\n",
      "validation loss: 18059646257339.953\n",
      "epoch: 546\n",
      "training loss: 29052675592488.438\n",
      "validation loss: 18051295392204.754\n",
      "epoch: 547\n",
      "training loss: 29049081697857.15\n",
      "validation loss: 18042973216036.625\n",
      "epoch: 548\n",
      "training loss: 29045500655070.418\n",
      "validation loss: 18034679617903.695\n",
      "epoch: 549\n",
      "training loss: 29041932405942.77\n",
      "validation loss: 18026414487361.12\n",
      "epoch: 550\n",
      "training loss: 29038376892610.72\n",
      "validation loss: 18018177714448.664\n",
      "epoch: 551\n",
      "training loss: 29034834057530.793\n",
      "validation loss: 18009969189688.336\n",
      "epoch: 552\n",
      "training loss: 29031303843477.492\n",
      "validation loss: 18001788804082.008\n",
      "epoch: 553\n",
      "training loss: 29027786193541.293\n",
      "validation loss: 17993636449109.062\n",
      "epoch: 554\n",
      "training loss: 29024281051126.67\n",
      "validation loss: 17985512016724.055\n",
      "epoch: 555\n",
      "training loss: 29020788359950.13\n",
      "validation loss: 17977415399354.38\n",
      "epoch: 556\n",
      "training loss: 29017308064038.227\n",
      "validation loss: 17969346489897.934\n",
      "epoch: 557\n",
      "training loss: 29013840107725.6\n",
      "validation loss: 17961305181720.844\n",
      "epoch: 558\n",
      "training loss: 29010384435653.11\n",
      "validation loss: 17953291368655.168\n",
      "epoch: 559\n",
      "training loss: 29006940992765.836\n",
      "validation loss: 17945304944996.582\n",
      "epoch: 560\n",
      "training loss: 29003509724311.22\n",
      "validation loss: 17937345805502.145\n",
      "epoch: 561\n",
      "training loss: 29000090575837.13\n",
      "validation loss: 17929413845388.04\n",
      "epoch: 562\n",
      "training loss: 28996683493190.03\n",
      "validation loss: 17921508960327.316\n",
      "epoch: 563\n",
      "training loss: 28993288422513.07\n",
      "validation loss: 17913631046447.684\n",
      "epoch: 564\n",
      "training loss: 28989905310244.223\n",
      "validation loss: 17905780000329.254\n",
      "epoch: 565\n",
      "training loss: 28986534103114.46\n",
      "validation loss: 17897955719002.38\n",
      "epoch: 566\n",
      "training loss: 28983174748145.902\n",
      "validation loss: 17890158099945.434\n",
      "epoch: 567\n",
      "training loss: 28979827192650.043\n",
      "validation loss: 17882387041082.633\n",
      "epoch: 568\n",
      "training loss: 28976491384225.84\n",
      "validation loss: 17874642440781.855\n",
      "epoch: 569\n",
      "training loss: 28973167270758.055\n",
      "validation loss: 17866924197852.52\n",
      "epoch: 570\n",
      "training loss: 28969854800415.33\n",
      "validation loss: 17859232211543.38\n",
      "epoch: 571\n",
      "training loss: 28966553921648.516\n",
      "validation loss: 17851566381540.457\n",
      "epoch: 572\n",
      "training loss: 28963264583188.87\n",
      "validation loss: 17843926607964.863\n",
      "epoch: 573\n",
      "training loss: 28959986734046.31\n",
      "validation loss: 17836312791370.703\n",
      "epoch: 574\n",
      "training loss: 28956720323507.69\n",
      "validation loss: 17828724832743.0\n",
      "epoch: 575\n",
      "training loss: 28953465301135.047\n",
      "validation loss: 17821162633495.582\n",
      "epoch: 576\n",
      "training loss: 28950221616763.906\n",
      "validation loss: 17813626095468.99\n",
      "epoch: 577\n",
      "training loss: 28946989220501.586\n",
      "validation loss: 17806115120928.473\n",
      "epoch: 578\n",
      "training loss: 28943768062725.48\n",
      "validation loss: 17798629612561.85\n",
      "epoch: 579\n",
      "training loss: 28940558094081.414\n",
      "validation loss: 17791169473477.562\n",
      "epoch: 580\n",
      "training loss: 28937359265481.938\n",
      "validation loss: 17783734607202.55\n",
      "epoch: 581\n",
      "training loss: 28934171528104.69\n",
      "validation loss: 17776324917680.297\n",
      "epoch: 582\n",
      "training loss: 28930994833390.75\n",
      "validation loss: 17768940309268.785\n",
      "epoch: 583\n",
      "training loss: 28927829133042.99\n",
      "validation loss: 17761580686738.527\n",
      "epoch: 584\n",
      "training loss: 28924674379024.473\n",
      "validation loss: 17754245955270.562\n",
      "epoch: 585\n",
      "training loss: 28921530523556.805\n",
      "validation loss: 17746936020454.47\n",
      "epoch: 586\n",
      "training loss: 28918397519118.562\n",
      "validation loss: 17739650788286.414\n",
      "epoch: 587\n",
      "training loss: 28915275318443.7\n",
      "validation loss: 17732390165167.227\n",
      "epoch: 588\n",
      "training loss: 28912163874519.926\n",
      "validation loss: 17725154057900.387\n",
      "epoch: 589\n",
      "training loss: 28909063140587.188\n",
      "validation loss: 17717942373690.168\n",
      "epoch: 590\n",
      "training loss: 28905973070136.082\n",
      "validation loss: 17710755020139.67\n",
      "epoch: 591\n",
      "training loss: 28902893616906.293\n",
      "validation loss: 17703591905248.93\n",
      "epoch: 592\n",
      "training loss: 28899824734885.082\n",
      "validation loss: 17696452937413.004\n",
      "epoch: 593\n",
      "training loss: 28896766378305.73\n",
      "validation loss: 17689338025420.105\n",
      "epoch: 594\n",
      "training loss: 28893718501646.04\n",
      "validation loss: 17682247078449.7\n",
      "epoch: 595\n",
      "training loss: 28890681059626.797\n",
      "validation loss: 17675180006070.66\n",
      "epoch: 596\n",
      "training loss: 28887654007210.31\n",
      "validation loss: 17668136718239.395\n",
      "epoch: 597\n",
      "training loss: 28884637299598.902\n",
      "validation loss: 17661117125298.027\n",
      "epoch: 598\n",
      "training loss: 28881630892233.402\n",
      "validation loss: 17654121137972.51\n",
      "epoch: 599\n",
      "training loss: 28878634740791.727\n",
      "validation loss: 17647148667370.855\n",
      "epoch: 600\n",
      "training loss: 28875648801187.4\n",
      "validation loss: 17640199624981.3\n",
      "epoch: 601\n",
      "training loss: 28872673029568.06\n",
      "validation loss: 17633273922670.5\n",
      "epoch: 602\n",
      "training loss: 28869707382314.105\n",
      "validation loss: 17626371472681.734\n",
      "epoch: 603\n",
      "training loss: 28866751816037.176\n",
      "validation loss: 17619492187633.12\n",
      "epoch: 604\n",
      "training loss: 28863806287578.793\n",
      "validation loss: 17612635980515.875\n",
      "epoch: 605\n",
      "training loss: 28860870754008.93\n",
      "validation loss: 17605802764692.516\n",
      "epoch: 606\n",
      "training loss: 28857945172624.562\n",
      "validation loss: 17598992453895.12\n",
      "epoch: 607\n",
      "training loss: 28855029500948.367\n",
      "validation loss: 17592204962223.59\n",
      "epoch: 608\n",
      "training loss: 28852123696727.25\n",
      "validation loss: 17585440204143.941\n",
      "epoch: 609\n",
      "training loss: 28849227717931.027\n",
      "validation loss: 17578698094486.545\n",
      "epoch: 610\n",
      "training loss: 28846341522751.027\n",
      "validation loss: 17571978548444.451\n",
      "epoch: 611\n",
      "training loss: 28843465069598.758\n",
      "validation loss: 17565281481571.68\n",
      "epoch: 612\n",
      "training loss: 28840598317104.535\n",
      "validation loss: 17558606809781.541\n",
      "epoch: 613\n",
      "training loss: 28837741224116.15\n",
      "validation loss: 17551954449344.951\n",
      "epoch: 614\n",
      "training loss: 28834893749697.566\n",
      "validation loss: 17545324316888.777\n",
      "epoch: 615\n",
      "training loss: 28832055853127.57\n",
      "validation loss: 17538716329394.17\n",
      "epoch: 616\n",
      "training loss: 28829227493898.42\n",
      "validation loss: 17532130404194.928\n",
      "epoch: 617\n",
      "training loss: 28826408631714.668\n",
      "validation loss: 17525566458975.873\n",
      "epoch: 618\n",
      "training loss: 28823599226491.71\n",
      "validation loss: 17519024411771.201\n",
      "epoch: 619\n",
      "training loss: 28820799238354.61\n",
      "validation loss: 17512504180962.916\n",
      "epoch: 620\n",
      "training loss: 28818008627636.78\n",
      "validation loss: 17506005685279.18\n",
      "epoch: 621\n",
      "training loss: 28815227354878.69\n",
      "validation loss: 17499528843792.771\n",
      "epoch: 622\n",
      "training loss: 28812455380826.668\n",
      "validation loss: 17493073575919.467\n",
      "epoch: 623\n",
      "training loss: 28809692666431.57\n",
      "validation loss: 17486639801416.512\n",
      "epoch: 624\n",
      "training loss: 28806939172847.62\n",
      "validation loss: 17480227440381.045\n",
      "epoch: 625\n",
      "training loss: 28804194861431.1\n",
      "validation loss: 17473836413248.547\n",
      "epoch: 626\n",
      "training loss: 28801459693739.145\n",
      "validation loss: 17467466640791.338\n",
      "epoch: 627\n",
      "training loss: 28798733631528.547\n",
      "validation loss: 17461118044117.045\n",
      "epoch: 628\n",
      "training loss: 28796016636754.504\n",
      "validation loss: 17454790544667.082\n",
      "epoch: 629\n",
      "training loss: 28793308671569.47\n",
      "validation loss: 17448484064215.195\n",
      "epoch: 630\n",
      "training loss: 28790609698321.88\n",
      "validation loss: 17442198524865.922\n",
      "epoch: 631\n",
      "training loss: 28787919679555.035\n",
      "validation loss: 17435933849053.186\n",
      "epoch: 632\n",
      "training loss: 28785238578005.895\n",
      "validation loss: 17429689959538.793\n",
      "epoch: 633\n",
      "training loss: 28782566356603.883\n",
      "validation loss: 17423466779411.01\n",
      "epoch: 634\n",
      "training loss: 28779902978469.758\n",
      "validation loss: 17417264232083.139\n",
      "epoch: 635\n",
      "training loss: 28777248406914.45\n",
      "validation loss: 17411082241292.09\n",
      "epoch: 636\n",
      "training loss: 28774602605437.86\n",
      "validation loss: 17404920731096.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 637\n",
      "training loss: 28771965537727.812\n",
      "validation loss: 17398779625877.793\n",
      "epoch: 638\n",
      "training loss: 28769337167658.832\n",
      "validation loss: 17392658850333.918\n",
      "epoch: 639\n",
      "training loss: 28766717459291.062\n",
      "validation loss: 17386558329482.879\n",
      "epoch: 640\n",
      "training loss: 28764106376869.137\n",
      "validation loss: 17380477988658.955\n",
      "epoch: 641\n",
      "training loss: 28761503884821.062\n",
      "validation loss: 17374417753511.867\n",
      "epoch: 642\n",
      "training loss: 28758909947757.13\n",
      "validation loss: 17368377550005.484\n",
      "epoch: 643\n",
      "training loss: 28756324530468.777\n",
      "validation loss: 17362357304416.508\n",
      "epoch: 644\n",
      "training loss: 28753747597927.56\n",
      "validation loss: 17356356943333.23\n",
      "epoch: 645\n",
      "training loss: 28751179115284.0\n",
      "validation loss: 17350376393654.271\n",
      "epoch: 646\n",
      "training loss: 28748619047866.574\n",
      "validation loss: 17344415582587.355\n",
      "epoch: 647\n",
      "training loss: 28746067361180.598\n",
      "validation loss: 17338474437648.104\n",
      "epoch: 648\n",
      "training loss: 28743524020907.18\n",
      "validation loss: 17332552886658.844\n",
      "epoch: 649\n",
      "training loss: 28740988992902.156\n",
      "validation loss: 17326650857747.45\n",
      "epoch: 650\n",
      "training loss: 28738462243195.086\n",
      "validation loss: 17320768279346.207\n",
      "epoch: 651\n",
      "training loss: 28735943737988.133\n",
      "validation loss: 17314905080190.695\n",
      "epoch: 652\n",
      "training loss: 28733433443655.12\n",
      "validation loss: 17309061189318.701\n",
      "epoch: 653\n",
      "training loss: 28730931326740.418\n",
      "validation loss: 17303236536069.166\n",
      "epoch: 654\n",
      "training loss: 28728437353957.984\n",
      "validation loss: 17297431050081.162\n",
      "epoch: 655\n",
      "training loss: 28725951492190.31\n",
      "validation loss: 17291644661292.9\n",
      "epoch: 656\n",
      "training loss: 28723473708487.44\n",
      "validation loss: 17285877299940.754\n",
      "epoch: 657\n",
      "training loss: 28721003970065.973\n",
      "validation loss: 17280128896558.361\n",
      "epoch: 658\n",
      "training loss: 28718542244308.047\n",
      "validation loss: 17274399381975.719\n",
      "epoch: 659\n",
      "training loss: 28716088498760.34\n",
      "validation loss: 17268688687318.355\n",
      "epoch: 660\n",
      "training loss: 28713642701133.12\n",
      "validation loss: 17262996744006.54\n",
      "epoch: 661\n",
      "training loss: 28711204819299.293\n",
      "validation loss: 17257323483754.504\n",
      "epoch: 662\n",
      "training loss: 28708774821293.344\n",
      "validation loss: 17251668838569.785\n",
      "epoch: 663\n",
      "training loss: 28706352675310.465\n",
      "validation loss: 17246032740752.566\n",
      "epoch: 664\n",
      "training loss: 28703938349705.56\n",
      "validation loss: 17240415122895.094\n",
      "epoch: 665\n",
      "training loss: 28701531812992.29\n",
      "validation loss: 17234815917881.188\n",
      "epoch: 666\n",
      "training loss: 28699133033842.133\n",
      "validation loss: 17229235058885.773\n",
      "epoch: 667\n",
      "training loss: 28696741981083.477\n",
      "validation loss: 17223672479374.527\n",
      "epoch: 668\n",
      "training loss: 28694358623700.65\n",
      "validation loss: 17218128113103.621\n",
      "epoch: 669\n",
      "training loss: 28691982930833.004\n",
      "validation loss: 17212601894119.508\n",
      "epoch: 670\n",
      "training loss: 28689614871774.0\n",
      "validation loss: 17207093756758.846\n",
      "epoch: 671\n",
      "training loss: 28687254415970.324\n",
      "validation loss: 17201603635648.545\n",
      "epoch: 672\n",
      "training loss: 28684901533020.914\n",
      "validation loss: 17196131465705.883\n",
      "epoch: 673\n",
      "training loss: 28682556192676.105\n",
      "validation loss: 17190677182138.826\n",
      "epoch: 674\n",
      "training loss: 28680218364836.75\n",
      "validation loss: 17185240720446.424\n",
      "epoch: 675\n",
      "training loss: 28677888019553.27\n",
      "validation loss: 17179822016419.44\n",
      "epoch: 676\n",
      "training loss: 28675565127024.836\n",
      "validation loss: 17174421006141.092\n",
      "epoch: 677\n",
      "training loss: 28673249657598.438\n",
      "validation loss: 17169037625988.057\n",
      "epoch: 678\n",
      "training loss: 28670941581768.055\n",
      "validation loss: 17163671812631.654\n",
      "epoch: 679\n",
      "training loss: 28668640870173.742\n",
      "validation loss: 17158323503039.287\n",
      "epoch: 680\n",
      "training loss: 28666347493600.812\n",
      "validation loss: 17152992634476.182\n",
      "epoch: 681\n",
      "training loss: 28664061422978.98\n",
      "validation loss: 17147679144507.371\n",
      "epoch: 682\n",
      "training loss: 28661782629381.465\n",
      "validation loss: 17142382971000.135\n",
      "epoch: 683\n",
      "training loss: 28659511084024.195\n",
      "validation loss: 17137104052126.688\n",
      "epoch: 684\n",
      "training loss: 28657246758264.957\n",
      "validation loss: 17131842326367.404\n",
      "epoch: 685\n",
      "training loss: 28654989623602.547\n",
      "validation loss: 17126597732514.475\n",
      "epoch: 686\n",
      "training loss: 28652739651675.957\n",
      "validation loss: 17121370209676.133\n",
      "epoch: 687\n",
      "training loss: 28650496814263.547\n",
      "validation loss: 17116159697281.5\n",
      "epoch: 688\n",
      "training loss: 28648261083282.23\n",
      "validation loss: 17110966135086.092\n",
      "epoch: 689\n",
      "training loss: 28646032430786.656\n",
      "validation loss: 17105789463178.14\n",
      "epoch: 690\n",
      "training loss: 28643810828968.414\n",
      "validation loss: 17100629621985.812\n",
      "epoch: 691\n",
      "training loss: 28641596250155.227\n",
      "validation loss: 17095486552285.387\n",
      "epoch: 692\n",
      "training loss: 28639388666810.152\n",
      "validation loss: 17090360195210.605\n",
      "epoch: 693\n",
      "training loss: 28637188051530.78\n",
      "validation loss: 17085250492263.293\n",
      "epoch: 694\n",
      "training loss: 28634994377048.47\n",
      "validation loss: 17080157385325.404\n",
      "epoch: 695\n",
      "training loss: 28632807616227.574\n",
      "validation loss: 17075080816672.701\n",
      "epoch: 696\n",
      "training loss: 28630627742064.613\n",
      "validation loss: 17070020728990.205\n",
      "epoch: 697\n",
      "training loss: 28628454727687.566\n",
      "validation loss: 17064977065389.67\n",
      "epoch: 698\n",
      "training loss: 28626288546355.074\n",
      "validation loss: 17059949769429.168\n",
      "epoch: 699\n",
      "training loss: 28624129171455.656\n",
      "validation loss: 17054938785134.938\n",
      "epoch: 700\n",
      "training loss: 28621976576507.035\n",
      "validation loss: 17049944057025.523\n",
      "epoch: 701\n",
      "training loss: 28619830735155.277\n",
      "validation loss: 17044965530137.992\n",
      "epoch: 702\n",
      "training loss: 28617691621174.11\n",
      "validation loss: 17040003150055.65\n",
      "epoch: 703\n",
      "training loss: 28615559208464.2\n",
      "validation loss: 17035056862936.016\n",
      "epoch: 704\n",
      "training loss: 28613433471052.355\n",
      "validation loss: 17030126615536.635\n",
      "epoch: 705\n",
      "training loss: 28611314383090.832\n",
      "validation loss: 17025212355234.002\n",
      "epoch: 706\n",
      "training loss: 28609201918856.594\n",
      "validation loss: 17020314030027.072\n",
      "epoch: 707\n",
      "training loss: 28607096052750.598\n",
      "validation loss: 17015431588509.197\n",
      "epoch: 708\n",
      "training loss: 28604996759297.055\n",
      "validation loss: 17010564979778.457\n",
      "epoch: 709\n",
      "training loss: 28602904013142.7\n",
      "validation loss: 17005714153228.41\n",
      "epoch: 710\n",
      "training loss: 28600817789056.117\n",
      "validation loss: 17000879058104.73\n",
      "epoch: 711\n",
      "training loss: 28598738061926.926\n",
      "validation loss: 16996059642591.822\n",
      "epoch: 712\n",
      "training loss: 28596664806765.04\n",
      "validation loss: 16991255851920.049\n",
      "epoch: 713\n",
      "training loss: 28594597998699.766\n",
      "validation loss: 16986467624330.65\n",
      "epoch: 714\n",
      "training loss: 28592537612978.44\n",
      "validation loss: 16981694882084.375\n",
      "epoch: 715\n",
      "training loss: 28590483624963.383\n",
      "validation loss: 16976937510432.412\n",
      "epoch: 716\n",
      "training loss: 28588436010119.125\n",
      "validation loss: 16972195308071.54\n",
      "epoch: 717\n",
      "training loss: 28586394743919.637\n",
      "validation loss: 16967467898803.992\n",
      "epoch: 718\n",
      "training loss: 28584359801918.105\n",
      "validation loss: 16962754787767.469\n",
      "epoch: 719\n",
      "training loss: 28582331161553.203\n",
      "validation loss: 16958053766214.098\n",
      "epoch: 720\n",
      "training loss: 28580308796646.168\n",
      "validation loss: 16953372106832.514\n",
      "epoch: 721\n",
      "training loss: 28578292683692.12\n",
      "validation loss: 16948704798620.145\n",
      "epoch: 722\n",
      "training loss: 28576282800283.598\n",
      "validation loss: 16944050652336.516\n",
      "epoch: 723\n",
      "training loss: 28574279120658.28\n",
      "validation loss: 16939413236036.838\n",
      "epoch: 724\n",
      "training loss: 28572281623415.406\n",
      "validation loss: 16934788594425.021\n",
      "epoch: 725\n",
      "training loss: 28570290282933.35\n",
      "validation loss: 16930181077270.193\n",
      "epoch: 726\n",
      "training loss: 28568305077387.24\n",
      "validation loss: 16925587086461.307\n",
      "epoch: 727\n",
      "training loss: 28566325982347.402\n",
      "validation loss: 16921008260888.07\n",
      "epoch: 728\n",
      "training loss: 28564352976463.86\n",
      "validation loss: 16916441931572.059\n",
      "epoch: 729\n",
      "training loss: 28562386034559.406\n",
      "validation loss: 16911893532524.912\n",
      "epoch: 730\n",
      "training loss: 28560425133962.832\n",
      "validation loss: 16907358811094.16\n",
      "epoch: 731\n",
      "training loss: 28558470254499.22\n",
      "validation loss: 16902831895417.178\n",
      "epoch: 732\n",
      "training loss: 28556521370335.934\n",
      "validation loss: 16898327610539.523\n",
      "epoch: 733\n",
      "training loss: 28554578459967.55\n",
      "validation loss: 16893837663565.76\n",
      "epoch: 734\n",
      "training loss: 28552641500797.684\n",
      "validation loss: 16889361724469.332\n",
      "epoch: 735\n",
      "training loss: 28550710469718.63\n",
      "validation loss: 16884899273027.387\n",
      "epoch: 736\n",
      "training loss: 28548785347350.414\n",
      "validation loss: 16880444719132.986\n",
      "epoch: 737\n",
      "training loss: 28546866108236.07\n",
      "validation loss: 16876012005003.982\n",
      "epoch: 738\n",
      "training loss: 28544952731415.145\n",
      "validation loss: 16871593321724.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 739\n",
      "training loss: 28543045194724.59\n",
      "validation loss: 16867188267163.441\n",
      "epoch: 740\n",
      "training loss: 28541143475737.23\n",
      "validation loss: 16862796321261.121\n",
      "epoch: 741\n",
      "training loss: 28539247555776.34\n",
      "validation loss: 16858409638162.438\n",
      "epoch: 742\n",
      "training loss: 28537357409461.79\n",
      "validation loss: 16854047203735.688\n",
      "epoch: 743\n",
      "training loss: 28535473016555.848\n",
      "validation loss: 16849698732391.258\n",
      "epoch: 744\n",
      "training loss: 28533594355565.7\n",
      "validation loss: 16845364066749.277\n",
      "epoch: 745\n",
      "training loss: 28531721405025.363\n",
      "validation loss: 16841042883524.63\n",
      "epoch: 746\n",
      "training loss: 28529854142722.25\n",
      "validation loss: 16836734641210.688\n",
      "epoch: 747\n",
      "training loss: 28527992551056.89\n",
      "validation loss: 16832432256996.432\n",
      "epoch: 748\n",
      "training loss: 28526136605029.34\n",
      "validation loss: 16828152625841.238\n",
      "epoch: 749\n",
      "training loss: 28524286285014.363\n",
      "validation loss: 16823886611744.209\n",
      "epoch: 750\n",
      "training loss: 28522441570078.984\n",
      "validation loss: 16819633963454.498\n",
      "epoch: 751\n",
      "training loss: 28520602438958.914\n",
      "validation loss: 16815394182950.602\n",
      "epoch: 752\n",
      "training loss: 28518768872433.367\n",
      "validation loss: 16811166522393.832\n",
      "epoch: 753\n",
      "training loss: 28516940847138.02\n",
      "validation loss: 16806952926672.95\n",
      "epoch: 754\n",
      "training loss: 28515118347726.844\n",
      "validation loss: 16802734832865.217\n",
      "epoch: 755\n",
      "training loss: 28513301348573.3\n",
      "validation loss: 16798548997886.969\n",
      "epoch: 756\n",
      "training loss: 28511489831227.715\n",
      "validation loss: 16794376546225.28\n",
      "epoch: 757\n",
      "training loss: 28509683775464.52\n",
      "validation loss: 16790217438539.197\n",
      "epoch: 758\n",
      "training loss: 28507883161150.78\n",
      "validation loss: 16786071634782.627\n",
      "epoch: 759\n",
      "training loss: 28506087968245.535\n",
      "validation loss: 16781939092264.783\n",
      "epoch: 760\n",
      "training loss: 28504298176798.97\n",
      "validation loss: 16777819761208.568\n",
      "epoch: 761\n",
      "training loss: 28502513766951.055\n",
      "validation loss: 16773713574054.824\n",
      "epoch: 762\n",
      "training loss: 28500734718927.19\n",
      "validation loss: 16769620417945.47\n",
      "epoch: 763\n",
      "training loss: 28498961013012.324\n",
      "validation loss: 16765540061454.238\n",
      "epoch: 764\n",
      "training loss: 28497192629187.54\n",
      "validation loss: 16761472013615.965\n",
      "epoch: 765\n",
      "training loss: 28495429548541.652\n",
      "validation loss: 16757415723836.025\n",
      "epoch: 766\n",
      "training loss: 28493671752968.15\n",
      "validation loss: 16753371073437.941\n",
      "epoch: 767\n",
      "training loss: 28491919219813.598\n",
      "validation loss: 16749341357176.79\n",
      "epoch: 768\n",
      "training loss: 28490171933664.152\n",
      "validation loss: 16745322389929.86\n",
      "epoch: 769\n",
      "training loss: 28488429871237.63\n",
      "validation loss: 16741318027886.855\n",
      "epoch: 770\n",
      "training loss: 28486693018438.3\n",
      "validation loss: 16737322094122.86\n",
      "epoch: 771\n",
      "training loss: 28484961352069.605\n",
      "validation loss: 16733343906424.408\n",
      "epoch: 772\n",
      "training loss: 28483234854177.57\n",
      "validation loss: 16729377652351.041\n",
      "epoch: 773\n",
      "training loss: 28481513506769.484\n",
      "validation loss: 16725422779615.705\n",
      "epoch: 774\n",
      "training loss: 28479797292209.734\n",
      "validation loss: 16721479543181.104\n",
      "epoch: 775\n",
      "training loss: 28478086187502.566\n",
      "validation loss: 16717550349523.94\n",
      "epoch: 776\n",
      "training loss: 28476380181521.68\n",
      "validation loss: 16713616454609.082\n",
      "epoch: 777\n",
      "training loss: 28474679249113.816\n",
      "validation loss: 16709712676365.445\n",
      "epoch: 778\n",
      "training loss: 28472983374191.82\n",
      "validation loss: 16705821308997.795\n",
      "epoch: 779\n",
      "training loss: 28471292538454.535\n",
      "validation loss: 16701942317196.291\n",
      "epoch: 780\n",
      "training loss: 28469606723682.65\n",
      "validation loss: 16698075663585.01\n",
      "epoch: 781\n",
      "training loss: 28467925911737.992\n",
      "validation loss: 16694221304728.234\n",
      "epoch: 782\n",
      "training loss: 28466250084562.22\n",
      "validation loss: 16690379181306.852\n",
      "epoch: 783\n",
      "training loss: 28464579224172.855\n",
      "validation loss: 16686549192169.117\n",
      "epoch: 784\n",
      "training loss: 28462913312639.125\n",
      "validation loss: 16682731121658.273\n",
      "epoch: 785\n",
      "training loss: 28461252331698.137\n",
      "validation loss: 16678924474964.824\n",
      "epoch: 786\n",
      "training loss: 28459596261681.957\n",
      "validation loss: 16675128728424.688\n",
      "epoch: 787\n",
      "training loss: 28457945093994.953\n",
      "validation loss: 16671298387282.033\n",
      "epoch: 788\n",
      "training loss: 28456298800771.16\n",
      "validation loss: 16667526409435.96\n",
      "epoch: 789\n",
      "training loss: 28454657368161.17\n",
      "validation loss: 16663766285941.61\n",
      "epoch: 790\n",
      "training loss: 28453020778742.016\n",
      "validation loss: 16660017976049.334\n",
      "epoch: 791\n",
      "training loss: 28451389015167.945\n",
      "validation loss: 16656281439172.912\n",
      "epoch: 792\n",
      "training loss: 28449762060169.996\n",
      "validation loss: 16652556634889.193\n",
      "epoch: 793\n",
      "training loss: 28448139896555.645\n",
      "validation loss: 16648843522937.746\n",
      "epoch: 794\n",
      "training loss: 28446522507208.27\n",
      "validation loss: 16645142063220.59\n",
      "epoch: 795\n",
      "training loss: 28444909875086.85\n",
      "validation loss: 16641452215801.916\n",
      "epoch: 796\n",
      "training loss: 28443301983225.504\n",
      "validation loss: 16637773940907.86\n",
      "epoch: 797\n",
      "training loss: 28441698814733.082\n",
      "validation loss: 16634107198926.316\n",
      "epoch: 798\n",
      "training loss: 28440100352792.77\n",
      "validation loss: 16630451950406.791\n",
      "epoch: 799\n",
      "training loss: 28438506580661.68\n",
      "validation loss: 16626808156060.285\n",
      "epoch: 800\n",
      "training loss: 28436917481670.473\n",
      "validation loss: 16623175776759.262\n",
      "epoch: 801\n",
      "training loss: 28435333039222.91\n",
      "validation loss: 16619554773537.611\n",
      "epoch: 802\n",
      "training loss: 28433753236795.48\n",
      "validation loss: 16615945107590.738\n",
      "epoch: 803\n",
      "training loss: 28432178057936.98\n",
      "validation loss: 16612346740275.666\n",
      "epoch: 804\n",
      "training loss: 28430607486268.12\n",
      "validation loss: 16608759633111.219\n",
      "epoch: 805\n",
      "training loss: 28429041505481.016\n",
      "validation loss: 16605183747778.299\n",
      "epoch: 806\n",
      "training loss: 28427480099338.613\n",
      "validation loss: 16601619046120.213\n",
      "epoch: 807\n",
      "training loss: 28425923251673.66\n",
      "validation loss: 16598065490143.123\n",
      "epoch: 808\n",
      "training loss: 28424370946385.44\n",
      "validation loss: 16594523042016.572\n",
      "epoch: 809\n",
      "training loss: 28422823167418.477\n",
      "validation loss: 16590991664074.04\n",
      "epoch: 810\n",
      "training loss: 28421279898119.73\n",
      "validation loss: 16587471318813.434\n",
      "epoch: 811\n",
      "training loss: 28419686412458.96\n",
      "validation loss: 16583961968891.643\n",
      "epoch: 812\n",
      "training loss: 28418152045670.145\n",
      "validation loss: 16580463476329.777\n",
      "epoch: 813\n",
      "training loss: 28416622142075.652\n",
      "validation loss: 16576975905420.969\n",
      "epoch: 814\n",
      "training loss: 28415096685993.812\n",
      "validation loss: 16573499219331.861\n",
      "epoch: 815\n",
      "training loss: 28413575661810.883\n",
      "validation loss: 16570033381400.27\n",
      "epoch: 816\n",
      "training loss: 28412059053980.55\n",
      "validation loss: 16566578355137.05\n",
      "epoch: 817\n",
      "training loss: 28410546847023.7\n",
      "validation loss: 16563134104228.27\n",
      "epoch: 818\n",
      "training loss: 28409039025527.938\n",
      "validation loss: 16559700592537.637\n",
      "epoch: 819\n",
      "training loss: 28407535574147.28\n",
      "validation loss: 16556277784109.326\n",
      "epoch: 820\n",
      "training loss: 28406036477601.81\n",
      "validation loss: 16552865643171.15\n",
      "epoch: 821\n",
      "training loss: 28404541720677.29\n",
      "validation loss: 16549464134138.182\n",
      "epoch: 822\n",
      "training loss: 28403051288224.82\n",
      "validation loss: 16546073221616.87\n",
      "epoch: 823\n",
      "training loss: 28401565165160.527\n",
      "validation loss: 16542692870409.725\n",
      "epoch: 824\n",
      "training loss: 28400083336465.164\n",
      "validation loss: 16539323045520.643\n",
      "epoch: 825\n",
      "training loss: 28398605787183.83\n",
      "validation loss: 16535963712160.969\n",
      "epoch: 826\n",
      "training loss: 28397132502425.55\n",
      "validation loss: 16532614835756.398\n",
      "epoch: 827\n",
      "training loss: 28395663467363.03\n",
      "validation loss: 16529276381954.87\n",
      "epoch: 828\n",
      "training loss: 28394198667232.234\n",
      "validation loss: 16525948316635.559\n",
      "epoch: 829\n",
      "training loss: 28392738087332.11\n",
      "validation loss: 16522630605919.176\n",
      "epoch: 830\n",
      "training loss: 28391281713024.242\n",
      "validation loss: 16519323216179.766\n",
      "epoch: 831\n",
      "training loss: 28389829529732.484\n",
      "validation loss: 16516026114058.236\n",
      "epoch: 832\n",
      "training loss: 28388381522942.703\n",
      "validation loss: 16512739266477.926\n",
      "epoch: 833\n",
      "training loss: 28386937678202.38\n",
      "validation loss: 16509462640662.537\n",
      "epoch: 834\n",
      "training loss: 28385497981120.33\n",
      "validation loss: 16506196204156.82\n",
      "epoch: 835\n",
      "training loss: 28384062417366.37\n",
      "validation loss: 16502939924850.479\n",
      "epoch: 836\n",
      "training loss: 28382630972671.004\n",
      "validation loss: 16499693771005.85\n",
      "epoch: 837\n",
      "training loss: 28381203632825.086\n",
      "validation loss: 16496457711290.008\n",
      "epoch: 838\n",
      "training loss: 28379780383679.516\n",
      "validation loss: 16493231714811.91\n",
      "epoch: 839\n",
      "training loss: 28378361211144.92\n",
      "validation loss: 16490015751165.53\n",
      "epoch: 840\n",
      "training loss: 28376946101191.375\n",
      "validation loss: 16486809790479.64\n",
      "epoch: 841\n",
      "training loss: 28375535039848.027\n",
      "validation loss: 16483613803475.102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 842\n",
      "training loss: 28374128013202.836\n",
      "validation loss: 16480427761530.04\n",
      "epoch: 843\n",
      "training loss: 28372725007402.258\n",
      "validation loss: 16477251636752.713\n",
      "epoch: 844\n",
      "training loss: 28371326008650.918\n",
      "validation loss: 16474085402060.045\n",
      "epoch: 845\n",
      "training loss: 28369931003211.35\n",
      "validation loss: 16470929031256.451\n",
      "epoch: 846\n",
      "training loss: 28368539977403.617\n",
      "validation loss: 16467782499099.791\n",
      "epoch: 847\n",
      "training loss: 28367152917605.09\n",
      "validation loss: 16464645781325.188\n",
      "epoch: 848\n",
      "training loss: 28365769810250.074\n",
      "validation loss: 16461518854561.668\n",
      "epoch: 849\n",
      "training loss: 28364390641829.508\n",
      "validation loss: 16458401695993.701\n",
      "epoch: 850\n",
      "training loss: 28363015398890.58\n",
      "validation loss: 16455294282418.57\n",
      "epoch: 851\n",
      "training loss: 28361644068036.285\n",
      "validation loss: 16452196587820.604\n",
      "epoch: 852\n",
      "training loss: 28360276635924.562\n",
      "validation loss: 16449108577054.418\n",
      "epoch: 853\n",
      "training loss: 28358913089265.887\n",
      "validation loss: 16446030188292.523\n",
      "epoch: 854\n",
      "training loss: 28357553414810.2\n",
      "validation loss: 16442961279142.775\n",
      "epoch: 855\n",
      "training loss: 28356197599164.215\n",
      "validation loss: 16439901453850.652\n",
      "epoch: 856\n",
      "training loss: 28354845562401.047\n",
      "validation loss: 16436849926190.016\n",
      "epoch: 857\n",
      "training loss: 28355766108140.824\n",
      "validation loss: 16439212680626.39\n",
      "epoch: 858\n",
      "training loss: 28354408022869.37\n",
      "validation loss: 16436147473223.92\n",
      "epoch: 859\n",
      "training loss: 28353053792086.07\n",
      "validation loss: 16433091721702.172\n",
      "epoch: 860\n",
      "training loss: 28351703402801.836\n",
      "validation loss: 16430045394378.342\n",
      "epoch: 861\n",
      "training loss: 28350356842033.246\n",
      "validation loss: 16427008459687.621\n",
      "epoch: 862\n",
      "training loss: 28349014096850.094\n",
      "validation loss: 16423980886182.578\n",
      "epoch: 863\n",
      "training loss: 28347675154375.152\n",
      "validation loss: 16420962642532.66\n",
      "epoch: 864\n",
      "training loss: 28346340001783.863\n",
      "validation loss: 16417953697523.697\n",
      "epoch: 865\n",
      "training loss: 28345008626304.113\n",
      "validation loss: 16414954020057.426\n",
      "epoch: 866\n",
      "training loss: 28343681015215.914\n",
      "validation loss: 16411963579151.018\n",
      "epoch: 867\n",
      "training loss: 28342357155851.195\n",
      "validation loss: 16408982343936.55\n",
      "epoch: 868\n",
      "training loss: 28341037035593.5\n",
      "validation loss: 16406010283660.594\n",
      "epoch: 869\n",
      "training loss: 28339720641877.742\n",
      "validation loss: 16403047367683.676\n",
      "epoch: 870\n",
      "training loss: 28338407962189.957\n",
      "validation loss: 16400093565479.836\n",
      "epoch: 871\n",
      "training loss: 28337098984067.008\n",
      "validation loss: 16397148846636.154\n",
      "epoch: 872\n",
      "training loss: 28335793695096.363\n",
      "validation loss: 16394213180852.26\n",
      "epoch: 873\n",
      "training loss: 28334492082915.824\n",
      "validation loss: 16391286537939.88\n",
      "epoch: 874\n",
      "training loss: 28333194135213.266\n",
      "validation loss: 16388368887822.377\n",
      "epoch: 875\n",
      "training loss: 28331899839726.414\n",
      "validation loss: 16385460200534.264\n",
      "epoch: 876\n",
      "training loss: 28330609184242.55\n",
      "validation loss: 16382560446220.764\n",
      "epoch: 877\n",
      "training loss: 28329322156598.277\n",
      "validation loss: 16379669595137.336\n",
      "epoch: 878\n",
      "training loss: 28328038744679.285\n",
      "validation loss: 16376787617649.22\n",
      "epoch: 879\n",
      "training loss: 28326758936420.04\n",
      "validation loss: 16373914484230.998\n",
      "epoch: 880\n",
      "training loss: 28325482719803.55\n",
      "validation loss: 16371050165466.105\n",
      "epoch: 881\n",
      "training loss: 28324210082861.062\n",
      "validation loss: 16368194632046.414\n",
      "epoch: 882\n",
      "training loss: 28322941013671.656\n",
      "validation loss: 16365347854771.773\n",
      "epoch: 883\n",
      "training loss: 28321675500361.645\n",
      "validation loss: 16362509804549.549\n",
      "epoch: 884\n",
      "training loss: 28320413531103.02\n",
      "validation loss: 16359680452394.203\n",
      "epoch: 885\n",
      "training loss: 28319155094107.246\n",
      "validation loss: 16356859769426.846\n",
      "epoch: 886\n",
      "training loss: 28317900177569.27\n",
      "validation loss: 16354047726874.852\n",
      "epoch: 887\n",
      "training loss: 28316648764506.438\n",
      "validation loss: 16351244296071.725\n",
      "epoch: 888\n",
      "training loss: 28315424635496.293\n",
      "validation loss: 16348449448818.879\n",
      "epoch: 889\n",
      "training loss: 28314179838966.223\n",
      "validation loss: 16345663185004.145\n",
      "epoch: 890\n",
      "training loss: 28341862050123.87\n",
      "validation loss: 16278976872550.164\n",
      "epoch: 891\n",
      "training loss: 28340648180420.01\n",
      "validation loss: 16276242789247.154\n",
      "epoch: 892\n",
      "training loss: 28339437773427.305\n",
      "validation loss: 16273517290919.814\n",
      "epoch: 893\n",
      "training loss: 28338230816792.73\n",
      "validation loss: 16270800348214.459\n",
      "epoch: 894\n",
      "training loss: 28337027298216.96\n",
      "validation loss: 16268091931889.232\n",
      "epoch: 895\n",
      "training loss: 28335827205454.105\n",
      "validation loss: 16265392012813.682\n",
      "epoch: 896\n",
      "training loss: 28334630526311.426\n",
      "validation loss: 16262700561968.242\n",
      "epoch: 897\n",
      "training loss: 28333437248649.035\n",
      "validation loss: 16260017550443.78\n",
      "epoch: 898\n",
      "training loss: 28332247360379.65\n",
      "validation loss: 16257342949441.127\n",
      "epoch: 899\n",
      "training loss: 28331060849468.266\n",
      "validation loss: 16254676730270.592\n",
      "epoch: 900\n",
      "training loss: 28329877703931.945\n",
      "validation loss: 16252018864351.512\n",
      "epoch: 901\n",
      "training loss: 28328697911839.473\n",
      "validation loss: 16249369323211.768\n",
      "epoch: 902\n",
      "training loss: 28327521461311.14\n",
      "validation loss: 16246728078487.342\n",
      "epoch: 903\n",
      "training loss: 28326348340518.453\n",
      "validation loss: 16244095101921.848\n",
      "epoch: 904\n",
      "training loss: 28325178537683.832\n",
      "validation loss: 16241470365366.066\n",
      "epoch: 905\n",
      "training loss: 28324012041080.41\n",
      "validation loss: 16238853840777.496\n",
      "epoch: 906\n",
      "training loss: 28322848839031.71\n",
      "validation loss: 16236245500219.908\n",
      "epoch: 907\n",
      "training loss: 28321688919911.383\n",
      "validation loss: 16233645315862.863\n",
      "epoch: 908\n",
      "training loss: 28320532272143.0\n",
      "validation loss: 16231053259981.312\n",
      "epoch: 909\n",
      "training loss: 28319378884199.715\n",
      "validation loss: 16228469304955.08\n",
      "epoch: 910\n",
      "training loss: 28318228744604.047\n",
      "validation loss: 16225893423268.492\n",
      "epoch: 911\n",
      "training loss: 28317081841927.605\n",
      "validation loss: 16223325587509.896\n",
      "epoch: 912\n",
      "training loss: 28315938164790.84\n",
      "validation loss: 16220765770371.203\n",
      "epoch: 913\n",
      "training loss: 28314797701862.754\n",
      "validation loss: 16218213944647.47\n",
      "epoch: 914\n",
      "training loss: 28313660441860.625\n",
      "validation loss: 16215670083236.473\n",
      "epoch: 915\n",
      "training loss: 28312526373549.73\n",
      "validation loss: 16213134159138.256\n",
      "epoch: 916\n",
      "training loss: 28311395485742.92\n",
      "validation loss: 16210606145454.686\n",
      "epoch: 917\n",
      "training loss: 28310267767299.977\n",
      "validation loss: 16208086015389.059\n",
      "epoch: 918\n",
      "training loss: 28309143207125.875\n",
      "validation loss: 16205573742245.617\n",
      "epoch: 919\n",
      "training loss: 28308021794162.426\n",
      "validation loss: 16203069299429.111\n",
      "epoch: 920\n",
      "training loss: 28306903517290.168\n",
      "validation loss: 16200572660444.012\n",
      "epoch: 921\n",
      "training loss: 28305788330127.676\n",
      "validation loss: 16198083798786.94\n",
      "epoch: 922\n",
      "training loss: 28311152218582.773\n",
      "validation loss: 16199057015792.543\n",
      "epoch: 923\n",
      "training loss: 28310028460227.254\n",
      "validation loss: 16196553985664.902\n",
      "epoch: 924\n",
      "training loss: 28308907839555.01\n",
      "validation loss: 16194058754062.12\n",
      "epoch: 925\n",
      "training loss: 28307790345582.344\n",
      "validation loss: 16191571294640.354\n",
      "epoch: 926\n",
      "training loss: 28306675967351.984\n",
      "validation loss: 16189091581094.686\n",
      "epoch: 927\n",
      "training loss: 28305564693953.547\n",
      "validation loss: 16186619587219.451\n",
      "epoch: 928\n",
      "training loss: 28304456514523.242\n",
      "validation loss: 16184155286907.855\n",
      "epoch: 929\n",
      "training loss: 28303351418243.668\n",
      "validation loss: 16181698654151.531\n",
      "epoch: 930\n",
      "training loss: 28302249394343.543\n",
      "validation loss: 16179249663040.113\n",
      "epoch: 931\n",
      "training loss: 28301150432097.484\n",
      "validation loss: 16176808287760.832\n",
      "epoch: 932\n",
      "training loss: 28300054520825.758\n",
      "validation loss: 16174374502598.123\n",
      "epoch: 933\n",
      "training loss: 28298961649894.027\n",
      "validation loss: 16171948281933.186\n",
      "epoch: 934\n",
      "training loss: 28297871808713.168\n",
      "validation loss: 16169529600243.578\n",
      "epoch: 935\n",
      "training loss: 28296784986738.945\n",
      "validation loss: 16167118432102.8\n",
      "epoch: 936\n",
      "training loss: 28295701173471.863\n",
      "validation loss: 16164714752179.947\n",
      "epoch: 937\n",
      "training loss: 28294620358456.89\n",
      "validation loss: 16162318535239.225\n",
      "epoch: 938\n",
      "training loss: 28293542531283.23\n",
      "validation loss: 16159929756139.598\n",
      "epoch: 939\n",
      "training loss: 28292467681584.1\n",
      "validation loss: 16157548389834.38\n",
      "epoch: 940\n",
      "training loss: 28291395799036.516\n",
      "validation loss: 16155174411370.822\n",
      "epoch: 941\n",
      "training loss: 28290326873361.02\n",
      "validation loss: 16152807795889.74\n",
      "epoch: 942\n",
      "training loss: 28289260894321.504\n",
      "validation loss: 16150448518625.094\n",
      "epoch: 943\n",
      "training loss: 28288197851724.973\n",
      "validation loss: 16148096554903.615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 944\n",
      "training loss: 28287137735421.293\n",
      "validation loss: 16145751880144.418\n",
      "epoch: 945\n",
      "training loss: 28286080535303.0\n",
      "validation loss: 16143414469858.58\n",
      "epoch: 946\n",
      "training loss: 28285026241305.074\n",
      "validation loss: 16141084299648.81\n",
      "epoch: 947\n",
      "training loss: 28283974843404.73\n",
      "validation loss: 16138761345209.004\n",
      "epoch: 948\n",
      "training loss: 28282926331621.156\n",
      "validation loss: 16136445582323.89\n",
      "epoch: 949\n",
      "training loss: 28281880696015.344\n",
      "validation loss: 16134136986868.672\n",
      "epoch: 950\n",
      "training loss: 28280837926689.875\n",
      "validation loss: 16131835534808.602\n",
      "epoch: 951\n",
      "training loss: 28279798013788.645\n",
      "validation loss: 16129541202198.62\n",
      "epoch: 952\n",
      "training loss: 28278760947496.75\n",
      "validation loss: 16127253965183.012\n",
      "epoch: 953\n",
      "training loss: 28277726718040.184\n",
      "validation loss: 16124973799994.975\n",
      "epoch: 954\n",
      "training loss: 28276695315685.66\n",
      "validation loss: 16122700682956.285\n",
      "epoch: 955\n",
      "training loss: 28275666730740.453\n",
      "validation loss: 16120434590476.912\n",
      "epoch: 956\n",
      "training loss: 28274640953552.098\n",
      "validation loss: 16118175499054.668\n",
      "epoch: 957\n",
      "training loss: 28273617974508.254\n",
      "validation loss: 16115923385274.824\n",
      "epoch: 958\n",
      "training loss: 28272597784036.477\n",
      "validation loss: 16113678225809.748\n",
      "epoch: 959\n",
      "training loss: 28271580372604.016\n",
      "validation loss: 16111439997418.533\n",
      "epoch: 960\n",
      "training loss: 28270565730717.605\n",
      "validation loss: 16109208676946.645\n",
      "epoch: 961\n",
      "training loss: 28269553848923.277\n",
      "validation loss: 16106984241325.564\n",
      "epoch: 962\n",
      "training loss: 28268544717806.145\n",
      "validation loss: 16104766667572.414\n",
      "epoch: 963\n",
      "training loss: 28267538327990.223\n",
      "validation loss: 16102555932789.635\n",
      "epoch: 964\n",
      "training loss: 28266534670138.21\n",
      "validation loss: 16100352014164.582\n",
      "epoch: 965\n",
      "training loss: 28265533734951.293\n",
      "validation loss: 16098154888969.186\n",
      "epoch: 966\n",
      "training loss: 28264535513168.992\n",
      "validation loss: 16095964534559.668\n",
      "epoch: 967\n",
      "training loss: 28263539995568.895\n",
      "validation loss: 16093780928376.055\n",
      "epoch: 968\n",
      "training loss: 28262547172966.523\n",
      "validation loss: 16091604047941.977\n",
      "epoch: 969\n",
      "training loss: 28261557036215.105\n",
      "validation loss: 16089433870864.186\n",
      "epoch: 970\n",
      "training loss: 28260569576205.426\n",
      "validation loss: 16087270374832.32\n",
      "epoch: 971\n",
      "training loss: 28259584783865.586\n",
      "validation loss: 16085113537618.516\n",
      "epoch: 972\n",
      "training loss: 28258602650160.855\n",
      "validation loss: 16082963337077.033\n",
      "epoch: 973\n",
      "training loss: 28257623166093.445\n",
      "validation loss: 16080819751143.975\n",
      "epoch: 974\n",
      "training loss: 28256646322702.375\n",
      "validation loss: 16078682757836.887\n",
      "epoch: 975\n",
      "training loss: 28255672111063.24\n",
      "validation loss: 16076552335254.498\n",
      "epoch: 976\n",
      "training loss: 28254700522288.04\n",
      "validation loss: 16074428461576.275\n",
      "epoch: 977\n",
      "training loss: 28253731547525.004\n",
      "validation loss: 16072311115062.207\n",
      "epoch: 978\n",
      "training loss: 28252765177958.418\n",
      "validation loss: 16070200274052.404\n",
      "epoch: 979\n",
      "training loss: 28251801404808.414\n",
      "validation loss: 16068095916966.744\n",
      "epoch: 980\n",
      "training loss: 28250840219330.816\n",
      "validation loss: 16065998022304.617\n",
      "epoch: 981\n",
      "training loss: 28249881612816.953\n",
      "validation loss: 16063906568644.535\n",
      "epoch: 982\n",
      "training loss: 28248925576593.47\n",
      "validation loss: 16061821534643.848\n",
      "epoch: 983\n",
      "training loss: 28247972102022.168\n",
      "validation loss: 16059742899038.373\n",
      "epoch: 984\n",
      "training loss: 28247021180499.832\n",
      "validation loss: 16057670640642.105\n",
      "epoch: 985\n",
      "training loss: 28246072803458.02\n",
      "validation loss: 16055604738346.873\n",
      "epoch: 986\n",
      "training loss: 28245126962362.95\n",
      "validation loss: 16053545171122.062\n",
      "epoch: 987\n",
      "training loss: 28244183648715.266\n",
      "validation loss: 16051491918014.203\n",
      "epoch: 988\n",
      "training loss: 28243242854049.902\n",
      "validation loss: 16049444958146.777\n",
      "epoch: 989\n",
      "training loss: 28242304569935.91\n",
      "validation loss: 16047404270719.809\n",
      "epoch: 990\n",
      "training loss: 28241368787976.27\n",
      "validation loss: 16045369835009.598\n",
      "epoch: 991\n",
      "training loss: 28240435499807.754\n",
      "validation loss: 16043341630368.344\n",
      "epoch: 992\n",
      "training loss: 28239504697100.72\n",
      "validation loss: 16041319636223.885\n",
      "epoch: 993\n",
      "training loss: 28238576371558.98\n",
      "validation loss: 16039303832079.438\n",
      "epoch: 994\n",
      "training loss: 28237650514919.6\n",
      "validation loss: 16037294197513.172\n",
      "epoch: 995\n",
      "training loss: 28236727118952.777\n",
      "validation loss: 16035290712177.979\n",
      "epoch: 996\n",
      "training loss: 28235806175461.64\n",
      "validation loss: 16033293355801.14\n",
      "epoch: 997\n",
      "training loss: 28234887676282.098\n",
      "validation loss: 16031302108184.025\n",
      "epoch: 998\n",
      "training loss: 28233971613282.688\n",
      "validation loss: 16029316949201.8\n",
      "epoch: 999\n",
      "training loss: 28233057978364.395\n",
      "validation loss: 16027337858803.105\n",
      "epoch: 1000\n",
      "training loss: 28232146763460.523\n",
      "validation loss: 16025364817009.79\n",
      "epoch: 1001\n",
      "training loss: 28231237960536.492\n",
      "validation loss: 16023397803916.555\n",
      "epoch: 1002\n",
      "training loss: 28230331561589.734\n",
      "validation loss: 16021436799690.717\n",
      "epoch: 1003\n",
      "training loss: 28229427558649.48\n",
      "validation loss: 16019481784571.879\n",
      "epoch: 1004\n",
      "training loss: 28228525943776.645\n",
      "validation loss: 16017532738871.63\n",
      "epoch: 1005\n",
      "training loss: 28227626709063.633\n",
      "validation loss: 16015589642973.277\n",
      "epoch: 1006\n",
      "training loss: 28226729846634.254\n",
      "validation loss: 16013652477331.516\n",
      "epoch: 1007\n",
      "training loss: 28225835348643.477\n",
      "validation loss: 16011721222472.184\n",
      "epoch: 1008\n",
      "training loss: 28224943207277.36\n",
      "validation loss: 16009795858991.934\n",
      "epoch: 1009\n",
      "training loss: 28224053414752.83\n",
      "validation loss: 16007876367557.965\n",
      "epoch: 1010\n",
      "training loss: 28223165963317.582\n",
      "validation loss: 16005962728907.68\n",
      "epoch: 1011\n",
      "training loss: 28222280845249.93\n",
      "validation loss: 16004054923848.547\n",
      "epoch: 1012\n",
      "training loss: 28221398052858.61\n",
      "validation loss: 16002152933257.613\n",
      "epoch: 1013\n",
      "training loss: 28220517578482.668\n",
      "validation loss: 16000256738081.38\n",
      "epoch: 1014\n",
      "training loss: 28219639414491.316\n",
      "validation loss: 15998366319335.459\n",
      "epoch: 1015\n",
      "training loss: 28218763553283.766\n",
      "validation loss: 15996481658104.309\n",
      "epoch: 1016\n",
      "training loss: 28217889987289.09\n",
      "validation loss: 15994602735540.938\n",
      "epoch: 1017\n",
      "training loss: 28217018708966.1\n",
      "validation loss: 15992729532866.58\n",
      "epoch: 1018\n",
      "training loss: 28216149710803.164\n",
      "validation loss: 15990862031370.584\n",
      "epoch: 1019\n",
      "training loss: 28215282985318.09\n",
      "validation loss: 15989000212409.906\n",
      "epoch: 1020\n",
      "training loss: 28214418525057.96\n",
      "validation loss: 15987144057409.025\n",
      "epoch: 1021\n",
      "training loss: 28213556322599.04\n",
      "validation loss: 15985293547859.527\n",
      "epoch: 1022\n",
      "training loss: 28212696370546.54\n",
      "validation loss: 15983448665319.967\n",
      "epoch: 1023\n",
      "training loss: 28211838661534.61\n",
      "validation loss: 15981609391415.594\n",
      "epoch: 1024\n",
      "training loss: 28210983188226.082\n",
      "validation loss: 15979775707837.846\n",
      "epoch: 1025\n",
      "training loss: 28210129943312.387\n",
      "validation loss: 15977947596344.443\n",
      "epoch: 1026\n",
      "training loss: 28209278919513.418\n",
      "validation loss: 15976125038758.814\n",
      "epoch: 1027\n",
      "training loss: 28208430109577.37\n",
      "validation loss: 15974308016970.008\n",
      "epoch: 1028\n",
      "training loss: 28207583506280.625\n",
      "validation loss: 15972496512932.316\n",
      "epoch: 1029\n",
      "training loss: 28206739102427.613\n",
      "validation loss: 15970690508665.195\n",
      "epoch: 1030\n",
      "training loss: 28205896890850.656\n",
      "validation loss: 15968889986252.693\n",
      "epoch: 1031\n",
      "training loss: 28205056864409.875\n",
      "validation loss: 15967094927843.547\n",
      "epoch: 1032\n",
      "training loss: 28204219015993.027\n",
      "validation loss: 15965305315650.63\n",
      "epoch: 1033\n",
      "training loss: 28203383338515.367\n",
      "validation loss: 15963521131950.88\n",
      "epoch: 1034\n",
      "training loss: 28202549824919.56\n",
      "validation loss: 15961742359084.896\n",
      "epoch: 1035\n",
      "training loss: 28201718468175.484\n",
      "validation loss: 15959968979456.832\n",
      "epoch: 1036\n",
      "training loss: 28200889261280.152\n",
      "validation loss: 15958200975534.002\n",
      "epoch: 1037\n",
      "training loss: 28200062197257.59\n",
      "validation loss: 15956438329846.756\n",
      "epoch: 1038\n",
      "training loss: 28199237269158.645\n",
      "validation loss: 15954681024988.166\n",
      "epoch: 1039\n",
      "training loss: 28198414470060.918\n",
      "validation loss: 15952929043613.703\n",
      "epoch: 1040\n",
      "training loss: 28197593793068.63\n",
      "validation loss: 15951182368441.13\n",
      "epoch: 1041\n",
      "training loss: 28196775231312.46\n",
      "validation loss: 15949440982250.094\n",
      "epoch: 1042\n",
      "training loss: 28195958777949.46\n",
      "validation loss: 15947704867882.043\n",
      "epoch: 1043\n",
      "training loss: 28195144426162.906\n",
      "validation loss: 15945974008239.822\n",
      "epoch: 1044\n",
      "training loss: 28194332169162.18\n",
      "validation loss: 15944248386287.58\n",
      "epoch: 1045\n",
      "training loss: 28193522000182.645\n",
      "validation loss: 15942527985050.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1046\n",
      "training loss: 28192713912485.535\n",
      "validation loss: 15940812787614.041\n",
      "epoch: 1047\n",
      "training loss: 28191907899357.81\n",
      "validation loss: 15939102777124.883\n",
      "epoch: 1048\n",
      "training loss: 28191103954112.062\n",
      "validation loss: 15937397936789.426\n",
      "epoch: 1049\n",
      "training loss: 28190302070086.367\n",
      "validation loss: 15935698249874.299\n",
      "epoch: 1050\n",
      "training loss: 28189502240644.184\n",
      "validation loss: 15934003699705.89\n",
      "epoch: 1051\n",
      "training loss: 28188704459174.223\n",
      "validation loss: 15932314269670.047\n",
      "epoch: 1052\n",
      "training loss: 28187908719090.344\n",
      "validation loss: 15930629943211.922\n",
      "epoch: 1053\n",
      "training loss: 28187115013831.426\n",
      "validation loss: 15928950703835.701\n",
      "epoch: 1054\n",
      "training loss: 28186323336861.242\n",
      "validation loss: 15927276535104.5\n",
      "epoch: 1055\n",
      "training loss: 28185533681668.37\n",
      "validation loss: 15925607420640.031\n",
      "epoch: 1056\n",
      "training loss: 28184746041766.04\n",
      "validation loss: 15923943344122.256\n",
      "epoch: 1057\n",
      "training loss: 28183960410692.05\n",
      "validation loss: 15922284289289.28\n",
      "epoch: 1058\n",
      "training loss: 28183176782008.64\n",
      "validation loss: 15920630239937.072\n",
      "epoch: 1059\n",
      "training loss: 28182395149302.37\n",
      "validation loss: 15918981179919.396\n",
      "epoch: 1060\n",
      "training loss: 28181615506184.023\n",
      "validation loss: 15917337093147.316\n",
      "epoch: 1061\n",
      "training loss: 28180837846288.48\n",
      "validation loss: 15915697963589.23\n",
      "epoch: 1062\n",
      "training loss: 28180062163274.61\n",
      "validation loss: 15914063775270.299\n",
      "epoch: 1063\n",
      "training loss: 28179288450825.156\n",
      "validation loss: 15912434512272.639\n",
      "epoch: 1064\n",
      "training loss: 28178516702646.65\n",
      "validation loss: 15910810158734.91\n",
      "epoch: 1065\n",
      "training loss: 28177746912469.25\n",
      "validation loss: 15909190698851.844\n",
      "epoch: 1066\n",
      "training loss: 28176979074046.684\n",
      "validation loss: 15907576116874.475\n",
      "epoch: 1067\n",
      "training loss: 28176213181156.105\n",
      "validation loss: 15905966397109.59\n",
      "epoch: 1068\n",
      "training loss: 28175449227598.01\n",
      "validation loss: 15904361523919.73\n",
      "epoch: 1069\n",
      "training loss: 28174687207196.1\n",
      "validation loss: 15902761481722.613\n",
      "epoch: 1070\n",
      "training loss: 28173927113797.223\n",
      "validation loss: 15901166254991.39\n",
      "epoch: 1071\n",
      "training loss: 28173168941271.188\n",
      "validation loss: 15899575828254.078\n",
      "epoch: 1072\n",
      "training loss: 28172412683510.75\n",
      "validation loss: 15897990186093.45\n",
      "epoch: 1073\n",
      "training loss: 28171658334431.445\n",
      "validation loss: 15896409313146.945\n",
      "epoch: 1074\n",
      "training loss: 28170905887971.496\n",
      "validation loss: 15894833194106.229\n",
      "epoch: 1075\n",
      "training loss: 28170155338091.72\n",
      "validation loss: 15893261813716.932\n",
      "epoch: 1076\n",
      "training loss: 28169406678775.41\n",
      "validation loss: 15891695156778.852\n",
      "epoch: 1077\n",
      "training loss: 28168659904028.27\n",
      "validation loss: 15890133208145.486\n",
      "epoch: 1078\n",
      "training loss: 28167915007878.25\n",
      "validation loss: 15888575952723.54\n",
      "epoch: 1079\n",
      "training loss: 28167171984375.49\n",
      "validation loss: 15887023375473.115\n",
      "epoch: 1080\n",
      "training loss: 28166430827592.2\n",
      "validation loss: 15885475461407.52\n",
      "epoch: 1081\n",
      "training loss: 28165691531622.574\n",
      "validation loss: 15883932195592.764\n",
      "epoch: 1082\n",
      "training loss: 28164954090582.684\n",
      "validation loss: 15882393563147.506\n",
      "epoch: 1083\n",
      "training loss: 28164218498610.355\n",
      "validation loss: 15880859549242.691\n",
      "epoch: 1084\n",
      "training loss: 28163484749865.117\n",
      "validation loss: 15879330139101.926\n",
      "epoch: 1085\n",
      "training loss: 28162752838528.055\n",
      "validation loss: 15877805318000.408\n",
      "epoch: 1086\n",
      "training loss: 28162022758801.746\n",
      "validation loss: 15876285071265.299\n",
      "epoch: 1087\n",
      "training loss: 28161294504910.15\n",
      "validation loss: 15874769384275.373\n",
      "epoch: 1088\n",
      "training loss: 28160568071098.492\n",
      "validation loss: 15873258242460.965\n",
      "epoch: 1089\n",
      "training loss: 28159843451633.203\n",
      "validation loss: 15871751631303.338\n",
      "epoch: 1090\n",
      "training loss: 28159120640801.81\n",
      "validation loss: 15870249536334.889\n",
      "epoch: 1091\n",
      "training loss: 28158399632912.832\n",
      "validation loss: 15868751943139.002\n",
      "epoch: 1092\n",
      "training loss: 28157680422295.67\n",
      "validation loss: 15867258837349.354\n",
      "epoch: 1093\n",
      "training loss: 28156963003300.58\n",
      "validation loss: 15865770204650.36\n",
      "epoch: 1094\n",
      "training loss: 28156247370298.484\n",
      "validation loss: 15864286030776.484\n",
      "epoch: 1095\n",
      "training loss: 28155533517680.945\n",
      "validation loss: 15862806301512.045\n",
      "epoch: 1096\n",
      "training loss: 28154821439860.055\n",
      "validation loss: 15861331002691.533\n",
      "epoch: 1097\n",
      "training loss: 28154111131268.34\n",
      "validation loss: 15859860120198.703\n",
      "epoch: 1098\n",
      "training loss: 28153402586358.668\n",
      "validation loss: 15858393639967.117\n",
      "epoch: 1099\n",
      "training loss: 28152695799604.164\n",
      "validation loss: 15856931547979.215\n",
      "epoch: 1100\n",
      "training loss: 28151990765498.113\n",
      "validation loss: 15855473830266.855\n",
      "epoch: 1101\n",
      "training loss: 28151287478553.863\n",
      "validation loss: 15854020472910.377\n",
      "epoch: 1102\n",
      "training loss: 28150585933304.754\n",
      "validation loss: 15852571462038.984\n",
      "epoch: 1103\n",
      "training loss: 28149886124304.01\n",
      "validation loss: 15851126783830.555\n",
      "epoch: 1104\n",
      "training loss: 28149188046124.676\n",
      "validation loss: 15849686424510.867\n",
      "epoch: 1105\n",
      "training loss: 28148491693359.49\n",
      "validation loss: 15848250370353.994\n",
      "epoch: 1106\n",
      "training loss: 28147797060620.83\n",
      "validation loss: 15846818607681.93\n",
      "epoch: 1107\n",
      "training loss: 28147104142540.598\n",
      "validation loss: 15845391122864.38\n",
      "epoch: 1108\n",
      "training loss: 28146412933770.188\n",
      "validation loss: 15843967902318.635\n",
      "epoch: 1109\n",
      "training loss: 28145723428980.31\n",
      "validation loss: 15842548932509.3\n",
      "epoch: 1110\n",
      "training loss: 28145035622860.984\n",
      "validation loss: 15841134199948.176\n",
      "epoch: 1111\n",
      "training loss: 28144349510121.43\n",
      "validation loss: 15839723691193.842\n",
      "epoch: 1112\n",
      "training loss: 28143665085489.95\n",
      "validation loss: 15838317392851.89\n",
      "epoch: 1113\n",
      "training loss: 28142982343713.902\n",
      "validation loss: 15836915291574.768\n",
      "epoch: 1114\n",
      "training loss: 28142301279559.57\n",
      "validation loss: 15835517374060.926\n",
      "epoch: 1115\n",
      "training loss: 28141621887812.086\n",
      "validation loss: 15834123627055.297\n",
      "epoch: 1116\n",
      "training loss: 28140944163275.375\n",
      "validation loss: 15832734037348.914\n",
      "epoch: 1117\n",
      "training loss: 28140268100772.06\n",
      "validation loss: 15831348591778.47\n",
      "epoch: 1118\n",
      "training loss: 28139593695143.332\n",
      "validation loss: 15829967277226.865\n",
      "epoch: 1119\n",
      "training loss: 28138920941248.953\n",
      "validation loss: 15828590080622.086\n",
      "epoch: 1120\n",
      "training loss: 28138249833967.105\n",
      "validation loss: 15827216988937.71\n",
      "epoch: 1121\n",
      "training loss: 28137580368194.348\n",
      "validation loss: 15825847989192.549\n",
      "epoch: 1122\n",
      "training loss: 28136912538845.523\n",
      "validation loss: 15824483068450.416\n",
      "epoch: 1123\n",
      "training loss: 28136246340853.676\n",
      "validation loss: 15823122213819.742\n",
      "epoch: 1124\n",
      "training loss: 28135581769169.96\n",
      "validation loss: 15821765412453.861\n",
      "epoch: 1125\n",
      "training loss: 28134918818763.605\n",
      "validation loss: 15820412651550.65\n",
      "epoch: 1126\n",
      "training loss: 28134257484621.777\n",
      "validation loss: 15819063918352.299\n",
      "epoch: 1127\n",
      "training loss: 28133597761749.543\n",
      "validation loss: 15817719200145.174\n",
      "epoch: 1128\n",
      "training loss: 28132939645169.79\n",
      "validation loss: 15816378484259.354\n",
      "epoch: 1129\n",
      "training loss: 28132283129923.11\n",
      "validation loss: 15815041758069.018\n",
      "epoch: 1130\n",
      "training loss: 28131628211067.77\n",
      "validation loss: 15813709008991.994\n",
      "epoch: 1131\n",
      "training loss: 28130974883679.617\n",
      "validation loss: 15812380224489.314\n",
      "epoch: 1132\n",
      "training loss: 28130323142851.977\n",
      "validation loss: 15811055392065.613\n",
      "epoch: 1133\n",
      "training loss: 28129672983695.63\n",
      "validation loss: 15809734499268.68\n",
      "epoch: 1134\n",
      "training loss: 28129024401338.688\n",
      "validation loss: 15808417533689.293\n",
      "epoch: 1135\n",
      "training loss: 28128377390926.547\n",
      "validation loss: 15807104482960.734\n",
      "epoch: 1136\n",
      "training loss: 28127731947621.79\n",
      "validation loss: 15805795334759.26\n",
      "epoch: 1137\n",
      "training loss: 28127088066604.137\n",
      "validation loss: 15804490076803.613\n",
      "epoch: 1138\n",
      "training loss: 28126445743070.348\n",
      "validation loss: 15803188696854.549\n",
      "epoch: 1139\n",
      "training loss: 28125804972234.19\n",
      "validation loss: 15801891182715.295\n",
      "epoch: 1140\n",
      "training loss: 28125165749326.293\n",
      "validation loss: 15800597522231.098\n",
      "epoch: 1141\n",
      "training loss: 28124528069594.15\n",
      "validation loss: 15799307703289.031\n",
      "epoch: 1142\n",
      "training loss: 28123891928301.996\n",
      "validation loss: 15798021713817.537\n",
      "epoch: 1143\n",
      "training loss: 28123257320730.77\n",
      "validation loss: 15796739541786.56\n",
      "epoch: 1144\n",
      "training loss: 28122624242178.008\n",
      "validation loss: 15795461175208.076\n",
      "epoch: 1145\n",
      "training loss: 28121992687957.79\n",
      "validation loss: 15794186602134.602\n",
      "epoch: 1146\n",
      "training loss: 28121362653400.71\n",
      "validation loss: 15792915810659.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1147\n",
      "training loss: 28120734133853.703\n",
      "validation loss: 15791648788918.07\n",
      "epoch: 1148\n",
      "training loss: 28120107124680.082\n",
      "validation loss: 15790385525085.297\n",
      "epoch: 1149\n",
      "training loss: 28119481621259.406\n",
      "validation loss: 15789126007377.443\n",
      "epoch: 1150\n",
      "training loss: 28118857618987.426\n",
      "validation loss: 15787870224050.66\n",
      "epoch: 1151\n",
      "training loss: 28118235113276.03\n",
      "validation loss: 15786618163401.34\n",
      "epoch: 1152\n",
      "training loss: 28117614099553.137\n",
      "validation loss: 15785369813766.74\n",
      "epoch: 1153\n",
      "training loss: 28116994573262.68\n",
      "validation loss: 15784125163522.941\n",
      "epoch: 1154\n",
      "training loss: 28116376529864.492\n",
      "validation loss: 15782884201086.967\n",
      "epoch: 1155\n",
      "training loss: 28115759964834.28\n",
      "validation loss: 15781646914914.732\n",
      "epoch: 1156\n",
      "training loss: 28115144873663.504\n",
      "validation loss: 15780413293501.625\n",
      "epoch: 1157\n",
      "training loss: 28114531251859.38\n",
      "validation loss: 15779183325382.795\n",
      "epoch: 1158\n",
      "training loss: 28113919094944.72\n",
      "validation loss: 15777956999132.59\n",
      "epoch: 1159\n",
      "training loss: 28113308398457.973\n",
      "validation loss: 15776734303363.986\n",
      "epoch: 1160\n",
      "training loss: 28112699157953.09\n",
      "validation loss: 15775515226729.29\n",
      "epoch: 1161\n",
      "training loss: 28112091368999.457\n",
      "validation loss: 15774299757919.121\n",
      "epoch: 1162\n",
      "training loss: 28111485027181.875\n",
      "validation loss: 15773087885663.137\n",
      "epoch: 1163\n",
      "training loss: 28110880128100.445\n",
      "validation loss: 15771879598729.0\n",
      "epoch: 1164\n",
      "training loss: 28110276667370.547\n",
      "validation loss: 15770674885923.139\n",
      "epoch: 1165\n",
      "training loss: 28109674640622.734\n",
      "validation loss: 15769473736089.686\n",
      "epoch: 1166\n",
      "training loss: 28109074043502.707\n",
      "validation loss: 15768276138111.236\n",
      "epoch: 1167\n",
      "training loss: 28108474871671.21\n",
      "validation loss: 15767082080907.818\n",
      "epoch: 1168\n",
      "training loss: 28107877120804.035\n",
      "validation loss: 15765891553437.623\n",
      "epoch: 1169\n",
      "training loss: 28107280786591.855\n",
      "validation loss: 15764704544696.45\n",
      "epoch: 1170\n",
      "training loss: 28106685864740.273\n",
      "validation loss: 15763521043716.59\n",
      "epoch: 1171\n",
      "training loss: 28106092350969.67\n",
      "validation loss: 15762341039569.047\n",
      "epoch: 1172\n",
      "training loss: 28105500241015.21\n",
      "validation loss: 15761164521360.537\n",
      "epoch: 1173\n",
      "training loss: 28104909530626.72\n",
      "validation loss: 15759991478236.264\n",
      "epoch: 1174\n",
      "training loss: 28104320215568.688\n",
      "validation loss: 15758821899376.812\n",
      "epoch: 1175\n",
      "training loss: 28103732291620.145\n",
      "validation loss: 15757655774000.486\n",
      "epoch: 1176\n",
      "training loss: 28103145754574.66\n",
      "validation loss: 15756493091362.219\n",
      "epoch: 1177\n",
      "training loss: 28102560600240.21\n",
      "validation loss: 15755333840752.85\n",
      "epoch: 1178\n",
      "training loss: 28101976824439.203\n",
      "validation loss: 15754178011499.537\n",
      "epoch: 1179\n",
      "training loss: 28101394423008.348\n",
      "validation loss: 15753025592966.156\n",
      "epoch: 1180\n",
      "training loss: 28100813391798.637\n",
      "validation loss: 15751876574552.078\n",
      "epoch: 1181\n",
      "training loss: 28100233726675.258\n",
      "validation loss: 15750730945693.11\n",
      "epoch: 1182\n",
      "training loss: 28099655423517.574\n",
      "validation loss: 15749588695860.295\n",
      "epoch: 1183\n",
      "training loss: 28099078478219.02\n",
      "validation loss: 15748449814560.852\n",
      "epoch: 1184\n",
      "training loss: 28098502886687.07\n",
      "validation loss: 15747314291336.94\n",
      "epoch: 1185\n",
      "training loss: 28097928644843.18\n",
      "validation loss: 15746182115766.068\n",
      "epoch: 1186\n",
      "training loss: 28097355748622.715\n",
      "validation loss: 15745053277461.543\n",
      "epoch: 1187\n",
      "training loss: 28096784193974.918\n",
      "validation loss: 15743927766071.197\n",
      "epoch: 1188\n",
      "training loss: 28096213976862.824\n",
      "validation loss: 15742805571277.791\n",
      "epoch: 1189\n",
      "training loss: 28095645093263.223\n",
      "validation loss: 15741686682799.53\n",
      "epoch: 1190\n",
      "training loss: 28095077539166.59\n",
      "validation loss: 15740571090388.686\n",
      "epoch: 1191\n",
      "training loss: 28094511310577.03\n",
      "validation loss: 15739458783832.742\n",
      "epoch: 1192\n",
      "training loss: 28093946403512.25\n",
      "validation loss: 15738349752952.973\n",
      "epoch: 1193\n",
      "training loss: 28093382814003.45\n",
      "validation loss: 15737243987604.953\n",
      "epoch: 1194\n",
      "training loss: 28092820538095.324\n",
      "validation loss: 15736141477678.424\n",
      "epoch: 1195\n",
      "training loss: 28092259571845.973\n",
      "validation loss: 15735042213097.803\n",
      "epoch: 1196\n",
      "training loss: 28091699911326.82\n",
      "validation loss: 15733946183821.426\n",
      "epoch: 1197\n",
      "training loss: 28091141552622.67\n",
      "validation loss: 15732853379840.125\n",
      "epoch: 1198\n",
      "training loss: 28090584491831.504\n",
      "validation loss: 15731763791179.684\n",
      "epoch: 1199\n",
      "training loss: 28090028725064.535\n",
      "validation loss: 15730677407899.424\n",
      "epoch: 1200\n",
      "training loss: 28089474248446.098\n",
      "validation loss: 15729594220090.729\n",
      "epoch: 1201\n",
      "training loss: 28088921058113.637\n",
      "validation loss: 15728514217879.598\n",
      "epoch: 1202\n",
      "training loss: 28088369150217.625\n",
      "validation loss: 15727437391424.508\n",
      "epoch: 1203\n",
      "training loss: 28087818520921.496\n",
      "validation loss: 15726363730917.65\n",
      "epoch: 1204\n",
      "training loss: 28087269166401.656\n",
      "validation loss: 15725293226583.414\n",
      "epoch: 1205\n",
      "training loss: 28086721082847.344\n",
      "validation loss: 15724225868678.977\n",
      "epoch: 1206\n",
      "training loss: 28086174266460.64\n",
      "validation loss: 15723161647494.137\n",
      "epoch: 1207\n",
      "training loss: 28085628713456.42\n",
      "validation loss: 15722100553351.951\n",
      "epoch: 1208\n",
      "training loss: 28085084420062.266\n",
      "validation loss: 15721042576607.162\n",
      "epoch: 1209\n",
      "training loss: 28084541382518.402\n",
      "validation loss: 15719987707646.787\n",
      "epoch: 1210\n",
      "training loss: 28083999597077.72\n",
      "validation loss: 15718935936890.76\n",
      "epoch: 1211\n",
      "training loss: 28083459060005.645\n",
      "validation loss: 15717887254789.521\n",
      "epoch: 1212\n",
      "training loss: 28082919767580.145\n",
      "validation loss: 15716841651826.957\n",
      "epoch: 1213\n",
      "training loss: 28082381716091.617\n",
      "validation loss: 15715799118518.766\n",
      "epoch: 1214\n",
      "training loss: 28081844901842.934\n",
      "validation loss: 15714759645410.77\n",
      "epoch: 1215\n",
      "training loss: 28081309321149.28\n",
      "validation loss: 15713723223081.902\n",
      "epoch: 1216\n",
      "training loss: 28080774970338.195\n",
      "validation loss: 15712689842141.75\n",
      "epoch: 1217\n",
      "training loss: 28080241845749.473\n",
      "validation loss: 15711659493231.223\n",
      "epoch: 1218\n",
      "training loss: 28079709943735.117\n",
      "validation loss: 15710632167023.244\n",
      "epoch: 1219\n",
      "training loss: 28079179260659.324\n",
      "validation loss: 15709607854221.002\n",
      "epoch: 1220\n",
      "training loss: 28078649792898.402\n",
      "validation loss: 15708586545557.807\n",
      "epoch: 1221\n",
      "training loss: 28078121536840.73\n",
      "validation loss: 15707568231800.346\n",
      "epoch: 1222\n",
      "training loss: 28077594488886.71\n",
      "validation loss: 15706552903743.506\n",
      "epoch: 1223\n",
      "training loss: 28077068645448.74\n",
      "validation loss: 15705540552214.494\n",
      "epoch: 1224\n",
      "training loss: 28076544002951.12\n",
      "validation loss: 15704531168069.281\n",
      "epoch: 1225\n",
      "training loss: 28076020557830.06\n",
      "validation loss: 15703524742195.979\n",
      "epoch: 1226\n",
      "training loss: 28075498306533.586\n",
      "validation loss: 15702521265512.076\n",
      "epoch: 1227\n",
      "training loss: 28074977245521.535\n",
      "validation loss: 15701520728966.148\n",
      "epoch: 1228\n",
      "training loss: 28074457371265.45\n",
      "validation loss: 15700523123534.998\n",
      "epoch: 1229\n",
      "training loss: 28073938680248.605\n",
      "validation loss: 15699528440227.176\n",
      "epoch: 1230\n",
      "training loss: 28073421168965.906\n",
      "validation loss: 15698536670080.146\n",
      "epoch: 1231\n",
      "training loss: 28072904833923.863\n",
      "validation loss: 15697547804161.066\n",
      "epoch: 1232\n",
      "training loss: 28072389671640.55\n",
      "validation loss: 15696561833566.705\n",
      "epoch: 1233\n",
      "training loss: 28071875678645.547\n",
      "validation loss: 15695578749424.258\n",
      "epoch: 1234\n",
      "training loss: 28071362851479.895\n",
      "validation loss: 15694598542888.38\n",
      "epoch: 1235\n",
      "training loss: 28070851186696.062\n",
      "validation loss: 15693621205144.92\n",
      "epoch: 1236\n",
      "training loss: 28070340680857.906\n",
      "validation loss: 15692646727407.912\n",
      "epoch: 1237\n",
      "training loss: 28069831330540.58\n",
      "validation loss: 15691675100920.447\n",
      "epoch: 1238\n",
      "training loss: 28069323132330.57\n",
      "validation loss: 15690706316954.523\n",
      "epoch: 1239\n",
      "training loss: 28068816082825.562\n",
      "validation loss: 15689740366810.98\n",
      "epoch: 1240\n",
      "training loss: 28068310178634.473\n",
      "validation loss: 15688777241820.41\n",
      "epoch: 1241\n",
      "training loss: 28067805416377.36\n",
      "validation loss: 15687816933339.936\n",
      "epoch: 1242\n",
      "training loss: 28067301792685.383\n",
      "validation loss: 15686859432757.27\n",
      "epoch: 1243\n",
      "training loss: 28066799304200.79\n",
      "validation loss: 15685904731487.48\n",
      "epoch: 1244\n",
      "training loss: 28066297947576.832\n",
      "validation loss: 15684952820973.941\n",
      "epoch: 1245\n",
      "training loss: 28065797719477.766\n",
      "validation loss: 15684003692688.191\n",
      "epoch: 1246\n",
      "training loss: 28065298616578.758\n",
      "validation loss: 15683057338129.85\n",
      "epoch: 1247\n",
      "training loss: 28064800635565.895\n",
      "validation loss: 15682113748826.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1248\n",
      "training loss: 28064303773136.11\n",
      "validation loss: 15681172916333.629\n",
      "epoch: 1249\n",
      "training loss: 28063808025997.137\n",
      "validation loss: 15680234832235.564\n",
      "epoch: 1250\n",
      "training loss: 28063313390867.496\n",
      "validation loss: 15679299488142.066\n",
      "epoch: 1251\n",
      "training loss: 28062819864476.43\n",
      "validation loss: 15678366875692.73\n",
      "epoch: 1252\n",
      "training loss: 28062327443563.848\n",
      "validation loss: 15677436986552.322\n",
      "epoch: 1253\n",
      "training loss: 28061836124880.324\n",
      "validation loss: 15676509812415.232\n",
      "epoch: 1254\n",
      "training loss: 28061345905187.047\n",
      "validation loss: 15675585345001.998\n",
      "epoch: 1255\n",
      "training loss: 28060856781255.742\n",
      "validation loss: 15674663576060.27\n",
      "epoch: 1256\n",
      "training loss: 28060368749868.668\n",
      "validation loss: 15673744497364.764\n",
      "epoch: 1257\n",
      "training loss: 28059881807818.547\n",
      "validation loss: 15672828100717.139\n",
      "epoch: 1258\n",
      "training loss: 28059395951908.59\n",
      "validation loss: 15671914377945.89\n",
      "epoch: 1259\n",
      "training loss: 28058911178952.355\n",
      "validation loss: 15671003320906.275\n",
      "epoch: 1260\n",
      "training loss: 28058427485773.78\n",
      "validation loss: 15670094921481.43\n",
      "epoch: 1261\n",
      "training loss: 28057944869207.137\n",
      "validation loss: 15669189171578.57\n",
      "epoch: 1262\n",
      "training loss: 28057463326096.965\n",
      "validation loss: 15668286063132.6\n",
      "epoch: 1263\n",
      "training loss: 28056982853298.03\n",
      "validation loss: 15667385588106.03\n",
      "epoch: 1264\n",
      "training loss: 28056503447675.324\n",
      "validation loss: 15666487738485.102\n",
      "epoch: 1265\n",
      "training loss: 28056025106104.01\n",
      "validation loss: 15665592506283.477\n",
      "epoch: 1266\n",
      "training loss: 28055547825469.35\n",
      "validation loss: 15664699883542.172\n",
      "epoch: 1267\n",
      "training loss: 28055071602666.688\n",
      "validation loss: 15663809862325.57\n",
      "epoch: 1268\n",
      "training loss: 28054596434601.445\n",
      "validation loss: 15662922434726.535\n",
      "epoch: 1269\n",
      "training loss: 28054122318189.02\n",
      "validation loss: 15662037592861.088\n",
      "epoch: 1270\n",
      "training loss: 28053649250354.797\n",
      "validation loss: 15661155328873.547\n",
      "epoch: 1271\n",
      "training loss: 28053177228034.094\n",
      "validation loss: 15660275634932.527\n",
      "epoch: 1272\n",
      "training loss: 28052706248172.12\n",
      "validation loss: 15659398503230.744\n",
      "epoch: 1273\n",
      "training loss: 28052236307723.938\n",
      "validation loss: 15658523925989.03\n",
      "epoch: 1274\n",
      "training loss: 28051767403654.43\n",
      "validation loss: 15657651895450.756\n",
      "epoch: 1275\n",
      "training loss: 28051299532938.266\n",
      "validation loss: 15656782403887.3\n",
      "epoch: 1276\n",
      "training loss: 28050832692559.855\n",
      "validation loss: 15655915443592.33\n",
      "epoch: 1277\n",
      "training loss: 28050366879513.31\n",
      "validation loss: 15655051006887.373\n",
      "epoch: 1278\n",
      "training loss: 28049902090802.434\n",
      "validation loss: 15654189086116.049\n",
      "epoch: 1279\n",
      "training loss: 28049438323440.65\n",
      "validation loss: 15653329673649.654\n",
      "epoch: 1280\n",
      "training loss: 28048975574450.957\n",
      "validation loss: 15652472761882.816\n",
      "epoch: 1281\n",
      "training loss: 28048513840865.965\n",
      "validation loss: 15651618343233.361\n",
      "epoch: 1282\n",
      "training loss: 28048053119727.76\n",
      "validation loss: 15650766410145.115\n",
      "epoch: 1283\n",
      "training loss: 28047593408087.957\n",
      "validation loss: 15649916955087.871\n",
      "epoch: 1284\n",
      "training loss: 28047134703007.598\n",
      "validation loss: 15649069970552.838\n",
      "epoch: 1285\n",
      "training loss: 28046677001557.133\n",
      "validation loss: 15648225449058.521\n",
      "epoch: 1286\n",
      "training loss: 28046220300816.42\n",
      "validation loss: 15647383383144.607\n",
      "epoch: 1287\n",
      "training loss: 28045764597874.645\n",
      "validation loss: 15646543765376.418\n",
      "epoch: 1288\n",
      "training loss: 28045309889830.285\n",
      "validation loss: 15645706588343.293\n",
      "epoch: 1289\n",
      "training loss: 28044856173791.098\n",
      "validation loss: 15644871844660.068\n",
      "epoch: 1290\n",
      "training loss: 28044403446874.047\n",
      "validation loss: 15644039526962.37\n",
      "epoch: 1291\n",
      "training loss: 28043951706205.254\n",
      "validation loss: 15643209627911.102\n",
      "epoch: 1292\n",
      "training loss: 28043500948919.92\n",
      "validation loss: 15642382140190.861\n",
      "epoch: 1293\n",
      "training loss: 28043051172162.156\n",
      "validation loss: 15641557056509.846\n",
      "epoch: 1294\n",
      "training loss: 28042602373084.598\n",
      "validation loss: 15640734369599.777\n",
      "epoch: 1295\n",
      "training loss: 28042154548847.145\n",
      "validation loss: 15639914072214.166\n",
      "epoch: 1296\n",
      "training loss: 28041707696610.38\n",
      "validation loss: 15639096157133.09\n",
      "epoch: 1297\n",
      "training loss: 28041261813467.363\n",
      "validation loss: 15638280617158.281\n",
      "epoch: 1298\n",
      "training loss: 28040816886473.86\n",
      "validation loss: 15637467445112.99\n",
      "epoch: 1299\n",
      "training loss: 28040110722929.15\n",
      "validation loss: 15636295646732.62\n",
      "epoch: 1300\n",
      "training loss: 28071528479471.727\n",
      "validation loss: 15667875174464.787\n",
      "epoch: 1301\n",
      "training loss: 28055470614945.645\n",
      "validation loss: 15643753751543.568\n",
      "epoch: 1302\n",
      "training loss: 28064056005935.926\n",
      "validation loss: 15636591840860.445\n",
      "epoch: 1303\n",
      "training loss: 28105366646376.29\n",
      "validation loss: 15784354937430.61\n",
      "epoch: 1304\n",
      "training loss: 28076888297353.867\n",
      "validation loss: 15638624820869.527\n",
      "epoch: 1305\n",
      "training loss: 28063715169689.703\n",
      "validation loss: 15613210995570.002\n",
      "epoch: 1306\n",
      "training loss: 28059740008408.227\n",
      "validation loss: 15610631940852.746\n",
      "epoch: 1307\n",
      "training loss: 28426342002192.93\n",
      "validation loss: 15800616874971.902\n",
      "epoch: 1308\n",
      "training loss: 28893286634750.562\n",
      "validation loss: 17639230421480.527\n",
      "epoch: 1309\n",
      "training loss: 29582098839529.074\n",
      "validation loss: 18889585587751.363\n",
      "epoch: 1310\n",
      "training loss: 29591251347657.777\n",
      "validation loss: 19595965977269.52\n",
      "epoch: 1311\n",
      "training loss: 29856730670461.625\n",
      "validation loss: 19867825594043.73\n",
      "epoch: 1312\n",
      "training loss: 29984022302031.668\n",
      "validation loss: 20199809227232.32\n",
      "epoch: 1313\n",
      "training loss: 30110654434598.81\n",
      "validation loss: 20668819861087.65\n",
      "epoch: 1314\n",
      "training loss: 30083384603493.008\n",
      "validation loss: 20373370332746.57\n",
      "epoch: 1315\n",
      "training loss: 30088906921191.773\n",
      "validation loss: 20393447072061.33\n",
      "epoch: 1316\n",
      "training loss: 30301508239942.29\n",
      "validation loss: 21153245384471.29\n",
      "epoch: 1317\n",
      "training loss: 30160290212016.566\n",
      "validation loss: 20166989031263.902\n",
      "epoch: 1318\n",
      "training loss: 30135340666855.285\n",
      "validation loss: 19898403454595.957\n",
      "epoch: 1319\n",
      "training loss: 30128165345113.098\n",
      "validation loss: 19882936838675.664\n",
      "epoch: 1320\n",
      "training loss: 30121873918722.902\n",
      "validation loss: 19874903731720.426\n",
      "epoch: 1321\n",
      "training loss: 30181138197523.44\n",
      "validation loss: 20428181120994.703\n",
      "epoch: 1322\n",
      "training loss: 30199695630665.64\n",
      "validation loss: 20301338078234.055\n",
      "epoch: 1323\n",
      "training loss: 30125432493960.684\n",
      "validation loss: 20239222038240.684\n",
      "epoch: 1324\n",
      "training loss: 30104390194233.43\n",
      "validation loss: 20101053772315.613\n",
      "epoch: 1325\n",
      "training loss: 30058723966141.305\n",
      "validation loss: 19947770214198.957\n",
      "epoch: 1326\n",
      "training loss: 30057820449749.727\n",
      "validation loss: 19941870480528.266\n",
      "epoch: 1327\n",
      "training loss: 30053291470577.78\n",
      "validation loss: 19930546476925.35\n",
      "epoch: 1328\n",
      "training loss: 30057148208121.33\n",
      "validation loss: 19915580612905.906\n",
      "epoch: 1329\n",
      "training loss: 30053162549924.92\n",
      "validation loss: 19907202285981.406\n",
      "epoch: 1330\n",
      "training loss: 30041076682625.016\n",
      "validation loss: 19914187192949.96\n",
      "epoch: 1331\n",
      "training loss: 30037430709849.105\n",
      "validation loss: 19906068028464.703\n",
      "epoch: 1332\n",
      "training loss: 30033292270072.22\n",
      "validation loss: 19897635675098.27\n",
      "epoch: 1333\n",
      "training loss: 30061148685563.527\n",
      "validation loss: 20063108606790.203\n",
      "epoch: 1334\n",
      "training loss: 30044966893059.004\n",
      "validation loss: 20036228000545.35\n",
      "epoch: 1335\n",
      "training loss: 30072938200736.64\n",
      "validation loss: 20276308877674.137\n",
      "epoch: 1336\n",
      "training loss: 30074897888068.523\n",
      "validation loss: 20274533298415.035\n",
      "epoch: 1337\n",
      "training loss: 30038722056915.445\n",
      "validation loss: 20225711785264.02\n",
      "epoch: 1338\n",
      "training loss: 30045425353517.992\n",
      "validation loss: 20247694847484.82\n",
      "epoch: 1339\n",
      "training loss: 30041823968205.87\n",
      "validation loss: 20238759256014.46\n",
      "epoch: 1340\n",
      "training loss: 30038231521123.246\n",
      "validation loss: 20229839822876.535\n",
      "epoch: 1341\n",
      "training loss: 30034647983182.348\n",
      "validation loss: 20220936520188.633\n",
      "epoch: 1342\n",
      "training loss: 30031073325028.33\n",
      "validation loss: 20212049320207.65\n",
      "epoch: 1343\n",
      "training loss: 30033648963687.625\n",
      "validation loss: 20268416881782.082\n",
      "epoch: 1344\n",
      "training loss: 30030096977689.203\n",
      "validation loss: 20259597426627.445\n",
      "epoch: 1345\n",
      "training loss: 30026553733349.555\n",
      "validation loss: 20250793871355.953\n",
      "epoch: 1346\n",
      "training loss: 30023019202614.79\n",
      "validation loss: 20242006189238.26\n",
      "epoch: 1347\n",
      "training loss: 30019493357549.07\n",
      "validation loss: 20233234353409.508\n",
      "epoch: 1348\n",
      "training loss: 30015976170345.45\n",
      "validation loss: 20224478337085.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1349\n",
      "training loss: 30012467613325.066\n",
      "validation loss: 20215738113558.45\n",
      "epoch: 1350\n",
      "training loss: 30008967658936.16\n",
      "validation loss: 20207013656190.43\n",
      "epoch: 1351\n",
      "training loss: 30005476279752.81\n",
      "validation loss: 20198304938411.375\n",
      "epoch: 1352\n",
      "training loss: 30001993448472.277\n",
      "validation loss: 20189611933714.715\n",
      "epoch: 1353\n",
      "training loss: 29998519137903.24\n",
      "validation loss: 20180934615654.496\n",
      "epoch: 1354\n",
      "training loss: 29995053320803.363\n",
      "validation loss: 20172272957842.73\n",
      "epoch: 1355\n",
      "training loss: 29991595599218.895\n",
      "validation loss: 20163626933946.754\n",
      "epoch: 1356\n",
      "training loss: 29984298076953.344\n",
      "validation loss: 20151938173473.992\n",
      "epoch: 1357\n",
      "training loss: 29980860367941.777\n",
      "validation loss: 20143318937474.207\n",
      "Mean absolute error: $6073935760.52\n",
      "[6492759.066518886, 6097805.846057779, 6198530.949583186, 6158860.9105913825, 7111780.459290973, 6958060.4762852825, 6050248.423598237, 6215120.875882364, 6676444.162470134, 6168218.017111915, 6106309.475293339, 6073935.760516125]\n"
     ]
    }
   ],
   "source": [
    "nodes = [50, 75, 100] # use to specify a number of hidden nodes per layer\n",
    "lrs = [0.00001, 0.000001, 0.0000001, 0.00000001]\n",
    "activations = [\"relu\"] # use if you want a diff activationFn per layer\n",
    "\n",
    "means = []\n",
    "\n",
    "for nod in nodes:\n",
    "    for ls in lrs:\n",
    "        print(\"Nodes:\", nod)\n",
    "        print(\"Learning Rate:\", ls)\n",
    "        nn = NeuralNetwork(layers=3, nnodes=nod, batchSize=50, \n",
    "                   activationFn=\"tanh\", lr=ls, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "        nn.fit(X_std, y)\n",
    "        \n",
    "        mae = mean_absolute_error(y, nn.predict(X_std))\n",
    "        means.append(mae)\n",
    "        print('Mean absolute error: $%0.2f'%(mae*1000))\n",
    "\n",
    "# nn = NeuralNetwork(layers=3, nnodes=50, batchSize=50, \n",
    "#                    activationFn=\"tanh\", lr=.000001, lr_type=\"constant\", \n",
    "#                    max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "# nn.fit(X_std, y)\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6492759.066518886, 6097805.846057779, 6198530.949583186, 6158860.9105913825, 7111780.459290973, 6958060.4762852825, 6050248.423598237, 6215120.875882364, 6676444.162470134, 6168218.017111915, 6106309.475293339, 6073935.760516125]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibles :\n",
    "\n",
    "- n = 75, lr = .0000001\n",
    "- n = 100, lr  = 0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 35352767859030.0\n",
      "validation loss: 33307756486710.598\n",
      "epoch: 1\n",
      "training loss: 34227907501473.59\n",
      "validation loss: 33498687172957.805\n",
      "epoch: 2\n",
      "training loss: 33875351568280.332\n",
      "validation loss: 32714941502544.19\n",
      "epoch: 3\n",
      "training loss: 33283208226642.773\n",
      "validation loss: 31368906831949.73\n",
      "epoch: 4\n",
      "training loss: 32637782787215.082\n",
      "validation loss: 31473820286607.617\n",
      "epoch: 5\n",
      "training loss: 32296323149179.965\n",
      "validation loss: 30762952762446.75\n",
      "epoch: 6\n",
      "training loss: 31930316363530.38\n",
      "validation loss: 30633776728142.215\n",
      "epoch: 7\n",
      "training loss: 31671665000501.098\n",
      "validation loss: 30567915615797.46\n",
      "epoch: 8\n",
      "training loss: 31487101891411.125\n",
      "validation loss: 30543618537369.56\n",
      "epoch: 9\n",
      "training loss: 31353865058275.754\n",
      "validation loss: 30546392084987.14\n",
      "epoch: 10\n",
      "training loss: 31256089507653.688\n",
      "validation loss: 30569476735759.742\n",
      "epoch: 11\n",
      "training loss: 31182902021270.21\n",
      "validation loss: 30606663514578.06\n",
      "epoch: 12\n",
      "training loss: 31126832001858.484\n",
      "validation loss: 30653595770871.188\n",
      "epoch: 13\n",
      "training loss: 31082583173499.656\n",
      "validation loss: 30707190692903.844\n",
      "epoch: 14\n",
      "training loss: 31046546812708.09\n",
      "validation loss: 30765296736798.637\n",
      "epoch: 15\n",
      "training loss: 31016232261812.566\n",
      "validation loss: 30826395913279.297\n",
      "epoch: 16\n",
      "training loss: 30989922279342.1\n",
      "validation loss: 30889413434614.562\n",
      "epoch: 17\n",
      "training loss: 30966434244647.297\n",
      "validation loss: 30953585189144.695\n",
      "epoch: 18\n",
      "training loss: 30944954772068.945\n",
      "validation loss: 31018365392834.676\n",
      "epoch: 19\n",
      "training loss: 30924925223056.04\n",
      "validation loss: 31083362164133.516\n",
      "epoch: 20\n",
      "training loss: 30905962498529.902\n",
      "validation loss: 31148292514876.734\n",
      "epoch: 21\n",
      "training loss: 30887804278120.535\n",
      "validation loss: 31212950849251.492\n",
      "epoch: 22\n",
      "training loss: 30870271192934.527\n",
      "validation loss: 31277186868048.652\n",
      "epoch: 23\n",
      "training loss: 30853240722180.344\n",
      "validation loss: 31340890027553.03\n",
      "epoch: 24\n",
      "training loss: 30836629194838.22\n",
      "validation loss: 31403978572160.242\n",
      "epoch: 25\n",
      "training loss: 30956731356476.617\n",
      "validation loss: 30989144524333.848\n",
      "epoch: 26\n",
      "training loss: 30945138689994.04\n",
      "validation loss: 31032679258262.402\n",
      "epoch: 27\n",
      "training loss: 30933772118663.945\n",
      "validation loss: 31075549592224.668\n",
      "epoch: 28\n",
      "training loss: 30922612640447.863\n",
      "validation loss: 31117766873547.043\n",
      "epoch: 29\n",
      "training loss: 30911645491039.25\n",
      "validation loss: 31159342318866.4\n",
      "epoch: 30\n",
      "training loss: 30900858996353.555\n",
      "validation loss: 31200286755216.137\n",
      "epoch: 31\n",
      "training loss: 30890243754073.49\n",
      "validation loss: 31240610485785.195\n",
      "epoch: 32\n",
      "training loss: 30879792046861.113\n",
      "validation loss: 31280323235183.406\n",
      "epoch: 33\n",
      "training loss: 30869497419221.723\n",
      "validation loss: 31319434143976.24\n",
      "epoch: 34\n",
      "training loss: 30859354370420.14\n",
      "validation loss: 31357951792398.703\n",
      "epoch: 35\n",
      "training loss: 30849358130056.934\n",
      "validation loss: 31395884240009.95\n",
      "epoch: 36\n",
      "training loss: 30839504492813.168\n",
      "validation loss: 31433239072953.27\n",
      "epoch: 37\n",
      "training loss: 30829789695783.35\n",
      "validation loss: 31470023453347.89\n",
      "epoch: 38\n",
      "training loss: 30820210326650.773\n",
      "validation loss: 31506244167681.92\n",
      "epoch: 39\n",
      "training loss: 30810763254348.66\n",
      "validation loss: 31541907672219.785\n",
      "epoch: 40\n",
      "training loss: 30801445576233.457\n",
      "validation loss: 31577020134608.35\n",
      "epoch: 41\n",
      "training loss: 30792254577476.402\n",
      "validation loss: 31611587471233.613\n",
      "epoch: 42\n",
      "training loss: 30783187699569.242\n",
      "validation loss: 31645615380241.887\n",
      "epoch: 43\n",
      "training loss: 30774242515684.797\n",
      "validation loss: 31679109370554.574\n",
      "epoch: 44\n",
      "training loss: 30765416711236.57\n",
      "validation loss: 31712074787013.38\n",
      "epoch: 45\n",
      "training loss: 30756708068414.86\n",
      "validation loss: 31744516832162.727\n",
      "epoch: 46\n",
      "training loss: 30748114453789.266\n",
      "validation loss: 31776440584918.562\n",
      "epoch: 47\n",
      "training loss: 30739633808294.82\n",
      "validation loss: 31807851016584.44\n",
      "epoch: 48\n",
      "training loss: 30731264139085.152\n",
      "validation loss: 31838753004438.844\n",
      "epoch: 49\n",
      "training loss: 30723003512858.582\n",
      "validation loss: 31869151343349.734\n",
      "epoch: 50\n",
      "training loss: 30714850050353.99\n",
      "validation loss: 31899050755544.062\n",
      "epoch: 51\n",
      "training loss: 30706801921781.434\n",
      "validation loss: 31928455898923.152\n",
      "epoch: 52\n",
      "training loss: 30698857343004.082\n",
      "validation loss: 31957371374038.707\n",
      "epoch: 53\n",
      "training loss: 30691014572326.883\n",
      "validation loss: 31985801729967.266\n",
      "epoch: 54\n",
      "training loss: 30683271907777.793\n",
      "validation loss: 32013751469238.77\n",
      "epoch: 55\n",
      "training loss: 30675627684790.438\n",
      "validation loss: 32041225051953.02\n",
      "epoch: 56\n",
      "training loss: 30668080274215.113\n",
      "validation loss: 32068226899250.027\n",
      "epoch: 57\n",
      "training loss: 30660628080599.67\n",
      "validation loss: 32094761396151.363\n",
      "epoch: 58\n",
      "training loss: 30653269540692.53\n",
      "validation loss: 32120832893941.54\n",
      "epoch: 59\n",
      "training loss: 30646003122129.516\n",
      "validation loss: 32146445712155.43\n",
      "epoch: 0\n",
      "training loss: 36520960002232.89\n",
      "validation loss: 32997046486644.883\n",
      "epoch: 1\n",
      "training loss: 36177577497558.12\n",
      "validation loss: 32401987150335.55\n",
      "epoch: 2\n",
      "training loss: 35753648869013.8\n",
      "validation loss: 31751078505922.26\n",
      "epoch: 3\n",
      "training loss: 35337722865168.65\n",
      "validation loss: 31312636673661.215\n",
      "epoch: 4\n",
      "training loss: 35106278676194.09\n",
      "validation loss: 30481363493948.867\n",
      "epoch: 5\n",
      "training loss: 34781068059310.53\n",
      "validation loss: 29802927539798.3\n",
      "epoch: 6\n",
      "training loss: 34468992223629.086\n",
      "validation loss: 29430819795319.883\n",
      "epoch: 7\n",
      "training loss: 34240115286622.445\n",
      "validation loss: 29291363849141.34\n",
      "epoch: 8\n",
      "training loss: 33986389834765.055\n",
      "validation loss: 28930569023724.965\n",
      "epoch: 9\n",
      "training loss: 33837499335776.41\n",
      "validation loss: 28593495157254.23\n",
      "epoch: 10\n",
      "training loss: 33630122632623.93\n",
      "validation loss: 28286444464650.89\n",
      "epoch: 11\n",
      "training loss: 33490938297337.117\n",
      "validation loss: 27919892837711.297\n",
      "epoch: 12\n",
      "training loss: 33328777209775.387\n",
      "validation loss: 27670693701384.92\n",
      "epoch: 13\n",
      "training loss: 33184519408374.4\n",
      "validation loss: 27445604148111.06\n",
      "epoch: 14\n",
      "training loss: 33056057529454.85\n",
      "validation loss: 27242356354256.785\n",
      "epoch: 15\n",
      "training loss: 32943822997989.742\n",
      "validation loss: 27086911582876.96\n",
      "epoch: 16\n",
      "training loss: 32873827015371.293\n",
      "validation loss: 27191515040802.44\n",
      "epoch: 17\n",
      "training loss: 32776228910643.28\n",
      "validation loss: 27033215401473.06\n",
      "epoch: 18\n",
      "training loss: 32688738993979.31\n",
      "validation loss: 26890536540880.844\n",
      "epoch: 19\n",
      "training loss: 32610187705746.785\n",
      "validation loss: 26762085413917.832\n",
      "epoch: 20\n",
      "training loss: 32539542539227.95\n",
      "validation loss: 26646604139285.72\n",
      "epoch: 21\n",
      "training loss: 32475891955150.266\n",
      "validation loss: 26542955807549.41\n",
      "epoch: 22\n",
      "training loss: 32418431184255.48\n",
      "validation loss: 26450111857683.707\n",
      "epoch: 23\n",
      "training loss: 32366449696323.363\n",
      "validation loss: 26367140843633.492\n",
      "epoch: 24\n",
      "training loss: 32319320140088.21\n",
      "validation loss: 26293198433049.227\n",
      "epoch: 25\n",
      "training loss: 32276488581577.375\n",
      "validation loss: 26227518498682.21\n",
      "epoch: 26\n",
      "training loss: 32237465788897.945\n",
      "validation loss: 26169405181386.457\n",
      "epoch: 27\n",
      "training loss: 32201796386270.54\n",
      "validation loss: 26118236307116.125\n",
      "epoch: 28\n",
      "training loss: 32161084863675.785\n",
      "validation loss: 26292208108986.297\n",
      "epoch: 29\n",
      "training loss: 32141726063010.17\n",
      "validation loss: 26222103066932.09\n",
      "epoch: 30\n",
      "training loss: 32113598458480.207\n",
      "validation loss: 26253579544363.414\n",
      "epoch: 31\n",
      "training loss: 32087971194258.992\n",
      "validation loss: 26222884656722.61\n",
      "epoch: 32\n",
      "training loss: 32064048497066.56\n",
      "validation loss: 26196892464968.72\n",
      "epoch: 33\n",
      "training loss: 32042166548310.17\n",
      "validation loss: 26175053707793.176\n",
      "epoch: 34\n",
      "training loss: 32021823601676.285\n",
      "validation loss: 26157001146200.543\n",
      "epoch: 35\n",
      "training loss: 32002848837784.18\n",
      "validation loss: 26142427350000.246\n",
      "epoch: 36\n",
      "training loss: 31985091526524.95\n",
      "validation loss: 26131050850815.133\n",
      "epoch: 37\n",
      "training loss: 31968418647557.75\n",
      "validation loss: 26122613677700.12\n",
      "epoch: 38\n",
      "training loss: 31952707950694.168\n",
      "validation loss: 26116861667425.383\n",
      "epoch: 39\n",
      "training loss: 31937865597993.836\n",
      "validation loss: 26113612661722.02\n",
      "epoch: 40\n",
      "training loss: 31923795111195.207\n",
      "validation loss: 26112649434560.7\n",
      "epoch: 41\n",
      "training loss: 31910415676025.207\n",
      "validation loss: 26113788328150.465\n",
      "epoch: 42\n",
      "training loss: 31897655908813.844\n",
      "validation loss: 26116860267596.707\n",
      "epoch: 43\n",
      "training loss: 31885451087786.98\n",
      "validation loss: 26121709473661.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44\n",
      "training loss: 31866074899617.766\n",
      "validation loss: 26135528063913.105\n",
      "epoch: 45\n",
      "training loss: 31854947738839.37\n",
      "validation loss: 26142913425042.414\n",
      "epoch: 46\n",
      "training loss: 31844224033336.46\n",
      "validation loss: 26151676707309.34\n",
      "epoch: 47\n",
      "training loss: 31833865465805.613\n",
      "validation loss: 26161704933294.42\n",
      "epoch: 48\n",
      "training loss: 31823838138015.57\n",
      "validation loss: 26172893634963.566\n",
      "epoch: 49\n",
      "training loss: 31813946580556.402\n",
      "validation loss: 26185878640674.812\n",
      "epoch: 50\n",
      "training loss: 31804494002946.55\n",
      "validation loss: 26199094576830.758\n",
      "epoch: 51\n",
      "training loss: 31795292657103.47\n",
      "validation loss: 26213202170959.27\n",
      "epoch: 52\n",
      "training loss: 31786321657763.188\n",
      "validation loss: 26228124555843.246\n",
      "epoch: 53\n",
      "training loss: 31777562475855.203\n",
      "validation loss: 26243790445001.805\n",
      "epoch: 54\n",
      "training loss: 31768998658546.38\n",
      "validation loss: 26260133695210.11\n",
      "epoch: 55\n",
      "training loss: 31760615583908.34\n",
      "validation loss: 26277092907359.1\n",
      "epoch: 56\n",
      "training loss: 31752400244754.203\n",
      "validation loss: 26294611062043.637\n",
      "epoch: 57\n",
      "training loss: 31744341058176.14\n",
      "validation loss: 26312635186506.375\n",
      "epoch: 58\n",
      "training loss: 31736427697727.566\n",
      "validation loss: 26331116049921.227\n",
      "epoch: 59\n",
      "training loss: 31728650945556.91\n",
      "validation loss: 26350007884316.46\n",
      "epoch: 60\n",
      "training loss: 31721002562120.258\n",
      "validation loss: 26369268128718.77\n",
      "epoch: 61\n",
      "training loss: 31713475171381.97\n",
      "validation loss: 26388857194350.332\n",
      "epoch: 62\n",
      "training loss: 31706062159661.15\n",
      "validation loss: 26408738248932.76\n",
      "epoch: 63\n",
      "training loss: 31698757586500.688\n",
      "validation loss: 26428877018350.957\n",
      "epoch: 64\n",
      "training loss: 31691556106128.74\n",
      "validation loss: 26449241604105.86\n",
      "epoch: 65\n",
      "training loss: 31684452898252.383\n",
      "validation loss: 26469802315143.082\n",
      "epoch: 66\n",
      "training loss: 31677443607073.273\n",
      "validation loss: 26490531512784.375\n",
      "epoch: 67\n",
      "training loss: 31670524287546.805\n",
      "validation loss: 26511403467615.004\n",
      "epoch: 68\n",
      "training loss: 31663691358023.023\n",
      "validation loss: 26532394227291.445\n",
      "epoch: 69\n",
      "training loss: 31656941558509.77\n",
      "validation loss: 26553481494334.29\n",
      "epoch: 70\n",
      "training loss: 31650271913888.88\n",
      "validation loss: 26574644513061.12\n",
      "epoch: 71\n",
      "training loss: 31643679701496.027\n",
      "validation loss: 26595863964893.742\n",
      "epoch: 72\n",
      "training loss: 31637162422544.758\n",
      "validation loss: 26617121871346.45\n",
      "epoch: 73\n",
      "training loss: 31630717776937.066\n",
      "validation loss: 26638401504066.57\n",
      "epoch: 74\n",
      "training loss: 31624343641057.438\n",
      "validation loss: 26659687301356.176\n",
      "epoch: 75\n",
      "training loss: 31618038048195.098\n",
      "validation loss: 26680964790655.71\n",
      "epoch: 76\n",
      "training loss: 31611799171281.58\n",
      "validation loss: 26702220516517.582\n",
      "epoch: 77\n",
      "training loss: 31605625307667.875\n",
      "validation loss: 26723441973639.195\n",
      "epoch: 78\n",
      "training loss: 31599514865698.383\n",
      "validation loss: 26744617544563.312\n",
      "epoch: 79\n",
      "training loss: 31593466352867.57\n",
      "validation loss: 26765736441687.562\n",
      "epoch: 80\n",
      "training loss: 31587478365371.043\n",
      "validation loss: 26786788653255.855\n",
      "epoch: 81\n",
      "training loss: 31581549578884.7\n",
      "validation loss: 26807764893032.4\n",
      "epoch: 82\n",
      "training loss: 31575678740426.04\n",
      "validation loss: 26828656553384.094\n",
      "epoch: 83\n",
      "training loss: 31569864661168.41\n",
      "validation loss: 26849455661519.957\n",
      "epoch: 84\n",
      "training loss: 31564106210095.01\n",
      "validation loss: 26870154838657.34\n",
      "epoch: 85\n",
      "training loss: 31558402308392.438\n",
      "validation loss: 26890747261902.703\n",
      "epoch: 86\n",
      "training loss: 31552751924495.77\n",
      "validation loss: 26911226628652.477\n",
      "epoch: 87\n",
      "training loss: 31547154069707.707\n",
      "validation loss: 26931587123334.7\n",
      "epoch: 88\n",
      "training loss: 31541607794323.273\n",
      "validation loss: 26951823386326.54\n",
      "epoch: 89\n",
      "training loss: 31536112184200.023\n",
      "validation loss: 26971930484895.082\n",
      "epoch: 90\n",
      "training loss: 31530666357720.62\n",
      "validation loss: 26991903886021.527\n",
      "epoch: 91\n",
      "training loss: 31525269463101.28\n",
      "validation loss: 27011739430978.41\n",
      "epoch: 0\n",
      "training loss: 36723612907011.85\n",
      "validation loss: 32781816693605.406\n",
      "epoch: 1\n",
      "training loss: 36321721001894.12\n",
      "validation loss: 32149986175672.656\n",
      "epoch: 2\n",
      "training loss: 35951270842532.64\n",
      "validation loss: 31557554600260.52\n",
      "epoch: 3\n",
      "training loss: 35639996998965.3\n",
      "validation loss: 31024336457495.31\n",
      "epoch: 4\n",
      "training loss: 35297146399897.305\n",
      "validation loss: 30404708698815.324\n",
      "epoch: 5\n",
      "training loss: 35171996091638.64\n",
      "validation loss: 30202250463782.824\n",
      "epoch: 6\n",
      "training loss: 34881630634435.25\n",
      "validation loss: 29664438532737.145\n",
      "epoch: 7\n",
      "training loss: 34592186397833.293\n",
      "validation loss: 29149956251786.13\n",
      "epoch: 8\n",
      "training loss: 34324405097029.516\n",
      "validation loss: 28670401733541.094\n",
      "epoch: 9\n",
      "training loss: 34076850565631.855\n",
      "validation loss: 28217503009975.72\n",
      "epoch: 10\n",
      "training loss: 33848311443715.36\n",
      "validation loss: 27792823197251.23\n",
      "epoch: 11\n",
      "training loss: 33637192478813.89\n",
      "validation loss: 27393015306278.17\n",
      "epoch: 12\n",
      "training loss: 33442165317567.16\n",
      "validation loss: 27016473218049.04\n",
      "epoch: 13\n",
      "training loss: 33262002861940.152\n",
      "validation loss: 26661701948091.92\n",
      "epoch: 14\n",
      "training loss: 33095571552379.426\n",
      "validation loss: 26327309616009.184\n",
      "epoch: 15\n",
      "training loss: 32941824238908.977\n",
      "validation loss: 26012000009888.492\n",
      "epoch: 16\n",
      "training loss: 32799811297944.605\n",
      "validation loss: 25712546329869.26\n",
      "epoch: 17\n",
      "training loss: 32668602473531.676\n",
      "validation loss: 25431828405391.0\n",
      "epoch: 18\n",
      "training loss: 32547391305420.934\n",
      "validation loss: 25166770054847.027\n",
      "epoch: 19\n",
      "training loss: 32435415533971.59\n",
      "validation loss: 24916439975180.387\n",
      "epoch: 20\n",
      "training loss: 32331970714735.473\n",
      "validation loss: 24679926403657.715\n",
      "epoch: 21\n",
      "training loss: 32236406109463.406\n",
      "validation loss: 24456378861784.297\n",
      "epoch: 22\n",
      "training loss: 32148120592662.812\n",
      "validation loss: 24245003804287.516\n",
      "epoch: 23\n",
      "training loss: 32066558870112.66\n",
      "validation loss: 24045060587238.156\n",
      "epoch: 24\n",
      "training loss: 31991207985563.24\n",
      "validation loss: 23855857731474.0\n",
      "epoch: 25\n",
      "training loss: 31921594093660.086\n",
      "validation loss: 23676749459287.785\n",
      "epoch: 26\n",
      "training loss: 31857279478804.516\n",
      "validation loss: 23507132484002.76\n",
      "epoch: 27\n",
      "training loss: 31797859801209.277\n",
      "validation loss: 23346443033594.582\n",
      "epoch: 28\n",
      "training loss: 31742961552836.043\n",
      "validation loss: 23194154090937.242\n",
      "epoch: 29\n",
      "training loss: 31692239707220.64\n",
      "validation loss: 23049772834562.285\n",
      "epoch: 30\n",
      "training loss: 31645375548410.727\n",
      "validation loss: 22912838265032.926\n",
      "epoch: 31\n",
      "training loss: 31602074665366.586\n",
      "validation loss: 22782919003154.957\n",
      "epoch: 32\n",
      "training loss: 31562065099215.773\n",
      "validation loss: 22659611247282.16\n",
      "epoch: 33\n",
      "training loss: 31525095631713.164\n",
      "validation loss: 22542536877931.18\n",
      "epoch: 34\n",
      "training loss: 31490934204145.7\n",
      "validation loss: 22431341698805.54\n",
      "epoch: 35\n",
      "training loss: 31459366456741.03\n",
      "validation loss: 22325693804146.78\n",
      "epoch: 36\n",
      "training loss: 31430194379396.78\n",
      "validation loss: 22225282063086.402\n",
      "epoch: 37\n",
      "training loss: 31403235065246.977\n",
      "validation loss: 22129814712371.5\n",
      "epoch: 38\n",
      "training loss: 31378319559228.72\n",
      "validation loss: 22039018049483.31\n",
      "epoch: 39\n",
      "training loss: 31355291794409.145\n",
      "validation loss: 21952635218764.797\n",
      "epoch: 40\n",
      "training loss: 31334007609384.74\n",
      "validation loss: 21870425083726.08\n",
      "epoch: 41\n",
      "training loss: 31314333840574.414\n",
      "validation loss: 21792161179206.926\n",
      "epoch: 42\n",
      "training loss: 31296147483698.938\n",
      "validation loss: 21717630737547.625\n",
      "epoch: 43\n",
      "training loss: 31279334919173.78\n",
      "validation loss: 21646633783355.906\n",
      "epoch: 44\n",
      "training loss: 31263791196544.68\n",
      "validation loss: 21578982291861.367\n",
      "epoch: 45\n",
      "training loss: 31249419373466.215\n",
      "validation loss: 21514499406221.945\n",
      "epoch: 46\n",
      "training loss: 31236129905066.566\n",
      "validation loss: 21453018709491.7\n",
      "epoch: 47\n",
      "training loss: 31223840079858.39\n",
      "validation loss: 21394383547278.953\n",
      "epoch: 48\n",
      "training loss: 31212473498648.484\n",
      "validation loss: 21338446397418.18\n",
      "epoch: 49\n",
      "training loss: 31201959593169.19\n",
      "validation loss: 21285068283252.316\n",
      "epoch: 50\n",
      "training loss: 31192233181404.12\n",
      "validation loss: 21234118227374.535\n",
      "epoch: 51\n",
      "training loss: 31183234056811.645\n",
      "validation loss: 21185472742911.688\n",
      "epoch: 52\n",
      "training loss: 31174906608862.61\n",
      "validation loss: 21139015359647.81\n",
      "epoch: 53\n",
      "training loss: 31167199472505.695\n",
      "validation loss: 21094636182485.55\n",
      "epoch: 54\n",
      "training loss: 31160065204355.535\n",
      "validation loss: 21052231479928.207\n",
      "epoch: 55\n",
      "training loss: 31153459983567.055\n",
      "validation loss: 21011703300435.887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 56\n",
      "training loss: 31147343335514.32\n",
      "validation loss: 20972959114667.297\n",
      "epoch: 57\n",
      "training loss: 31141677876535.984\n",
      "validation loss: 20935911481765.117\n",
      "epoch: 58\n",
      "training loss: 31136429078141.38\n",
      "validation loss: 20900477737977.965\n",
      "epoch: 59\n",
      "training loss: 31131565049194.297\n",
      "validation loss: 20866579706037.37\n",
      "epoch: 60\n",
      "training loss: 31127056334703.805\n",
      "validation loss: 20834143423823.777\n",
      "epoch: 61\n",
      "training loss: 31122875729956.65\n",
      "validation loss: 20803098890963.105\n",
      "epoch: 62\n",
      "training loss: 31118998108821.582\n",
      "validation loss: 20773379832094.215\n",
      "epoch: 63\n",
      "training loss: 31115400265145.562\n",
      "validation loss: 20744923475639.73\n",
      "epoch: 64\n",
      "training loss: 31112060766243.773\n",
      "validation loss: 20717670346997.695\n",
      "epoch: 65\n",
      "training loss: 31108959817561.723\n",
      "validation loss: 20691564075149.73\n",
      "epoch: 66\n",
      "training loss: 31106079137657.63\n",
      "validation loss: 20666551211754.83\n",
      "epoch: 67\n",
      "training loss: 31103401842718.68\n",
      "validation loss: 20642581061864.773\n",
      "epoch: 68\n",
      "training loss: 31100912339884.1\n",
      "validation loss: 20619605525459.914\n",
      "epoch: 69\n",
      "training loss: 31098596228703.867\n",
      "validation loss: 20597578949061.754\n",
      "epoch: 70\n",
      "training loss: 31096440210112.82\n",
      "validation loss: 20576457986732.105\n",
      "epoch: 71\n",
      "training loss: 31094432002347.277\n",
      "validation loss: 20556201469818.477\n",
      "epoch: 72\n",
      "training loss: 31092560263274.855\n",
      "validation loss: 20536770284850.938\n",
      "epoch: 73\n",
      "training loss: 31090814518648.594\n",
      "validation loss: 20518127259038.418\n",
      "epoch: 74\n",
      "training loss: 31089185095833.742\n",
      "validation loss: 20500237052851.754\n",
      "epoch: 75\n",
      "training loss: 31087663062589.934\n",
      "validation loss: 20483066059217.12\n",
      "epoch: 76\n",
      "training loss: 31086240170523.406\n",
      "validation loss: 20466582308877.586\n",
      "epoch: 77\n",
      "training loss: 31084908802853.133\n",
      "validation loss: 20450755381511.465\n",
      "epoch: 78\n",
      "training loss: 31083661926161.973\n",
      "validation loss: 20435556322225.664\n",
      "epoch: 79\n",
      "training loss: 31082493045829.02\n",
      "validation loss: 20420957563068.68\n",
      "epoch: 80\n",
      "training loss: 31081396164862.434\n",
      "validation loss: 20406932849233.08\n",
      "epoch: 81\n",
      "training loss: 31080365745873.535\n",
      "validation loss: 20393457169640.35\n",
      "epoch: 82\n",
      "training loss: 31079396675952.51\n",
      "validation loss: 20380506691622.586\n",
      "epoch: 83\n",
      "training loss: 31078484234224.633\n",
      "validation loss: 20368058699435.125\n",
      "epoch: 84\n",
      "training loss: 31077624061882.4\n",
      "validation loss: 20356091536352.918\n",
      "epoch: 85\n",
      "training loss: 31076812134504.953\n",
      "validation loss: 20344584550120.344\n",
      "epoch: 86\n",
      "training loss: 31076044736490.29\n",
      "validation loss: 20333518041540.207\n",
      "epoch: 87\n",
      "training loss: 31075318437439.027\n",
      "validation loss: 20322873216002.293\n",
      "epoch: 88\n",
      "training loss: 31074630070341.1\n",
      "validation loss: 20312632137765.535\n",
      "epoch: 89\n",
      "training loss: 31073976711427.63\n",
      "validation loss: 20302777686820.61\n",
      "epoch: 90\n",
      "training loss: 31073355661561.195\n",
      "validation loss: 20293293518171.387\n",
      "epoch: 91\n",
      "training loss: 31072764429047.008\n",
      "validation loss: 20284164023384.73\n",
      "epoch: 92\n",
      "training loss: 31072200713756.633\n",
      "validation loss: 20275374294268.383\n",
      "epoch: 93\n",
      "training loss: 31071662392464.24\n",
      "validation loss: 20266910088545.816\n",
      "epoch: 94\n",
      "training loss: 31071147505302.67\n",
      "validation loss: 20258757797406.0\n",
      "epoch: 95\n",
      "training loss: 31070654243254.17\n",
      "validation loss: 20250904414814.016\n",
      "epoch: 96\n",
      "training loss: 31070180936596.59\n",
      "validation loss: 20243337508475.98\n",
      "epoch: 97\n",
      "training loss: 31069726044232.32\n",
      "validation loss: 20236045192358.93\n",
      "epoch: 98\n",
      "training loss: 31069288143832.52\n",
      "validation loss: 20229016100672.723\n",
      "epoch: 99\n",
      "training loss: 31068865922734.56\n",
      "validation loss: 20222239363227.164\n",
      "epoch: 100\n",
      "training loss: 31068458169534.97\n",
      "validation loss: 20215704582083.207\n",
      "epoch: 101\n",
      "training loss: 31068063766325.098\n",
      "validation loss: 20209401809422.28\n",
      "epoch: 102\n",
      "training loss: 31067681681520.168\n",
      "validation loss: 20203321526562.863\n",
      "epoch: 103\n",
      "training loss: 31067310963236.625\n",
      "validation loss: 20197454624057.75\n",
      "epoch: 104\n",
      "training loss: 31066950733175.83\n",
      "validation loss: 20191792382809.938\n",
      "epoch: 105\n",
      "training loss: 31066600180975.59\n",
      "validation loss: 20186326456148.75\n",
      "epoch: 106\n",
      "training loss: 31066258558993.51\n",
      "validation loss: 20181048852811.863\n",
      "epoch: 107\n",
      "training loss: 31065925177489.527\n",
      "validation loss: 20175951920781.938\n",
      "epoch: 108\n",
      "training loss: 31065599400176.99\n",
      "validation loss: 20171028331929.953\n",
      "epoch: 109\n",
      "training loss: 31065280640114.133\n",
      "validation loss: 20166271067420.445\n",
      "epoch: 110\n",
      "training loss: 31064968355909.992\n",
      "validation loss: 20161673403836.344\n",
      "epoch: 111\n",
      "training loss: 31064662048220.79\n",
      "validation loss: 20157228899983.883\n",
      "epoch: 112\n",
      "training loss: 31064361256514.387\n",
      "validation loss: 20152931384340.57\n",
      "epoch: 113\n",
      "training loss: 31064065556082.617\n",
      "validation loss: 20148774943111.168\n",
      "epoch: 114\n",
      "training loss: 31063774555282.34\n",
      "validation loss: 20144753908859.004\n",
      "epoch: 115\n",
      "training loss: 31063487892987.773\n",
      "validation loss: 20140862849681.855\n",
      "epoch: 116\n",
      "training loss: 31063205236237.957\n",
      "validation loss: 20137096558903.42\n",
      "epoch: 117\n",
      "training loss: 31062926278064.2\n",
      "validation loss: 20133450045253.24\n",
      "epoch: 118\n",
      "training loss: 31062650735483.66\n",
      "validation loss: 20129918523509.543\n",
      "epoch: 119\n",
      "training loss: 31062378347645.016\n",
      "validation loss: 20126497405581.117\n",
      "epoch: 120\n",
      "training loss: 31062108874102.36\n",
      "validation loss: 20123182292005.69\n",
      "epoch: 121\n",
      "training loss: 31057510110854.285\n",
      "validation loss: 20113503931512.504\n",
      "epoch: 122\n",
      "training loss: 31061181088203.645\n",
      "validation loss: 20278158834773.402\n",
      "epoch: 123\n",
      "training loss: 31060577705688.016\n",
      "validation loss: 20268699508399.418\n",
      "epoch: 124\n",
      "training loss: 31060004154974.64\n",
      "validation loss: 20259595770260.555\n",
      "epoch: 125\n",
      "training loss: 31059458131634.406\n",
      "validation loss: 20250832663110.324\n",
      "epoch: 126\n",
      "training loss: 31058937507594.617\n",
      "validation loss: 20242395899519.906\n",
      "epoch: 127\n",
      "training loss: 31058440306291.45\n",
      "validation loss: 20234271843485.87\n",
      "epoch: 128\n",
      "training loss: 31057762335592.973\n",
      "validation loss: 20226667359124.242\n",
      "epoch: 129\n",
      "training loss: 31035335852325.957\n",
      "validation loss: 20285644546936.426\n",
      "epoch: 130\n",
      "training loss: 31033418445418.68\n",
      "validation loss: 20312069361892.902\n",
      "epoch: 131\n",
      "training loss: 31032835648826.273\n",
      "validation loss: 20304545910820.586\n",
      "epoch: 132\n",
      "training loss: 31032285742915.855\n",
      "validation loss: 20297577033951.785\n",
      "epoch: 133\n",
      "training loss: 31031751125156.19\n",
      "validation loss: 20290855078999.117\n",
      "epoch: 134\n",
      "training loss: 31031230617912.438\n",
      "validation loss: 20284369828693.582\n",
      "epoch: 135\n",
      "training loss: 31030723134900.062\n",
      "validation loss: 20278111509673.3\n",
      "epoch: 136\n",
      "training loss: 31030227674144.184\n",
      "validation loss: 20272070771731.266\n",
      "epoch: 137\n",
      "training loss: 31029743311479.832\n",
      "validation loss: 20266238668134.176\n",
      "epoch: 138\n",
      "training loss: 31029269194551.508\n",
      "validation loss: 20260606636950.71\n",
      "epoch: 139\n",
      "training loss: 31028804537273.754\n",
      "validation loss: 20255166483331.28\n",
      "epoch: 140\n",
      "training loss: 31028348614717.51\n",
      "validation loss: 20249910362685.19\n",
      "epoch: 141\n",
      "training loss: 31027900758389.387\n",
      "validation loss: 20244830764704.51\n",
      "epoch: 142\n",
      "training loss: 31027460351873.92\n",
      "validation loss: 20239920498187.02\n",
      "epoch: 143\n",
      "training loss: 31027026826810.9\n",
      "validation loss: 20235172676613.633\n",
      "epoch: 144\n",
      "training loss: 31026599659181.887\n",
      "validation loss: 20230580704438.586\n",
      "epoch: 145\n",
      "training loss: 31026178365882.586\n",
      "validation loss: 20226138264053.03\n",
      "epoch: 146\n",
      "training loss: 31025762501558.664\n",
      "validation loss: 20221839303385.17\n",
      "epoch: 147\n",
      "training loss: 31025351655685.234\n",
      "validation loss: 20217678024102.516\n",
      "epoch: 148\n",
      "training loss: 31024945449870.945\n",
      "validation loss: 20213648870383.504\n",
      "epoch: 149\n",
      "training loss: 31024543535369.746\n",
      "validation loss: 20209746518228.184\n",
      "epoch: 150\n",
      "training loss: 31024145590784.117\n",
      "validation loss: 20205965865279.04\n",
      "epoch: 151\n",
      "training loss: 31023751319945.234\n",
      "validation loss: 20202302021125.066\n",
      "epoch: 152\n",
      "training loss: 31023360449956.44\n",
      "validation loss: 20198750298063.633\n",
      "epoch: 153\n",
      "training loss: 31022972729387.46\n",
      "validation loss: 20195306202296.047\n",
      "epoch: 154\n",
      "training loss: 31022587926607.76\n",
      "validation loss: 20191965425534.242\n",
      "epoch: 155\n",
      "training loss: 31022205828248.426\n",
      "validation loss: 20188723836996.688\n",
      "epoch: 156\n",
      "training loss: 31021826237782.47\n",
      "validation loss: 20185577475770.75\n",
      "epoch: 157\n",
      "training loss: 31021448974214.164\n",
      "validation loss: 20182522543506.188\n",
      "epoch: 158\n",
      "training loss: 31021073870863.043\n",
      "validation loss: 20179555397224.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 159\n",
      "training loss: 31020700773624.04\n",
      "validation loss: 20176672521150.6\n",
      "epoch: 160\n",
      "training loss: 31020315269419.12\n",
      "validation loss: 20173390715491.34\n",
      "epoch: 161\n",
      "training loss: 31019945771965.97\n",
      "validation loss: 20170666338956.785\n",
      "epoch: 162\n",
      "training loss: 31019577890523.023\n",
      "validation loss: 20168016616684.133\n",
      "epoch: 163\n",
      "training loss: 31019211514905.05\n",
      "validation loss: 20165438590173.734\n",
      "epoch: 164\n",
      "training loss: 31018846543725.516\n",
      "validation loss: 20162929421032.137\n",
      "epoch: 165\n",
      "training loss: 31018482883707.93\n",
      "validation loss: 20160486385937.457\n",
      "epoch: 166\n",
      "training loss: 31018120449050.586\n",
      "validation loss: 20158106871827.473\n",
      "epoch: 167\n",
      "training loss: 31017759160840.406\n",
      "validation loss: 20155788371299.67\n",
      "epoch: 168\n",
      "training loss: 31017398946512.25\n",
      "validation loss: 20153528478213.312\n",
      "epoch: 169\n",
      "training loss: 31017039739350.13\n",
      "validation loss: 20151324883483.676\n",
      "epoch: 170\n",
      "training loss: 31016681478027.113\n",
      "validation loss: 20149175371059.664\n",
      "epoch: 171\n",
      "training loss: 31016324106180.87\n",
      "validation loss: 20147077814076.03\n",
      "epoch: 172\n",
      "training loss: 31015967572022.33\n",
      "validation loss: 20145030171172.04\n",
      "epoch: 0\n",
      "training loss: 36999228074671.46\n",
      "validation loss: 33397090927406.285\n",
      "epoch: 1\n",
      "training loss: 36911030029515.08\n",
      "validation loss: 33331469808847.062\n",
      "epoch: 2\n",
      "training loss: 36809307068557.37\n",
      "validation loss: 33283913235420.414\n",
      "epoch: 3\n",
      "training loss: 36696549958247.47\n",
      "validation loss: 33190282992281.4\n",
      "epoch: 4\n",
      "training loss: 36633763881922.43\n",
      "validation loss: 33141490891488.883\n",
      "epoch: 5\n",
      "training loss: 36533678177472.6\n",
      "validation loss: 33020844079413.246\n",
      "epoch: 6\n",
      "training loss: 36476247578627.29\n",
      "validation loss: 32974225277019.332\n",
      "epoch: 7\n",
      "training loss: 36380991223703.81\n",
      "validation loss: 32846497920007.246\n",
      "epoch: 8\n",
      "training loss: 36246350217408.03\n",
      "validation loss: 32574592263079.406\n",
      "epoch: 9\n",
      "training loss: 36236412221511.83\n",
      "validation loss: 32502718354486.707\n",
      "epoch: 10\n",
      "training loss: 36182845850712.55\n",
      "validation loss: 32493531640059.38\n",
      "epoch: 11\n",
      "training loss: 36178601100769.55\n",
      "validation loss: 32507441595139.3\n",
      "epoch: 12\n",
      "training loss: 36201623193946.625\n",
      "validation loss: 32599042257731.67\n",
      "epoch: 13\n",
      "training loss: 36093677920567.29\n",
      "validation loss: 32349649356250.312\n",
      "epoch: 14\n",
      "training loss: 36034085301606.8\n",
      "validation loss: 32286757484816.824\n",
      "epoch: 15\n",
      "training loss: 35965167667103.8\n",
      "validation loss: 32336528826611.094\n",
      "epoch: 16\n",
      "training loss: 35915722365784.16\n",
      "validation loss: 32226321186353.746\n",
      "epoch: 17\n",
      "training loss: 35984662272995.71\n",
      "validation loss: 32290133372441.867\n",
      "epoch: 18\n",
      "training loss: 35877826571804.39\n",
      "validation loss: 32254679847444.773\n",
      "epoch: 19\n",
      "training loss: 35859435309303.29\n",
      "validation loss: 32292034322810.902\n",
      "epoch: 20\n",
      "training loss: 35794546669009.055\n",
      "validation loss: 32251233930795.766\n",
      "epoch: 21\n",
      "training loss: 35725092338320.164\n",
      "validation loss: 32198749177372.977\n",
      "epoch: 22\n",
      "training loss: 35658868682003.82\n",
      "validation loss: 32148968263285.99\n",
      "epoch: 23\n",
      "training loss: 35595496709693.945\n",
      "validation loss: 32100896330951.887\n",
      "epoch: 24\n",
      "training loss: 35534840935484.74\n",
      "validation loss: 32054438853413.75\n",
      "epoch: 25\n",
      "training loss: 35476772317132.54\n",
      "validation loss: 32009506179819.184\n",
      "epoch: 26\n",
      "training loss: 35421167971957.81\n",
      "validation loss: 31966013340571.41\n",
      "epoch: 27\n",
      "training loss: 35367910830762.84\n",
      "validation loss: 31923879812984.996\n",
      "epoch: 28\n",
      "training loss: 35316832618589.625\n",
      "validation loss: 31882989856115.184\n",
      "epoch: 29\n",
      "training loss: 35267934345016.1\n",
      "validation loss: 31843348106873.81\n",
      "epoch: 30\n",
      "training loss: 35221065145288.36\n",
      "validation loss: 31804848869712.33\n",
      "epoch: 31\n",
      "training loss: 35176128772669.01\n",
      "validation loss: 31767427270822.633\n",
      "epoch: 32\n",
      "training loss: 35133029520272.145\n",
      "validation loss: 31731021872585.74\n",
      "epoch: 33\n",
      "training loss: 35091687872030.32\n",
      "validation loss: 31695574195886.38\n",
      "epoch: 34\n",
      "training loss: 35052017017978.637\n",
      "validation loss: 31661029463048.113\n",
      "epoch: 35\n",
      "training loss: 35013937815178.855\n",
      "validation loss: 31627335526587.957\n",
      "epoch: 36\n",
      "training loss: 34977374897351.125\n",
      "validation loss: 31594443013240.85\n",
      "epoch: 37\n",
      "training loss: 34942241570187.492\n",
      "validation loss: 31562305156230.566\n",
      "epoch: 38\n",
      "training loss: 34908498402905.414\n",
      "validation loss: 31530877791252.4\n",
      "epoch: 39\n",
      "training loss: 34876066386766.895\n",
      "validation loss: 31500118475674.34\n",
      "epoch: 40\n",
      "training loss: 34844883545075.19\n",
      "validation loss: 31469987800443.305\n",
      "epoch: 41\n",
      "training loss: 34814882701629.477\n",
      "validation loss: 31440437266266.195\n",
      "epoch: 42\n",
      "training loss: 34786024301634.934\n",
      "validation loss: 31411461218861.48\n",
      "epoch: 43\n",
      "training loss: 34758246451157.48\n",
      "validation loss: 31383008661329.133\n",
      "epoch: 44\n",
      "training loss: 34731498181930.242\n",
      "validation loss: 31355048055017.742\n",
      "epoch: 45\n",
      "training loss: 34705730958710.566\n",
      "validation loss: 31327549605963.1\n",
      "epoch: 46\n",
      "training loss: 34680898563119.906\n",
      "validation loss: 31300485175419.598\n",
      "epoch: 47\n",
      "training loss: 34656956983033.53\n",
      "validation loss: 31273828195379.23\n",
      "epoch: 48\n",
      "training loss: 34633864307245.367\n",
      "validation loss: 31247553590126.28\n",
      "epoch: 49\n",
      "training loss: 34611580625020.855\n",
      "validation loss: 31221637707776.27\n",
      "epoch: 50\n",
      "training loss: 34590063707962.598\n",
      "validation loss: 31196058236478.64\n",
      "epoch: 51\n",
      "training loss: 34569285664544.32\n",
      "validation loss: 31170794186468.5\n",
      "epoch: 52\n",
      "training loss: 34549207966247.14\n",
      "validation loss: 31145825578502.46\n",
      "epoch: 53\n",
      "training loss: 34529797793673.223\n",
      "validation loss: 31121133689052.92\n",
      "epoch: 54\n",
      "training loss: 34511023894015.92\n",
      "validation loss: 31096700874498.727\n",
      "epoch: 55\n",
      "training loss: 34492856506211.074\n",
      "validation loss: 31072510514463.84\n",
      "epoch: 56\n",
      "training loss: 34475267289710.34\n",
      "validation loss: 31048546957997.99\n",
      "epoch: 57\n",
      "training loss: 34458229256654.406\n",
      "validation loss: 31024795472458.96\n",
      "epoch: 58\n",
      "training loss: 34441716707283.508\n",
      "validation loss: 31001242194964.617\n",
      "epoch: 59\n",
      "training loss: 34425705168430.02\n",
      "validation loss: 30977874086292.324\n",
      "epoch: 60\n",
      "training loss: 34410171334936.336\n",
      "validation loss: 30954678887145.04\n",
      "epoch: 61\n",
      "training loss: 34395093012657.867\n",
      "validation loss: 30931645079870.18\n",
      "epoch: 62\n",
      "training loss: 34380500453661.945\n",
      "validation loss: 30909505407368.04\n",
      "epoch: 63\n",
      "training loss: 34369184061608.125\n",
      "validation loss: 30879152704984.383\n",
      "epoch: 64\n",
      "training loss: 34355404183396.52\n",
      "validation loss: 30856484748650.535\n",
      "epoch: 65\n",
      "training loss: 34341998620693.473\n",
      "validation loss: 30833938824232.97\n",
      "epoch: 66\n",
      "training loss: 34328950102713.207\n",
      "validation loss: 30811506586988.418\n",
      "epoch: 67\n",
      "training loss: 34316242183354.625\n",
      "validation loss: 30789180219139.46\n",
      "epoch: 68\n",
      "training loss: 34303859201783.54\n",
      "validation loss: 30766952401125.246\n",
      "epoch: 69\n",
      "training loss: 34291786244394.914\n",
      "validation loss: 30744816284321.21\n",
      "epoch: 70\n",
      "training loss: 34279837628571.246\n",
      "validation loss: 30722766277057.93\n",
      "epoch: 71\n",
      "training loss: 34268341449652.562\n",
      "validation loss: 30700796941581.363\n",
      "epoch: 72\n",
      "training loss: 34257114790783.586\n",
      "validation loss: 30678901358854.984\n",
      "epoch: 73\n",
      "training loss: 34246145384371.17\n",
      "validation loss: 30657074315439.76\n",
      "epoch: 74\n",
      "training loss: 34235421548377.742\n",
      "validation loss: 30635310952197.562\n",
      "epoch: 75\n",
      "training loss: 34224932158330.37\n",
      "validation loss: 30613606744407.2\n",
      "epoch: 76\n",
      "training loss: 34214666620651.184\n",
      "validation loss: 30591957482909.58\n",
      "epoch: 77\n",
      "training loss: 34204614842664.75\n",
      "validation loss: 30570359256230.688\n",
      "epoch: 78\n",
      "training loss: 34194558034537.39\n",
      "validation loss: 30548809045059.02\n",
      "epoch: 79\n",
      "training loss: 34184904419407.703\n",
      "validation loss: 30527303315319.664\n",
      "epoch: 80\n",
      "training loss: 34175437178520.566\n",
      "validation loss: 30505838520471.56\n",
      "epoch: 81\n",
      "training loss: 34166148011796.504\n",
      "validation loss: 30484411777060.566\n",
      "epoch: 82\n",
      "training loss: 34157029015055.402\n",
      "validation loss: 30463020423093.6\n",
      "epoch: 83\n",
      "training loss: 34148072661087.71\n",
      "validation loss: 30441662005076.914\n",
      "epoch: 84\n",
      "training loss: 34139271781620.77\n",
      "validation loss: 30420334265737.72\n",
      "epoch: 85\n",
      "training loss: 34130619549180.51\n",
      "validation loss: 30399035132392.07\n",
      "epoch: 86\n",
      "training loss: 34119889129820.645\n",
      "validation loss: 30384655014710.08\n",
      "epoch: 87\n",
      "training loss: 34111350921322.996\n",
      "validation loss: 30363742726959.797\n",
      "epoch: 88\n",
      "training loss: 34102953487208.055\n",
      "validation loss: 30342859415968.637\n",
      "epoch: 89\n",
      "training loss: 34094690693005.113\n",
      "validation loss: 30322003281918.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90\n",
      "training loss: 34086556697116.746\n",
      "validation loss: 30301172679315.797\n",
      "epoch: 91\n",
      "training loss: 34078545936787.617\n",
      "validation loss: 30280366107639.805\n",
      "epoch: 92\n",
      "training loss: 34070653114743.977\n",
      "validation loss: 30259582202482.617\n",
      "epoch: 93\n",
      "training loss: 34062873186455.832\n",
      "validation loss: 30238819727172.766\n",
      "epoch: 94\n",
      "training loss: 34055201296743.4\n",
      "validation loss: 30218077564850.164\n",
      "epoch: 95\n",
      "training loss: 34050060780240.246\n",
      "validation loss: 30181786348354.508\n",
      "epoch: 96\n",
      "training loss: 34042614454371.082\n",
      "validation loss: 30160619238230.273\n",
      "epoch: 97\n",
      "training loss: 34035257705587.23\n",
      "validation loss: 30139467172845.57\n",
      "epoch: 98\n",
      "training loss: 34027986852249.668\n",
      "validation loss: 30118329636698.31\n",
      "epoch: 99\n",
      "training loss: 34020798387661.94\n",
      "validation loss: 30097206190231.52\n",
      "epoch: 100\n",
      "training loss: 34013688971705.055\n",
      "validation loss: 30076096464689.164\n",
      "epoch: 101\n",
      "training loss: 34006655422873.734\n",
      "validation loss: 30055000157266.742\n",
      "epoch: 102\n",
      "training loss: 33999694710693.176\n",
      "validation loss: 30033917026532.9\n",
      "epoch: 103\n",
      "training loss: 33992803948498.137\n",
      "validation loss: 30012846888108.305\n",
      "epoch: 104\n",
      "training loss: 33985980386556.996\n",
      "validation loss: 29991789610588.3\n",
      "epoch: 105\n",
      "training loss: 33979221405524.363\n",
      "validation loss: 29970745111696.777\n",
      "epoch: 106\n",
      "training loss: 33972524510206.5\n",
      "validation loss: 29949713354659.19\n",
      "epoch: 107\n",
      "training loss: 33965887323624.71\n",
      "validation loss: 29928694344783.43\n",
      "epoch: 108\n",
      "training loss: 33959307581362.363\n",
      "validation loss: 29907688126237.555\n",
      "epoch: 109\n",
      "training loss: 33952783126182.016\n",
      "validation loss: 29886694779014.188\n",
      "epoch: 110\n",
      "training loss: 33946311902899.848\n",
      "validation loss: 29865714416071.79\n",
      "epoch: 111\n",
      "training loss: 33939891953505.004\n",
      "validation loss: 29844747180643.4\n",
      "epoch: 112\n",
      "training loss: 33933521412512.16\n",
      "validation loss: 29823793243704.234\n",
      "epoch: 113\n",
      "training loss: 33927198502536.227\n",
      "validation loss: 29802852801589.527\n",
      "epoch: 114\n",
      "training loss: 33920921530078.54\n",
      "validation loss: 29781926073754.746\n",
      "epoch: 115\n",
      "training loss: 33914688881514.49\n",
      "validation loss: 29761013300670.777\n",
      "epoch: 116\n",
      "training loss: 33908499019272.883\n",
      "validation loss: 29740114741846.496\n",
      "epoch: 117\n",
      "training loss: 33902350478197.94\n",
      "validation loss: 29719230673972.406\n",
      "epoch: 118\n",
      "training loss: 33896241862085.223\n",
      "validation loss: 29698361389178.49\n",
      "epoch: 119\n",
      "training loss: 33890171840383.086\n",
      "validation loss: 29677507193400.273\n",
      "epoch: 120\n",
      "training loss: 33884139145051.906\n",
      "validation loss: 29656668404847.31\n",
      "epoch: 121\n",
      "training loss: 33878142567573.38\n",
      "validation loss: 29635845352568.41\n",
      "epoch: 122\n",
      "training loss: 33872180956102.85\n",
      "validation loss: 29615038375108.562\n",
      "epoch: 123\n",
      "training loss: 33866253212757.766\n",
      "validation loss: 29594247819252.32\n",
      "epoch: 124\n",
      "training loss: 33860358291035.78\n",
      "validation loss: 29573474038849.133\n",
      "epoch: 125\n",
      "training loss: 33854495193356.31\n",
      "validation loss: 29552717393715.953\n",
      "epoch: 126\n",
      "training loss: 33848662968719.664\n",
      "validation loss: 29531978248613.02\n",
      "epoch: 127\n",
      "training loss: 33842860710478.094\n",
      "validation loss: 29511256972288.652\n",
      "epoch: 128\n",
      "training loss: 33837087554213.438\n",
      "validation loss: 29490553936589.285\n",
      "epoch: 129\n",
      "training loss: 33831342675716.246\n",
      "validation loss: 29469869515631.027\n",
      "epoch: 130\n",
      "training loss: 33825625289061.617\n",
      "validation loss: 29449204085029.438\n",
      "epoch: 131\n",
      "training loss: 33819934644777.01\n",
      "validation loss: 29428558021184.086\n",
      "epoch: 132\n",
      "training loss: 33814270028097.695\n",
      "validation loss: 29407931700614.93\n",
      "epoch: 133\n",
      "training loss: 33808630757305.664\n",
      "validation loss: 29387325499347.418\n",
      "epoch: 134\n",
      "training loss: 33803016182148.062\n",
      "validation loss: 29366739792343.703\n",
      "epoch: 135\n",
      "training loss: 33797425682331.12\n",
      "validation loss: 29346174952977.184\n",
      "epoch: 136\n",
      "training loss: 33791858666086.348\n",
      "validation loss: 29325631352547.957\n",
      "epoch: 137\n",
      "training loss: 33786314568805.145\n",
      "validation loss: 29305109359836.715\n",
      "epoch: 138\n",
      "training loss: 33780792851738.863\n",
      "validation loss: 29284609340694.953\n",
      "epoch: 139\n",
      "training loss: 33775293000761.04\n",
      "validation loss: 29264131657669.273\n",
      "epoch: 140\n",
      "training loss: 33769814525188.785\n",
      "validation loss: 29243676669657.75\n",
      "epoch: 141\n",
      "training loss: 33764356956660.715\n",
      "validation loss: 29223244731596.527\n",
      "epoch: 142\n",
      "training loss: 33758919848068.484\n",
      "validation loss: 29202836194174.836\n",
      "epoch: 143\n",
      "training loss: 33753502772539.48\n",
      "validation loss: 29182451403576.516\n",
      "epoch: 144\n",
      "training loss: 33748105322468.23\n",
      "validation loss: 29162090701246.773\n",
      "epoch: 145\n",
      "training loss: 33742727108594.18\n",
      "validation loss: 29141754423682.297\n",
      "epoch: 146\n",
      "training loss: 33737367759123.543\n",
      "validation loss: 29121442902243.48\n",
      "epoch: 147\n",
      "training loss: 33732026918893.363\n",
      "validation loss: 29101156462987.31\n",
      "epoch: 148\n",
      "training loss: 33726704248575.39\n",
      "validation loss: 29080895426519.63\n",
      "epoch: 149\n",
      "training loss: 33721399423918.25\n",
      "validation loss: 29060660107865.547\n",
      "epoch: 150\n",
      "training loss: 33716112135025.816\n",
      "validation loss: 29040450816356.777\n",
      "epoch: 151\n",
      "training loss: 33710842085670.164\n",
      "validation loss: 29020267855534.938\n",
      "epoch: 152\n",
      "training loss: 33705588992637.434\n",
      "validation loss: 29000111523069.598\n",
      "epoch: 153\n",
      "training loss: 33700352585105.016\n",
      "validation loss: 28979982110690.25\n",
      "epoch: 154\n",
      "training loss: 33695132604048.61\n",
      "validation loss: 28959879904131.176\n",
      "epoch: 155\n",
      "training loss: 33689928801677.63\n",
      "validation loss: 28939805183088.34\n",
      "epoch: 156\n",
      "training loss: 33684740940897.69\n",
      "validation loss: 28919758221187.523\n",
      "epoch: 157\n",
      "training loss: 33679568794798.87\n",
      "validation loss: 28899739285962.934\n",
      "epoch: 158\n",
      "training loss: 33674412146168.46\n",
      "validation loss: 28879748638845.418\n",
      "epoch: 159\n",
      "training loss: 33669270787027.113\n",
      "validation loss: 28859786535159.766\n",
      "epoch: 160\n",
      "training loss: 33664144518187.125\n",
      "validation loss: 28839853224130.293\n",
      "epoch: 161\n",
      "training loss: 33659033148832.047\n",
      "validation loss: 28819948948894.254\n",
      "epoch: 162\n",
      "training loss: 33653936496116.3\n",
      "validation loss: 28800073946522.24\n",
      "epoch: 163\n",
      "training loss: 33648854384784.066\n",
      "validation loss: 28780228448045.277\n",
      "epoch: 164\n",
      "training loss: 33643786646806.46\n",
      "validation loss: 28760412678488.004\n",
      "epoch: 165\n",
      "training loss: 33638733121036.035\n",
      "validation loss: 28740626856907.34\n",
      "epoch: 166\n",
      "training loss: 33633693652877.934\n",
      "validation loss: 28720871196436.406\n",
      "epoch: 167\n",
      "training loss: 33628668093976.773\n",
      "validation loss: 28701145904332.99\n",
      "epoch: 168\n",
      "training loss: 33623656301918.53\n",
      "validation loss: 28681451182032.43\n",
      "epoch: 169\n",
      "training loss: 33618658139946.812\n",
      "validation loss: 28661787225204.316\n",
      "epoch: 170\n",
      "training loss: 33613673476692.633\n",
      "validation loss: 28642154223812.758\n",
      "epoch: 171\n",
      "training loss: 33608702185917.242\n",
      "validation loss: 28622552362179.902\n",
      "epoch: 172\n",
      "training loss: 33603744146267.24\n",
      "validation loss: 28602981819052.184\n",
      "epoch: 173\n",
      "training loss: 33598799241041.48\n",
      "validation loss: 28583442767669.4\n",
      "epoch: 174\n",
      "training loss: 33593867357969.18\n",
      "validation loss: 28563935375835.895\n",
      "epoch: 175\n",
      "training loss: 33588948388998.617\n",
      "validation loss: 28544459805993.867\n",
      "epoch: 176\n",
      "training loss: 33584042230096.047\n",
      "validation loss: 28525016215298.51\n",
      "epoch: 177\n",
      "training loss: 33579148781054.23\n",
      "validation loss: 28505604755694.68\n",
      "epoch: 178\n",
      "training loss: 33574267945310.156\n",
      "validation loss: 28486225573994.918\n",
      "epoch: 179\n",
      "training loss: 33569399629771.582\n",
      "validation loss: 28466878811958.684\n",
      "epoch: 180\n",
      "training loss: 33564543744651.824\n",
      "validation loss: 28447564606372.48\n",
      "epoch: 181\n",
      "training loss: 33559700203312.527\n",
      "validation loss: 28428283089130.875\n",
      "epoch: 182\n",
      "training loss: 33554868922114.043\n",
      "validation loss: 28409034387318.062\n",
      "epoch: 183\n",
      "training loss: 33550049820272.85\n",
      "validation loss: 28389818623289.938\n",
      "epoch: 184\n",
      "training loss: 33545242819726.04\n",
      "validation loss: 28370635914756.527\n",
      "epoch: 185\n",
      "training loss: 33540447845002.113\n",
      "validation loss: 28351486374864.62\n",
      "epoch: 186\n",
      "training loss: 33535664823098.11\n",
      "validation loss: 28332370112280.46\n",
      "epoch: 187\n",
      "training loss: 33530893683362.68\n",
      "validation loss: 28313287231272.418\n",
      "epoch: 188\n",
      "training loss: 33526134357384.594\n",
      "validation loss: 28294237831793.582\n",
      "epoch: 189\n",
      "training loss: 33521386778886.887\n",
      "validation loss: 28275222009564.023\n",
      "epoch: 190\n",
      "training loss: 33516650883625.836\n",
      "validation loss: 28256239856152.83\n",
      "epoch: 191\n",
      "training loss: 33511926609295.01\n",
      "validation loss: 28237291459059.684\n",
      "epoch: 192\n",
      "training loss: 33507213895433.777\n",
      "validation loss: 28218376901795.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 193\n",
      "training loss: 33502512683340.316\n",
      "validation loss: 28199496263965.367\n",
      "epoch: 194\n",
      "training loss: 33497822915988.81\n",
      "validation loss: 28180649621343.746\n",
      "epoch: 195\n",
      "training loss: 33493144537950.547\n",
      "validation loss: 28161837045958.523\n",
      "epoch: 196\n",
      "training loss: 33488477495318.95\n",
      "validation loss: 28143058606167.164\n",
      "epoch: 197\n",
      "training loss: 33483821735638.1\n",
      "validation loss: 28124314366734.945\n",
      "epoch: 198\n",
      "training loss: 33479177207834.79\n",
      "validation loss: 28105604388911.996\n",
      "epoch: 199\n",
      "training loss: 33474543862153.785\n",
      "validation loss: 28086928730509.34\n",
      "epoch: 200\n",
      "training loss: 33469921650096.266\n",
      "validation loss: 28068287445974.09\n",
      "epoch: 201\n",
      "training loss: 33465310524361.195\n",
      "validation loss: 28049680586463.8\n",
      "epoch: 202\n",
      "training loss: 33460710438789.55\n",
      "validation loss: 28031108199919.73\n",
      "epoch: 203\n",
      "training loss: 33456121348311.184\n",
      "validation loss: 28012570331139.23\n",
      "epoch: 204\n",
      "training loss: 33451543208894.35\n",
      "validation loss: 27994067021847.1\n",
      "epoch: 205\n",
      "training loss: 33446975977497.57\n",
      "validation loss: 27975598310765.895\n",
      "epoch: 206\n",
      "training loss: 33442419612023.844\n",
      "validation loss: 27957164233685.2\n",
      "epoch: 207\n",
      "training loss: 33437874071277.125\n",
      "validation loss: 27938764823529.93\n",
      "epoch: 208\n",
      "training loss: 33433339314920.86\n",
      "validation loss: 27920400110427.47\n",
      "epoch: 209\n",
      "training loss: 33428815303438.492\n",
      "validation loss: 27902070121773.76\n",
      "epoch: 210\n",
      "training loss: 33424301998095.92\n",
      "validation loss: 27883774882298.387\n",
      "epoch: 211\n",
      "training loss: 33419799360905.844\n",
      "validation loss: 27865514414128.465\n",
      "epoch: 212\n",
      "training loss: 33415307354593.613\n",
      "validation loss: 27847288736851.52\n",
      "epoch: 213\n",
      "training loss: 33410825942564.992\n",
      "validation loss: 27829097867577.27\n",
      "epoch: 214\n",
      "training loss: 33406355088875.29\n",
      "validation loss: 27810941820998.27\n",
      "epoch: 215\n",
      "training loss: 33401894758200.09\n",
      "validation loss: 27792820609449.51\n",
      "epoch: 216\n",
      "training loss: 33397444915807.316\n",
      "validation loss: 27774734242966.95\n",
      "epoch: 217\n",
      "training loss: 33393005527530.746\n",
      "validation loss: 27756682729344.883\n",
      "epoch: 218\n",
      "training loss: 33388576559744.72\n",
      "validation loss: 27738666074192.3\n",
      "epoch: 219\n",
      "training loss: 33384157979340.094\n",
      "validation loss: 27720684280988.21\n",
      "epoch: 220\n",
      "training loss: 33379749753701.438\n",
      "validation loss: 27702737351135.742\n",
      "epoch: 221\n",
      "training loss: 33375351850685.18\n",
      "validation loss: 27684825284015.395\n",
      "epoch: 222\n",
      "training loss: 33370964238599.0\n",
      "validation loss: 27666948077037.086\n",
      "epoch: 223\n",
      "training loss: 33366586886182.027\n",
      "validation loss: 27649105725691.234\n",
      "epoch: 224\n",
      "training loss: 33362219762586.184\n",
      "validation loss: 27631298223598.848\n",
      "epoch: 225\n",
      "training loss: 33357862837358.26\n",
      "validation loss: 27613525562560.473\n",
      "epoch: 226\n",
      "training loss: 33353516080423.04\n",
      "validation loss: 27595787732604.383\n",
      "epoch: 227\n",
      "training loss: 33349179462067.04\n",
      "validation loss: 27578084722033.586\n",
      "epoch: 228\n",
      "training loss: 33344852952923.23\n",
      "validation loss: 27560416517472.09\n",
      "epoch: 229\n",
      "training loss: 33340536523956.355\n",
      "validation loss: 27542783103910.34\n",
      "epoch: 230\n",
      "training loss: 33336230146449.066\n",
      "validation loss: 27525184464750.0\n",
      "epoch: 231\n",
      "training loss: 33331933791988.617\n",
      "validation loss: 27507620581848.848\n",
      "epoch: 232\n",
      "training loss: 33327647432454.293\n",
      "validation loss: 27490091435567.0\n",
      "epoch: 233\n",
      "training loss: 33323371040005.34\n",
      "validation loss: 27472597004820.008\n",
      "epoch: 234\n",
      "training loss: 33319104587069.26\n",
      "validation loss: 27455137267161.414\n",
      "epoch: 235\n",
      "training loss: 33314848046329.51\n",
      "validation loss: 27437712199041.754\n",
      "epoch: 236\n",
      "training loss: 33310601390697.938\n",
      "validation loss: 27420321778233.316\n",
      "epoch: 237\n",
      "training loss: 33306364589102.15\n",
      "validation loss: 27402966460375.555\n",
      "epoch: 238\n",
      "training loss: 33302439395696.23\n",
      "validation loss: 27388139488736.406\n",
      "epoch: 239\n",
      "training loss: 33298222648861.812\n",
      "validation loss: 27370859815434.926\n",
      "epoch: 240\n",
      "training loss: 33294015648969.74\n",
      "validation loss: 27353614613972.324\n",
      "epoch: 241\n",
      "training loss: 33403663792802.574\n",
      "validation loss: 27878877130474.824\n",
      "epoch: 242\n",
      "training loss: 33399489885606.723\n",
      "validation loss: 27861818078456.48\n",
      "epoch: 243\n",
      "training loss: 33395325616344.934\n",
      "validation loss: 27844799292498.984\n",
      "epoch: 244\n",
      "training loss: 33391170929496.71\n",
      "validation loss: 27827820479460.22\n",
      "epoch: 245\n",
      "training loss: 33387025771236.863\n",
      "validation loss: 27810881353031.64\n",
      "epoch: 246\n",
      "training loss: 33382890089357.98\n",
      "validation loss: 27793981633530.375\n",
      "epoch: 247\n",
      "training loss: 33378763833196.59\n",
      "validation loss: 27777121047698.02\n",
      "epoch: 248\n",
      "training loss: 33374646953562.73\n",
      "validation loss: 27760299328506.16\n",
      "epoch: 249\n",
      "training loss: 33370539402672.984\n",
      "validation loss: 27743516214968.37\n",
      "epoch: 250\n",
      "training loss: 33366441134086.555\n",
      "validation loss: 27726771451958.305\n",
      "epoch: 251\n",
      "training loss: 33362352102644.45\n",
      "validation loss: 27710064790033.805\n",
      "epoch: 252\n",
      "training loss: 33358272264411.46\n",
      "validation loss: 27693395985266.793\n",
      "epoch: 253\n",
      "training loss: 33354201576620.977\n",
      "validation loss: 27676764799078.7\n",
      "epoch: 254\n",
      "training loss: 33350139997622.312\n",
      "validation loss: 27660170998081.215\n",
      "epoch: 255\n",
      "training loss: 33346087486830.574\n",
      "validation loss: 27643614353922.26\n",
      "epoch: 256\n",
      "training loss: 33342044004678.86\n",
      "validation loss: 27627094643136.906\n",
      "epoch: 257\n",
      "training loss: 33338009512572.785\n",
      "validation loss: 27610611647003.117\n",
      "epoch: 258\n",
      "training loss: 33333983972847.016\n",
      "validation loss: 27594165151402.16\n",
      "epoch: 259\n",
      "training loss: 33329967348724.004\n",
      "validation loss: 27577754946683.47\n",
      "epoch: 260\n",
      "training loss: 33325959604274.523\n",
      "validation loss: 27561380827533.78\n",
      "epoch: 261\n",
      "training loss: 33321960704380.2\n",
      "validation loss: 27545042592850.598\n",
      "epoch: 262\n",
      "training loss: 33317970614697.668\n",
      "validation loss: 27528740045619.51\n",
      "epoch: 263\n",
      "training loss: 33313989301624.55\n",
      "validation loss: 27512472992795.51\n",
      "epoch: 264\n",
      "training loss: 33310016732266.9\n",
      "validation loss: 27496241245188.094\n",
      "epoch: 265\n",
      "training loss: 33306052874408.27\n",
      "validation loss: 27480044617349.816\n",
      "epoch: 266\n",
      "training loss: 33302097696480.188\n",
      "validation loss: 27463882927468.605\n",
      "epoch: 267\n",
      "training loss: 33298151167534.06\n",
      "validation loss: 27447755997263.273\n",
      "epoch: 268\n",
      "training loss: 33294213257214.3\n",
      "validation loss: 27431663651882.348\n",
      "epoch: 269\n",
      "training loss: 33290283935732.875\n",
      "validation loss: 27415605719806.125\n",
      "epoch: 270\n",
      "training loss: 33286363173844.81\n",
      "validation loss: 27399582032751.68\n",
      "epoch: 271\n",
      "training loss: 33282450942825.137\n",
      "validation loss: 27383592425580.914\n",
      "epoch: 272\n",
      "training loss: 33278547214446.617\n",
      "validation loss: 27367636736211.414\n",
      "epoch: 273\n",
      "training loss: 33274651960958.74\n",
      "validation loss: 27351714805530.047\n",
      "epoch: 274\n",
      "training loss: 33270765155067.566\n",
      "validation loss: 27335826477309.305\n",
      "epoch: 275\n",
      "training loss: 33266886769916.58\n",
      "validation loss: 27319971598126.13\n",
      "epoch: 276\n",
      "training loss: 33263016779068.39\n",
      "validation loss: 27304150017283.277\n",
      "epoch: 277\n",
      "training loss: 33259155156487.316\n",
      "validation loss: 27288361586733.113\n",
      "epoch: 278\n",
      "training loss: 33255301876522.785\n",
      "validation loss: 27272606161003.707\n",
      "epoch: 279\n",
      "training loss: 33251456913893.45\n",
      "validation loss: 27256883597127.164\n",
      "epoch: 280\n",
      "training loss: 33247620243672.133\n",
      "validation loss: 27241193754570.2\n",
      "epoch: 281\n",
      "training loss: 33243791841271.387\n",
      "validation loss: 27225536495166.754\n",
      "epoch: 282\n",
      "training loss: 33239971682429.785\n",
      "validation loss: 27209911683052.72\n",
      "epoch: 283\n",
      "training loss: 33236159743198.793\n",
      "validation loss: 27194319184602.547\n",
      "epoch: 284\n",
      "training loss: 33232355999930.3\n",
      "validation loss: 27178758868367.89\n",
      "epoch: 285\n",
      "training loss: 33228560429264.703\n",
      "validation loss: 27163230605017.97\n",
      "epoch: 286\n",
      "training loss: 33224773008119.535\n",
      "validation loss: 27147734267281.82\n",
      "epoch: 287\n",
      "training loss: 33220993713678.633\n",
      "validation loss: 27132269729892.246\n",
      "epoch: 288\n",
      "training loss: 33217222523381.797\n",
      "validation loss: 27116836869531.383\n",
      "epoch: 289\n",
      "training loss: 33213459414914.934\n",
      "validation loss: 27101435564778.04\n",
      "epoch: 290\n",
      "training loss: 33209704366200.652\n",
      "validation loss: 27086065696056.434\n",
      "epoch: 291\n",
      "training loss: 33205957355389.266\n",
      "validation loss: 27070727145586.598\n",
      "epoch: 292\n",
      "training loss: 33202218360850.266\n",
      "validation loss: 27055419797336.137\n",
      "epoch: 293\n",
      "training loss: 33198487361164.117\n",
      "validation loss: 27040143536973.52\n",
      "epoch: 294\n",
      "training loss: 33194764335114.48\n",
      "validation loss: 27024898251822.676\n",
      "epoch: 295\n",
      "training loss: 33191049261680.79\n",
      "validation loss: 27009683830818.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 296\n",
      "training loss: 33187342120031.113\n",
      "validation loss: 26994500164466.5\n",
      "epoch: 297\n",
      "training loss: 33183642889515.426\n",
      "validation loss: 26979347144796.51\n",
      "epoch: 298\n",
      "training loss: 33179951549659.08\n",
      "validation loss: 26964224665327.25\n",
      "epoch: 299\n",
      "training loss: 33176268080156.71\n",
      "validation loss: 26949132621024.83\n",
      "epoch: 300\n",
      "training loss: 33172592460866.29\n",
      "validation loss: 26934070908265.26\n",
      "epoch: 301\n",
      "training loss: 33168924671803.516\n",
      "validation loss: 26919039424797.695\n",
      "epoch: 302\n",
      "training loss: 33165264693136.457\n",
      "validation loss: 26904038069708.574\n",
      "epoch: 303\n",
      "training loss: 33161612505180.414\n",
      "validation loss: 26889066743386.973\n",
      "epoch: 304\n",
      "training loss: 33157968088393.016\n",
      "validation loss: 26874125347490.867\n",
      "epoch: 305\n",
      "training loss: 33154331423369.566\n",
      "validation loss: 26859213784914.41\n",
      "epoch: 306\n",
      "training loss: 33150702490838.582\n",
      "validation loss: 26844331959756.11\n",
      "epoch: 307\n",
      "training loss: 33147081271657.484\n",
      "validation loss: 26829479777288.03\n",
      "epoch: 308\n",
      "training loss: 33143467746808.598\n",
      "validation loss: 26814657143925.734\n",
      "epoch: 309\n",
      "training loss: 33139861897395.195\n",
      "validation loss: 26799863967199.24\n",
      "epoch: 310\n",
      "training loss: 33136263704637.82\n",
      "validation loss: 26785100155724.707\n",
      "epoch: 311\n",
      "training loss: 33132673149870.742\n",
      "validation loss: 26770365619176.977\n",
      "epoch: 312\n",
      "training loss: 33129090214538.496\n",
      "validation loss: 26755660268262.9\n",
      "epoch: 313\n",
      "training loss: 33125514880192.754\n",
      "validation loss: 26740984014695.43\n",
      "epoch: 314\n",
      "training loss: 33121947128489.11\n",
      "validation loss: 26726336771168.457\n",
      "epoch: 315\n",
      "training loss: 33118386941184.207\n",
      "validation loss: 26711718451332.33\n",
      "epoch: 316\n",
      "training loss: 33114834300132.836\n",
      "validation loss: 26697128969770.1\n",
      "epoch: 317\n",
      "training loss: 33111289187285.3\n",
      "validation loss: 26682568241974.496\n",
      "epoch: 318\n",
      "training loss: 33107751584684.754\n",
      "validation loss: 26668036184325.426\n",
      "epoch: 319\n",
      "training loss: 33104221474464.773\n",
      "validation loss: 26653532714068.254\n",
      "epoch: 320\n",
      "training loss: 33100698838847.004\n",
      "validation loss: 26639057749292.59\n",
      "epoch: 321\n",
      "training loss: 33097183660138.816\n",
      "validation loss: 26624611208911.707\n",
      "epoch: 322\n",
      "training loss: 33093675920731.242\n",
      "validation loss: 26610193012642.625\n",
      "epoch: 323\n",
      "training loss: 33090175603096.836\n",
      "validation loss: 26595803080986.6\n",
      "epoch: 324\n",
      "training loss: 33086682689787.69\n",
      "validation loss: 26581441335210.297\n",
      "epoch: 325\n",
      "training loss: 33083197163433.59\n",
      "validation loss: 26567107697327.434\n",
      "epoch: 326\n",
      "training loss: 33079719006740.137\n",
      "validation loss: 26552802090080.957\n",
      "epoch: 327\n",
      "training loss: 33076248202487.035\n",
      "validation loss: 26538524436925.684\n",
      "epoch: 328\n",
      "training loss: 33072784733526.45\n",
      "validation loss: 26524274662011.51\n",
      "epoch: 329\n",
      "training loss: 33069328582781.363\n",
      "validation loss: 26510052690166.992\n",
      "epoch: 330\n",
      "training loss: 33065879733244.082\n",
      "validation loss: 26495858446883.438\n",
      "epoch: 331\n",
      "training loss: 33062438167974.758\n",
      "validation loss: 26481691858299.477\n",
      "epoch: 332\n",
      "training loss: 33059003870100.0\n",
      "validation loss: 26467552851185.99\n",
      "epoch: 333\n",
      "training loss: 33055576822811.523\n",
      "validation loss: 26453441352931.496\n",
      "epoch: 334\n",
      "training loss: 33052157009364.863\n",
      "validation loss: 26439357291527.938\n",
      "epoch: 335\n",
      "training loss: 33048744413078.125\n",
      "validation loss: 26425300595556.863\n",
      "epoch: 336\n",
      "training loss: 33045339017330.83\n",
      "validation loss: 26411271194176.04\n",
      "epoch: 337\n",
      "training loss: 33041940805562.758\n",
      "validation loss: 26397269017106.305\n",
      "epoch: 338\n",
      "training loss: 33038549761272.86\n",
      "validation loss: 26383293994618.957\n",
      "epoch: 339\n",
      "training loss: 33035165868018.246\n",
      "validation loss: 26369346057523.34\n",
      "epoch: 340\n",
      "training loss: 33031789109413.117\n",
      "validation loss: 26355425137154.867\n",
      "epoch: 341\n",
      "training loss: 33028419469127.883\n",
      "validation loss: 26341531165363.387\n",
      "epoch: 342\n",
      "training loss: 33025056930888.164\n",
      "validation loss: 26327664074501.734\n",
      "epoch: 343\n",
      "training loss: 33021701478473.957\n",
      "validation loss: 26313823797414.78\n",
      "epoch: 344\n",
      "training loss: 33018353095718.773\n",
      "validation loss: 26300010267428.645\n",
      "epoch: 345\n",
      "training loss: 33015011766508.79\n",
      "validation loss: 26286223418340.277\n",
      "epoch: 346\n",
      "training loss: 33011677474782.11\n",
      "validation loss: 26272463184407.3\n",
      "epoch: 347\n",
      "training loss: 33008350204527.973\n",
      "validation loss: 26258729500338.125\n",
      "epoch: 348\n",
      "training loss: 33005029939786.027\n",
      "validation loss: 26245022301282.324\n",
      "epoch: 349\n",
      "training loss: 33001716664645.668\n",
      "validation loss: 26231341522821.348\n",
      "epoch: 350\n",
      "training loss: 32998410363245.336\n",
      "validation loss: 26217687100959.37\n",
      "epoch: 351\n",
      "training loss: 32995111019771.863\n",
      "validation loss: 26204058972114.496\n",
      "epoch: 352\n",
      "training loss: 32991818618459.87\n",
      "validation loss: 26190457073110.14\n",
      "epoch: 353\n",
      "training loss: 32988533143591.16\n",
      "validation loss: 26176881341166.633\n",
      "epoch: 354\n",
      "training loss: 32985254579494.137\n",
      "validation loss: 26163331713893.152\n",
      "epoch: 355\n",
      "training loss: 32981982910543.273\n",
      "validation loss: 26149808129279.742\n",
      "epoch: 356\n",
      "training loss: 32978718121158.535\n",
      "validation loss: 26136310525689.656\n",
      "epoch: 357\n",
      "training loss: 32975460195804.902\n",
      "validation loss: 26122838841851.812\n",
      "epoch: 358\n",
      "training loss: 32972209118991.812\n",
      "validation loss: 26109393016853.547\n",
      "epoch: 359\n",
      "training loss: 32968964875272.734\n",
      "validation loss: 26095972990133.5\n",
      "epoch: 360\n",
      "training loss: 32965727449244.63\n",
      "validation loss: 26082578701474.71\n",
      "epoch: 361\n",
      "training loss: 32962496825547.39\n",
      "validation loss: 26069210090997.895\n",
      "epoch: 362\n",
      "training loss: 32959272988863.023\n",
      "validation loss: 26055867099154.934\n",
      "epoch: 363\n",
      "training loss: 32956055923912.32\n",
      "validation loss: 26042549666722.434\n",
      "epoch: 364\n",
      "training loss: 32952845615399.04\n",
      "validation loss: 26029257734795.504\n",
      "epoch: 365\n",
      "training loss: 32949643877189.527\n",
      "validation loss: 26016016796271.88\n",
      "epoch: 366\n",
      "training loss: 32943226940024.645\n",
      "validation loss: 25996952381255.12\n",
      "epoch: 367\n",
      "training loss: 32939989384026.676\n",
      "validation loss: 25983544223460.938\n",
      "epoch: 368\n",
      "training loss: 32936758756012.223\n",
      "validation loss: 25970205757542.61\n",
      "epoch: 369\n",
      "training loss: 32933535038496.504\n",
      "validation loss: 25956893746757.008\n",
      "epoch: 370\n",
      "training loss: 32930318214216.617\n",
      "validation loss: 25943608111511.773\n",
      "epoch: 371\n",
      "training loss: 32927109260215.98\n",
      "validation loss: 25930396404872.59\n",
      "epoch: 372\n",
      "training loss: 32926393986716.15\n",
      "validation loss: 25921675874037.527\n",
      "epoch: 373\n",
      "training loss: 32923234826422.656\n",
      "validation loss: 25908582118794.406\n",
      "epoch: 374\n",
      "training loss: 32920082316893.113\n",
      "validation loss: 25895513499643.223\n",
      "epoch: 375\n",
      "training loss: 32916936443070.086\n",
      "validation loss: 25882469957741.1\n",
      "epoch: 376\n",
      "training loss: 32913797189939.293\n",
      "validation loss: 25869451434578.31\n",
      "epoch: 377\n",
      "training loss: 32910664542528.957\n",
      "validation loss: 25856457871971.76\n",
      "epoch: 378\n",
      "training loss: 32907538485909.39\n",
      "validation loss: 25843489212058.734\n",
      "epoch: 379\n",
      "training loss: 32904419005192.582\n",
      "validation loss: 25830545397290.703\n",
      "epoch: 380\n",
      "training loss: 32901306085531.734\n",
      "validation loss: 25817626370427.312\n",
      "epoch: 381\n",
      "training loss: 32898199712120.95\n",
      "validation loss: 25804732074530.6\n",
      "epoch: 382\n",
      "training loss: 32895099870194.758\n",
      "validation loss: 25791862452959.305\n",
      "epoch: 383\n",
      "training loss: 32892006545027.824\n",
      "validation loss: 25779017449363.37\n",
      "epoch: 384\n",
      "training loss: 32888919721934.58\n",
      "validation loss: 25766197007678.555\n",
      "epoch: 385\n",
      "training loss: 32885839386268.83\n",
      "validation loss: 25753401072121.19\n",
      "epoch: 386\n",
      "training loss: 32882765523423.48\n",
      "validation loss: 25740629587183.16\n",
      "epoch: 387\n",
      "training loss: 32879698118830.188\n",
      "validation loss: 25727882497626.855\n",
      "epoch: 388\n",
      "training loss: 32876637157959.06\n",
      "validation loss: 25715159748480.465\n",
      "epoch: 389\n",
      "training loss: 32873582626318.33\n",
      "validation loss: 25702461285033.188\n",
      "epoch: 390\n",
      "training loss: 32870534509454.098\n",
      "validation loss: 25689787052830.73\n",
      "epoch: 391\n",
      "training loss: 32867492792950.043\n",
      "validation loss: 25677136997670.848\n",
      "epoch: 392\n",
      "training loss: 32864457462427.105\n",
      "validation loss: 25664511065598.984\n",
      "epoch: 393\n",
      "training loss: 32861428503543.273\n",
      "validation loss: 25651909202904.117\n",
      "epoch: 394\n",
      "training loss: 32858405901993.31\n",
      "validation loss: 25639331356114.62\n",
      "epoch: 395\n",
      "training loss: 32855389643508.48\n",
      "validation loss: 25626777471994.28\n",
      "epoch: 396\n",
      "training loss: 32852379713856.33\n",
      "validation loss: 25614247497538.402\n",
      "epoch: 397\n",
      "training loss: 32849376098840.438\n",
      "validation loss: 25601741379970.043\n",
      "epoch: 398\n",
      "training loss: 32846378784300.2\n",
      "validation loss: 25589259066736.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 399\n",
      "training loss: 32843387756110.586\n",
      "validation loss: 25576800505504.805\n",
      "epoch: 400\n",
      "training loss: 32840403000181.934\n",
      "validation loss: 25564365644160.047\n",
      "epoch: 401\n",
      "training loss: 32837424502459.723\n",
      "validation loss: 25551954430800.188\n",
      "epoch: 402\n",
      "training loss: 32834452248924.402\n",
      "validation loss: 25539566813733.58\n",
      "epoch: 403\n",
      "training loss: 32831486225591.15\n",
      "validation loss: 25527202741475.65\n",
      "epoch: 404\n",
      "training loss: 32828526418509.69\n",
      "validation loss: 25514862162745.668\n",
      "epoch: 405\n",
      "training loss: 32825572813764.13\n",
      "validation loss: 25502545026463.766\n",
      "epoch: 406\n",
      "training loss: 32822625397472.715\n",
      "validation loss: 25490251281747.867\n",
      "epoch: 407\n",
      "training loss: 32819684155787.695\n",
      "validation loss: 25477980877910.863\n",
      "epoch: 408\n",
      "training loss: 32816749074895.156\n",
      "validation loss: 25465733764457.734\n",
      "epoch: 409\n",
      "training loss: 32813820141014.793\n",
      "validation loss: 25453509891082.793\n",
      "epoch: 410\n",
      "training loss: 32810897340399.793\n",
      "validation loss: 25441309207667.02\n",
      "epoch: 411\n",
      "training loss: 32807980659336.637\n",
      "validation loss: 25429131664275.438\n",
      "epoch: 412\n",
      "training loss: 32805070084144.957\n",
      "validation loss: 25416977211154.54\n",
      "epoch: 413\n",
      "training loss: 32802165601177.375\n",
      "validation loss: 25404845798729.816\n",
      "epoch: 414\n",
      "training loss: 32799267196819.344\n",
      "validation loss: 25392737377603.36\n",
      "epoch: 415\n",
      "training loss: 32796374857488.996\n",
      "validation loss: 25380651898551.46\n",
      "epoch: 416\n",
      "training loss: 32793488569637.008\n",
      "validation loss: 25368589312522.32\n",
      "epoch: 417\n",
      "training loss: 32790608319746.42\n",
      "validation loss: 25356549570633.83\n",
      "epoch: 418\n",
      "training loss: 32787734094332.57\n",
      "validation loss: 25344532624171.36\n",
      "epoch: 419\n",
      "training loss: 32784865879942.844\n",
      "validation loss: 25332538424585.63\n",
      "epoch: 420\n",
      "training loss: 32782003663156.656\n",
      "validation loss: 25320566923490.652\n",
      "epoch: 421\n",
      "training loss: 32779147430585.203\n",
      "validation loss: 25308618072661.684\n",
      "epoch: 422\n",
      "training loss: 32776297168871.445\n",
      "validation loss: 25296691824033.27\n",
      "epoch: 423\n",
      "training loss: 32773452864689.863\n",
      "validation loss: 25284788129697.305\n",
      "epoch: 424\n",
      "training loss: 32770614504746.46\n",
      "validation loss: 25272906941901.16\n",
      "epoch: 425\n",
      "training loss: 32767782075778.508\n",
      "validation loss: 25261048213045.867\n",
      "epoch: 426\n",
      "training loss: 32764955564554.52\n",
      "validation loss: 25249211895684.297\n",
      "epoch: 427\n",
      "training loss: 32762134957874.066\n",
      "validation loss: 25237397942519.47\n",
      "epoch: 428\n",
      "training loss: 32759320242567.71\n",
      "validation loss: 25225606306402.81\n",
      "epoch: 429\n",
      "training loss: 32756511405496.863\n",
      "validation loss: 25213836940332.52\n",
      "epoch: 430\n",
      "training loss: 32753708433553.664\n",
      "validation loss: 25202089797451.94\n",
      "epoch: 431\n",
      "training loss: 32750911313660.895\n",
      "validation loss: 25190364831048.016\n",
      "epoch: 432\n",
      "training loss: 32748120032771.84\n",
      "validation loss: 25178661994549.7\n",
      "epoch: 433\n",
      "training loss: 32745334577870.21\n",
      "validation loss: 25166981241526.504\n",
      "epoch: 434\n",
      "training loss: 32742554935969.977\n",
      "validation loss: 25155322525686.984\n",
      "epoch: 435\n",
      "training loss: 32739781094115.348\n",
      "validation loss: 25143685800877.395\n",
      "epoch: 436\n",
      "training loss: 32737013039380.61\n",
      "validation loss: 25132071021080.184\n",
      "epoch: 437\n",
      "training loss: 32734250758870.03\n",
      "validation loss: 25120478140412.72\n",
      "epoch: 438\n",
      "training loss: 32731494239717.76\n",
      "validation loss: 25108907113125.93\n",
      "epoch: 439\n",
      "training loss: 32728743469087.758\n",
      "validation loss: 25097357893602.98\n",
      "epoch: 440\n",
      "training loss: 32725998434173.633\n",
      "validation loss: 25085830436358.08\n",
      "epoch: 441\n",
      "training loss: 32723259122198.63\n",
      "validation loss: 25074324696035.152\n",
      "epoch: 442\n",
      "training loss: 32720525520415.457\n",
      "validation loss: 25062840627406.707\n",
      "epoch: 443\n",
      "training loss: 32717797616106.24\n",
      "validation loss: 25051378185372.625\n",
      "epoch: 444\n",
      "training loss: 32715075396582.406\n",
      "validation loss: 25039937324958.992\n",
      "epoch: 445\n",
      "training loss: 32712358849184.586\n",
      "validation loss: 25028518001317.016\n",
      "epoch: 446\n",
      "training loss: 32709647961282.562\n",
      "validation loss: 25017120169721.9\n",
      "epoch: 447\n",
      "training loss: 32706942720275.125\n",
      "validation loss: 25005743785571.76\n",
      "epoch: 448\n",
      "training loss: 32704243113589.996\n",
      "validation loss: 24994388804386.62\n",
      "epoch: 449\n",
      "training loss: 32701549128683.797\n",
      "validation loss: 24983055181807.34\n",
      "epoch: 450\n",
      "training loss: 32698860753041.88\n",
      "validation loss: 24971742873594.637\n",
      "epoch: 451\n",
      "training loss: 32696177974178.273\n",
      "validation loss: 24960451835628.098\n",
      "epoch: 452\n",
      "training loss: 32693500779635.637\n",
      "validation loss: 24949182023905.25\n",
      "epoch: 453\n",
      "training loss: 32690829156985.113\n",
      "validation loss: 24937933394540.582\n",
      "epoch: 454\n",
      "training loss: 32688163093826.285\n",
      "validation loss: 24926705903764.668\n",
      "epoch: 455\n",
      "training loss: 32685502577787.05\n",
      "validation loss: 24915499507923.242\n",
      "epoch: 456\n",
      "training loss: 32682847596523.61\n",
      "validation loss: 24904314163476.36\n",
      "epoch: 457\n",
      "training loss: 32680198137720.32\n",
      "validation loss: 24893149826997.523\n",
      "epoch: 458\n",
      "training loss: 32677554189089.637\n",
      "validation loss: 24882006455172.84\n",
      "epoch: 459\n",
      "training loss: 32674915738372.047\n",
      "validation loss: 24870884004800.22\n",
      "epoch: 460\n",
      "training loss: 32672282773335.977\n",
      "validation loss: 24859782432788.56\n",
      "epoch: 461\n",
      "training loss: 32669655281777.68\n",
      "validation loss: 24848701696156.97\n",
      "epoch: 462\n",
      "training loss: 32667033251521.25\n",
      "validation loss: 24837641752034.008\n",
      "epoch: 463\n",
      "training loss: 32664416670418.45\n",
      "validation loss: 24826602557656.945\n",
      "epoch: 464\n",
      "training loss: 32661805526348.67\n",
      "validation loss: 24815584070370.977\n",
      "epoch: 465\n",
      "training loss: 32659199807218.875\n",
      "validation loss: 24804586247628.59\n",
      "epoch: 466\n",
      "training loss: 32656599500963.496\n",
      "validation loss: 24793609046988.777\n",
      "epoch: 467\n",
      "training loss: 32654004595544.36\n",
      "validation loss: 24782652426116.4\n",
      "epoch: 468\n",
      "training loss: 32651415078950.64\n",
      "validation loss: 24771716342781.508\n",
      "epoch: 469\n",
      "training loss: 32648830939198.746\n",
      "validation loss: 24760800754858.645\n",
      "epoch: 470\n",
      "training loss: 32646252164332.29\n",
      "validation loss: 24749905620326.246\n",
      "epoch: 471\n",
      "training loss: 32643678742421.977\n",
      "validation loss: 24739030897265.97\n",
      "epoch: 472\n",
      "training loss: 32641110661565.547\n",
      "validation loss: 24728176543862.11\n",
      "epoch: 473\n",
      "training loss: 32638547909887.727\n",
      "validation loss: 24717342518400.94\n",
      "epoch: 474\n",
      "training loss: 32635990475540.12\n",
      "validation loss: 24706528779270.18\n",
      "epoch: 475\n",
      "training loss: 32633438346701.164\n",
      "validation loss: 24695735284958.355\n",
      "epoch: 476\n",
      "training loss: 32630891511576.03\n",
      "validation loss: 24684961994054.234\n",
      "epoch: 477\n",
      "training loss: 32628349958396.6\n",
      "validation loss: 24674208865246.297\n",
      "epoch: 478\n",
      "training loss: 32625813675421.36\n",
      "validation loss: 24663475857322.156\n",
      "epoch: 479\n",
      "training loss: 32623282650935.324\n",
      "validation loss: 24652762929168.008\n",
      "epoch: 480\n",
      "training loss: 32620756873250.008\n",
      "validation loss: 24642070039768.14\n",
      "epoch: 481\n",
      "training loss: 32618236330703.332\n",
      "validation loss: 24631397148204.375\n",
      "epoch: 482\n",
      "training loss: 32615721011659.543\n",
      "validation loss: 24620744213655.582\n",
      "epoch: 483\n",
      "training loss: 32613210904509.188\n",
      "validation loss: 24610111195397.145\n",
      "epoch: 484\n",
      "training loss: 32610705997668.996\n",
      "validation loss: 24599498052800.508\n",
      "epoch: 485\n",
      "training loss: 32608206279581.85\n",
      "validation loss: 24588904745332.67\n",
      "epoch: 486\n",
      "training loss: 32605711738716.71\n",
      "validation loss: 24578331232555.723\n",
      "epoch: 487\n",
      "training loss: 32603222363568.547\n",
      "validation loss: 24567777474126.37\n",
      "epoch: 488\n",
      "training loss: 32600738142658.258\n",
      "validation loss: 24557243429795.484\n",
      "epoch: 489\n",
      "training loss: 32598259064532.637\n",
      "validation loss: 24546729059407.633\n",
      "epoch: 490\n",
      "training loss: 32595785117764.277\n",
      "validation loss: 24536234322900.67\n",
      "epoch: 491\n",
      "training loss: 32593316290951.55\n",
      "validation loss: 24525759180305.293\n",
      "epoch: 492\n",
      "training loss: 32590852572718.484\n",
      "validation loss: 24515303591744.598\n",
      "epoch: 493\n",
      "training loss: 32588393951714.734\n",
      "validation loss: 24504867517433.664\n",
      "epoch: 494\n",
      "training loss: 32585940416615.535\n",
      "validation loss: 24494450917679.176\n",
      "epoch: 495\n",
      "training loss: 32583491956121.598\n",
      "validation loss: 24484053752878.957\n",
      "epoch: 496\n",
      "training loss: 32581048558959.066\n",
      "validation loss: 24473675983521.633\n",
      "epoch: 497\n",
      "training loss: 32578610213879.473\n",
      "validation loss: 24463317570186.2\n",
      "epoch: 498\n",
      "training loss: 32576176909659.62\n",
      "validation loss: 24452978473541.637\n",
      "epoch: 499\n",
      "training loss: 32573748635101.613\n",
      "validation loss: 24442658654346.574\n",
      "epoch: 500\n",
      "training loss: 32571325379032.7\n",
      "validation loss: 24432358073448.848\n",
      "epoch: 501\n",
      "training loss: 32568907130305.258\n",
      "validation loss: 24422076691785.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 502\n",
      "training loss: 32566493877796.75\n",
      "validation loss: 24411814470380.832\n",
      "epoch: 503\n",
      "training loss: 32564085610409.617\n",
      "validation loss: 24401571370349.184\n",
      "epoch: 504\n",
      "training loss: 32561682317071.242\n",
      "validation loss: 24391347352891.42\n",
      "epoch: 505\n",
      "training loss: 32559283986733.918\n",
      "validation loss: 24381142379296.227\n",
      "epoch: 506\n",
      "training loss: 32556890608374.72\n",
      "validation loss: 24370956410939.348\n",
      "epoch: 507\n",
      "training loss: 32554502170995.516\n",
      "validation loss: 24360789409283.363\n",
      "epoch: 508\n",
      "training loss: 32552118663622.86\n",
      "validation loss: 24350641335877.273\n",
      "epoch: 509\n",
      "training loss: 32549740075307.945\n",
      "validation loss: 24340512152356.207\n",
      "epoch: 510\n",
      "training loss: 32547366395126.574\n",
      "validation loss: 24330401820441.09\n",
      "epoch: 511\n",
      "training loss: 32544997612179.035\n",
      "validation loss: 24320310301938.355\n",
      "epoch: 512\n",
      "training loss: 32542633715590.13\n",
      "validation loss: 24310237558739.605\n",
      "epoch: 513\n",
      "training loss: 32540274694509.02\n",
      "validation loss: 24300183552821.258\n",
      "epoch: 514\n",
      "training loss: 32537920538109.258\n",
      "validation loss: 24290148246244.383\n",
      "epoch: 515\n",
      "training loss: 32535571235588.668\n",
      "validation loss: 24280131601154.215\n",
      "epoch: 516\n",
      "training loss: 32533226776169.31\n",
      "validation loss: 24270133579780.004\n",
      "epoch: 517\n",
      "training loss: 32530887149097.426\n",
      "validation loss: 24260154144434.688\n",
      "epoch: 518\n",
      "training loss: 32528552343643.375\n",
      "validation loss: 24250193257514.53\n",
      "epoch: 519\n",
      "training loss: 32526222349101.594\n",
      "validation loss: 24240250881498.938\n",
      "epoch: 520\n",
      "training loss: 32523897154790.51\n",
      "validation loss: 24230326978950.13\n",
      "epoch: 521\n",
      "training loss: 32521576750052.504\n",
      "validation loss: 24220421512512.832\n",
      "epoch: 522\n",
      "training loss: 32519261124253.848\n",
      "validation loss: 24210534444914.066\n",
      "epoch: 523\n",
      "training loss: 32516950266784.65\n",
      "validation loss: 24200665738962.824\n",
      "epoch: 524\n",
      "training loss: 32514644167058.82\n",
      "validation loss: 24190815357549.848\n",
      "epoch: 525\n",
      "training loss: 32512342814513.97\n",
      "validation loss: 24180983263647.3\n",
      "epoch: 526\n",
      "training loss: 32510046198611.395\n",
      "validation loss: 24171169420308.586\n",
      "epoch: 527\n",
      "training loss: 32507754308836.0\n",
      "validation loss: 24161373790668.016\n",
      "epoch: 528\n",
      "training loss: 32505467134696.254\n",
      "validation loss: 24151596337940.586\n",
      "epoch: 529\n",
      "training loss: 32503184665724.12\n",
      "validation loss: 24141837025421.746\n",
      "epoch: 530\n",
      "training loss: 32500906891475.023\n",
      "validation loss: 24132095816487.086\n",
      "epoch: 531\n",
      "training loss: 32498633801527.79\n",
      "validation loss: 24122372674592.152\n",
      "epoch: 532\n",
      "training loss: 32496365385484.566\n",
      "validation loss: 24112667563272.188\n",
      "epoch: 533\n",
      "training loss: 32494101632970.8\n",
      "validation loss: 24102980446141.85\n",
      "epoch: 534\n",
      "training loss: 32491842533635.168\n",
      "validation loss: 24093311286895.016\n",
      "epoch: 535\n",
      "training loss: 32489588077149.523\n",
      "validation loss: 24083660049304.54\n",
      "epoch: 536\n",
      "training loss: 32487338253208.848\n",
      "validation loss: 24074026697221.984\n",
      "epoch: 537\n",
      "training loss: 32485093051531.203\n",
      "validation loss: 24064411194577.426\n",
      "epoch: 538\n",
      "training loss: 32482852461857.64\n",
      "validation loss: 24054813505379.2\n",
      "epoch: 539\n",
      "training loss: 32480616473952.21\n",
      "validation loss: 24045233593713.684\n",
      "epoch: 540\n",
      "training loss: 32478385077601.85\n",
      "validation loss: 24035671423745.074\n",
      "epoch: 541\n",
      "training loss: 32476158262616.367\n",
      "validation loss: 24026126959715.15\n",
      "epoch: 542\n",
      "training loss: 32473936018828.367\n",
      "validation loss: 24016600165943.074\n",
      "epoch: 543\n",
      "training loss: 32471718336093.23\n",
      "validation loss: 24007091006825.12\n",
      "epoch: 544\n",
      "training loss: 32469505204289.0\n",
      "validation loss: 23997599446834.53\n",
      "epoch: 545\n",
      "training loss: 32467296613316.406\n",
      "validation loss: 23988125450521.242\n",
      "epoch: 546\n",
      "training loss: 32465092553098.754\n",
      "validation loss: 23978668982511.707\n",
      "epoch: 547\n",
      "training loss: 32462893013581.9\n",
      "validation loss: 23969230007508.63\n",
      "epoch: 548\n",
      "training loss: 32460697984734.184\n",
      "validation loss: 23959808490290.824\n",
      "epoch: 549\n",
      "training loss: 32458507456546.402\n",
      "validation loss: 23950404395712.973\n",
      "epoch: 550\n",
      "training loss: 32456321419031.746\n",
      "validation loss: 23941017688705.4\n",
      "epoch: 551\n",
      "training loss: 32454139862225.71\n",
      "validation loss: 23931648334273.883\n",
      "epoch: 552\n",
      "training loss: 32451962776186.11\n",
      "validation loss: 23922296297499.473\n",
      "epoch: 553\n",
      "training loss: 32449790150992.992\n",
      "validation loss: 23912961543538.26\n",
      "epoch: 554\n",
      "training loss: 32447621976748.574\n",
      "validation loss: 23903644037621.18\n",
      "epoch: 555\n",
      "training loss: 32445458243577.215\n",
      "validation loss: 23894343745053.816\n",
      "epoch: 556\n",
      "training loss: 32443298941625.363\n",
      "validation loss: 23885060631216.215\n",
      "epoch: 557\n",
      "training loss: 32441144061061.504\n",
      "validation loss: 23875794661562.668\n",
      "epoch: 558\n",
      "training loss: 32438993592076.082\n",
      "validation loss: 23866545801621.543\n",
      "epoch: 559\n",
      "training loss: 32436847524881.49\n",
      "validation loss: 23857314016995.047\n",
      "epoch: 560\n",
      "training loss: 32434705849712.023\n",
      "validation loss: 23848099273359.098\n",
      "epoch: 561\n",
      "training loss: 32432568556823.773\n",
      "validation loss: 23838901536463.094\n",
      "epoch: 562\n",
      "training loss: 32430435636494.645\n",
      "validation loss: 23829720772129.688\n",
      "epoch: 563\n",
      "training loss: 32428307079024.25\n",
      "validation loss: 23820556946254.688\n",
      "epoch: 564\n",
      "training loss: 32426182874733.918\n",
      "validation loss: 23811410024806.816\n",
      "epoch: 565\n",
      "training loss: 32424063013966.574\n",
      "validation loss: 23802279973827.51\n",
      "epoch: 566\n",
      "training loss: 32421947487086.754\n",
      "validation loss: 23793166759430.773\n",
      "epoch: 567\n",
      "training loss: 32419836284480.527\n",
      "validation loss: 23784070347802.977\n",
      "epoch: 568\n",
      "training loss: 32417729396555.445\n",
      "validation loss: 23774990705202.68\n",
      "epoch: 569\n",
      "training loss: 32415626813740.5\n",
      "validation loss: 23765927797960.434\n",
      "epoch: 570\n",
      "training loss: 32413528526486.08\n",
      "validation loss: 23756881592478.633\n",
      "epoch: 571\n",
      "training loss: 32411434525263.895\n",
      "validation loss: 23747852055231.312\n",
      "epoch: 572\n",
      "training loss: 32409344800566.96\n",
      "validation loss: 23738839152763.98\n",
      "epoch: 573\n",
      "training loss: 32407259342909.547\n",
      "validation loss: 23729842851693.41\n",
      "epoch: 574\n",
      "training loss: 32405178142827.11\n",
      "validation loss: 23720863118707.53\n",
      "epoch: 575\n",
      "training loss: 32403101190876.242\n",
      "validation loss: 23711899920565.203\n",
      "epoch: 576\n",
      "training loss: 32401028477634.664\n",
      "validation loss: 23702953224096.047\n",
      "epoch: 577\n",
      "training loss: 32398959993701.12\n",
      "validation loss: 23694022996200.277\n",
      "epoch: 578\n",
      "training loss: 32396895729695.387\n",
      "validation loss: 23685109203848.54\n",
      "epoch: 579\n",
      "training loss: 32394835676258.16\n",
      "validation loss: 23676211814081.734\n",
      "epoch: 580\n",
      "training loss: 32392779824051.08\n",
      "validation loss: 23667330794010.84\n",
      "epoch: 581\n",
      "training loss: 32390728163756.645\n",
      "validation loss: 23658466110816.754\n",
      "epoch: 582\n",
      "training loss: 32388680686078.15\n",
      "validation loss: 23649617731750.113\n",
      "epoch: 583\n",
      "training loss: 32386637381739.664\n",
      "validation loss: 23640785624131.16\n",
      "epoch: 584\n",
      "training loss: 32384598241485.996\n",
      "validation loss: 23631969755349.51\n",
      "epoch: 585\n",
      "training loss: 32382563256082.6\n",
      "validation loss: 23623170092864.05\n",
      "epoch: 586\n",
      "training loss: 32380532416315.586\n",
      "validation loss: 23614386604202.742\n",
      "epoch: 587\n",
      "training loss: 32378505712991.61\n",
      "validation loss: 23605619256962.473\n",
      "epoch: 588\n",
      "training loss: 32376483136937.89\n",
      "validation loss: 23596868018808.863\n",
      "epoch: 589\n",
      "training loss: 32374464679002.113\n",
      "validation loss: 23588132857476.16\n",
      "epoch: 590\n",
      "training loss: 32372450330052.418\n",
      "validation loss: 23579413740767.016\n",
      "epoch: 591\n",
      "training loss: 32370440080977.3\n",
      "validation loss: 23570710636552.348\n",
      "epoch: 592\n",
      "training loss: 32368433922685.633\n",
      "validation loss: 23562023512771.21\n",
      "epoch: 593\n",
      "training loss: 32366431846106.516\n",
      "validation loss: 23553352337430.58\n",
      "epoch: 594\n",
      "training loss: 32364433842189.28\n",
      "validation loss: 23544697078605.227\n",
      "epoch: 595\n",
      "training loss: 32362439901903.254\n",
      "validation loss: 23536057704437.58\n",
      "epoch: 596\n",
      "training loss: 32360450016237.24\n",
      "validation loss: 23527434183137.5\n",
      "epoch: 597\n",
      "training loss: 32358464176196.45\n",
      "validation loss: 23518826482982.2\n",
      "epoch: 598\n",
      "training loss: 32356482372744.51\n",
      "validation loss: 23510234572316.004\n",
      "epoch: 599\n",
      "training loss: 32354502155935.67\n",
      "validation loss: 23501658419549.473\n",
      "epoch: 600\n",
      "training loss: 32346428661916.8\n",
      "validation loss: 23475018385798.945\n",
      "epoch: 601\n",
      "training loss: 32344401859477.785\n",
      "validation loss: 23466315613039.26\n",
      "epoch: 602\n",
      "training loss: 32342378149821.18\n",
      "validation loss: 23457629493537.098\n",
      "epoch: 603\n",
      "training loss: 32340545976301.33\n",
      "validation loss: 23448688557264.84\n",
      "epoch: 604\n",
      "training loss: 32338532501881.387\n",
      "validation loss: 23440036061063.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 605\n",
      "training loss: 32336523282587.043\n",
      "validation loss: 23431400061925.316\n",
      "epoch: 606\n",
      "training loss: 32334518312802.43\n",
      "validation loss: 23422780515625.973\n",
      "epoch: 607\n",
      "training loss: 32332517582823.258\n",
      "validation loss: 23414177378311.797\n",
      "epoch: 608\n",
      "training loss: 32330521081896.13\n",
      "validation loss: 23405590606552.902\n",
      "epoch: 609\n",
      "training loss: 32328528798908.754\n",
      "validation loss: 23397020157345.707\n",
      "epoch: 610\n",
      "training loss: 32326540722627.863\n",
      "validation loss: 23388465988105.59\n",
      "epoch: 611\n",
      "training loss: 32324556841799.848\n",
      "validation loss: 23379928056656.94\n",
      "epoch: 612\n",
      "training loss: 32322577145198.69\n",
      "validation loss: 23371406321222.504\n",
      "epoch: 613\n",
      "training loss: 32320601621650.633\n",
      "validation loss: 23362900740412.574\n",
      "epoch: 614\n",
      "training loss: 32318630260047.066\n",
      "validation loss: 23354411273214.367\n",
      "epoch: 615\n",
      "training loss: 32316663049351.47\n",
      "validation loss: 23345937878981.598\n",
      "epoch: 616\n",
      "training loss: 32314699978602.74\n",
      "validation loss: 23337480517424.37\n",
      "epoch: 617\n",
      "training loss: 32312741036916.58\n",
      "validation loss: 23329039148599.348\n",
      "epoch: 618\n",
      "training loss: 32310786213485.586\n",
      "validation loss: 23320613732900.24\n",
      "epoch: 619\n",
      "training loss: 32308835497578.586\n",
      "validation loss: 23312204231048.594\n",
      "epoch: 620\n",
      "training loss: 32306888878539.55\n",
      "validation loss: 23303810604084.867\n",
      "epoch: 621\n",
      "training loss: 32304946345786.133\n",
      "validation loss: 23295432813359.85\n",
      "epoch: 622\n",
      "training loss: 32303007888808.082\n",
      "validation loss: 23287070820526.258\n",
      "epoch: 623\n",
      "training loss: 32301073497165.58\n",
      "validation loss: 23278724587530.734\n",
      "epoch: 624\n",
      "training loss: 32299143160487.426\n",
      "validation loss: 23270394076606.0\n",
      "epoch: 625\n",
      "training loss: 32297216868469.36\n",
      "validation loss: 23262079250263.37\n",
      "epoch: 626\n",
      "training loss: 32295294610872.242\n",
      "validation loss: 23253780071285.418\n",
      "epoch: 627\n",
      "training loss: 32293376377520.36\n",
      "validation loss: 23245496502718.957\n",
      "epoch: 628\n",
      "training loss: 32291462158299.742\n",
      "validation loss: 23237228507868.203\n",
      "epoch: 629\n",
      "training loss: 32289551943156.484\n",
      "validation loss: 23228976050288.19\n",
      "epoch: 630\n",
      "training loss: 32287645722095.188\n",
      "validation loss: 23220739093778.434\n",
      "epoch: 631\n",
      "training loss: 32285743485177.375\n",
      "validation loss: 23212517602376.695\n",
      "epoch: 632\n",
      "training loss: 32283845222520.027\n",
      "validation loss: 23204311540353.094\n",
      "epoch: 633\n",
      "training loss: 32281950924294.098\n",
      "validation loss: 23196120872204.285\n",
      "epoch: 634\n",
      "training loss: 32280060580723.152\n",
      "validation loss: 23187945562647.92\n",
      "epoch: 635\n",
      "training loss: 32278174182082.04\n",
      "validation loss: 23179785576617.266\n",
      "epoch: 636\n",
      "training loss: 32276291718695.574\n",
      "validation loss: 23171640879255.938\n",
      "epoch: 637\n",
      "training loss: 32274413180937.285\n",
      "validation loss: 23163511435912.88\n",
      "epoch: 638\n",
      "training loss: 32272538559228.29\n",
      "validation loss: 23155397212137.51\n",
      "epoch: 639\n",
      "training loss: 32270667844036.07\n",
      "validation loss: 23147298173674.97\n",
      "epoch: 640\n",
      "training loss: 32268801025873.426\n",
      "validation loss: 23139214286461.54\n",
      "epoch: 641\n",
      "training loss: 32266938095297.406\n",
      "validation loss: 23131145516620.273\n",
      "epoch: 642\n",
      "training loss: 32265079042908.29\n",
      "validation loss: 23123091830456.66\n",
      "epoch: 643\n",
      "training loss: 32263223859348.62\n",
      "validation loss: 23115053194454.527\n",
      "epoch: 644\n",
      "training loss: 32261372535302.266\n",
      "validation loss: 23107029575272.035\n",
      "epoch: 645\n",
      "training loss: 32259525061493.508\n",
      "validation loss: 23099020939737.773\n",
      "epoch: 646\n",
      "training loss: 32257681428686.227\n",
      "validation loss: 23091027254847.047\n",
      "epoch: 647\n",
      "training loss: 32255841627683.03\n",
      "validation loss: 23083048487758.223\n",
      "epoch: 648\n",
      "training loss: 32254005649324.496\n",
      "validation loss: 23075084605789.234\n",
      "epoch: 649\n",
      "training loss: 32252173484488.363\n",
      "validation loss: 23067135576414.17\n",
      "epoch: 650\n",
      "training loss: 32250345124088.844\n",
      "validation loss: 23059201367260.004\n",
      "epoch: 651\n",
      "training loss: 32248520559075.9\n",
      "validation loss: 23051281946103.367\n",
      "epoch: 652\n",
      "training loss: 32246699780434.58\n",
      "validation loss: 23043377280867.51\n",
      "epoch: 653\n",
      "training loss: 32244882779184.37\n",
      "validation loss: 23035487339619.332\n",
      "epoch: 654\n",
      "training loss: 32243069546378.55\n",
      "validation loss: 23027612090566.42\n",
      "epoch: 655\n",
      "training loss: 32241260073103.65\n",
      "validation loss: 23019751502054.33\n",
      "epoch: 656\n",
      "training loss: 32239454350478.773\n",
      "validation loss: 23011905542563.82\n",
      "epoch: 657\n",
      "training loss: 32237652369655.18\n",
      "validation loss: 23004074180708.29\n",
      "epoch: 658\n",
      "training loss: 32235854121815.64\n",
      "validation loss: 22996257385231.18\n",
      "epoch: 659\n",
      "training loss: 32234059598173.98\n",
      "validation loss: 22988455125003.55\n",
      "epoch: 660\n",
      "training loss: 32232268789974.586\n",
      "validation loss: 22980667369021.69\n",
      "epoch: 661\n",
      "training loss: 32230481688491.91\n",
      "validation loss: 22972894086404.83\n",
      "epoch: 662\n",
      "training loss: 32228698285030.04\n",
      "validation loss: 22965135246392.875\n",
      "epoch: 663\n",
      "training loss: 32226918570922.246\n",
      "validation loss: 22957390818344.27\n",
      "epoch: 664\n",
      "training loss: 32225142537530.58\n",
      "validation loss: 22949660771733.906\n",
      "epoch: 665\n",
      "training loss: 32223370176245.42\n",
      "validation loss: 22941945076151.07\n",
      "epoch: 666\n",
      "training loss: 32221601478485.176\n",
      "validation loss: 22934243701297.51\n",
      "epoch: 667\n",
      "training loss: 32219836435695.785\n",
      "validation loss: 22926556616985.508\n",
      "epoch: 668\n",
      "training loss: 32218075039350.477\n",
      "validation loss: 22918883793136.047\n",
      "epoch: 669\n",
      "training loss: 32216317280949.3\n",
      "validation loss: 22911225199777.023\n",
      "epoch: 670\n",
      "training loss: 32214563152018.918\n",
      "validation loss: 22903580807041.516\n",
      "epoch: 671\n",
      "training loss: 32212812644112.17\n",
      "validation loss: 22895950585166.11\n",
      "epoch: 672\n",
      "training loss: 32211065748807.82\n",
      "validation loss: 22888334504489.266\n",
      "epoch: 673\n",
      "training loss: 32209322457710.24\n",
      "validation loss: 22880732535449.758\n",
      "epoch: 674\n",
      "training loss: 32207582762449.105\n",
      "validation loss: 22873144648585.137\n",
      "epoch: 675\n",
      "training loss: 32205846654679.14\n",
      "validation loss: 22865570814530.258\n",
      "epoch: 676\n",
      "training loss: 32204114126079.824\n",
      "validation loss: 22858011004015.836\n",
      "epoch: 677\n",
      "training loss: 32202385168355.12\n",
      "validation loss: 22850465187867.08\n",
      "epoch: 678\n",
      "training loss: 32200659773233.266\n",
      "validation loss: 22842933337002.305\n",
      "epoch: 679\n",
      "training loss: 32198937932466.48\n",
      "validation loss: 22835415422431.688\n",
      "epoch: 680\n",
      "training loss: 32197219637830.746\n",
      "validation loss: 22827911415255.938\n",
      "epoch: 681\n",
      "training loss: 32195504881125.58\n",
      "validation loss: 22820421286665.117\n",
      "epoch: 682\n",
      "training loss: 32193793654173.816\n",
      "validation loss: 22812945007937.43\n",
      "epoch: 683\n",
      "training loss: 32192085948821.363\n",
      "validation loss: 22805482550438.04\n",
      "epoch: 684\n",
      "training loss: 32190381756937.066\n",
      "validation loss: 22798033885618.04\n",
      "epoch: 685\n",
      "training loss: 32188681070412.4\n",
      "validation loss: 22790598985013.266\n",
      "epoch: 686\n",
      "training loss: 32186983881161.363\n",
      "validation loss: 22783177820243.297\n",
      "epoch: 687\n",
      "training loss: 32185290181120.27\n",
      "validation loss: 22775770363010.438\n",
      "epoch: 688\n",
      "training loss: 32183599962247.516\n",
      "validation loss: 22768376585098.703\n",
      "epoch: 689\n",
      "training loss: 32181913216523.465\n",
      "validation loss: 22760996458372.887\n",
      "epoch: 690\n",
      "training loss: 32180229935950.258\n",
      "validation loss: 22753629954777.598\n",
      "epoch: 691\n",
      "training loss: 32178550112551.61\n",
      "validation loss: 22746277046336.387\n",
      "epoch: 692\n",
      "training loss: 32176873738372.715\n",
      "validation loss: 22738937705150.832\n",
      "epoch: 693\n",
      "training loss: 32175200805479.996\n",
      "validation loss: 22731611903399.727\n",
      "epoch: 694\n",
      "training loss: 32173531305961.055\n",
      "validation loss: 22724299613338.25\n",
      "epoch: 695\n",
      "training loss: 32171865231924.45\n",
      "validation loss: 22717000807297.098\n",
      "epoch: 696\n",
      "training loss: 32170202575499.586\n",
      "validation loss: 22709715457681.816\n",
      "epoch: 697\n",
      "training loss: 32168543328836.547\n",
      "validation loss: 22702443536971.95\n",
      "epoch: 698\n",
      "training loss: 32166887484105.98\n",
      "validation loss: 22695185017720.332\n",
      "epoch: 699\n",
      "training loss: 32165235033498.965\n",
      "validation loss: 22687939872552.418\n",
      "epoch: 700\n",
      "training loss: 32163585969226.88\n",
      "validation loss: 22680708074165.543\n",
      "epoch: 701\n",
      "training loss: 32161940283521.24\n",
      "validation loss: 22673489595328.254\n",
      "epoch: 702\n",
      "training loss: 32160297968633.64\n",
      "validation loss: 22666284408879.664\n",
      "epoch: 703\n",
      "training loss: 32158659016835.57\n",
      "validation loss: 22659092487728.83\n",
      "epoch: 704\n",
      "training loss: 32157023420418.363\n",
      "validation loss: 22651913804854.13\n",
      "epoch: 705\n",
      "training loss: 32155391171693.016\n",
      "validation loss: 22644748333302.65\n",
      "epoch: 706\n",
      "training loss: 32153762262990.117\n",
      "validation loss: 22637596046189.594\n",
      "epoch: 707\n",
      "training loss: 32152136686659.723\n",
      "validation loss: 22630456916697.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 708\n",
      "training loss: 32150514435071.266\n",
      "validation loss: 22623330918076.992\n",
      "epoch: 709\n",
      "training loss: 32148895500613.434\n",
      "validation loss: 22616218023643.53\n",
      "epoch: 710\n",
      "training loss: 32147279875694.074\n",
      "validation loss: 22609118206779.656\n",
      "epoch: 711\n",
      "training loss: 32145667552740.12\n",
      "validation loss: 22602031440933.08\n",
      "epoch: 712\n",
      "training loss: 32144058524197.43\n",
      "validation loss: 22594957699616.47\n",
      "epoch: 713\n",
      "training loss: 32142452782530.77\n",
      "validation loss: 22587896956407.0\n",
      "epoch: 714\n",
      "training loss: 32140850320223.668\n",
      "validation loss: 22580849184945.84\n",
      "epoch: 715\n",
      "training loss: 32139251129778.34\n",
      "validation loss: 22573814358937.742\n",
      "epoch: 716\n",
      "training loss: 32137655203715.586\n",
      "validation loss: 22566792452150.57\n",
      "epoch: 717\n",
      "training loss: 32136062534574.746\n",
      "validation loss: 22559783438414.906\n",
      "epoch: 718\n",
      "training loss: 32134473114913.566\n",
      "validation loss: 22552787291623.58\n",
      "epoch: 719\n",
      "training loss: 32132886937308.156\n",
      "validation loss: 22545803985731.312\n",
      "epoch: 720\n",
      "training loss: 32131303994352.836\n",
      "validation loss: 22538833494754.28\n",
      "epoch: 721\n",
      "training loss: 32129724278660.168\n",
      "validation loss: 22531875792769.77\n",
      "epoch: 722\n",
      "training loss: 32128147782860.77\n",
      "validation loss: 22524930853915.758\n",
      "epoch: 723\n",
      "training loss: 32126574499603.31\n",
      "validation loss: 22517998652390.57\n",
      "epoch: 724\n",
      "training loss: 32125004421554.375\n",
      "validation loss: 22511079162452.516\n",
      "epoch: 725\n",
      "training loss: 32123437541398.45\n",
      "validation loss: 22504172358419.535\n",
      "epoch: 726\n",
      "training loss: 32121873851837.8\n",
      "validation loss: 22497278214668.88\n",
      "epoch: 727\n",
      "training loss: 32120313345592.42\n",
      "validation loss: 22490396705636.746\n",
      "epoch: 728\n",
      "training loss: 32118756015399.977\n",
      "validation loss: 22483527805817.996\n",
      "epoch: 729\n",
      "training loss: 32117201854015.695\n",
      "validation loss: 22476671489765.785\n",
      "epoch: 730\n",
      "training loss: 32115650854212.336\n",
      "validation loss: 22469827732091.312\n",
      "epoch: 731\n",
      "training loss: 32114103008780.09\n",
      "validation loss: 22462996507463.477\n",
      "epoch: 732\n",
      "training loss: 32112558310526.56\n",
      "validation loss: 22456177790608.62\n",
      "epoch: 733\n",
      "training loss: 32111016752276.63\n",
      "validation loss: 22449371556310.19\n",
      "epoch: 734\n",
      "training loss: 32109478326872.465\n",
      "validation loss: 22442577779408.53\n",
      "epoch: 735\n",
      "training loss: 32107943027173.402\n",
      "validation loss: 22435796434800.52\n",
      "epoch: 736\n",
      "training loss: 32106410846055.906\n",
      "validation loss: 22429027497439.4\n",
      "epoch: 737\n",
      "training loss: 32104881776413.508\n",
      "validation loss: 22422270942334.426\n",
      "epoch: 738\n",
      "training loss: 32103355811156.75\n",
      "validation loss: 22415526744550.668\n",
      "epoch: 739\n",
      "training loss: 32101832943213.098\n",
      "validation loss: 22408794879208.74\n",
      "epoch: 740\n",
      "training loss: 32100313165526.91\n",
      "validation loss: 22402075321484.58\n",
      "epoch: 741\n",
      "training loss: 32098796471059.367\n",
      "validation loss: 22395368046609.16\n",
      "epoch: 742\n",
      "training loss: 32097282852788.445\n",
      "validation loss: 22388673029868.29\n",
      "epoch: 743\n",
      "training loss: 32095772303708.785\n",
      "validation loss: 22381990246602.39\n",
      "epoch: 744\n",
      "training loss: 32094264816831.7\n",
      "validation loss: 22375319672206.25\n",
      "epoch: 745\n",
      "training loss: 32092760385185.105\n",
      "validation loss: 22368661282128.81\n",
      "epoch: 746\n",
      "training loss: 32091259001813.47\n",
      "validation loss: 22362015051872.953\n",
      "epoch: 747\n",
      "training loss: 32089760659777.727\n",
      "validation loss: 22355380956995.312\n",
      "epoch: 748\n",
      "training loss: 32088265352155.266\n",
      "validation loss: 22348758973106.016\n",
      "epoch: 749\n",
      "training loss: 32086773072039.84\n",
      "validation loss: 22342149075868.52\n",
      "epoch: 750\n",
      "training loss: 32085283812541.555\n",
      "validation loss: 22335551240999.402\n",
      "epoch: 751\n",
      "training loss: 32083797566786.77\n",
      "validation loss: 22328965444268.17\n",
      "epoch: 752\n",
      "training loss: 32082314327918.09\n",
      "validation loss: 22322391661497.05\n",
      "epoch: 753\n",
      "training loss: 32080834089094.285\n",
      "validation loss: 22315829868560.83\n",
      "epoch: 754\n",
      "training loss: 32079356843490.246\n",
      "validation loss: 22309280041386.645\n",
      "epoch: 755\n",
      "training loss: 32077882584296.96\n",
      "validation loss: 22302742155953.82\n",
      "epoch: 756\n",
      "training loss: 32076411304721.426\n",
      "validation loss: 22296216188293.67\n",
      "epoch: 757\n",
      "training loss: 32074942997986.62\n",
      "validation loss: 22289702114489.36\n",
      "epoch: 758\n",
      "training loss: 32073477657331.453\n",
      "validation loss: 22283199910675.668\n",
      "epoch: 759\n",
      "training loss: 32072015276010.703\n",
      "validation loss: 22276709553038.883\n",
      "epoch: 760\n",
      "training loss: 32070555847295.004\n",
      "validation loss: 22270231017816.598\n",
      "epoch: 761\n",
      "training loss: 32069099364470.777\n",
      "validation loss: 22263764281297.582\n",
      "epoch: 762\n",
      "training loss: 32067645820840.15\n",
      "validation loss: 22257309319821.562\n",
      "epoch: 763\n",
      "training loss: 32066195209720.98\n",
      "validation loss: 22250866109779.113\n",
      "epoch: 764\n",
      "training loss: 32064747524446.754\n",
      "validation loss: 22244434627611.504\n",
      "epoch: 765\n",
      "training loss: 32063302758366.594\n",
      "validation loss: 22238014849810.496\n",
      "epoch: 766\n",
      "training loss: 32061860904845.13\n",
      "validation loss: 22231606752918.24\n",
      "epoch: 767\n",
      "training loss: 32060421957262.55\n",
      "validation loss: 22225210313527.1\n",
      "epoch: 768\n",
      "training loss: 32058985909014.484\n",
      "validation loss: 22218825508279.523\n",
      "epoch: 769\n",
      "training loss: 32057552753512.004\n",
      "validation loss: 22212452313867.867\n",
      "epoch: 770\n",
      "training loss: 32056122484181.55\n",
      "validation loss: 22206090707034.297\n",
      "epoch: 771\n",
      "training loss: 32054695094464.93\n",
      "validation loss: 22199740664570.61\n",
      "epoch: 772\n",
      "training loss: 32053270577819.19\n",
      "validation loss: 22193402163318.094\n",
      "epoch: 773\n",
      "training loss: 32051848927716.707\n",
      "validation loss: 22187075180167.418\n",
      "epoch: 774\n",
      "training loss: 32050430137645.004\n",
      "validation loss: 22180759692058.484\n",
      "epoch: 775\n",
      "training loss: 32049014201106.805\n",
      "validation loss: 22174455675980.266\n",
      "epoch: 776\n",
      "training loss: 32047601111619.953\n",
      "validation loss: 22168163108970.742\n",
      "epoch: 777\n",
      "training loss: 32046190862717.383\n",
      "validation loss: 22161881968116.676\n",
      "epoch: 778\n",
      "training loss: 32044783447947.062\n",
      "validation loss: 22155612230553.555\n",
      "epoch: 779\n",
      "training loss: 32043378860871.99\n",
      "validation loss: 22149353873465.438\n",
      "epoch: 780\n",
      "training loss: 32041977095070.086\n",
      "validation loss: 22143106874084.824\n",
      "epoch: 781\n",
      "training loss: 32040578144134.24\n",
      "validation loss: 22136871209692.543\n",
      "epoch: 782\n",
      "training loss: 32039182001672.19\n",
      "validation loss: 22130646857617.61\n",
      "epoch: 783\n",
      "training loss: 32037788661306.543\n",
      "validation loss: 22124433795237.12\n",
      "epoch: 784\n",
      "training loss: 32036398116674.69\n",
      "validation loss: 22118231999976.117\n",
      "epoch: 785\n",
      "training loss: 32035010361428.79\n",
      "validation loss: 22112041449307.47\n",
      "epoch: 786\n",
      "training loss: 32033625389235.74\n",
      "validation loss: 22105862120751.766\n",
      "epoch: 787\n",
      "training loss: 32032243193777.113\n",
      "validation loss: 22099693991877.19\n",
      "epoch: 788\n",
      "training loss: 32030863768749.15\n",
      "validation loss: 22093537040299.4\n",
      "epoch: 789\n",
      "training loss: 32029487107862.664\n",
      "validation loss: 22087391243681.406\n",
      "epoch: 790\n",
      "training loss: 32028113204843.074\n",
      "validation loss: 22081256579733.473\n",
      "epoch: 791\n",
      "training loss: 32026742053430.33\n",
      "validation loss: 22075133026213.004\n",
      "epoch: 792\n",
      "training loss: 32025373647378.85\n",
      "validation loss: 22069020560924.402\n",
      "epoch: 793\n",
      "training loss: 32024007980457.547\n",
      "validation loss: 22062919161718.984\n",
      "epoch: 794\n",
      "training loss: 32022645046449.742\n",
      "validation loss: 22056828806494.867\n",
      "epoch: 795\n",
      "training loss: 32021284839153.117\n",
      "validation loss: 22050749473196.836\n",
      "epoch: 796\n",
      "training loss: 32019927352379.742\n",
      "validation loss: 22044681139816.28\n",
      "epoch: 797\n",
      "training loss: 32018572579955.96\n",
      "validation loss: 22038623784391.02\n",
      "epoch: 798\n",
      "training loss: 32017220515722.43\n",
      "validation loss: 22032577385005.266\n",
      "epoch: 799\n",
      "training loss: 32015871153534.004\n",
      "validation loss: 22026541919789.45\n",
      "epoch: 800\n",
      "training loss: 32014524487259.766\n",
      "validation loss: 22020517366920.17\n",
      "epoch: 801\n",
      "training loss: 32013180510782.957\n",
      "validation loss: 22014503704620.047\n",
      "epoch: 802\n",
      "training loss: 32011839218000.938\n",
      "validation loss: 22008500911157.65\n",
      "epoch: 803\n",
      "training loss: 32010500602825.18\n",
      "validation loss: 22002508964847.35\n",
      "epoch: 804\n",
      "training loss: 32009164659181.2\n",
      "validation loss: 21996527844049.28\n",
      "epoch: 805\n",
      "training loss: 32007831381008.543\n",
      "validation loss: 21990557527169.164\n",
      "epoch: 806\n",
      "training loss: 32006500762260.754\n",
      "validation loss: 21984597992658.25\n",
      "epoch: 807\n",
      "training loss: 32005172796905.305\n",
      "validation loss: 21978649219013.227\n",
      "epoch: 808\n",
      "training loss: 32003847478923.605\n",
      "validation loss: 21972711184776.062\n",
      "epoch: 809\n",
      "training loss: 32002524802310.95\n",
      "validation loss: 21966783868533.97\n",
      "epoch: 810\n",
      "training loss: 32001204761076.473\n",
      "validation loss: 21960867248919.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 811\n",
      "training loss: 31999887349243.12\n",
      "validation loss: 21954961304609.266\n",
      "epoch: 812\n",
      "training loss: 31998572560847.637\n",
      "validation loss: 21949066014326.227\n",
      "epoch: 813\n",
      "training loss: 31997260389940.484\n",
      "validation loss: 21943181356837.195\n",
      "epoch: 814\n",
      "training loss: 31995950830585.875\n",
      "validation loss: 21937307310953.965\n",
      "epoch: 815\n",
      "training loss: 31994643876861.66\n",
      "validation loss: 21931443855532.918\n",
      "epoch: 816\n",
      "training loss: 31993339522859.363\n",
      "validation loss: 21925590969475.0\n",
      "epoch: 817\n",
      "training loss: 31992037762684.11\n",
      "validation loss: 21919748631725.53\n",
      "epoch: 818\n",
      "training loss: 31990738590454.598\n",
      "validation loss: 21913916821274.19\n",
      "epoch: 819\n",
      "training loss: 31989442000303.082\n",
      "validation loss: 21908095517154.902\n",
      "epoch: 820\n",
      "training loss: 31988147986375.324\n",
      "validation loss: 21902284698445.7\n",
      "epoch: 821\n",
      "training loss: 31986856542830.547\n",
      "validation loss: 21896484344268.67\n",
      "epoch: 822\n",
      "training loss: 31985567663841.46\n",
      "validation loss: 21890694433789.86\n",
      "epoch: 823\n",
      "training loss: 31984281343594.133\n",
      "validation loss: 21884914946219.137\n",
      "epoch: 824\n",
      "training loss: 31982997576288.062\n",
      "validation loss: 21879145860810.152\n",
      "epoch: 825\n",
      "training loss: 31981716356136.06\n",
      "validation loss: 21873387156860.207\n",
      "epoch: 826\n",
      "training loss: 31980437677364.277\n",
      "validation loss: 21867638813710.176\n",
      "epoch: 827\n",
      "training loss: 31979161534212.125\n",
      "validation loss: 21861900810744.418\n",
      "epoch: 828\n",
      "training loss: 31977887920932.28\n",
      "validation loss: 21856173127390.676\n",
      "epoch: 829\n",
      "training loss: 31976616831790.633\n",
      "validation loss: 21850455743119.977\n",
      "epoch: 830\n",
      "training loss: 31975348261066.26\n",
      "validation loss: 21844748637446.543\n",
      "epoch: 831\n",
      "training loss: 31974082203051.406\n",
      "validation loss: 21839051789927.71\n",
      "epoch: 832\n",
      "training loss: 31972818652051.414\n",
      "validation loss: 21833365180163.832\n",
      "epoch: 833\n",
      "training loss: 31971557602384.73\n",
      "validation loss: 21827688787798.176\n",
      "epoch: 834\n",
      "training loss: 31970299048382.863\n",
      "validation loss: 21822022592516.85\n",
      "epoch: 835\n",
      "training loss: 31969042984390.34\n",
      "validation loss: 21816366574048.715\n",
      "epoch: 836\n",
      "training loss: 31967789404764.684\n",
      "validation loss: 21810720712165.258\n",
      "epoch: 837\n",
      "training loss: 31966538303876.38\n",
      "validation loss: 21805084986680.543\n",
      "epoch: 838\n",
      "training loss: 31965289676108.816\n",
      "validation loss: 21799459377451.1\n",
      "epoch: 839\n",
      "training loss: 31964043515858.293\n",
      "validation loss: 21793843864375.84\n",
      "epoch: 840\n",
      "training loss: 31962799817533.89\n",
      "validation loss: 21788238427395.98\n",
      "epoch: 841\n",
      "training loss: 31961558575557.406\n",
      "validation loss: 21782643046494.918\n",
      "epoch: 842\n",
      "training loss: 31960319784362.965\n",
      "validation loss: 21777057701698.17\n",
      "epoch: 843\n",
      "training loss: 31959083438395.35\n",
      "validation loss: 21771482373073.29\n",
      "epoch: 844\n",
      "training loss: 31957849532090.17\n",
      "validation loss: 21765917040729.773\n",
      "epoch: 845\n",
      "training loss: 31956618044099.63\n",
      "validation loss: 21760361684819.176\n",
      "epoch: 846\n",
      "training loss: 31955313891893.535\n",
      "validation loss: 21754725517533.05\n",
      "epoch: 847\n",
      "training loss: 31954087054842.254\n",
      "validation loss: 21749189807294.4\n",
      "epoch: 848\n",
      "training loss: 31952862636406.56\n",
      "validation loss: 21743664010170.027\n",
      "epoch: 849\n",
      "training loss: 31951640631100.07\n",
      "validation loss: 21738148106578.6\n",
      "epoch: 850\n",
      "training loss: 31950421033449.254\n",
      "validation loss: 21732642076978.465\n",
      "epoch: 851\n",
      "training loss: 31949203837993.29\n",
      "validation loss: 21727145901867.53\n",
      "epoch: 852\n",
      "training loss: 31947989039284.13\n",
      "validation loss: 21721659561783.312\n",
      "epoch: 853\n",
      "training loss: 31946776631886.38\n",
      "validation loss: 21716183037302.77\n",
      "epoch: 854\n",
      "training loss: 31945566610377.312\n",
      "validation loss: 21710716309042.29\n",
      "epoch: 855\n",
      "training loss: 31944358969346.83\n",
      "validation loss: 21705259357657.613\n",
      "epoch: 856\n",
      "training loss: 31943153703397.383\n",
      "validation loss: 21699812163843.77\n",
      "epoch: 857\n",
      "training loss: 31941950807144.004\n",
      "validation loss: 21694374708335.0\n",
      "epoch: 858\n",
      "training loss: 31940750275214.223\n",
      "validation loss: 21688946971904.71\n",
      "epoch: 859\n",
      "training loss: 31939552102248.06\n",
      "validation loss: 21683528935365.402\n",
      "epoch: 860\n",
      "training loss: 31938356282897.996\n",
      "validation loss: 21678120579568.605\n",
      "epoch: 861\n",
      "training loss: 31937162811828.902\n",
      "validation loss: 21672721885404.824\n",
      "epoch: 862\n",
      "training loss: 31935971683718.043\n",
      "validation loss: 21667332833803.45\n",
      "epoch: 863\n",
      "training loss: 31934782893255.035\n",
      "validation loss: 21661953405732.723\n",
      "epoch: 864\n",
      "training loss: 31933596435141.812\n",
      "validation loss: 21656583582199.65\n",
      "epoch: 865\n",
      "training loss: 31932412304092.594\n",
      "validation loss: 21651223344249.945\n",
      "epoch: 866\n",
      "training loss: 31931230494833.84\n",
      "validation loss: 21645872672967.984\n",
      "epoch: 867\n",
      "training loss: 31930051002104.25\n",
      "validation loss: 21640531549476.703\n",
      "epoch: 868\n",
      "training loss: 31928873820654.68\n",
      "validation loss: 21635199954937.58\n",
      "epoch: 869\n",
      "training loss: 31927698945248.188\n",
      "validation loss: 21629877870550.52\n",
      "epoch: 870\n",
      "training loss: 31926526370659.918\n",
      "validation loss: 21624565277553.848\n",
      "epoch: 871\n",
      "training loss: 31925356091677.13\n",
      "validation loss: 21619262157224.19\n",
      "epoch: 872\n",
      "training loss: 31924188103099.12\n",
      "validation loss: 21613968490876.45\n",
      "epoch: 873\n",
      "training loss: 31923022399737.254\n",
      "validation loss: 21608684259863.707\n",
      "epoch: 874\n",
      "training loss: 31921858976414.87\n",
      "validation loss: 21603409445577.227\n",
      "epoch: 875\n",
      "training loss: 31920697827967.28\n",
      "validation loss: 21598144029446.28\n",
      "epoch: 876\n",
      "training loss: 31919538949241.746\n",
      "validation loss: 21592887992938.203\n",
      "epoch: 877\n",
      "training loss: 31918382335097.43\n",
      "validation loss: 21587641317558.223\n",
      "epoch: 878\n",
      "training loss: 31917227980405.375\n",
      "validation loss: 21582403984849.492\n",
      "epoch: 879\n",
      "training loss: 31916075880048.46\n",
      "validation loss: 21577175976392.94\n",
      "epoch: 880\n",
      "training loss: 31914926028921.406\n",
      "validation loss: 21571957273807.273\n",
      "epoch: 881\n",
      "training loss: 31913778421930.703\n",
      "validation loss: 21566747858748.867\n",
      "epoch: 882\n",
      "training loss: 31912633053994.613\n",
      "validation loss: 21561547712911.74\n",
      "epoch: 883\n",
      "training loss: 31911489920043.125\n",
      "validation loss: 21556356818027.43\n",
      "epoch: 884\n",
      "training loss: 31910349015017.914\n",
      "validation loss: 21551175155865.0\n",
      "epoch: 885\n",
      "training loss: 31909210333872.336\n",
      "validation loss: 21546002708230.934\n",
      "epoch: 886\n",
      "training loss: 31908073871571.4\n",
      "validation loss: 21540839456969.09\n",
      "epoch: 887\n",
      "training loss: 31906939623091.7\n",
      "validation loss: 21535685383960.61\n",
      "epoch: 888\n",
      "training loss: 31905807583421.45\n",
      "validation loss: 21530540471123.867\n",
      "epoch: 889\n",
      "training loss: 31904677747560.383\n",
      "validation loss: 21525404700414.426\n",
      "epoch: 890\n",
      "training loss: 31903550110519.777\n",
      "validation loss: 21520278053824.95\n",
      "epoch: 891\n",
      "training loss: 31902424667322.41\n",
      "validation loss: 21515160513385.133\n",
      "epoch: 892\n",
      "training loss: 31901301413002.508\n",
      "validation loss: 21510052061161.65\n",
      "epoch: 893\n",
      "training loss: 31900180342605.773\n",
      "validation loss: 21504952679258.094\n",
      "epoch: 894\n",
      "training loss: 31899061451189.29\n",
      "validation loss: 21499862349814.906\n",
      "epoch: 895\n",
      "training loss: 31897944733821.54\n",
      "validation loss: 21494781055009.293\n",
      "epoch: 896\n",
      "training loss: 31896830185582.36\n",
      "validation loss: 21489708777055.195\n",
      "epoch: 897\n",
      "training loss: 31895717801562.906\n",
      "validation loss: 21484645498203.2\n",
      "epoch: 898\n",
      "training loss: 31894607576865.656\n",
      "validation loss: 21479591200740.484\n",
      "epoch: 899\n",
      "training loss: 31893499506604.348\n",
      "validation loss: 21474545866990.734\n",
      "epoch: 900\n",
      "training loss: 31892393585903.973\n",
      "validation loss: 21469509479314.105\n",
      "epoch: 901\n",
      "training loss: 31891289809900.727\n",
      "validation loss: 21464482020107.137\n",
      "epoch: 902\n",
      "training loss: 31890188173742.008\n",
      "validation loss: 21459463471802.695\n",
      "epoch: 903\n",
      "training loss: 31889088672586.38\n",
      "validation loss: 21454453816869.906\n",
      "epoch: 904\n",
      "training loss: 31887991301603.547\n",
      "validation loss: 21449453037814.094\n",
      "epoch: 905\n",
      "training loss: 31886896055974.297\n",
      "validation loss: 21444461117176.71\n",
      "epoch: 906\n",
      "training loss: 31885802930890.547\n",
      "validation loss: 21439478037535.266\n",
      "epoch: 907\n",
      "training loss: 31884711921555.223\n",
      "validation loss: 21434503781503.26\n",
      "epoch: 908\n",
      "training loss: 31883623023182.31\n",
      "validation loss: 21429538331730.156\n",
      "epoch: 909\n",
      "training loss: 31882536230996.793\n",
      "validation loss: 21424581670901.26\n",
      "epoch: 910\n",
      "training loss: 31881451540234.613\n",
      "validation loss: 21419633781737.68\n",
      "epoch: 911\n",
      "training loss: 31880368946142.695\n",
      "validation loss: 21414694646996.26\n",
      "epoch: 912\n",
      "training loss: 31879288443978.863\n",
      "validation loss: 21409764249469.51\n",
      "epoch: 913\n",
      "training loss: 31878210029011.855\n",
      "validation loss: 21404842571985.566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 914\n",
      "training loss: 31877133696521.254\n",
      "validation loss: 21399929597408.062\n",
      "epoch: 915\n",
      "training loss: 31876059441797.51\n",
      "validation loss: 21395025308636.145\n",
      "epoch: 916\n",
      "training loss: 31874987260141.895\n",
      "validation loss: 21390129688604.332\n",
      "epoch: 917\n",
      "training loss: 31873917146866.46\n",
      "validation loss: 21385242720282.504\n",
      "epoch: 918\n",
      "training loss: 31872849097294.047\n",
      "validation loss: 21380364386675.805\n",
      "epoch: 919\n",
      "training loss: 31871783106758.2\n",
      "validation loss: 21375494670824.586\n",
      "epoch: 920\n",
      "training loss: 31870719170603.22\n",
      "validation loss: 21370633555804.344\n",
      "epoch: 921\n",
      "training loss: 31869657284184.082\n",
      "validation loss: 21365781024725.65\n",
      "epoch: 922\n",
      "training loss: 31868597442866.42\n",
      "validation loss: 21360937060734.074\n",
      "epoch: 923\n",
      "training loss: 31867539642026.53\n",
      "validation loss: 21356101647010.14\n",
      "epoch: 924\n",
      "training loss: 31866483877051.3\n",
      "validation loss: 21351274766769.258\n",
      "epoch: 925\n",
      "training loss: 31865430143338.203\n",
      "validation loss: 21346456403261.625\n",
      "epoch: 926\n",
      "training loss: 31864378436295.316\n",
      "validation loss: 21341646539772.19\n",
      "epoch: 927\n",
      "training loss: 31863328751341.21\n",
      "validation loss: 21336845159620.598\n",
      "epoch: 928\n",
      "training loss: 31862281083904.996\n",
      "validation loss: 21332052246161.07\n",
      "epoch: 929\n",
      "training loss: 31861235429426.26\n",
      "validation loss: 21327267782782.414\n",
      "epoch: 930\n",
      "training loss: 31860191783355.082\n",
      "validation loss: 21322491752907.887\n",
      "epoch: 931\n",
      "training loss: 31859150141151.93\n",
      "validation loss: 21317724139995.17\n",
      "epoch: 932\n",
      "training loss: 31858110498287.742\n",
      "validation loss: 21312964927536.285\n",
      "epoch: 933\n",
      "training loss: 31857072850243.82\n",
      "validation loss: 21308214099057.55\n",
      "epoch: 934\n",
      "training loss: 31856037192511.824\n",
      "validation loss: 21303471638119.484\n",
      "epoch: 935\n",
      "training loss: 31855003520593.77\n",
      "validation loss: 21298737528316.746\n",
      "epoch: 936\n",
      "training loss: 31853971830002.0\n",
      "validation loss: 21294011753278.098\n",
      "epoch: 937\n",
      "training loss: 31852942116259.133\n",
      "validation loss: 21289294296666.305\n",
      "epoch: 938\n",
      "training loss: 31851914374898.05\n",
      "validation loss: 21284585142178.074\n",
      "epoch: 939\n",
      "training loss: 31850888601461.902\n",
      "validation loss: 21279884273544.016\n",
      "epoch: 940\n",
      "training loss: 31849864791504.04\n",
      "validation loss: 21275191674528.53\n",
      "epoch: 941\n",
      "training loss: 31848842940588.01\n",
      "validation loss: 21270507328929.78\n",
      "epoch: 942\n",
      "training loss: 31847823044287.562\n",
      "validation loss: 21265831220579.637\n",
      "epoch: 943\n",
      "training loss: 31846805098186.562\n",
      "validation loss: 21261163333343.547\n",
      "epoch: 944\n",
      "training loss: 31845789097879.0\n",
      "validation loss: 21256503651120.535\n",
      "epoch: 945\n",
      "training loss: 31844775038968.992\n",
      "validation loss: 21251852157843.113\n",
      "epoch: 946\n",
      "training loss: 31843762917070.715\n",
      "validation loss: 21247208837477.19\n",
      "epoch: 947\n",
      "training loss: 31842752727808.4\n",
      "validation loss: 21242573674022.05\n",
      "epoch: 948\n",
      "training loss: 31841744466816.312\n",
      "validation loss: 21237946651510.266\n",
      "epoch: 949\n",
      "training loss: 31840738129738.723\n",
      "validation loss: 21233327754007.6\n",
      "epoch: 950\n",
      "training loss: 31839733712229.9\n",
      "validation loss: 21228716965613.023\n",
      "epoch: 951\n",
      "training loss: 31838731209954.03\n",
      "validation loss: 21224114270458.54\n",
      "epoch: 952\n",
      "training loss: 31837730618585.297\n",
      "validation loss: 21219519652709.215\n",
      "epoch: 953\n",
      "training loss: 31836731933807.746\n",
      "validation loss: 21214933096563.043\n",
      "epoch: 954\n",
      "training loss: 31835735151315.344\n",
      "validation loss: 21210354586250.94\n",
      "epoch: 955\n",
      "training loss: 31834740266811.918\n",
      "validation loss: 21205784106036.63\n",
      "epoch: 956\n",
      "training loss: 31833747276011.13\n",
      "validation loss: 21201221640216.59\n",
      "epoch: 957\n",
      "training loss: 31832756174636.465\n",
      "validation loss: 21196667173120.008\n",
      "epoch: 958\n",
      "training loss: 31831766958421.227\n",
      "validation loss: 21192120689108.68\n",
      "epoch: 959\n",
      "training loss: 31830779623108.473\n",
      "validation loss: 21187582172576.992\n",
      "epoch: 960\n",
      "training loss: 31829794164451.016\n",
      "validation loss: 21183051607951.805\n",
      "epoch: 961\n",
      "training loss: 31828810578211.42\n",
      "validation loss: 21178528979692.41\n",
      "epoch: 962\n",
      "training loss: 31827828860161.934\n",
      "validation loss: 21174014272290.47\n",
      "epoch: 963\n",
      "training loss: 31826849006084.496\n",
      "validation loss: 21169507470269.95\n",
      "epoch: 964\n",
      "training loss: 31825871011770.71\n",
      "validation loss: 21165008558187.04\n",
      "epoch: 965\n",
      "training loss: 31824894873021.824\n",
      "validation loss: 21160517520630.113\n",
      "epoch: 966\n",
      "training loss: 31823920585648.69\n",
      "validation loss: 21156034342219.637\n",
      "epoch: 967\n",
      "training loss: 31822948145471.79\n",
      "validation loss: 21151559007608.117\n",
      "epoch: 968\n",
      "training loss: 31821977548321.133\n",
      "validation loss: 21147091501480.035\n",
      "epoch: 969\n",
      "training loss: 31821008790036.293\n",
      "validation loss: 21142631808551.773\n",
      "epoch: 970\n",
      "training loss: 31820041866466.41\n",
      "validation loss: 21138179913571.57\n",
      "epoch: 971\n",
      "training loss: 31819076773470.07\n",
      "validation loss: 21133735801319.426\n",
      "epoch: 972\n",
      "training loss: 31818113506915.4\n",
      "validation loss: 21129299456607.062\n",
      "epoch: 973\n",
      "training loss: 31817152062679.938\n",
      "validation loss: 21124870864277.848\n",
      "epoch: 974\n",
      "training loss: 31816192436650.703\n",
      "validation loss: 21120450009206.734\n",
      "epoch: 975\n",
      "training loss: 31815234624724.113\n",
      "validation loss: 21116036876300.195\n",
      "epoch: 976\n",
      "training loss: 31814278622806.0\n",
      "validation loss: 21111631450496.133\n",
      "epoch: 977\n",
      "training loss: 31813324426811.543\n",
      "validation loss: 21107233716763.867\n",
      "epoch: 978\n",
      "training loss: 31812372032665.297\n",
      "validation loss: 21102843660104.035\n",
      "epoch: 979\n",
      "training loss: 31811421436301.14\n",
      "validation loss: 21098461265548.527\n",
      "epoch: 980\n",
      "training loss: 31810472633662.26\n",
      "validation loss: 21094086518160.42\n",
      "epoch: 981\n",
      "training loss: 31809525620701.14\n",
      "validation loss: 21089719403033.945\n",
      "epoch: 982\n",
      "training loss: 31808580393379.523\n",
      "validation loss: 21085359905294.363\n",
      "epoch: 983\n",
      "training loss: 31807636947668.402\n",
      "validation loss: 21081008010097.973\n",
      "epoch: 984\n",
      "training loss: 31806695279547.984\n",
      "validation loss: 21076663702632.0\n",
      "epoch: 985\n",
      "training loss: 31805755385007.7\n",
      "validation loss: 21072326968114.523\n",
      "epoch: 986\n",
      "training loss: 31804817260046.137\n",
      "validation loss: 21067997791794.457\n",
      "epoch: 987\n",
      "training loss: 31803880900671.062\n",
      "validation loss: 21063676158951.445\n",
      "epoch: 988\n",
      "training loss: 31802946302899.375\n",
      "validation loss: 21059362054895.816\n",
      "epoch: 989\n",
      "training loss: 31802013462757.09\n",
      "validation loss: 21055055464968.508\n",
      "epoch: 990\n",
      "training loss: 31801082376279.312\n",
      "validation loss: 21050756374541.035\n",
      "epoch: 991\n",
      "training loss: 31800153039510.25\n",
      "validation loss: 21046464769015.38\n",
      "epoch: 992\n",
      "training loss: 31799225448503.137\n",
      "validation loss: 21042180633823.957\n",
      "epoch: 993\n",
      "training loss: 31798299599320.25\n",
      "validation loss: 21037903954429.54\n",
      "epoch: 994\n",
      "training loss: 31797375488032.89\n",
      "validation loss: 21033634716325.223\n",
      "epoch: 995\n",
      "training loss: 31796453110721.332\n",
      "validation loss: 21029372905034.31\n",
      "epoch: 996\n",
      "training loss: 31795532463474.836\n",
      "validation loss: 21025118506110.293\n",
      "epoch: 997\n",
      "training loss: 31794613542391.617\n",
      "validation loss: 21020871505136.76\n",
      "epoch: 998\n",
      "training loss: 31793696343578.805\n",
      "validation loss: 21016631887727.367\n",
      "epoch: 999\n",
      "training loss: 31792780863152.44\n",
      "validation loss: 21012399639525.74\n",
      "epoch: 1000\n",
      "training loss: 31791867097237.473\n",
      "validation loss: 21008174746205.414\n",
      "epoch: 1001\n",
      "training loss: 31790955041967.703\n",
      "validation loss: 21003957193469.81\n",
      "epoch: 1002\n",
      "training loss: 31790044693485.777\n",
      "validation loss: 20999746967052.125\n",
      "epoch: 1003\n",
      "training loss: 31789136047943.176\n",
      "validation loss: 20995544052715.285\n",
      "epoch: 1004\n",
      "training loss: 31788229101500.207\n",
      "validation loss: 20991348436251.902\n",
      "epoch: 1005\n",
      "training loss: 31787323850325.918\n",
      "validation loss: 20987160103484.188\n",
      "epoch: 1006\n",
      "training loss: 31786420290598.168\n",
      "validation loss: 20982979040263.89\n",
      "epoch: 1007\n",
      "training loss: 31785518418503.54\n",
      "validation loss: 20978805232472.266\n",
      "epoch: 1008\n",
      "training loss: 31784618230237.35\n",
      "validation loss: 20974638666019.973\n",
      "epoch: 1009\n",
      "training loss: 31783719722003.625\n",
      "validation loss: 20970479326847.02\n",
      "epoch: 1010\n",
      "training loss: 31782822890015.07\n",
      "validation loss: 20966327200922.73\n",
      "epoch: 1011\n",
      "training loss: 31781927730493.055\n",
      "validation loss: 20962182274245.66\n",
      "epoch: 1012\n",
      "training loss: 31781034239667.6\n",
      "validation loss: 20958044532843.52\n",
      "epoch: 1013\n",
      "training loss: 31780142413777.363\n",
      "validation loss: 20953913962773.17\n",
      "epoch: 1014\n",
      "training loss: 31779252249069.586\n",
      "validation loss: 20949790550120.477\n",
      "epoch: 1015\n",
      "training loss: 31778363741800.105\n",
      "validation loss: 20945674281000.316\n",
      "epoch: 1016\n",
      "training loss: 31777476888233.344\n",
      "validation loss: 20941565141556.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1017\n",
      "training loss: 31776591684642.25\n",
      "validation loss: 20937463117961.715\n",
      "epoch: 1018\n",
      "training loss: 31775708127308.3\n",
      "validation loss: 20933368196417.4\n",
      "epoch: 1019\n",
      "training loss: 31774826212521.484\n",
      "validation loss: 20929280363153.793\n",
      "epoch: 1020\n",
      "training loss: 31773945936580.297\n",
      "validation loss: 20925199604429.79\n",
      "epoch: 1021\n",
      "training loss: 31773067295791.668\n",
      "validation loss: 20921125906532.902\n",
      "epoch: 1022\n",
      "training loss: 31772190286470.992\n",
      "validation loss: 20917059255779.223\n",
      "epoch: 1023\n",
      "training loss: 31771314904942.117\n",
      "validation loss: 20912999638513.332\n",
      "epoch: 1024\n",
      "training loss: 31770441147537.258\n",
      "validation loss: 20908947041108.25\n",
      "epoch: 1025\n",
      "training loss: 31769569010597.06\n",
      "validation loss: 20904901449965.38\n",
      "epoch: 1026\n",
      "training loss: 31768698490470.508\n",
      "validation loss: 20900862851514.44\n",
      "epoch: 1027\n",
      "training loss: 31767829583514.957\n",
      "validation loss: 20896831232213.414\n",
      "epoch: 1028\n",
      "training loss: 31766962286096.1\n",
      "validation loss: 20892806578548.477\n",
      "epoch: 1029\n",
      "training loss: 31766096594587.94\n",
      "validation loss: 20888788877033.953\n",
      "epoch: 1030\n",
      "training loss: 31765232505372.773\n",
      "validation loss: 20884778114212.223\n",
      "epoch: 1031\n",
      "training loss: 31764370014841.16\n",
      "validation loss: 20880774276653.707\n",
      "epoch: 1032\n",
      "training loss: 31763509119391.93\n",
      "validation loss: 20876777350956.777\n",
      "epoch: 1033\n",
      "training loss: 31762649815432.16\n",
      "validation loss: 20872787323747.707\n",
      "epoch: 1034\n",
      "training loss: 31761792099377.137\n",
      "validation loss: 20868804181680.6\n",
      "epoch: 1035\n",
      "training loss: 31760935967650.336\n",
      "validation loss: 20864827911437.367\n",
      "epoch: 1036\n",
      "training loss: 31760081416683.42\n",
      "validation loss: 20860858499727.6\n",
      "epoch: 1037\n",
      "training loss: 31759228442916.246\n",
      "validation loss: 20856895933288.59\n",
      "epoch: 1038\n",
      "training loss: 31758377042796.758\n",
      "validation loss: 20852940198885.195\n",
      "epoch: 1039\n",
      "training loss: 31757527212781.066\n",
      "validation loss: 20848991283309.855\n",
      "epoch: 1040\n",
      "training loss: 31756678949333.39\n",
      "validation loss: 20845049173382.457\n",
      "epoch: 1041\n",
      "training loss: 31755832248926.0\n",
      "validation loss: 20841113855950.355\n",
      "epoch: 1042\n",
      "training loss: 31754987108039.27\n",
      "validation loss: 20837185317888.22\n",
      "epoch: 1043\n",
      "training loss: 31754143523161.61\n",
      "validation loss: 20833263546098.082\n",
      "epoch: 1044\n",
      "training loss: 31753301490789.465\n",
      "validation loss: 20829348527509.195\n",
      "epoch: 1045\n",
      "training loss: 31752461007427.3\n",
      "validation loss: 20825440249077.992\n",
      "epoch: 1046\n",
      "training loss: 31751622069587.562\n",
      "validation loss: 20821538697788.082\n",
      "epoch: 1047\n",
      "training loss: 31750784673790.684\n",
      "validation loss: 20817643860650.117\n",
      "epoch: 1048\n",
      "training loss: 31749948816565.062\n",
      "validation loss: 20813755724701.766\n",
      "epoch: 1049\n",
      "training loss: 31749114494447.01\n",
      "validation loss: 20809874277007.676\n",
      "epoch: 1050\n",
      "training loss: 31748281703980.8\n",
      "validation loss: 20805999504659.37\n",
      "epoch: 1051\n",
      "training loss: 31747450441718.586\n",
      "validation loss: 20802131394775.246\n",
      "epoch: 1052\n",
      "training loss: 31746620704220.395\n",
      "validation loss: 20798269934500.477\n",
      "epoch: 1053\n",
      "training loss: 31745792488054.145\n",
      "validation loss: 20794415111006.94\n",
      "epoch: 1054\n",
      "training loss: 31744965789795.6\n",
      "validation loss: 20790566911493.227\n",
      "epoch: 1055\n",
      "training loss: 31744140606028.348\n",
      "validation loss: 20786725323184.508\n",
      "epoch: 1056\n",
      "training loss: 31743316933343.785\n",
      "validation loss: 20782890333332.535\n",
      "epoch: 1057\n",
      "training loss: 31742494768341.13\n",
      "validation loss: 20779061929215.56\n",
      "epoch: 1058\n",
      "training loss: 31741674107627.348\n",
      "validation loss: 20775240098138.246\n",
      "epoch: 1059\n",
      "training loss: 31740854947817.17\n",
      "validation loss: 20771424827431.68\n",
      "epoch: 1060\n",
      "training loss: 31740037285533.09\n",
      "validation loss: 20767616104453.266\n",
      "epoch: 1061\n",
      "training loss: 31739221117405.316\n",
      "validation loss: 20763813916586.676\n",
      "epoch: 1062\n",
      "training loss: 31738406440071.742\n",
      "validation loss: 20760018251241.8\n",
      "epoch: 1063\n",
      "training loss: 31737593250177.97\n",
      "validation loss: 20756229095854.695\n",
      "epoch: 1064\n",
      "training loss: 31736781544377.293\n",
      "validation loss: 20752446437887.527\n",
      "epoch: 1065\n",
      "training loss: 31735971319330.61\n",
      "validation loss: 20748670264828.49\n",
      "epoch: 1066\n",
      "training loss: 31735162571706.492\n",
      "validation loss: 20744900564191.777\n",
      "epoch: 1067\n",
      "training loss: 31734355298181.125\n",
      "validation loss: 20741137323517.543\n",
      "epoch: 1068\n",
      "training loss: 31733549495438.28\n",
      "validation loss: 20737380530371.79\n",
      "epoch: 1069\n",
      "training loss: 31732745160169.324\n",
      "validation loss: 20733630172346.35\n",
      "epoch: 1070\n",
      "training loss: 31731942289073.2\n",
      "validation loss: 20729886237058.84\n",
      "epoch: 1071\n",
      "training loss: 31731140878856.363\n",
      "validation loss: 20726148712152.582\n",
      "epoch: 1072\n",
      "training loss: 31730340926232.844\n",
      "validation loss: 20722417585296.547\n",
      "epoch: 1073\n",
      "training loss: 31729542427924.156\n",
      "validation loss: 20718692844185.332\n",
      "epoch: 1074\n",
      "training loss: 31728745380659.33\n",
      "validation loss: 20714974476539.06\n",
      "epoch: 1075\n",
      "training loss: 31727949781174.86\n",
      "validation loss: 20711262470103.344\n",
      "epoch: 1076\n",
      "training loss: 31727155626214.72\n",
      "validation loss: 20707556812649.258\n",
      "epoch: 1077\n",
      "training loss: 31726362912530.31\n",
      "validation loss: 20703857491973.25\n",
      "epoch: 1078\n",
      "training loss: 31725571636880.48\n",
      "validation loss: 20700164495897.074\n",
      "epoch: 1079\n",
      "training loss: 31724781796031.473\n",
      "validation loss: 20696477812267.79\n",
      "epoch: 1080\n",
      "training loss: 31723993386756.93\n",
      "validation loss: 20692797428957.656\n",
      "epoch: 1081\n",
      "training loss: 31723206405837.89\n",
      "validation loss: 20689123333864.11\n",
      "epoch: 1082\n",
      "training loss: 31722420850062.72\n",
      "validation loss: 20685455514909.684\n",
      "epoch: 1083\n",
      "training loss: 31721636716227.152\n",
      "validation loss: 20681793960041.984\n",
      "epoch: 1084\n",
      "training loss: 31720854001134.24\n",
      "validation loss: 20678138657233.6\n",
      "epoch: 1085\n",
      "training loss: 31720072701594.344\n",
      "validation loss: 20674489594482.09\n",
      "epoch: 1086\n",
      "training loss: 31719292814425.117\n",
      "validation loss: 20670846759809.887\n",
      "epoch: 1087\n",
      "training loss: 31718514336451.5\n",
      "validation loss: 20667210141264.273\n",
      "epoch: 1088\n",
      "training loss: 31717737264505.688\n",
      "validation loss: 20663579726917.33\n",
      "epoch: 1089\n",
      "training loss: 31716961595427.1\n",
      "validation loss: 20659955504865.848\n",
      "epoch: 1090\n",
      "training loss: 31716187326062.42\n",
      "validation loss: 20656337463231.33\n",
      "epoch: 1091\n",
      "training loss: 31715414453265.51\n",
      "validation loss: 20652725590159.867\n",
      "epoch: 1092\n",
      "training loss: 31714642973897.44\n",
      "validation loss: 20649119873822.16\n",
      "epoch: 1093\n",
      "training loss: 31713872884826.44\n",
      "validation loss: 20645520302413.41\n",
      "epoch: 1094\n",
      "training loss: 31713104182927.934\n",
      "validation loss: 20641926864153.3\n",
      "epoch: 1095\n",
      "training loss: 31712336865084.45\n",
      "validation loss: 20638339547285.914\n",
      "epoch: 1096\n",
      "training loss: 31711570928185.684\n",
      "validation loss: 20634758340079.72\n",
      "epoch: 1097\n",
      "training loss: 31710806369128.402\n",
      "validation loss: 20631183230827.453\n",
      "epoch: 1098\n",
      "training loss: 31710043184816.5\n",
      "validation loss: 20627614207846.156\n",
      "epoch: 1099\n",
      "training loss: 31709281372160.93\n",
      "validation loss: 20624051259477.043\n",
      "epoch: 1100\n",
      "training loss: 31708520928079.723\n",
      "validation loss: 20620494374085.5\n",
      "epoch: 1101\n",
      "training loss: 31707761849497.94\n",
      "validation loss: 20616943540061.008\n",
      "epoch: 1102\n",
      "training loss: 31707004133347.68\n",
      "validation loss: 20613398745817.086\n",
      "epoch: 1103\n",
      "training loss: 31706247776568.066\n",
      "validation loss: 20609859979791.26\n",
      "epoch: 1104\n",
      "training loss: 31705492776105.21\n",
      "validation loss: 20606327230445.01\n",
      "epoch: 1105\n",
      "training loss: 31704739128912.19\n",
      "validation loss: 20602800486263.688\n",
      "epoch: 1106\n",
      "training loss: 31703986831949.08\n",
      "validation loss: 20599279735756.504\n",
      "epoch: 1107\n",
      "training loss: 31703235882182.88\n",
      "validation loss: 20595764967456.438\n",
      "epoch: 1108\n",
      "training loss: 31702486276587.55\n",
      "validation loss: 20592256169920.24\n",
      "epoch: 1109\n",
      "training loss: 31701738012143.934\n",
      "validation loss: 20588753331728.316\n",
      "epoch: 1110\n",
      "training loss: 31700991085839.8\n",
      "validation loss: 20585256441484.73\n",
      "epoch: 1111\n",
      "training loss: 31700245494669.8\n",
      "validation loss: 20581765487817.13\n",
      "epoch: 1112\n",
      "training loss: 31699501235635.46\n",
      "validation loss: 20578280459376.668\n",
      "epoch: 1113\n",
      "training loss: 31698758305745.152\n",
      "validation loss: 20574801344838.016\n",
      "epoch: 1114\n",
      "training loss: 31698016702014.082\n",
      "validation loss: 20571328132899.26\n",
      "epoch: 1115\n",
      "training loss: 31697276421464.31\n",
      "validation loss: 20567860812281.883\n",
      "epoch: 1116\n",
      "training loss: 31696537461124.664\n",
      "validation loss: 20564399371730.676\n",
      "epoch: 1117\n",
      "training loss: 31695799818030.78\n",
      "validation loss: 20560943800013.734\n",
      "epoch: 1118\n",
      "training loss: 31695063489225.09\n",
      "validation loss: 20557494085922.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1119\n",
      "training loss: 31694328471756.773\n",
      "validation loss: 20554050218271.06\n",
      "epoch: 1120\n",
      "training loss: 31693594762681.723\n",
      "validation loss: 20550612185897.445\n",
      "epoch: 1121\n",
      "training loss: 31692862359062.617\n",
      "validation loss: 20547179977662.24\n",
      "epoch: 1122\n",
      "training loss: 31692131257968.824\n",
      "validation loss: 20543753582449.156\n",
      "epoch: 1123\n",
      "training loss: 31691401456476.395\n",
      "validation loss: 20540332989164.926\n",
      "epoch: 1124\n",
      "training loss: 31690672951668.086\n",
      "validation loss: 20536918186739.19\n",
      "epoch: 1125\n",
      "training loss: 31689945740633.32\n",
      "validation loss: 20533509164124.492\n",
      "epoch: 1126\n",
      "training loss: 31689219820468.17\n",
      "validation loss: 20530105910296.168\n",
      "epoch: 1127\n",
      "training loss: 31688495188275.35\n",
      "validation loss: 20526708414252.383\n",
      "epoch: 1128\n",
      "training loss: 31687771841164.18\n",
      "validation loss: 20523316665014.0\n",
      "epoch: 1129\n",
      "training loss: 31687049776250.613\n",
      "validation loss: 20519930651624.58\n",
      "epoch: 1130\n",
      "training loss: 31686328990657.188\n",
      "validation loss: 20516550363150.32\n",
      "epoch: 1131\n",
      "training loss: 31685609481513.0\n",
      "validation loss: 20513175788680.0\n",
      "epoch: 1132\n",
      "training loss: 31684891245953.742\n",
      "validation loss: 20509806917324.938\n",
      "epoch: 1133\n",
      "training loss: 31684174281121.617\n",
      "validation loss: 20506443738218.934\n",
      "epoch: 1134\n",
      "training loss: 31683458584165.395\n",
      "validation loss: 20503086240518.254\n",
      "epoch: 1135\n",
      "training loss: 31682744152240.34\n",
      "validation loss: 20499734413401.516\n",
      "epoch: 1136\n",
      "training loss: 31682030982508.23\n",
      "validation loss: 20496388246069.71\n",
      "epoch: 1137\n",
      "training loss: 31681319072137.32\n",
      "validation loss: 20493047727746.105\n",
      "epoch: 1138\n",
      "training loss: 31680608418302.348\n",
      "validation loss: 20489712847676.24\n",
      "epoch: 1139\n",
      "training loss: 31679899018184.504\n",
      "validation loss: 20486383595127.81\n",
      "epoch: 1140\n",
      "training loss: 31679190868971.426\n",
      "validation loss: 20483059959390.7\n",
      "epoch: 1141\n",
      "training loss: 31678483967857.17\n",
      "validation loss: 20479741929776.89\n",
      "epoch: 1142\n",
      "training loss: 31677778312042.227\n",
      "validation loss: 20476429495620.395\n",
      "epoch: 1143\n",
      "training loss: 31677073898733.46\n",
      "validation loss: 20473122646277.254\n",
      "epoch: 1144\n",
      "training loss: 31676370725144.13\n",
      "validation loss: 20469821371125.465\n",
      "epoch: 1145\n",
      "training loss: 31675668788493.875\n",
      "validation loss: 20466525659564.938\n",
      "epoch: 1146\n",
      "training loss: 31674968086008.67\n",
      "validation loss: 20463235501017.434\n",
      "epoch: 1147\n",
      "training loss: 31674268614920.848\n",
      "validation loss: 20459950884926.547\n",
      "epoch: 1148\n",
      "training loss: 31673570372469.055\n",
      "validation loss: 20456671800757.637\n",
      "epoch: 1149\n",
      "training loss: 31672873355898.254\n",
      "validation loss: 20453398237997.79\n",
      "epoch: 1150\n",
      "training loss: 31672177562459.707\n",
      "validation loss: 20450130186155.754\n",
      "epoch: 1151\n",
      "training loss: 31671482989410.95\n",
      "validation loss: 20446867634761.92\n",
      "epoch: 1152\n",
      "training loss: 31670789634015.79\n",
      "validation loss: 20443610573368.26\n",
      "epoch: 1153\n",
      "training loss: 31670097493544.297\n",
      "validation loss: 20440358991548.273\n",
      "epoch: 1154\n",
      "training loss: 31669406565272.773\n",
      "validation loss: 20437112878896.957\n",
      "epoch: 1155\n",
      "training loss: 31668716846483.746\n",
      "validation loss: 20433872225030.76\n",
      "epoch: 1156\n",
      "training loss: 31668028334465.957\n",
      "validation loss: 20430637019587.504\n",
      "epoch: 1157\n",
      "training loss: 31667341026514.332\n",
      "validation loss: 20427407252226.383\n",
      "epoch: 1158\n",
      "training loss: 31666654919930.0\n",
      "validation loss: 20424182912627.875\n",
      "epoch: 1159\n",
      "training loss: 31665970012020.242\n",
      "validation loss: 20420963990493.742\n",
      "epoch: 1160\n",
      "training loss: 31665286300098.492\n",
      "validation loss: 20417750475546.926\n",
      "epoch: 1161\n",
      "training loss: 31664603781484.34\n",
      "validation loss: 20414542357531.562\n",
      "epoch: 1162\n",
      "training loss: 31663922453503.477\n",
      "validation loss: 20411339626212.89\n",
      "epoch: 1163\n",
      "training loss: 31663242313487.734\n",
      "validation loss: 20408142271377.242\n",
      "epoch: 1164\n",
      "training loss: 31662563358775.03\n",
      "validation loss: 20404950282831.965\n",
      "epoch: 1165\n",
      "training loss: 31661885586709.348\n",
      "validation loss: 20401763650405.38\n",
      "epoch: 1166\n",
      "training loss: 31661208994640.773\n",
      "validation loss: 20398582363946.777\n",
      "epoch: 1167\n",
      "training loss: 31660533579925.418\n",
      "validation loss: 20395406413326.31\n",
      "epoch: 1168\n",
      "training loss: 31659859339925.457\n",
      "validation loss: 20392235788434.992\n",
      "epoch: 1169\n",
      "training loss: 31659186272009.094\n",
      "validation loss: 20389070479184.652\n",
      "epoch: 1170\n",
      "training loss: 31658514373550.52\n",
      "validation loss: 20385910475507.86\n",
      "epoch: 1171\n",
      "training loss: 31657843641929.965\n",
      "validation loss: 20382755767357.91\n",
      "epoch: 1172\n",
      "training loss: 31657174074533.613\n",
      "validation loss: 20379606344708.754\n",
      "epoch: 1173\n",
      "training loss: 31656505668753.637\n",
      "validation loss: 20376462197554.96\n",
      "epoch: 1174\n",
      "training loss: 31655838421988.184\n",
      "validation loss: 20373323315911.71\n",
      "epoch: 1175\n",
      "training loss: 31655172331641.31\n",
      "validation loss: 20370189689814.684\n",
      "epoch: 1176\n",
      "training loss: 31654507395123.03\n",
      "validation loss: 20367061309320.07\n",
      "epoch: 1177\n",
      "training loss: 31653843609849.28\n",
      "validation loss: 20363938164504.516\n",
      "epoch: 1178\n",
      "training loss: 31653180973241.883\n",
      "validation loss: 20360820245465.03\n",
      "epoch: 1179\n",
      "training loss: 31652519482728.57\n",
      "validation loss: 20357707542319.027\n",
      "epoch: 1180\n",
      "training loss: 31651859135742.938\n",
      "validation loss: 20354600045204.195\n",
      "epoch: 1181\n",
      "training loss: 31651199929724.457\n",
      "validation loss: 20351497744278.523\n",
      "epoch: 1182\n",
      "training loss: 31650541862118.445\n",
      "validation loss: 20348400629720.21\n",
      "epoch: 1183\n",
      "training loss: 31649884930376.06\n",
      "validation loss: 20345308691727.65\n",
      "epoch: 1184\n",
      "training loss: 31649229131954.28\n",
      "validation loss: 20342221920519.344\n",
      "epoch: 1185\n",
      "training loss: 31648574464315.9\n",
      "validation loss: 20339140306333.938\n",
      "epoch: 1186\n",
      "training loss: 31647920924929.504\n",
      "validation loss: 20336063839430.09\n",
      "epoch: 1187\n",
      "training loss: 31647268511269.47\n",
      "validation loss: 20332992510086.48\n",
      "epoch: 1188\n",
      "training loss: 31646617220815.945\n",
      "validation loss: 20329926308601.758\n",
      "epoch: 1189\n",
      "training loss: 31645967051054.816\n",
      "validation loss: 20326865225294.492\n",
      "epoch: 1190\n",
      "training loss: 31645317999477.754\n",
      "validation loss: 20323809250503.13\n",
      "epoch: 1191\n",
      "training loss: 31644670063582.125\n",
      "validation loss: 20320758374585.957\n",
      "epoch: 1192\n",
      "training loss: 31644023240871.03\n",
      "validation loss: 20317712587921.06\n",
      "epoch: 1193\n",
      "training loss: 31643377528853.27\n",
      "validation loss: 20314671880906.26\n",
      "epoch: 1194\n",
      "training loss: 31642732925043.344\n",
      "validation loss: 20311636243959.105\n",
      "epoch: 1195\n",
      "training loss: 31642089426961.418\n",
      "validation loss: 20308605667516.793\n",
      "epoch: 1196\n",
      "training loss: 31641447032133.35\n",
      "validation loss: 20305580142036.152\n",
      "epoch: 1197\n",
      "training loss: 31640805738090.617\n",
      "validation loss: 20302559657993.59\n",
      "epoch: 1198\n",
      "training loss: 31640165542370.36\n",
      "validation loss: 20299544205885.055\n",
      "epoch: 1199\n",
      "training loss: 31639526442515.348\n",
      "validation loss: 20296533776226.0\n",
      "epoch: 1200\n",
      "training loss: 31638888436073.94\n",
      "validation loss: 20293528359551.305\n",
      "epoch: 1201\n",
      "training loss: 31638251520600.133\n",
      "validation loss: 20290527946415.297\n",
      "epoch: 1202\n",
      "training loss: 31637615693653.484\n",
      "validation loss: 20287532527391.656\n",
      "epoch: 1203\n",
      "training loss: 31636980952799.137\n",
      "validation loss: 20284542093073.39\n",
      "epoch: 1204\n",
      "training loss: 31636347295607.805\n",
      "validation loss: 20281556634072.797\n",
      "epoch: 1205\n",
      "training loss: 31635714719655.734\n",
      "validation loss: 20278576141021.414\n",
      "epoch: 1206\n",
      "training loss: 31635083222524.72\n",
      "validation loss: 20275600604570.004\n",
      "epoch: 1207\n",
      "training loss: 31634452801802.09\n",
      "validation loss: 20272630015388.46\n",
      "epoch: 1208\n",
      "training loss: 31633823455080.67\n",
      "validation loss: 20269664364165.83\n",
      "epoch: 1209\n",
      "training loss: 31633195179958.793\n",
      "validation loss: 20266703641610.21\n",
      "epoch: 1210\n",
      "training loss: 31632567974040.285\n",
      "validation loss: 20263747838448.766\n",
      "epoch: 1211\n",
      "training loss: 31631941834934.426\n",
      "validation loss: 20260796945427.625\n",
      "epoch: 1212\n",
      "training loss: 31631316760255.984\n",
      "validation loss: 20257850953311.906\n",
      "epoch: 1213\n",
      "training loss: 31630692747625.145\n",
      "validation loss: 20254909852885.63\n",
      "epoch: 1214\n",
      "training loss: 31630069794667.574\n",
      "validation loss: 20251973634951.688\n",
      "epoch: 1215\n",
      "training loss: 31629447899014.33\n",
      "validation loss: 20249042290331.805\n",
      "epoch: 1216\n",
      "training loss: 31628827058301.883\n",
      "validation loss: 20246115809866.516\n",
      "epoch: 1217\n",
      "training loss: 31628207270172.125\n",
      "validation loss: 20243194184415.09\n",
      "epoch: 1218\n",
      "training loss: 31627588532272.31\n",
      "validation loss: 20240277404855.516\n",
      "epoch: 1219\n",
      "training loss: 31626970842255.082\n",
      "validation loss: 20237365462084.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1220\n",
      "training loss: 31626354197778.453\n",
      "validation loss: 20234458347017.203\n",
      "epoch: 1221\n",
      "training loss: 31625738596505.773\n",
      "validation loss: 20231556050587.645\n",
      "epoch: 1222\n",
      "training loss: 31625124036105.734\n",
      "validation loss: 20228658563748.215\n",
      "epoch: 1223\n",
      "training loss: 31624510514252.344\n",
      "validation loss: 20225765877469.88\n",
      "epoch: 1224\n",
      "training loss: 31623898028624.96\n",
      "validation loss: 20222877982742.03\n",
      "epoch: 1225\n",
      "training loss: 31623286576908.19\n",
      "validation loss: 20219994870572.54\n",
      "epoch: 1226\n",
      "training loss: 31622676156791.973\n",
      "validation loss: 20217116531987.652\n",
      "epoch: 1227\n",
      "training loss: 31622066765971.504\n",
      "validation loss: 20214242958031.973\n",
      "epoch: 1228\n",
      "training loss: 31621458402147.25\n",
      "validation loss: 20211374139768.395\n",
      "epoch: 1229\n",
      "training loss: 31620851063024.93\n",
      "validation loss: 20208510068278.125\n",
      "epoch: 1230\n",
      "training loss: 31620244746315.5\n",
      "validation loss: 20205650734660.57\n",
      "epoch: 1231\n",
      "training loss: 31619639449735.152\n",
      "validation loss: 20202796130033.35\n",
      "epoch: 1232\n",
      "training loss: 31619035171005.3\n",
      "validation loss: 20199946245532.24\n",
      "epoch: 1233\n",
      "training loss: 31618431907852.547\n",
      "validation loss: 20197101072311.11\n",
      "epoch: 1234\n",
      "training loss: 31617829658008.703\n",
      "validation loss: 20194260601541.953\n",
      "epoch: 1235\n",
      "training loss: 31617228419210.754\n",
      "validation loss: 20191424824414.75\n",
      "epoch: 1236\n",
      "training loss: 31616628189200.844\n",
      "validation loss: 20188593732137.52\n",
      "epoch: 1237\n",
      "training loss: 31616028965726.3\n",
      "validation loss: 20185767315936.223\n",
      "epoch: 1238\n",
      "training loss: 31615430746539.58\n",
      "validation loss: 20182945567054.734\n",
      "epoch: 1239\n",
      "training loss: 31614833529398.277\n",
      "validation loss: 20180128476754.848\n",
      "epoch: 1240\n",
      "training loss: 31614237312065.098\n",
      "validation loss: 20177316036316.17\n",
      "epoch: 1241\n",
      "training loss: 31613642092307.875\n",
      "validation loss: 20174508237036.12\n",
      "epoch: 1242\n",
      "training loss: 31613047867899.54\n",
      "validation loss: 20171705070229.9\n",
      "epoch: 1243\n",
      "training loss: 31612454636618.09\n",
      "validation loss: 20168906527230.418\n",
      "epoch: 1244\n",
      "training loss: 31611862396246.63\n",
      "validation loss: 20166112599388.31\n",
      "epoch: 1245\n",
      "training loss: 31611271144573.297\n",
      "validation loss: 20163323278071.83\n",
      "epoch: 1246\n",
      "training loss: 31610680879391.297\n",
      "validation loss: 20160538554666.855\n",
      "epoch: 1247\n",
      "training loss: 31610091598498.887\n",
      "validation loss: 20157758420576.867\n",
      "epoch: 1248\n",
      "training loss: 31609503299699.336\n",
      "validation loss: 20154982867222.86\n",
      "epoch: 1249\n",
      "training loss: 31608915980800.926\n",
      "validation loss: 20152211886043.33\n",
      "epoch: 1250\n",
      "training loss: 31608329639616.953\n",
      "validation loss: 20149445468494.266\n",
      "epoch: 1251\n",
      "training loss: 31607744273965.727\n",
      "validation loss: 20146683606049.055\n",
      "epoch: 1252\n",
      "training loss: 31607159881670.504\n",
      "validation loss: 20143926290198.484\n",
      "epoch: 1253\n",
      "training loss: 31606576460559.547\n",
      "validation loss: 20141173512450.695\n",
      "epoch: 1254\n",
      "training loss: 31605994008466.05\n",
      "validation loss: 20138425264331.137\n",
      "epoch: 1255\n",
      "training loss: 31605412523228.19\n",
      "validation loss: 20135681537382.547\n",
      "epoch: 1256\n",
      "training loss: 31604832002689.04\n",
      "validation loss: 20132942323164.89\n",
      "epoch: 1257\n",
      "training loss: 31604252444696.63\n",
      "validation loss: 20130207613255.35\n",
      "epoch: 1258\n",
      "training loss: 31603673847103.883\n",
      "validation loss: 20127477399248.273\n",
      "epoch: 1259\n",
      "training loss: 31603096207768.664\n",
      "validation loss: 20124751672755.117\n",
      "epoch: 1260\n",
      "training loss: 31602519524553.695\n",
      "validation loss: 20122030425404.45\n",
      "epoch: 1261\n",
      "training loss: 31601943795326.574\n",
      "validation loss: 20119313648841.9\n",
      "epoch: 1262\n",
      "training loss: 31601369017959.8\n",
      "validation loss: 20116601334730.098\n",
      "epoch: 1263\n",
      "training loss: 31600795190330.73\n",
      "validation loss: 20113893474748.668\n",
      "epoch: 1264\n",
      "training loss: 31600222310321.52\n",
      "validation loss: 20111190060594.176\n",
      "epoch: 1265\n",
      "training loss: 31599650375819.227\n",
      "validation loss: 20108491083980.1\n",
      "epoch: 1266\n",
      "training loss: 31599079384715.7\n",
      "validation loss: 20105796536636.79\n",
      "epoch: 1267\n",
      "training loss: 31598509334907.594\n",
      "validation loss: 20103106410311.438\n",
      "epoch: 1268\n",
      "training loss: 31597940224296.395\n",
      "validation loss: 20100420696768.023\n",
      "epoch: 1269\n",
      "training loss: 31597372050788.36\n",
      "validation loss: 20097739387787.293\n",
      "epoch: 1270\n",
      "training loss: 31596804812294.543\n",
      "validation loss: 20095062475166.746\n",
      "epoch: 1271\n",
      "training loss: 31596238506730.75\n",
      "validation loss: 20092389950720.543\n",
      "epoch: 1272\n",
      "training loss: 31595673132017.56\n",
      "validation loss: 20089721806279.51\n",
      "epoch: 1273\n",
      "training loss: 31595108686080.312\n",
      "validation loss: 20087058033691.113\n",
      "epoch: 1274\n",
      "training loss: 31594545166849.055\n",
      "validation loss: 20084398624819.383\n",
      "epoch: 1275\n",
      "training loss: 31593982572258.6\n",
      "validation loss: 20081743571544.9\n",
      "epoch: 1276\n",
      "training loss: 31593420900248.426\n",
      "validation loss: 20079092865764.758\n",
      "epoch: 1277\n",
      "training loss: 31592860148762.766\n",
      "validation loss: 20076446499392.547\n",
      "epoch: 1278\n",
      "training loss: 31592300315750.523\n",
      "validation loss: 20073804464358.29\n",
      "epoch: 1279\n",
      "training loss: 31591741399165.285\n",
      "validation loss: 20071166752608.402\n",
      "epoch: 1280\n",
      "training loss: 31591183396965.324\n",
      "validation loss: 20068533356105.7\n",
      "epoch: 1281\n",
      "training loss: 31590626307113.562\n",
      "validation loss: 20065904266829.31\n",
      "epoch: 1282\n",
      "training loss: 31590070127577.58\n",
      "validation loss: 20063279476774.676\n",
      "epoch: 1283\n",
      "training loss: 31589514856329.594\n",
      "validation loss: 20060658977953.508\n",
      "epoch: 1284\n",
      "training loss: 31588960491346.473\n",
      "validation loss: 20058042762393.742\n",
      "epoch: 1285\n",
      "training loss: 31588407030609.67\n",
      "validation loss: 20055430822139.516\n",
      "epoch: 1286\n",
      "training loss: 31587854472105.277\n",
      "validation loss: 20052823149251.137\n",
      "epoch: 1287\n",
      "training loss: 31587302813823.957\n",
      "validation loss: 20050219735805.027\n",
      "epoch: 1288\n",
      "training loss: 31586752053760.99\n",
      "validation loss: 20047620573893.707\n",
      "epoch: 1289\n",
      "training loss: 31586202189916.223\n",
      "validation loss: 20045025655625.76\n",
      "epoch: 1290\n",
      "training loss: 31585653220294.066\n",
      "validation loss: 20042434973125.785\n",
      "epoch: 1291\n",
      "training loss: 31585105142903.49\n",
      "validation loss: 20039848518534.37\n",
      "epoch: 1292\n",
      "training loss: 31584557955758.016\n",
      "validation loss: 20037266284008.086\n",
      "epoch: 1293\n",
      "training loss: 31584011656875.68\n",
      "validation loss: 20034688261719.387\n",
      "epoch: 1294\n",
      "training loss: 31583466244279.074\n",
      "validation loss: 20032114443856.62\n",
      "epoch: 1295\n",
      "training loss: 31582921715995.28\n",
      "validation loss: 20029544822624.016\n",
      "epoch: 1296\n",
      "training loss: 31582378070055.918\n",
      "validation loss: 20026979390241.6\n",
      "epoch: 1297\n",
      "training loss: 31581835304497.06\n",
      "validation loss: 20024418138945.184\n",
      "epoch: 1298\n",
      "training loss: 31581293417359.285\n",
      "validation loss: 20021861060986.324\n",
      "epoch: 1299\n",
      "training loss: 31580752406687.65\n",
      "validation loss: 20019308148632.316\n",
      "epoch: 1300\n",
      "training loss: 31580212270531.664\n",
      "validation loss: 20016759394166.113\n",
      "epoch: 1301\n",
      "training loss: 31579673006945.297\n",
      "validation loss: 20014214789886.336\n",
      "epoch: 1302\n",
      "training loss: 31579134613986.97\n",
      "validation loss: 20011674328107.227\n",
      "epoch: 1303\n",
      "training loss: 31578597089719.508\n",
      "validation loss: 20009138001158.582\n",
      "epoch: 1304\n",
      "training loss: 31578060432210.188\n",
      "validation loss: 20006605801385.785\n",
      "epoch: 1305\n",
      "training loss: 31577524639530.695\n",
      "validation loss: 20004077721149.703\n",
      "epoch: 1306\n",
      "training loss: 31576989709757.11\n",
      "validation loss: 20001553752826.707\n",
      "epoch: 1307\n",
      "training loss: 31576455640969.914\n",
      "validation loss: 19999033888808.61\n",
      "epoch: 1308\n",
      "training loss: 31575922431253.953\n",
      "validation loss: 19996518121502.633\n",
      "epoch: 1309\n",
      "training loss: 31575390078698.457\n",
      "validation loss: 19994006443331.395\n",
      "epoch: 1310\n",
      "training loss: 31574858581397.035\n",
      "validation loss: 19991498846732.855\n",
      "epoch: 1311\n",
      "training loss: 31574327937447.62\n",
      "validation loss: 19988995324160.297\n",
      "epoch: 1312\n",
      "training loss: 31573798144952.508\n",
      "validation loss: 19986495868082.285\n",
      "epoch: 1313\n",
      "training loss: 31573269202018.312\n",
      "validation loss: 19984000470982.637\n",
      "epoch: 1314\n",
      "training loss: 31572741106755.984\n",
      "validation loss: 19981509125360.387\n",
      "epoch: 1315\n",
      "training loss: 31572213857280.78\n",
      "validation loss: 19979021823729.76\n",
      "epoch: 1316\n",
      "training loss: 31571687451712.26\n",
      "validation loss: 19976538558620.133\n",
      "epoch: 1317\n",
      "training loss: 31571161888174.27\n",
      "validation loss: 19974059322575.992\n",
      "epoch: 1318\n",
      "training loss: 31570637164794.95\n",
      "validation loss: 19971584108156.938\n",
      "epoch: 1319\n",
      "training loss: 31570113279706.71\n",
      "validation loss: 19969112907937.61\n",
      "epoch: 1320\n",
      "training loss: 31569590231046.23\n",
      "validation loss: 19966645714507.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1321\n",
      "training loss: 31569068016954.43\n",
      "validation loss: 19964182520471.8\n",
      "epoch: 1322\n",
      "training loss: 31568546635576.473\n",
      "validation loss: 19961723318449.594\n",
      "epoch: 1323\n",
      "training loss: 31568026085061.77\n",
      "validation loss: 19959268101075.617\n",
      "epoch: 1324\n",
      "training loss: 31567506363563.965\n",
      "validation loss: 19956816860999.31\n",
      "epoch: 1325\n",
      "training loss: 31566987469240.887\n",
      "validation loss: 19954369590884.98\n",
      "epoch: 1326\n",
      "training loss: 31566469400254.598\n",
      "validation loss: 19951926283411.773\n",
      "epoch: 1327\n",
      "training loss: 31565952154771.324\n",
      "validation loss: 19949486931273.63\n",
      "epoch: 1328\n",
      "training loss: 31565435730961.52\n",
      "validation loss: 19947051527179.26\n",
      "epoch: 1329\n",
      "training loss: 31564920126999.793\n",
      "validation loss: 19944620063852.12\n",
      "epoch: 1330\n",
      "training loss: 31564405341064.906\n",
      "validation loss: 19942192534030.355\n",
      "epoch: 1331\n",
      "training loss: 31563891371339.797\n",
      "validation loss: 19939768930466.8\n",
      "epoch: 1332\n",
      "training loss: 31563378216011.547\n",
      "validation loss: 19937349245928.914\n",
      "epoch: 1333\n",
      "training loss: 31562865873271.387\n",
      "validation loss: 19934933473198.8\n",
      "epoch: 1334\n",
      "training loss: 31562354341314.65\n",
      "validation loss: 19932521605073.11\n",
      "epoch: 1335\n",
      "training loss: 31561843618340.805\n",
      "validation loss: 19930113634363.055\n",
      "epoch: 1336\n",
      "training loss: 31561333702553.445\n",
      "validation loss: 19927709553894.37\n",
      "epoch: 1337\n",
      "training loss: 31560824592160.23\n",
      "validation loss: 19925309356507.266\n",
      "epoch: 1338\n",
      "training loss: 31560316285372.945\n",
      "validation loss: 19922913035056.42\n",
      "epoch: 1339\n",
      "training loss: 31559808780407.42\n",
      "validation loss: 19920520582410.93\n",
      "epoch: 1340\n",
      "training loss: 31559302075483.605\n",
      "validation loss: 19918131991454.28\n",
      "epoch: 1341\n",
      "training loss: 31558796168825.477\n",
      "validation loss: 19915747255084.344\n",
      "epoch: 1342\n",
      "training loss: 31558291058661.08\n",
      "validation loss: 19913366366213.29\n",
      "epoch: 1343\n",
      "training loss: 31557786743222.473\n",
      "validation loss: 19910989317767.617\n",
      "epoch: 1344\n",
      "training loss: 31557283220745.797\n",
      "validation loss: 19908616102688.08\n",
      "epoch: 1345\n",
      "training loss: 31556780489471.2\n",
      "validation loss: 19906246713929.688\n",
      "epoch: 1346\n",
      "training loss: 31556278547642.83\n",
      "validation loss: 19903881144461.637\n",
      "epoch: 1347\n",
      "training loss: 31555777393508.86\n",
      "validation loss: 19901519387267.332\n",
      "epoch: 1348\n",
      "training loss: 31555277025321.445\n",
      "validation loss: 19899161435344.316\n",
      "epoch: 1349\n",
      "training loss: 31554777441336.73\n",
      "validation loss: 19896807281704.24\n",
      "epoch: 1350\n",
      "training loss: 31554278639814.83\n",
      "validation loss: 19894456919372.848\n",
      "epoch: 1351\n",
      "training loss: 31553780619019.797\n",
      "validation loss: 19892110341389.957\n",
      "epoch: 1352\n",
      "training loss: 31553283377219.598\n",
      "validation loss: 19889767540809.406\n",
      "epoch: 1353\n",
      "training loss: 31552786912685.996\n",
      "validation loss: 19887428510699.027\n",
      "epoch: 1354\n",
      "training loss: 31552291223694.223\n",
      "validation loss: 19885093244140.633\n",
      "epoch: 1355\n",
      "training loss: 31551796308520.844\n",
      "validation loss: 19882761734229.973\n",
      "epoch: 1356\n",
      "training loss: 31551302165414.87\n",
      "validation loss: 19880433974076.723\n",
      "epoch: 1357\n",
      "training loss: 31550808752148.062\n",
      "validation loss: 19878109956804.668\n",
      "epoch: 1358\n",
      "training loss: 31550230554390.324\n",
      "validation loss: 19875669195513.8\n",
      "epoch: 1359\n",
      "training loss: 31549738695123.02\n",
      "validation loss: 19873353150850.457\n",
      "epoch: 1360\n",
      "training loss: 31549247601376.805\n",
      "validation loss: 19871040817864.33\n",
      "epoch: 1361\n",
      "training loss: 31548757271450.01\n",
      "validation loss: 19868732189982.875\n",
      "epoch: 1362\n",
      "training loss: 31548267703644.984\n",
      "validation loss: 19866427260641.516\n",
      "epoch: 1363\n",
      "training loss: 31547778896268.02\n",
      "validation loss: 19864126023283.79\n",
      "epoch: 1364\n",
      "training loss: 31547290847629.402\n",
      "validation loss: 19861828471361.35\n",
      "epoch: 1365\n",
      "training loss: 31546803556043.36\n",
      "validation loss: 19859534598334.188\n",
      "epoch: 1366\n",
      "training loss: 31546317019828.082\n",
      "validation loss: 19857244397670.645\n",
      "epoch: 1367\n",
      "training loss: 31545831237305.67\n",
      "validation loss: 19854957862847.52\n",
      "epoch: 1368\n",
      "training loss: 31545346206802.18\n",
      "validation loss: 19852674987350.168\n",
      "epoch: 1369\n",
      "training loss: 31544861926647.543\n",
      "validation loss: 19850395764672.586\n",
      "epoch: 1370\n",
      "training loss: 31544378395175.6\n",
      "validation loss: 19848120188317.484\n",
      "epoch: 1371\n",
      "training loss: 31543895610724.09\n",
      "validation loss: 19845848251796.383\n",
      "epoch: 1372\n",
      "training loss: 31543413571634.625\n",
      "validation loss: 19843579948629.684\n",
      "epoch: 1373\n",
      "training loss: 31542932276252.684\n",
      "validation loss: 19841315272346.74\n",
      "epoch: 1374\n",
      "training loss: 31542451722927.605\n",
      "validation loss: 19839054216485.945\n",
      "epoch: 1375\n",
      "training loss: 31541971910012.555\n",
      "validation loss: 19836796774594.793\n",
      "epoch: 1376\n",
      "training loss: 31541492835864.555\n",
      "validation loss: 19834542940229.957\n",
      "epoch: 1377\n",
      "training loss: 31541014498844.438\n",
      "validation loss: 19832292706957.348\n",
      "epoch: 1378\n",
      "training loss: 31540536897316.848\n",
      "validation loss: 19830046068352.164\n",
      "epoch: 1379\n",
      "training loss: 31540060029650.234\n",
      "validation loss: 19827803017999.004\n",
      "epoch: 1380\n",
      "training loss: 31539583894216.848\n",
      "validation loss: 19825563549491.88\n",
      "epoch: 1381\n",
      "training loss: 31539108489392.7\n",
      "validation loss: 19823327656434.273\n",
      "epoch: 1382\n",
      "training loss: 31538633813557.59\n",
      "validation loss: 19821095332439.258\n",
      "epoch: 1383\n",
      "training loss: 31538159865095.066\n",
      "validation loss: 19818866571129.47\n",
      "epoch: 1384\n",
      "training loss: 31537686642392.453\n",
      "validation loss: 19816641366137.215\n",
      "epoch: 1385\n",
      "training loss: 31537214143840.773\n",
      "validation loss: 19814419711104.508\n",
      "epoch: 1386\n",
      "training loss: 31536742367834.816\n",
      "validation loss: 19812201599683.105\n",
      "epoch: 1387\n",
      "training loss: 31536271312773.062\n",
      "validation loss: 19809987025534.586\n",
      "epoch: 1388\n",
      "training loss: 31535800977057.73\n",
      "validation loss: 19807775982330.367\n",
      "epoch: 1389\n",
      "training loss: 31535331359094.73\n",
      "validation loss: 19805568463751.754\n",
      "epoch: 1390\n",
      "training loss: 31534862457293.645\n",
      "validation loss: 19803364463490.016\n",
      "epoch: 1551\n",
      "training loss: 31467680728565.08\n",
      "validation loss: 19490236480148.824\n",
      "epoch: 1552\n",
      "training loss: 31467309257214.293\n",
      "validation loss: 19488526697440.95\n",
      "epoch: 1553\n",
      "training loss: 31466938286035.695\n",
      "validation loss: 19486819543638.86\n",
      "epoch: 1554\n",
      "training loss: 31466567813937.06\n",
      "validation loss: 19485115014099.383\n",
      "epoch: 1555\n",
      "training loss: 31466197839828.652\n",
      "validation loss: 19483413104188.434\n",
      "epoch: 1556\n",
      "training loss: 31465828362623.227\n",
      "validation loss: 19481713809280.918\n",
      "epoch: 1557\n",
      "training loss: 31465459381236.008\n",
      "validation loss: 19480017124760.8\n",
      "epoch: 1558\n",
      "training loss: 31465090894584.695\n",
      "validation loss: 19478323046021.027\n",
      "epoch: 1559\n",
      "training loss: 31464722901589.445\n",
      "validation loss: 19476631568463.535\n",
      "epoch: 1560\n",
      "training loss: 31464355401172.895\n",
      "validation loss: 19474942687499.22\n",
      "epoch: 1561\n",
      "training loss: 31463988392260.12\n",
      "validation loss: 19473256398547.945\n",
      "epoch: 1562\n",
      "training loss: 31463621873778.65\n",
      "validation loss: 19471572697038.496\n",
      "epoch: 1563\n",
      "training loss: 31463255844658.457\n",
      "validation loss: 19469891578408.56\n",
      "epoch: 1564\n",
      "training loss: 31462890303831.95\n",
      "validation loss: 19468213038104.73\n",
      "epoch: 1565\n",
      "training loss: 31462525250233.977\n",
      "validation loss: 19466537071582.496\n",
      "epoch: 1566\n",
      "training loss: 31462160682801.8\n",
      "validation loss: 19464863674306.17\n",
      "epoch: 1567\n",
      "training loss: 31461796600475.12\n",
      "validation loss: 19463192841748.938\n",
      "epoch: 1568\n",
      "training loss: 31461433002196.035\n",
      "validation loss: 19461524569392.797\n",
      "epoch: 1569\n",
      "training loss: 31461069886909.062\n",
      "validation loss: 19459858852728.56\n",
      "epoch: 1570\n",
      "training loss: 31460707253561.13\n",
      "validation loss: 19458195687255.832\n",
      "epoch: 1571\n",
      "training loss: 31460345101101.56\n",
      "validation loss: 19456535068482.984\n",
      "epoch: 1572\n",
      "training loss: 31459983428482.047\n",
      "validation loss: 19454876991927.145\n",
      "epoch: 1573\n",
      "training loss: 31459622234656.703\n",
      "validation loss: 19453221453114.164\n",
      "epoch: 1574\n",
      "training loss: 31459261518582.023\n",
      "validation loss: 19451568447578.652\n",
      "epoch: 1575\n",
      "training loss: 31458901279216.863\n",
      "validation loss: 19449917970863.895\n",
      "epoch: 1576\n",
      "training loss: 31458541515522.445\n",
      "validation loss: 19448270018521.86\n",
      "epoch: 1577\n",
      "training loss: 31458182226462.375\n",
      "validation loss: 19446624586113.203\n",
      "epoch: 1578\n",
      "training loss: 31457823411002.61\n",
      "validation loss: 19444981669207.21\n",
      "epoch: 1579\n",
      "training loss: 31457465068111.477\n",
      "validation loss: 19443341263381.816\n",
      "epoch: 1580\n",
      "training loss: 31457107196759.63\n",
      "validation loss: 19441703364223.56\n",
      "epoch: 1581\n",
      "training loss: 31456749795920.066\n",
      "validation loss: 19440067967327.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1582\n",
      "training loss: 31456392864568.164\n",
      "validation loss: 19438435068297.61\n",
      "epoch: 1583\n",
      "training loss: 31456036401681.582\n",
      "validation loss: 19436804662745.934\n",
      "epoch: 1584\n",
      "training loss: 31455680406240.344\n",
      "validation loss: 19435176746293.383\n",
      "epoch: 1585\n",
      "training loss: 31455324877226.766\n",
      "validation loss: 19433551314569.32\n",
      "epoch: 1586\n",
      "training loss: 31454969813625.51\n",
      "validation loss: 19431928363211.625\n",
      "epoch: 1587\n",
      "training loss: 31454615214423.55\n",
      "validation loss: 19430307887866.676\n",
      "epoch: 1588\n",
      "training loss: 31454261078610.15\n",
      "validation loss: 19428689884189.31\n",
      "epoch: 1589\n",
      "training loss: 31453907405176.87\n",
      "validation loss: 19427074347842.836\n",
      "epoch: 1590\n",
      "training loss: 31453554193117.6\n",
      "validation loss: 19425461274499.008\n",
      "epoch: 1591\n",
      "training loss: 31453201441428.48\n",
      "validation loss: 19423850659838.01\n",
      "epoch: 1592\n",
      "training loss: 31452849149107.97\n",
      "validation loss: 19422242499548.43\n",
      "epoch: 1593\n",
      "training loss: 31452497315156.8\n",
      "validation loss: 19420636789327.246\n",
      "epoch: 1594\n",
      "training loss: 31452145938577.96\n",
      "validation loss: 19419033524879.81\n",
      "epoch: 1595\n",
      "training loss: 31451795018376.74\n",
      "validation loss: 19417432701919.832\n",
      "epoch: 1596\n",
      "training loss: 31451444553560.652\n",
      "validation loss: 19415834316169.367\n",
      "epoch: 1597\n",
      "training loss: 31451094543139.527\n",
      "validation loss: 19414238363358.81\n",
      "epoch: 1598\n",
      "training loss: 31450744986125.383\n",
      "validation loss: 19412644839226.816\n",
      "epoch: 1599\n",
      "training loss: 31450395881532.555\n",
      "validation loss: 19411053739520.383\n",
      "epoch: 1600\n",
      "training loss: 31450047228377.58\n",
      "validation loss: 19409465059994.754\n",
      "epoch: 1601\n",
      "training loss: 31449699025679.223\n",
      "validation loss: 19407878796413.426\n",
      "epoch: 1602\n",
      "training loss: 31449351272458.527\n",
      "validation loss: 19406294944548.137\n",
      "epoch: 1603\n",
      "training loss: 31449003967738.746\n",
      "validation loss: 19404713500178.875\n",
      "epoch: 1604\n",
      "training loss: 31448657110545.336\n",
      "validation loss: 19403134459093.8\n",
      "epoch: 1605\n",
      "training loss: 31448310699905.992\n",
      "validation loss: 19401557817089.28\n",
      "epoch: 1606\n",
      "training loss: 31447964734850.633\n",
      "validation loss: 19399983569969.844\n",
      "epoch: 1607\n",
      "training loss: 31447619214411.355\n",
      "validation loss: 19398411713548.17\n",
      "epoch: 1608\n",
      "training loss: 31447274137622.484\n",
      "validation loss: 19396842243645.105\n",
      "epoch: 1609\n",
      "training loss: 31446929503520.555\n",
      "validation loss: 19395275156089.594\n",
      "epoch: 1610\n",
      "training loss: 31446585311144.25\n",
      "validation loss: 19393710446718.688\n",
      "epoch: 1611\n",
      "training loss: 31446241559534.477\n",
      "validation loss: 19392148111377.527\n",
      "epoch: 1612\n",
      "training loss: 31445898247734.32\n",
      "validation loss: 19390588145919.344\n",
      "epoch: 1613\n",
      "training loss: 31445555374789.04\n",
      "validation loss: 19389030546205.402\n",
      "epoch: 1614\n",
      "training loss: 31445212939746.066\n",
      "validation loss: 19387475308105.016\n",
      "epoch: 1615\n",
      "training loss: 31444870941655.004\n",
      "validation loss: 19385922427495.52\n",
      "epoch: 1616\n",
      "training loss: 31444529379567.62\n",
      "validation loss: 19384371900262.258\n",
      "epoch: 1617\n",
      "training loss: 31444188252537.84\n",
      "validation loss: 19382823722298.56\n",
      "epoch: 1618\n",
      "training loss: 31443847559621.74\n",
      "validation loss: 19381277889505.73\n",
      "epoch: 1619\n",
      "training loss: 31443507299877.543\n",
      "validation loss: 19379734397793.04\n",
      "epoch: 1620\n",
      "training loss: 31443167472365.633\n",
      "validation loss: 19378193243077.684\n",
      "epoch: 1621\n",
      "training loss: 31442828076148.5\n",
      "validation loss: 19376654421284.793\n",
      "epoch: 1622\n",
      "training loss: 31442489110290.797\n",
      "validation loss: 19375117928347.402\n",
      "epoch: 1623\n",
      "training loss: 31442150573859.297\n",
      "validation loss: 19373583760206.438\n",
      "epoch: 1624\n",
      "training loss: 31441812465922.895\n",
      "validation loss: 19372051912810.71\n",
      "epoch: 1625\n",
      "training loss: 31441474785552.605\n",
      "validation loss: 19370522382116.867\n",
      "epoch: 1626\n",
      "training loss: 31441137531821.56\n",
      "validation loss: 19368995164089.426\n",
      "epoch: 1627\n",
      "training loss: 31440800703804.992\n",
      "validation loss: 19367470254700.72\n",
      "epoch: 1628\n",
      "training loss: 31440464300580.25\n",
      "validation loss: 19365947649930.9\n",
      "epoch: 1629\n",
      "training loss: 31440128321226.79\n",
      "validation loss: 19364427345767.902\n",
      "epoch: 1630\n",
      "training loss: 31439792764826.13\n",
      "validation loss: 19362909338207.44\n",
      "epoch: 1631\n",
      "training loss: 31439457630461.91\n",
      "validation loss: 19361393623252.992\n",
      "epoch: 1632\n",
      "training loss: 31439122917219.848\n",
      "validation loss: 19359880196915.79\n",
      "epoch: 1633\n",
      "training loss: 31438788624187.746\n",
      "validation loss: 19358369055214.8\n",
      "epoch: 1634\n",
      "training loss: 31438454750455.457\n",
      "validation loss: 19356860194176.688\n",
      "epoch: 1635\n",
      "training loss: 31438121295114.94\n",
      "validation loss: 19355353609835.832\n",
      "epoch: 1636\n",
      "training loss: 31437788257260.203\n",
      "validation loss: 19353849298234.285\n",
      "epoch: 1637\n",
      "training loss: 31437455635987.31\n",
      "validation loss: 19352347255421.77\n",
      "epoch: 1638\n",
      "training loss: 31437123430394.395\n",
      "validation loss: 19350847477455.668\n",
      "epoch: 1639\n",
      "training loss: 31436791639581.633\n",
      "validation loss: 19349349960400.98\n",
      "epoch: 1640\n",
      "training loss: 31436460262651.258\n",
      "validation loss: 19347854700330.355\n",
      "epoch: 1641\n",
      "training loss: 31436129298707.543\n",
      "validation loss: 19346361693324.008\n",
      "epoch: 1642\n",
      "training loss: 31435798746856.797\n",
      "validation loss: 19344870935469.76\n",
      "epoch: 1643\n",
      "training loss: 31435468606207.355\n",
      "validation loss: 19343382422863.027\n",
      "epoch: 1644\n",
      "training loss: 31435138875869.6\n",
      "validation loss: 19341896151606.754\n",
      "epoch: 1645\n",
      "training loss: 31434809554955.93\n",
      "validation loss: 19340412117811.434\n",
      "epoch: 1646\n",
      "training loss: 31434480642580.742\n",
      "validation loss: 19338930317595.086\n",
      "epoch: 1647\n",
      "training loss: 31434152137860.49\n",
      "validation loss: 19337450747083.246\n",
      "epoch: 1648\n",
      "training loss: 31433824039913.6\n",
      "validation loss: 19335973402408.95\n",
      "epoch: 1649\n",
      "training loss: 31433496347860.53\n",
      "validation loss: 19334498279712.684\n",
      "epoch: 1650\n",
      "training loss: 31433169060823.715\n",
      "validation loss: 19333025375142.42\n",
      "epoch: 1651\n",
      "training loss: 31432842177927.62\n",
      "validation loss: 19331554684853.6\n",
      "epoch: 1652\n",
      "training loss: 31432515698298.66\n",
      "validation loss: 19330086205009.06\n",
      "epoch: 1653\n",
      "training loss: 31432189621065.273\n",
      "validation loss: 19328619931779.06\n",
      "epoch: 1654\n",
      "training loss: 31431863945357.87\n",
      "validation loss: 19327155861341.29\n",
      "epoch: 1655\n",
      "training loss: 31431538670308.824\n",
      "validation loss: 19325693989880.8\n",
      "epoch: 1656\n",
      "training loss: 31431213795052.51\n",
      "validation loss: 19324234313590.03\n",
      "epoch: 1657\n",
      "training loss: 31430889318725.24\n",
      "validation loss: 19322776828668.754\n",
      "epoch: 1658\n",
      "training loss: 31430565240465.32\n",
      "validation loss: 19321321531324.117\n",
      "epoch: 1659\n",
      "training loss: 31430241559412.996\n",
      "validation loss: 19319868417770.562\n",
      "epoch: 1660\n",
      "training loss: 31429918274710.496\n",
      "validation loss: 19318417484229.867\n",
      "epoch: 1661\n",
      "training loss: 31429595385501.96\n",
      "validation loss: 19316968726931.086\n",
      "epoch: 1662\n",
      "training loss: 31429272890933.504\n",
      "validation loss: 19315522142110.562\n",
      "epoch: 1663\n",
      "training loss: 31428950790153.19\n",
      "validation loss: 19314077726011.926\n",
      "epoch: 1664\n",
      "training loss: 31428629082310.984\n",
      "validation loss: 19312635474886.016\n",
      "epoch: 1665\n",
      "training loss: 31428307766558.832\n",
      "validation loss: 19311195384990.93\n",
      "epoch: 1666\n",
      "training loss: 31427986842050.57\n",
      "validation loss: 19309757452591.99\n",
      "epoch: 1667\n",
      "training loss: 31427666307941.984\n",
      "validation loss: 19308321673961.723\n",
      "epoch: 1668\n",
      "training loss: 31427346163390.76\n",
      "validation loss: 19306888045379.836\n",
      "epoch: 1669\n",
      "training loss: 31427026407556.523\n",
      "validation loss: 19305456563133.223\n",
      "epoch: 1670\n",
      "training loss: 31426707039600.79\n",
      "validation loss: 19304027223515.938\n",
      "epoch: 1671\n",
      "training loss: 31426388058686.984\n",
      "validation loss: 19302600022829.184\n",
      "epoch: 1672\n",
      "training loss: 31426069463980.45\n",
      "validation loss: 19301174957381.277\n",
      "epoch: 1673\n",
      "training loss: 31425751254648.418\n",
      "validation loss: 19299752023487.668\n",
      "epoch: 1674\n",
      "training loss: 31425433429860.016\n",
      "validation loss: 19298331217470.914\n",
      "epoch: 1675\n",
      "training loss: 31425115988786.26\n",
      "validation loss: 19296912535660.64\n",
      "epoch: 1676\n",
      "training loss: 31424798930600.055\n",
      "validation loss: 19295495974393.562\n",
      "epoch: 1677\n",
      "training loss: 31424482254476.18\n",
      "validation loss: 19294081530013.453\n",
      "epoch: 1678\n",
      "training loss: 31424165959591.277\n",
      "validation loss: 19292669198871.113\n",
      "epoch: 1679\n",
      "training loss: 31423850045123.906\n",
      "validation loss: 19291258977324.387\n",
      "epoch: 1680\n",
      "training loss: 31423534510254.457\n",
      "validation loss: 19289850861738.133\n",
      "epoch: 1681\n",
      "training loss: 31423219354165.188\n",
      "validation loss: 19288444848484.195\n",
      "epoch: 1682\n",
      "training loss: 31422904576040.223\n",
      "validation loss: 19287040933941.426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1683\n",
      "training loss: 31422590175065.543\n",
      "validation loss: 19285639114495.633\n",
      "epoch: 1684\n",
      "training loss: 31422276150428.98\n",
      "validation loss: 19284239386539.574\n",
      "epoch: 1685\n",
      "training loss: 31421962501320.22\n",
      "validation loss: 19282841746472.97\n",
      "epoch: 1686\n",
      "training loss: 31421649226930.773\n",
      "validation loss: 19281446190702.453\n",
      "epoch: 1687\n",
      "training loss: 31421336326453.984\n",
      "validation loss: 19280052715641.58\n",
      "epoch: 1688\n",
      "training loss: 31421023799085.08\n",
      "validation loss: 19278661317710.79\n",
      "epoch: 1689\n",
      "training loss: 31420711644021.05\n",
      "validation loss: 19277271993337.426\n",
      "epoch: 1690\n",
      "training loss: 31420399860460.758\n",
      "validation loss: 19275884738955.695\n",
      "epoch: 1691\n",
      "training loss: 31420088447604.867\n",
      "validation loss: 19274499551006.65\n",
      "epoch: 1692\n",
      "training loss: 31419777404655.875\n",
      "validation loss: 19273116425938.203\n",
      "epoch: 1693\n",
      "training loss: 31419466730818.08\n",
      "validation loss: 19271735360205.086\n",
      "epoch: 1694\n",
      "training loss: 31419156425297.58\n",
      "validation loss: 19270356350268.836\n",
      "epoch: 1695\n",
      "training loss: 31418846487302.305\n",
      "validation loss: 19268979392597.816\n",
      "epoch: 1696\n",
      "training loss: 31418536916041.97\n",
      "validation loss: 19267604483667.137\n",
      "epoch: 1697\n",
      "training loss: 31418227710728.074\n",
      "validation loss: 19266231619958.72\n",
      "epoch: 1698\n",
      "training loss: 31417918870573.95\n",
      "validation loss: 19264860797961.223\n",
      "epoch: 1699\n",
      "training loss: 31417610394794.656\n",
      "validation loss: 19263492014170.047\n",
      "epoch: 1700\n",
      "training loss: 31417302282607.1\n",
      "validation loss: 19262125265087.32\n",
      "epoch: 1701\n",
      "training loss: 31416994533229.938\n",
      "validation loss: 19260760547221.906\n",
      "epoch: 1702\n",
      "training loss: 31416687145883.58\n",
      "validation loss: 19259397857089.344\n",
      "epoch: 1703\n",
      "training loss: 31416380119790.266\n",
      "validation loss: 19258037191211.89\n",
      "epoch: 1704\n",
      "training loss: 31416073454173.957\n",
      "validation loss: 19256678546118.44\n",
      "epoch: 1705\n",
      "training loss: 31415767148260.39\n",
      "validation loss: 19255321918344.594\n",
      "epoch: 1706\n",
      "training loss: 31415461201277.07\n",
      "validation loss: 19253967304432.55\n",
      "epoch: 1707\n",
      "training loss: 31415155612453.26\n",
      "validation loss: 19252614700931.184\n",
      "epoch: 1708\n",
      "training loss: 31414850381019.957\n",
      "validation loss: 19251264104395.953\n",
      "epoch: 1709\n",
      "training loss: 31414545506209.92\n",
      "validation loss: 19249915511388.95\n",
      "epoch: 1710\n",
      "training loss: 31414240987257.66\n",
      "validation loss: 19248568918478.836\n",
      "epoch: 1711\n",
      "training loss: 31413936823399.4\n",
      "validation loss: 19247224322240.863\n",
      "epoch: 1712\n",
      "training loss: 31413633013873.13\n",
      "validation loss: 19245881719256.848\n",
      "epoch: 1713\n",
      "training loss: 31413329557918.555\n",
      "validation loss: 19244541106115.156\n",
      "epoch: 1714\n",
      "training loss: 31413026454777.113\n",
      "validation loss: 19243202479410.703\n",
      "epoch: 1715\n",
      "training loss: 31412723703691.973\n",
      "validation loss: 19241865835744.895\n",
      "epoch: 1716\n",
      "training loss: 31412421303907.992\n",
      "validation loss: 19240531171725.668\n",
      "epoch: 1717\n",
      "training loss: 31412119254671.79\n",
      "validation loss: 19239198483967.473\n",
      "epoch: 1718\n",
      "training loss: 31411817555231.668\n",
      "validation loss: 19237867769091.22\n",
      "epoch: 1719\n",
      "training loss: 31411516204837.652\n",
      "validation loss: 19236539023724.3\n",
      "epoch: 1720\n",
      "training loss: 31411215202741.465\n",
      "validation loss: 19235212244500.56\n",
      "epoch: 1721\n",
      "training loss: 31410914548196.516\n",
      "validation loss: 19233887428060.28\n",
      "epoch: 1722\n",
      "training loss: 31410614240457.95\n",
      "validation loss: 19232564571050.188\n",
      "epoch: 1723\n",
      "training loss: 31410314278782.56\n",
      "validation loss: 19231243670123.414\n",
      "epoch: 1724\n",
      "training loss: 31410014662428.86\n",
      "validation loss: 19229924721939.496\n",
      "epoch: 1725\n",
      "training loss: 31409715390657.02\n",
      "validation loss: 19228607723164.37\n",
      "epoch: 1726\n",
      "training loss: 31409416462728.94\n",
      "validation loss: 19227292670470.34\n",
      "epoch: 1727\n",
      "training loss: 31409117877908.14\n",
      "validation loss: 19225979560536.08\n",
      "epoch: 1728\n",
      "training loss: 31408819635459.848\n",
      "validation loss: 19224668390046.6\n",
      "epoch: 1729\n",
      "training loss: 31408521734650.94\n",
      "validation loss: 19223359155693.277\n",
      "epoch: 1730\n",
      "training loss: 31408224174750.01\n",
      "validation loss: 19222051854173.777\n",
      "epoch: 1731\n",
      "training loss: 31407926955027.24\n",
      "validation loss: 19220746482192.11\n",
      "epoch: 1732\n",
      "training loss: 31407630074754.51\n",
      "validation loss: 19219443036458.562\n",
      "epoch: 1733\n",
      "training loss: 31407333533205.375\n",
      "validation loss: 19218141513689.72\n",
      "epoch: 1734\n",
      "training loss: 31407037329654.99\n",
      "validation loss: 19216841910608.43\n",
      "epoch: 1735\n",
      "training loss: 31406741463380.19\n",
      "validation loss: 19215544223943.805\n",
      "epoch: 1736\n",
      "training loss: 31406445933659.45\n",
      "validation loss: 19214248450431.2\n",
      "epoch: 1737\n",
      "training loss: 31406150739772.89\n",
      "validation loss: 19212954586812.215\n",
      "epoch: 1738\n",
      "training loss: 31405855881002.24\n",
      "validation loss: 19211662629834.656\n",
      "epoch: 1739\n",
      "training loss: 31405561356630.88\n",
      "validation loss: 19210372576252.547\n",
      "epoch: 1740\n",
      "training loss: 31405267165943.83\n",
      "validation loss: 19209084422826.117\n",
      "epoch: 1741\n",
      "training loss: 31404973308227.703\n",
      "validation loss: 19207798166321.74\n",
      "epoch: 1742\n",
      "training loss: 31404679782770.773\n",
      "validation loss: 19206513803512.008\n",
      "epoch: 1743\n",
      "training loss: 31404386588862.895\n",
      "validation loss: 19205231331175.637\n",
      "epoch: 1744\n",
      "training loss: 31404093725795.543\n",
      "validation loss: 19203950746097.484\n",
      "epoch: 1745\n",
      "training loss: 31403801192861.83\n",
      "validation loss: 19202672045068.57\n",
      "epoch: 1746\n",
      "training loss: 31403508989356.44\n",
      "validation loss: 19201395224886.004\n",
      "epoch: 1747\n",
      "training loss: 31403217114575.68\n",
      "validation loss: 19200120282353.027\n",
      "epoch: 1748\n",
      "training loss: 31402925567817.45\n",
      "validation loss: 19198847214278.94\n",
      "epoch: 1749\n",
      "training loss: 31402634348381.227\n",
      "validation loss: 19197576017479.152\n",
      "epoch: 1750\n",
      "training loss: 31402343455568.117\n",
      "validation loss: 19196306688775.14\n",
      "epoch: 1751\n",
      "training loss: 31402052888680.785\n",
      "validation loss: 19195039224994.41\n",
      "epoch: 1752\n",
      "training loss: 31401762647023.484\n",
      "validation loss: 19193773622970.562\n",
      "epoch: 1753\n",
      "training loss: 31401472729902.055\n",
      "validation loss: 19192509879543.176\n",
      "epoch: 1754\n",
      "training loss: 31401183136623.918\n",
      "validation loss: 19191247991557.875\n",
      "epoch: 1755\n",
      "training loss: 31400893866498.055\n",
      "validation loss: 19189987955866.285\n",
      "epoch: 1756\n",
      "training loss: 31400604918835.023\n",
      "validation loss: 19188729769326.03\n",
      "epoch: 1757\n",
      "training loss: 31400316292946.945\n",
      "validation loss: 19187473428800.703\n",
      "epoch: 1758\n",
      "training loss: 31400027988147.496\n",
      "validation loss: 19186218931159.88\n",
      "epoch: 1759\n",
      "training loss: 31399740003751.934\n",
      "validation loss: 19184966273279.086\n",
      "epoch: 1760\n",
      "training loss: 31399452339077.06\n",
      "validation loss: 19183715452039.793\n",
      "epoch: 1761\n",
      "training loss: 31399164993441.207\n",
      "validation loss: 19182466464329.402\n",
      "epoch: 1762\n",
      "training loss: 31398877966164.305\n",
      "validation loss: 19181219307041.246\n",
      "epoch: 1763\n",
      "training loss: 31398591256567.76\n",
      "validation loss: 19179973977074.55\n",
      "epoch: 1764\n",
      "training loss: 31398304863974.594\n",
      "validation loss: 19178730471334.445\n",
      "epoch: 1765\n",
      "training loss: 31398018787709.31\n",
      "validation loss: 19177488786731.945\n",
      "epoch: 1766\n",
      "training loss: 31397733027097.965\n",
      "validation loss: 19176248920183.92\n",
      "epoch: 1767\n",
      "training loss: 31397447581468.145\n",
      "validation loss: 19175010868613.133\n",
      "epoch: 1768\n",
      "training loss: 31397162450148.973\n",
      "validation loss: 19173774628948.152\n",
      "epoch: 1769\n",
      "training loss: 31396877632471.08\n",
      "validation loss: 19172540198123.418\n",
      "epoch: 1770\n",
      "training loss: 31396593127766.62\n",
      "validation loss: 19171307573079.17\n",
      "epoch: 1771\n",
      "training loss: 31396308935369.29\n",
      "validation loss: 19170076750761.477\n",
      "epoch: 1772\n",
      "training loss: 31396025054614.246\n",
      "validation loss: 19168847728122.19\n",
      "epoch: 1773\n",
      "training loss: 31395741484838.2\n",
      "validation loss: 19167620502118.957\n",
      "epoch: 1774\n",
      "training loss: 31395458225379.35\n",
      "validation loss: 19166395069715.203\n",
      "epoch: 1775\n",
      "training loss: 31395175275577.414\n",
      "validation loss: 19165171427880.113\n",
      "epoch: 1776\n",
      "training loss: 31394892634773.594\n",
      "validation loss: 19163949573588.625\n",
      "epoch: 1777\n",
      "training loss: 31394610302310.58\n",
      "validation loss: 19162729503821.406\n",
      "epoch: 1778\n",
      "training loss: 31394328277532.58\n",
      "validation loss: 19161511215564.87\n",
      "epoch: 1779\n",
      "training loss: 31394046559785.26\n",
      "validation loss: 19160294705811.133\n",
      "epoch: 1780\n",
      "training loss: 31393765148415.8\n",
      "validation loss: 19159079971558.027\n",
      "epoch: 1781\n",
      "training loss: 31393484042772.85\n",
      "validation loss: 19157867009809.07\n",
      "epoch: 1782\n",
      "training loss: 31393203242206.543\n",
      "validation loss: 19156655817573.45\n",
      "epoch: 1783\n",
      "training loss: 31392922746068.477\n",
      "validation loss: 19155446391866.05\n",
      "epoch: 1784\n",
      "training loss: 31392642553711.72\n",
      "validation loss: 19154238729707.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1785\n",
      "training loss: 31392362664490.84\n",
      "validation loss: 19153032828123.652\n",
      "epoch: 1786\n",
      "training loss: 31392083077761.83\n",
      "validation loss: 19151828684146.625\n",
      "epoch: 1787\n",
      "training loss: 31391803792882.17\n",
      "validation loss: 19150626294813.754\n",
      "epoch: 1788\n",
      "training loss: 31391524809210.79\n",
      "validation loss: 19149425657168.074\n",
      "epoch: 1789\n",
      "training loss: 31391246126108.074\n",
      "validation loss: 19148226768258.23\n",
      "epoch: 1790\n",
      "training loss: 31390967742935.867\n",
      "validation loss: 19147029625138.445\n",
      "epoch: 1791\n",
      "training loss: 31390689659057.45\n",
      "validation loss: 19145834224868.535\n",
      "epoch: 1792\n",
      "training loss: 31390411873837.56\n",
      "validation loss: 19144640564513.86\n",
      "epoch: 1793\n",
      "training loss: 31390134386642.37\n",
      "validation loss: 19143448641145.35\n",
      "epoch: 1794\n",
      "training loss: 31389857196839.492\n",
      "validation loss: 19142258451839.477\n",
      "epoch: 1795\n",
      "training loss: 31389580303797.99\n",
      "validation loss: 19141069993678.242\n",
      "epoch: 1796\n",
      "training loss: 31389303706888.32\n",
      "validation loss: 19139883263749.156\n",
      "epoch: 1797\n",
      "training loss: 31389027405482.406\n",
      "validation loss: 19138698259145.242\n",
      "epoch: 1798\n",
      "training loss: 31388751398953.594\n",
      "validation loss: 19137514976965.04\n",
      "epoch: 1799\n",
      "training loss: 31388475686676.617\n",
      "validation loss: 19136333414312.547\n",
      "epoch: 1800\n",
      "training loss: 31388200268027.664\n",
      "validation loss: 19135153568297.242\n",
      "epoch: 1801\n",
      "training loss: 31387925142384.332\n",
      "validation loss: 19133975436034.086\n",
      "epoch: 1802\n",
      "training loss: 31387650309125.625\n",
      "validation loss: 19132799014643.477\n",
      "epoch: 1803\n",
      "training loss: 31387375767631.97\n",
      "validation loss: 19131624301251.242\n",
      "epoch: 1804\n",
      "training loss: 31387101517285.156\n",
      "validation loss: 19130451292988.656\n",
      "epoch: 1805\n",
      "training loss: 31386827557468.426\n",
      "validation loss: 19129279986992.406\n",
      "epoch: 1806\n",
      "training loss: 31386553887566.406\n",
      "validation loss: 19128110380404.594\n",
      "epoch: 1807\n",
      "training loss: 31386280506965.113\n",
      "validation loss: 19126942470372.695\n",
      "epoch: 1808\n",
      "training loss: 31386007415051.957\n",
      "validation loss: 19125776254049.605\n",
      "epoch: 1809\n",
      "training loss: 31385734611215.754\n",
      "validation loss: 19124611728593.562\n",
      "epoch: 1810\n",
      "training loss: 31385462094846.684\n",
      "validation loss: 19123448891168.18\n",
      "epoch: 1811\n",
      "training loss: 31385189865336.32\n",
      "validation loss: 19122287738942.42\n",
      "epoch: 1812\n",
      "training loss: 31384917922077.62\n",
      "validation loss: 19121128269090.598\n",
      "epoch: 1813\n",
      "training loss: 31384646264464.918\n",
      "validation loss: 19119970478792.332\n",
      "epoch: 1814\n",
      "training loss: 31384374891893.914\n",
      "validation loss: 19118814365232.59\n",
      "epoch: 1815\n",
      "training loss: 31384103803761.695\n",
      "validation loss: 19117659925601.62\n",
      "epoch: 1816\n",
      "training loss: 31383832999466.71\n",
      "validation loss: 19116507157094.98\n",
      "epoch: 1817\n",
      "training loss: 31383562478408.75\n",
      "validation loss: 19115356056913.523\n",
      "epoch: 1818\n",
      "training loss: 31383292239989.008\n",
      "validation loss: 19114206622263.35\n",
      "epoch: 1819\n",
      "training loss: 31383022283610.008\n",
      "validation loss: 19113058850355.844\n",
      "epoch: 1820\n",
      "training loss: 31382752608675.63\n",
      "validation loss: 19111912738407.633\n",
      "epoch: 1821\n",
      "training loss: 31382483214591.12\n",
      "validation loss: 19110768283640.6\n",
      "epoch: 1822\n",
      "training loss: 31382214100763.066\n",
      "validation loss: 19109625483281.84\n",
      "epoch: 1823\n",
      "training loss: 31381945266599.395\n",
      "validation loss: 19108484334563.645\n",
      "epoch: 1824\n",
      "training loss: 31381676711509.402\n",
      "validation loss: 19107344834723.566\n",
      "epoch: 1825\n",
      "training loss: 31381408434903.68\n",
      "validation loss: 19106206981004.297\n",
      "epoch: 1826\n",
      "training loss: 31381140436194.207\n",
      "validation loss: 19105070770653.74\n",
      "epoch: 1827\n",
      "training loss: 31380872714794.254\n",
      "validation loss: 19103936200924.94\n",
      "epoch: 1828\n",
      "training loss: 31380605270118.445\n",
      "validation loss: 19102803269076.113\n",
      "epoch: 1829\n",
      "training loss: 31380338101582.734\n",
      "validation loss: 19101671972370.59\n",
      "epoch: 1830\n",
      "training loss: 31380071208604.375\n",
      "validation loss: 19100542308076.81\n",
      "epoch: 1831\n",
      "training loss: 31379804590601.965\n",
      "validation loss: 19099414273468.312\n",
      "epoch: 1832\n",
      "training loss: 31379538246995.41\n",
      "validation loss: 19098287865823.645\n",
      "epoch: 1833\n",
      "training loss: 31379272177205.93\n",
      "validation loss: 19097163082426.367\n",
      "epoch: 1834\n",
      "training loss: 31379006380656.047\n",
      "validation loss: 19096039920564.914\n",
      "epoch: 1835\n",
      "training loss: 31378740856769.58\n",
      "validation loss: 19094918377532.414\n",
      "epoch: 1836\n",
      "training loss: 31378475604971.63\n",
      "validation loss: 19093798450626.367\n",
      "epoch: 1837\n",
      "training loss: 31378210624688.535\n",
      "validation loss: 19092680137147.832\n",
      "epoch: 1838\n",
      "training loss: 31377945915347.758\n",
      "validation loss: 19091563434399.273\n",
      "epoch: 1839\n",
      "training loss: 31377681476377.418\n",
      "validation loss: 19090448339677.434\n",
      "epoch: 1840\n",
      "training loss: 31377417307203.332\n",
      "validation loss: 19089334850239.184\n",
      "epoch: 1841\n",
      "training loss: 31377153407193.95\n",
      "validation loss: 19088222962958.703\n",
      "epoch: 1842\n",
      "training loss: 31376887812115.83\n",
      "validation loss: 19087112522922.043\n",
      "epoch: 1843\n",
      "training loss: 31380400616103.47\n",
      "validation loss: 19068330385334.75\n",
      "epoch: 1844\n",
      "training loss: 31380049987678.2\n",
      "validation loss: 19067911681846.145\n",
      "epoch: 1845\n",
      "training loss: 31379703492893.332\n",
      "validation loss: 19067479456714.34\n",
      "epoch: 1846\n",
      "training loss: 31379360953078.97\n",
      "validation loss: 19067033989461.176\n",
      "epoch: 1847\n",
      "training loss: 31379022197604.97\n",
      "validation loss: 19066575559753.035\n",
      "epoch: 1848\n",
      "training loss: 31378687063813.445\n",
      "validation loss: 19066104443919.9\n",
      "epoch: 1849\n",
      "training loss: 31378355396642.086\n",
      "validation loss: 19065620914874.58\n",
      "epoch: 1850\n",
      "training loss: 31378027048265.43\n",
      "validation loss: 19065125242041.51\n",
      "epoch: 1851\n",
      "training loss: 31377701877753.08\n",
      "validation loss: 19064617691294.453\n",
      "epoch: 1852\n",
      "training loss: 31377379750744.18\n",
      "validation loss: 19064098524902.605\n",
      "epoch: 1853\n",
      "training loss: 31377060539137.324\n",
      "validation loss: 19063568001484.6\n",
      "epoch: 1854\n",
      "training loss: 31376744120795.18\n",
      "validation loss: 19063026375969.8\n",
      "epoch: 1855\n",
      "training loss: 31376430379263.06\n",
      "validation loss: 19062473899566.555\n",
      "epoch: 1856\n",
      "training loss: 31376119203500.96\n",
      "validation loss: 19061910819736.83\n",
      "epoch: 1857\n",
      "training loss: 31375810487628.145\n",
      "validation loss: 19061337380176.887\n",
      "epoch: 1858\n",
      "training loss: 31375504130679.984\n",
      "validation loss: 19060753820803.6\n",
      "epoch: 1859\n",
      "training loss: 31375200036376.22\n",
      "validation loss: 19060160377745.953\n",
      "epoch: 1860\n",
      "training loss: 31374898112900.246\n",
      "validation loss: 19059557283341.465\n",
      "epoch: 1861\n",
      "training loss: 31374598272688.887\n",
      "validation loss: 19058944766137.137\n",
      "epoch: 1862\n",
      "training loss: 31374300432232.043\n",
      "validation loss: 19058323050894.625\n",
      "epoch: 1863\n",
      "training loss: 31374004511881.918\n",
      "validation loss: 19057692358599.305\n",
      "epoch: 1864\n",
      "training loss: 31373710435671.26\n",
      "validation loss: 19057052906473.023\n",
      "epoch: 1865\n",
      "training loss: 31373418131140.137\n",
      "validation loss: 19056404907990.16\n",
      "epoch: 1866\n",
      "training loss: 31373127529171.04\n",
      "validation loss: 19055748572896.824\n",
      "epoch: 1867\n",
      "training loss: 31372838563831.684\n",
      "validation loss: 19055084107232.938\n",
      "epoch: 1868\n",
      "training loss: 31372551172225.316\n",
      "validation loss: 19054411713356.914\n",
      "epoch: 1869\n",
      "training loss: 31372265294348.08\n",
      "validation loss: 19053731589972.805\n",
      "epoch: 1870\n",
      "training loss: 31371980872953.125\n",
      "validation loss: 19053043932159.664\n",
      "epoch: 1871\n",
      "training loss: 31371697853421.16\n",
      "validation loss: 19052348931402.953\n",
      "epoch: 1872\n",
      "training loss: 31371416183637.14\n",
      "validation loss: 19051646775627.81\n",
      "epoch: 1873\n",
      "training loss: 31371135813872.74\n",
      "validation loss: 19050937649233.97\n",
      "epoch: 1874\n",
      "training loss: 31370856696674.46\n",
      "validation loss: 19050221733132.305\n",
      "epoch: 1875\n",
      "training loss: 31370578786756.91\n",
      "validation loss: 19049499204782.65\n",
      "epoch: 1876\n",
      "training loss: 31370302040901.273\n",
      "validation loss: 19048770238232.96\n",
      "epoch: 1877\n",
      "training loss: 31370026417858.465\n",
      "validation loss: 19048035004159.523\n",
      "epoch: 1878\n",
      "training loss: 31369751878256.91\n",
      "validation loss: 19047293669908.168\n",
      "epoch: 1879\n",
      "training loss: 31369478384514.65\n",
      "validation loss: 19046546399536.395\n",
      "epoch: 1880\n",
      "training loss: 31369205900755.652\n",
      "validation loss: 19045793353856.18\n",
      "epoch: 1881\n",
      "training loss: 31368934392730.023\n",
      "validation loss: 19045034690477.58\n",
      "epoch: 1882\n",
      "training loss: 31368663827737.996\n",
      "validation loss: 19044270563852.727\n",
      "epoch: 1883\n",
      "training loss: 31368394174557.523\n",
      "validation loss: 19043501125320.46\n",
      "epoch: 1884\n",
      "training loss: 31368125403375.277\n",
      "validation loss: 19042726523151.28\n",
      "epoch: 1885\n",
      "training loss: 31367857485720.895\n",
      "validation loss: 19041946902592.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1886\n",
      "training loss: 31367590394404.34\n",
      "validation loss: 19041162405914.258\n",
      "epoch: 1887\n",
      "training loss: 31367324103456.227\n",
      "validation loss: 19040373172454.254\n",
      "epoch: 1888\n",
      "training loss: 31367058588070.9\n",
      "validation loss: 19039579338664.496\n",
      "epoch: 1889\n",
      "training loss: 31366793824552.277\n",
      "validation loss: 19038781038156.684\n",
      "epoch: 1890\n",
      "training loss: 31366529790262.22\n",
      "validation loss: 19037978401748.145\n",
      "epoch: 1891\n",
      "training loss: 31366266463571.25\n",
      "validation loss: 19037171557507.625\n",
      "epoch: 1892\n",
      "training loss: 31366003823811.746\n",
      "validation loss: 19036360630800.973\n",
      "epoch: 1893\n",
      "training loss: 31365741851233.188\n",
      "validation loss: 19035545744336.668\n",
      "epoch: 1894\n",
      "training loss: 31365480526959.613\n",
      "validation loss: 19034727018211.184\n",
      "epoch: 1895\n",
      "training loss: 31365219832949.016\n",
      "validation loss: 19033904569954.09\n",
      "epoch: 1896\n",
      "training loss: 31364959751954.69\n",
      "validation loss: 19033078514572.926\n",
      "epoch: 1897\n",
      "training loss: 31364700267488.363\n",
      "validation loss: 19032248964597.742\n",
      "epoch: 1898\n",
      "training loss: 31364441363785.074\n",
      "validation loss: 19031416030125.367\n",
      "epoch: 1899\n",
      "training loss: 31364183025769.746\n",
      "validation loss: 19030579818863.25\n",
      "epoch: 1900\n",
      "training loss: 31363925239025.21\n",
      "validation loss: 19029740436173.02\n",
      "epoch: 1901\n",
      "training loss: 31363667989761.848\n",
      "validation loss: 19028897985113.566\n",
      "epoch: 1902\n",
      "training loss: 31363411264788.652\n",
      "validation loss: 19028052566483.73\n",
      "epoch: 1903\n",
      "training loss: 31363155051485.527\n",
      "validation loss: 19027204278864.582\n",
      "epoch: 1904\n",
      "training loss: 31362899337777.04\n",
      "validation loss: 19026353218661.164\n",
      "epoch: 1905\n",
      "training loss: 31362644112107.32\n",
      "validation loss: 19025499480143.883\n",
      "epoch: 1906\n",
      "training loss: 31362389363416.113\n",
      "validation loss: 19024643155489.293\n",
      "epoch: 1907\n",
      "training loss: 31362135081116.027\n",
      "validation loss: 19023784334820.492\n",
      "epoch: 1908\n",
      "training loss: 31361881255070.83\n",
      "validation loss: 19022923106246.92\n",
      "epoch: 1909\n",
      "training loss: 31361627875574.645\n",
      "validation loss: 19022059555903.688\n",
      "epoch: 1910\n",
      "training loss: 31361374933332.36\n",
      "validation loss: 19021193767990.383\n",
      "epoch: 1911\n",
      "training loss: 31361122419440.676\n",
      "validation loss: 19020325824809.332\n",
      "epoch: 1912\n",
      "training loss: 31360870325370.25\n",
      "validation loss: 19019455806803.324\n",
      "epoch: 1913\n",
      "training loss: 31360618642948.574\n",
      "validation loss: 19018583792592.793\n",
      "epoch: 1914\n",
      "training loss: 31360367364343.69\n",
      "validation loss: 19017709859012.465\n",
      "epoch: 1915\n",
      "training loss: 31360116482048.637\n",
      "validation loss: 19016834081147.418\n",
      "epoch: 1916\n",
      "training loss: 31359865988866.633\n",
      "validation loss: 19015956532368.66\n",
      "epoch: 1917\n",
      "training loss: 31359615877896.98\n",
      "validation loss: 19015077284368.086\n",
      "epoch: 1918\n",
      "training loss: 31359366142521.57\n",
      "validation loss: 19014196407192.895\n",
      "epoch: 1919\n",
      "training loss: 31359116776392.06\n",
      "validation loss: 19013313969279.5\n",
      "epoch: 1920\n",
      "training loss: 31358867773417.64\n",
      "validation loss: 19012430037486.83\n",
      "epoch: 1921\n",
      "training loss: 31358619127753.36\n",
      "validation loss: 19011544677129.08\n",
      "epoch: 1922\n",
      "training loss: 31358370833788.977\n",
      "validation loss: 19010657952007.99\n",
      "epoch: 1923\n",
      "training loss: 31358122886138.41\n",
      "validation loss: 19009769924444.453\n",
      "epoch: 1924\n",
      "training loss: 31357875279629.523\n",
      "validation loss: 19008880655309.668\n",
      "epoch: 1925\n",
      "training loss: 31357628009294.586\n",
      "validation loss: 19007990204055.734\n",
      "epoch: 1926\n",
      "training loss: 31357381070360.99\n",
      "validation loss: 19007098628745.67\n",
      "epoch: 1927\n",
      "training loss: 31357134458242.51\n",
      "validation loss: 19006205986082.945\n",
      "epoch: 1928\n",
      "training loss: 31356888168530.957\n",
      "validation loss: 19005312331440.473\n",
      "epoch: 1929\n",
      "training loss: 31356642196988.15\n",
      "validation loss: 19004417718889.008\n",
      "epoch: 1930\n",
      "training loss: 31356396539538.344\n",
      "validation loss: 19003522201225.14\n",
      "epoch: 1931\n",
      "training loss: 31356151192260.94\n",
      "validation loss: 19002625829998.656\n",
      "epoch: 1932\n",
      "training loss: 31355906151383.625\n",
      "validation loss: 19001728655539.516\n",
      "epoch: 1933\n",
      "training loss: 31355661413275.664\n",
      "validation loss: 19000830726984.156\n",
      "epoch: 1934\n",
      "training loss: 31355416974441.707\n",
      "validation loss: 18999932092301.473\n",
      "epoch: 1935\n",
      "training loss: 31355172831515.715\n",
      "validation loss: 18999032798318.184\n",
      "epoch: 1936\n",
      "training loss: 31354928981255.23\n",
      "validation loss: 18998132890743.75\n",
      "epoch: 1937\n",
      "training loss: 31354685420535.957\n",
      "validation loss: 18997232414194.82\n",
      "epoch: 1938\n",
      "training loss: 31354442146346.504\n",
      "validation loss: 18996331412219.184\n",
      "epoch: 1939\n",
      "training loss: 31354199155783.383\n",
      "validation loss: 18995429927319.24\n",
      "epoch: 1940\n",
      "training loss: 31353956446046.34\n",
      "validation loss: 18994528000975.07\n",
      "epoch: 1941\n",
      "training loss: 31353714014433.734\n",
      "validation loss: 18993625673666.957\n",
      "epoch: 1942\n",
      "training loss: 31353471858338.297\n",
      "validation loss: 18992722984897.547\n",
      "epoch: 1943\n",
      "training loss: 31353229975242.95\n",
      "validation loss: 18991819973213.5\n",
      "epoch: 1944\n",
      "training loss: 31352988362716.87\n",
      "validation loss: 18990916676226.727\n",
      "epoch: 1945\n",
      "training loss: 31352747018411.793\n",
      "validation loss: 18990013130635.223\n",
      "epoch: 1946\n",
      "training loss: 31352505940058.336\n",
      "validation loss: 18989109372243.406\n",
      "epoch: 1947\n",
      "training loss: 31352265125462.664\n",
      "validation loss: 18988205435982.15\n",
      "epoch: 1948\n",
      "training loss: 31352024572503.15\n",
      "validation loss: 18987301355928.285\n",
      "epoch: 1949\n",
      "training loss: 31351784279127.32\n",
      "validation loss: 18986397165323.79\n",
      "epoch: 1950\n",
      "training loss: 31351544243348.844\n",
      "validation loss: 18985492896594.53\n",
      "epoch: 1951\n",
      "training loss: 31351304463244.69\n",
      "validation loss: 18984588581368.625\n",
      "epoch: 1952\n",
      "training loss: 31351064936952.44\n",
      "validation loss: 18983684250494.473\n",
      "epoch: 1953\n",
      "training loss: 31350825662667.668\n",
      "validation loss: 18982779934058.285\n",
      "epoch: 1954\n",
      "training loss: 31350586638641.51\n",
      "validation loss: 18981875661401.387\n",
      "epoch: 1955\n",
      "training loss: 31350347863178.273\n",
      "validation loss: 18980971461137.062\n",
      "epoch: 1956\n",
      "training loss: 31350109334633.18\n",
      "validation loss: 18980067361167.074\n",
      "epoch: 1957\n",
      "training loss: 31349871051410.223\n",
      "validation loss: 18979163388697.844\n",
      "epoch: 1958\n",
      "training loss: 31349633011960.15\n",
      "validation loss: 18978259570256.246\n",
      "epoch: 1959\n",
      "training loss: 31349395214778.42\n",
      "validation loss: 18977355931705.105\n",
      "epoch: 1960\n",
      "training loss: 31349157658403.41\n",
      "validation loss: 18976452498258.35\n",
      "epoch: 1961\n",
      "training loss: 31348920341414.566\n",
      "validation loss: 18975549294495.844\n",
      "epoch: 1962\n",
      "training loss: 31348683262430.742\n",
      "validation loss: 18974646344377.86\n",
      "epoch: 1963\n",
      "training loss: 31348446420108.523\n",
      "validation loss: 18973743671259.297\n",
      "epoch: 1964\n",
      "training loss: 31348209813140.695\n",
      "validation loss: 18972841297903.562\n",
      "epoch: 1965\n",
      "training loss: 31347973440254.73\n",
      "validation loss: 18971939246496.145\n",
      "epoch: 1966\n",
      "training loss: 31347737300211.38\n",
      "validation loss: 18971037538657.902\n",
      "epoch: 1967\n",
      "training loss: 31347501391803.33\n",
      "validation loss: 18970136195458.05\n",
      "epoch: 1968\n",
      "training loss: 31347265713853.85\n",
      "validation loss: 18969235237426.9\n",
      "epoch: 1969\n",
      "training loss: 31347030265215.598\n",
      "validation loss: 18968334684568.25\n",
      "epoch: 1970\n",
      "training loss: 31346795044769.4\n",
      "validation loss: 18967434556371.58\n",
      "epoch: 1971\n",
      "training loss: 31346560051423.133\n",
      "validation loss: 18966534871823.914\n",
      "epoch: 1972\n",
      "training loss: 31346325284110.65\n",
      "validation loss: 18965635649421.457\n",
      "epoch: 1973\n",
      "training loss: 31346090741790.703\n",
      "validation loss: 18964736907180.984\n",
      "epoch: 1974\n",
      "training loss: 31345856423445.992\n",
      "validation loss: 18963838662650.934\n",
      "epoch: 1975\n",
      "training loss: 31345622328082.2\n",
      "validation loss: 18962940932922.277\n",
      "epoch: 1976\n",
      "training loss: 31345388454727.08\n",
      "validation loss: 18962043734639.18\n",
      "epoch: 1977\n",
      "training loss: 31345154802429.574\n",
      "validation loss: 18961147084009.344\n",
      "epoch: 1978\n",
      "training loss: 31344921370259.06\n",
      "validation loss: 18960250996814.234\n",
      "epoch: 1979\n",
      "training loss: 31344688157304.438\n",
      "validation loss: 18959355488418.95\n",
      "epoch: 1980\n",
      "training loss: 31344455162673.477\n",
      "validation loss: 18958460573781.95\n",
      "epoch: 1981\n",
      "training loss: 31344222385492.023\n",
      "validation loss: 18957566267464.574\n",
      "epoch: 1982\n",
      "training loss: 31343989824903.344\n",
      "validation loss: 18956672583640.293\n",
      "epoch: 1983\n",
      "training loss: 31343757480067.457\n",
      "validation loss: 18955779536103.785\n",
      "epoch: 1984\n",
      "training loss: 31343525350160.445\n",
      "validation loss: 18954887138279.805\n",
      "epoch: 1985\n",
      "training loss: 31343293434373.945\n",
      "validation loss: 18953995403231.855\n",
      "epoch: 1986\n",
      "training loss: 31343061731914.46\n",
      "validation loss: 18953104343670.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1987\n",
      "training loss: 31342830242002.875\n",
      "validation loss: 18952213971962.348\n",
      "epoch: 1988\n",
      "training loss: 31342598963873.9\n",
      "validation loss: 18951324300136.76\n",
      "epoch: 1989\n",
      "training loss: 31342367896775.555\n",
      "validation loss: 18950435339895.125\n",
      "epoch: 1990\n",
      "training loss: 31342137039968.668\n",
      "validation loss: 18949547102617.91\n",
      "epoch: 1991\n",
      "training loss: 31341906392726.45\n",
      "validation loss: 18948659599372.35\n",
      "epoch: 1992\n",
      "training loss: 31341675954333.992\n",
      "validation loss: 18947772840919.82\n",
      "epoch: 1993\n",
      "training loss: 31341445724087.883\n",
      "validation loss: 18946886837723.027\n",
      "epoch: 1994\n",
      "training loss: 31341215701295.766\n",
      "validation loss: 18946001599953.11\n",
      "epoch: 1995\n",
      "training loss: 31340985885275.95\n",
      "validation loss: 18945117137496.453\n",
      "epoch: 1996\n",
      "training loss: 31340756275357.05\n",
      "validation loss: 18944233459961.484\n",
      "epoch: 1997\n",
      "training loss: 31340526870877.617\n",
      "validation loss: 18943350576685.19\n",
      "epoch: 1998\n",
      "training loss: 31340297671185.746\n",
      "validation loss: 18942468496739.58\n",
      "epoch: 1999\n",
      "training loss: 31340068675638.84\n",
      "validation loss: 18941587228937.93\n",
      "epoch: 0\n",
      "training loss: 37087380608677.13\n",
      "validation loss: 33593954068271.49\n",
      "epoch: 1\n",
      "training loss: 36975311419349.48\n",
      "validation loss: 33332105478230.508\n",
      "epoch: 2\n",
      "training loss: 36887137712248.19\n",
      "validation loss: 33156261605376.855\n",
      "epoch: 3\n",
      "training loss: 36809417803046.61\n",
      "validation loss: 33112426949795.883\n",
      "epoch: 4\n",
      "training loss: 36670742827676.98\n",
      "validation loss: 32969931938714.242\n",
      "epoch: 5\n",
      "training loss: 36564857153078.31\n",
      "validation loss: 32718982710846.582\n",
      "epoch: 6\n",
      "training loss: 36475305652706.56\n",
      "validation loss: 32869572772433.566\n",
      "epoch: 7\n",
      "training loss: 36386768690078.76\n",
      "validation loss: 32528767087199.355\n",
      "epoch: 8\n",
      "training loss: 36263671461227.21\n",
      "validation loss: 32227067284792.625\n",
      "epoch: 9\n",
      "training loss: 36139112264088.53\n",
      "validation loss: 32010634928903.2\n",
      "epoch: 10\n",
      "training loss: 36047530587170.086\n",
      "validation loss: 31896026399876.844\n",
      "epoch: 11\n",
      "training loss: 35921470847547.164\n",
      "validation loss: 31888687580663.594\n",
      "epoch: 12\n",
      "training loss: 35844358180455.33\n",
      "validation loss: 31866635807265.305\n",
      "epoch: 13\n",
      "training loss: 35813837043301.67\n",
      "validation loss: 31837730589637.914\n",
      "epoch: 14\n",
      "training loss: 35780849316566.92\n",
      "validation loss: 31742174330428.56\n",
      "epoch: 15\n",
      "training loss: 35653304162710.08\n",
      "validation loss: 31392188525635.855\n",
      "epoch: 16\n",
      "training loss: 35552945304168.805\n",
      "validation loss: 31783026470933.35\n",
      "epoch: 17\n",
      "training loss: 35455484104424.336\n",
      "validation loss: 31313555210591.96\n",
      "epoch: 18\n",
      "training loss: 35349052469416.7\n",
      "validation loss: 30834582027025.2\n",
      "epoch: 19\n",
      "training loss: 35243754893177.164\n",
      "validation loss: 30433968078586.94\n",
      "epoch: 20\n",
      "training loss: 35217755039690.41\n",
      "validation loss: 30565516956053.17\n",
      "epoch: 21\n",
      "training loss: 35100904392757.12\n",
      "validation loss: 30170768746196.465\n",
      "epoch: 22\n",
      "training loss: 34993818774299.867\n",
      "validation loss: 29913491863526.562\n",
      "epoch: 23\n",
      "training loss: 34909060293343.324\n",
      "validation loss: 29755512561847.387\n",
      "epoch: 24\n",
      "training loss: 34802737146517.395\n",
      "validation loss: 29647563817324.13\n",
      "epoch: 25\n",
      "training loss: 34757604389196.902\n",
      "validation loss: 29645384582422.08\n",
      "epoch: 26\n",
      "training loss: 34654549682226.656\n",
      "validation loss: 29478376412910.78\n",
      "epoch: 27\n",
      "training loss: 34583502663894.07\n",
      "validation loss: 29336448459267.066\n",
      "epoch: 28\n",
      "training loss: 34507751015559.77\n",
      "validation loss: 29199729903649.3\n",
      "epoch: 29\n",
      "training loss: 34433824206070.586\n",
      "validation loss: 29065369981424.562\n",
      "epoch: 30\n",
      "training loss: 34361640419732.793\n",
      "validation loss: 28933388269413.945\n",
      "epoch: 31\n",
      "training loss: 34291150995134.867\n",
      "validation loss: 28803717112302.38\n",
      "epoch: 32\n",
      "training loss: 34222315365980.145\n",
      "validation loss: 28676312363166.758\n",
      "epoch: 33\n",
      "training loss: 34155093668437.582\n",
      "validation loss: 28551130723752.184\n",
      "epoch: 34\n",
      "training loss: 34162816483317.203\n",
      "validation loss: 29347634526109.547\n",
      "epoch: 35\n",
      "training loss: 34277878867669.81\n",
      "validation loss: 29011570928822.992\n",
      "epoch: 36\n",
      "training loss: 34176883908136.387\n",
      "validation loss: 28793657528703.38\n",
      "epoch: 37\n",
      "training loss: 34083524793669.305\n",
      "validation loss: 27982797808696.15\n",
      "epoch: 38\n",
      "training loss: 34200945692641.27\n",
      "validation loss: 28738296366515.25\n",
      "epoch: 39\n",
      "training loss: 34081036406582.445\n",
      "validation loss: 28240938744110.277\n",
      "epoch: 40\n",
      "training loss: 33990716898750.445\n",
      "validation loss: 28216016611875.973\n",
      "epoch: 41\n",
      "training loss: 33996579978952.965\n",
      "validation loss: 28209626087424.066\n",
      "epoch: 42\n",
      "training loss: 34014852573781.84\n",
      "validation loss: 28388641821691.938\n",
      "epoch: 43\n",
      "training loss: 34032831743635.88\n",
      "validation loss: 28447136508420.4\n",
      "epoch: 44\n",
      "training loss: 33962991681114.605\n",
      "validation loss: 28551517879258.973\n",
      "epoch: 45\n",
      "training loss: 33870443015386.59\n",
      "validation loss: 27919545466944.453\n",
      "epoch: 46\n",
      "training loss: 33817899894545.652\n",
      "validation loss: 27713796360419.09\n",
      "epoch: 47\n",
      "training loss: 33750165959368.95\n",
      "validation loss: 27569328055695.887\n",
      "epoch: 48\n",
      "training loss: 33700296075924.113\n",
      "validation loss: 27462582570430.027\n",
      "epoch: 49\n",
      "training loss: 33664456667437.637\n",
      "validation loss: 27374325207770.223\n",
      "epoch: 50\n",
      "training loss: 33617161086951.695\n",
      "validation loss: 27271539766925.195\n",
      "epoch: 51\n",
      "training loss: 33569323969143.69\n",
      "validation loss: 27163058046257.47\n",
      "epoch: 52\n",
      "training loss: 33524018863361.098\n",
      "validation loss: 27063274982296.746\n",
      "epoch: 53\n",
      "training loss: 33479849054265.754\n",
      "validation loss: 26965115530476.098\n",
      "epoch: 54\n",
      "training loss: 33436664207199.773\n",
      "validation loss: 26868669187684.6\n",
      "epoch: 55\n",
      "training loss: 33394486075277.387\n",
      "validation loss: 26773831298559.914\n",
      "epoch: 56\n",
      "training loss: 33353290514925.445\n",
      "validation loss: 26680572461838.12\n",
      "epoch: 57\n",
      "training loss: 33313053959364.11\n",
      "validation loss: 26588863858652.72\n",
      "epoch: 58\n",
      "training loss: 33273753407550.36\n",
      "validation loss: 26498677253622.414\n",
      "epoch: 59\n",
      "training loss: 33235366297741.445\n",
      "validation loss: 26409984982088.78\n",
      "epoch: 60\n",
      "training loss: 33197776768197.418\n",
      "validation loss: 26322740843493.51\n",
      "epoch: 61\n",
      "training loss: 33161140064210.305\n",
      "validation loss: 26236929224514.32\n",
      "epoch: 62\n",
      "training loss: 33125352915553.223\n",
      "validation loss: 26152532304172.504\n",
      "epoch: 63\n",
      "training loss: 33090394955794.727\n",
      "validation loss: 26069524583414.2\n",
      "epoch: 64\n",
      "training loss: 33056246307300.777\n",
      "validation loss: 25987881074323.16\n",
      "epoch: 65\n",
      "training loss: 33022887569503.914\n",
      "validation loss: 25907577289050.844\n",
      "epoch: 66\n",
      "training loss: 32990299807453.656\n",
      "validation loss: 25828589228996.773\n",
      "epoch: 67\n",
      "training loss: 32958464540641.785\n",
      "validation loss: 25750893374234.734\n",
      "epoch: 68\n",
      "training loss: 32927363732095.695\n",
      "validation loss: 25674466673178.934\n",
      "epoch: 69\n",
      "training loss: 32896979777733.54\n",
      "validation loss: 25599286532484.57\n",
      "epoch: 70\n",
      "training loss: 32867295495974.812\n",
      "validation loss: 25525330807177.332\n",
      "epoch: 71\n",
      "training loss: 32838294117600.22\n",
      "validation loss: 25452577791006.406\n",
      "epoch: 72\n",
      "training loss: 32809959275854.598\n",
      "validation loss: 25381006207015.9\n",
      "epoch: 73\n",
      "training loss: 32782274996786.246\n",
      "validation loss: 25310595198329.44\n",
      "epoch: 74\n",
      "training loss: 32755225689805.715\n",
      "validation loss: 25241324319143.074\n",
      "epoch: 75\n",
      "training loss: 32728796137515.676\n",
      "validation loss: 25173173525920.996\n",
      "epoch: 76\n",
      "training loss: 32702637365739.71\n",
      "validation loss: 25106122893039.824\n",
      "epoch: 77\n",
      "training loss: 32715706555364.824\n",
      "validation loss: 25047490276428.387\n",
      "epoch: 78\n",
      "training loss: 32690127400422.54\n",
      "validation loss: 24983483378996.215\n",
      "epoch: 79\n",
      "training loss: 32643005793624.434\n",
      "validation loss: 24910612804490.766\n",
      "epoch: 80\n",
      "training loss: 32619533855095.89\n",
      "validation loss: 24848027858415.42\n",
      "epoch: 81\n",
      "training loss: 32596593808506.91\n",
      "validation loss: 24786445455031.44\n",
      "epoch: 82\n",
      "training loss: 32574172936861.566\n",
      "validation loss: 24725848265961.336\n",
      "epoch: 83\n",
      "training loss: 32552258828002.89\n",
      "validation loss: 24666219298212.57\n",
      "epoch: 84\n",
      "training loss: 32530839367109.805\n",
      "validation loss: 24607541887093.46\n",
      "epoch: 85\n",
      "training loss: 32509902729574.348\n",
      "validation loss: 24549799689281.555\n",
      "epoch: 86\n",
      "training loss: 32489437374049.402\n",
      "validation loss: 24492976676046.97\n",
      "epoch: 87\n",
      "training loss: 32469432035663.004\n",
      "validation loss: 24437057126627.2\n",
      "epoch: 88\n",
      "training loss: 32449875719395.234\n",
      "validation loss: 24382025621749.816\n",
      "epoch: 89\n",
      "training loss: 32430757693613.688\n",
      "validation loss: 24327867037299.754\n",
      "epoch: 90\n",
      "training loss: 32412067483763.848\n",
      "validation loss: 24274566538127.684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 91\n",
      "training loss: 32393794866210.645\n",
      "validation loss: 24222109571996.41\n",
      "epoch: 92\n",
      "training loss: 32375929862227.418\n",
      "validation loss: 24170481863661.94\n",
      "epoch: 93\n",
      "training loss: 32358462732129.02\n",
      "validation loss: 24119669409086.21\n",
      "epoch: 94\n",
      "training loss: 32341383969545.33\n",
      "validation loss: 24069658469778.453\n",
      "epoch: 95\n",
      "training loss: 32324684295832.027\n",
      "validation loss: 24020435567262.1\n",
      "epoch: 96\n",
      "training loss: 32308354654615.215\n",
      "validation loss: 23971987477664.59\n",
      "epoch: 97\n",
      "training loss: 32292386206466.73\n",
      "validation loss: 23924301226426.934\n",
      "epoch: 98\n",
      "training loss: 32276770323707.04\n",
      "validation loss: 23877364083130.56\n",
      "epoch: 99\n",
      "training loss: 32261498585332.555\n",
      "validation loss: 23831163556438.61\n",
      "epoch: 100\n",
      "training loss: 32246562772064.58\n",
      "validation loss: 23785687389149.016\n",
      "epoch: 101\n",
      "training loss: 32231954861516.707\n",
      "validation loss: 23740923553356.92\n",
      "epoch: 102\n",
      "training loss: 32217667023478.14\n",
      "validation loss: 23696860245723.82\n",
      "epoch: 103\n",
      "training loss: 32203691615309.89\n",
      "validation loss: 23653485882851.05\n",
      "epoch: 104\n",
      "training loss: 32190021177451.266\n",
      "validation loss: 23610789096755.156\n",
      "epoch: 105\n",
      "training loss: 32176648429034.086\n",
      "validation loss: 23568758730442.85\n",
      "epoch: 106\n",
      "training loss: 32163566263601.867\n",
      "validation loss: 23527383833583.34\n",
      "epoch: 107\n",
      "training loss: 32150767744931.582\n",
      "validation loss: 23486653658275.633\n",
      "epoch: 108\n",
      "training loss: 32138246102955.54\n",
      "validation loss: 23446557654908.81\n",
      "epoch: 109\n",
      "training loss: 32125994729780.887\n",
      "validation loss: 23407085468113.04\n",
      "epoch: 110\n",
      "training loss: 32114007175804.57\n",
      "validation loss: 23368226932799.324\n",
      "epoch: 111\n",
      "training loss: 32102277145921.25\n",
      "validation loss: 23329972070285.863\n",
      "epoch: 112\n",
      "training loss: 32090798495822.113\n",
      "validation loss: 23292311084509.168\n",
      "epoch: 113\n",
      "training loss: 32079565228382.363\n",
      "validation loss: 23255234358317.91\n",
      "epoch: 114\n",
      "training loss: 32068571490135.2\n",
      "validation loss: 23218732449847.617\n",
      "epoch: 115\n",
      "training loss: 32057811567830.3\n",
      "validation loss: 23182796088974.44\n",
      "epoch: 116\n",
      "training loss: 32047279885074.71\n",
      "validation loss: 23147416173846.145\n",
      "epoch: 117\n",
      "training loss: 32036970999054.273\n",
      "validation loss: 23112583767488.496\n",
      "epoch: 118\n",
      "training loss: 32026879597333.504\n",
      "validation loss: 23078290094485.53\n",
      "epoch: 119\n",
      "training loss: 32017000494732.168\n",
      "validation loss: 23044526537731.75\n",
      "epoch: 120\n",
      "training loss: 32007328630276.723\n",
      "validation loss: 23011284635254.902\n",
      "epoch: 121\n",
      "training loss: 31997859064224.684\n",
      "validation loss: 22978556077107.426\n",
      "epoch: 122\n",
      "training loss: 31988586975160.41\n",
      "validation loss: 22946332702325.332\n",
      "epoch: 123\n",
      "training loss: 31979507657160.305\n",
      "validation loss: 22914606495952.723\n",
      "epoch: 124\n",
      "training loss: 31970616517026.08\n",
      "validation loss: 22883369586130.617\n",
      "epoch: 125\n",
      "training loss: 31961909071584.168\n",
      "validation loss: 22852614241248.53\n",
      "epoch: 126\n",
      "training loss: 31953380945049.914\n",
      "validation loss: 22822332867157.477\n",
      "epoch: 127\n",
      "training loss: 31945027866454.89\n",
      "validation loss: 22792518004442.938\n",
      "epoch: 128\n",
      "training loss: 31936845667135.773\n",
      "validation loss: 22763162325756.438\n",
      "epoch: 129\n",
      "training loss: 31928830278283.504\n",
      "validation loss: 22734258633204.465\n",
      "epoch: 130\n",
      "training loss: 31920977728551.01\n",
      "validation loss: 22705799855793.375\n",
      "epoch: 131\n",
      "training loss: 31913284141718.375\n",
      "validation loss: 22677779046929.05\n",
      "epoch: 132\n",
      "training loss: 31905745734413.8\n",
      "validation loss: 22650189381970.062\n",
      "epoch: 133\n",
      "training loss: 31898358813889.29\n",
      "validation loss: 22623024155833.13\n",
      "epoch: 134\n",
      "training loss: 31891119775849.44\n",
      "validation loss: 22596276780649.754\n",
      "epoch: 135\n",
      "training loss: 31884025102332.246\n",
      "validation loss: 22569940783472.727\n",
      "epoch: 136\n",
      "training loss: 31877071359640.16\n",
      "validation loss: 22544009804031.613\n",
      "epoch: 137\n",
      "training loss: 31870255196317.72\n",
      "validation loss: 22518477592535.824\n",
      "epoch: 138\n",
      "training loss: 31863573341129.168\n",
      "validation loss: 22493338007524.473\n",
      "epoch: 139\n",
      "training loss: 31857022243675.734\n",
      "validation loss: 22468585013760.145\n",
      "epoch: 140\n",
      "training loss: 31892611299298.93\n",
      "validation loss: 22826213640597.613\n",
      "epoch: 141\n",
      "training loss: 31894418919415.484\n",
      "validation loss: 22789790420935.477\n",
      "epoch: 142\n",
      "training loss: 31887275246286.72\n",
      "validation loss: 22762980813039.34\n",
      "epoch: 143\n",
      "training loss: 31880272841737.25\n",
      "validation loss: 22736578700728.88\n",
      "epoch: 144\n",
      "training loss: 31873408380254.645\n",
      "validation loss: 22710577855215.234\n",
      "epoch: 145\n",
      "training loss: 31866678615164.33\n",
      "validation loss: 22684972154635.645\n",
      "epoch: 146\n",
      "training loss: 31860080376760.324\n",
      "validation loss: 22659755581995.95\n",
      "epoch: 147\n",
      "training loss: 31853610570480.117\n",
      "validation loss: 22634922223156.742\n",
      "epoch: 148\n",
      "training loss: 31847266175122.848\n",
      "validation loss: 22610466264861.637\n",
      "epoch: 149\n",
      "training loss: 31841044241109.77\n",
      "validation loss: 22586381992806.973\n",
      "epoch: 150\n",
      "training loss: 31834941888785.86\n",
      "validation loss: 22562663789751.668\n",
      "epoch: 151\n",
      "training loss: 31828956306761.84\n",
      "validation loss: 22539306133666.773\n",
      "epoch: 152\n",
      "training loss: 31823084750295.39\n",
      "validation loss: 22516303595923.457\n",
      "epoch: 153\n",
      "training loss: 31817324539710.81\n",
      "validation loss: 22493650839518.766\n",
      "epoch: 154\n",
      "training loss: 31811673058856.098\n",
      "validation loss: 22471342617338.37\n",
      "epoch: 155\n",
      "training loss: 31806127753596.695\n",
      "validation loss: 22449373770455.316\n",
      "epoch: 156\n",
      "training loss: 31800686130344.996\n",
      "validation loss: 22427739226464.11\n",
      "epoch: 157\n",
      "training loss: 31795345754624.254\n",
      "validation loss: 22406433997849.324\n",
      "epoch: 158\n",
      "training loss: 31790104249667.273\n",
      "validation loss: 22385453180387.96\n",
      "epoch: 159\n",
      "training loss: 31784959295047.85\n",
      "validation loss: 22364791951584.832\n",
      "epoch: 160\n",
      "training loss: 31779908625344.79\n",
      "validation loss: 22344445569140.24\n",
      "epoch: 161\n",
      "training loss: 31774950028837.562\n",
      "validation loss: 22324409369449.207\n",
      "epoch: 162\n",
      "training loss: 31770081346232.94\n",
      "validation loss: 22304678766131.652\n",
      "epoch: 163\n",
      "training loss: 31765300469421.797\n",
      "validation loss: 22285249248592.688\n",
      "epoch: 164\n",
      "training loss: 31760605340265.34\n",
      "validation loss: 22266116380612.58\n",
      "epoch: 165\n",
      "training loss: 31755993949410.258\n",
      "validation loss: 22247275798965.52\n",
      "epoch: 166\n",
      "training loss: 31751464335131.777\n",
      "validation loss: 22228723212066.69\n",
      "epoch: 167\n",
      "training loss: 31747014582204.36\n",
      "validation loss: 22210454398647.027\n",
      "epoch: 168\n",
      "training loss: 31742642820798.953\n",
      "validation loss: 22192465206454.938\n",
      "epoch: 169\n",
      "training loss: 31738347225406.58\n",
      "validation loss: 22174751550984.586\n",
      "epoch: 170\n",
      "training loss: 31734126013787.348\n",
      "validation loss: 22157309414229.945\n",
      "epoch: 171\n",
      "training loss: 31729977445944.395\n",
      "validation loss: 22140134843464.31\n",
      "epoch: 172\n",
      "training loss: 31725899823122.195\n",
      "validation loss: 22123223950044.4\n",
      "epoch: 173\n",
      "training loss: 31721891486828.566\n",
      "validation loss: 22106572908238.84\n",
      "epoch: 174\n",
      "training loss: 31717950817879.945\n",
      "validation loss: 22090177954080.273\n",
      "epoch: 175\n",
      "training loss: 31714076235469.168\n",
      "validation loss: 22074035384240.68\n",
      "epoch: 176\n",
      "training loss: 31710266196255.51\n",
      "validation loss: 22058141554929.367\n",
      "epoch: 177\n",
      "training loss: 31706519193476.215\n",
      "validation loss: 22042492880813.164\n",
      "epoch: 178\n",
      "training loss: 31702833756079.05\n",
      "validation loss: 22027085833958.33\n",
      "epoch: 179\n",
      "training loss: 31699208447875.527\n",
      "validation loss: 22011916942793.703\n",
      "epoch: 180\n",
      "training loss: 31695641866714.098\n",
      "validation loss: 21996982791094.652\n",
      "epoch: 181\n",
      "training loss: 31692132643672.926\n",
      "validation loss: 21982280016987.32\n",
      "epoch: 182\n",
      "training loss: 31688679442271.605\n",
      "validation loss: 21967805311972.816\n",
      "epoch: 183\n",
      "training loss: 31685280957700.562\n",
      "validation loss: 21953555419970.844\n",
      "epoch: 184\n",
      "training loss: 31681935916060.633\n",
      "validation loss: 21939527136382.38\n",
      "epoch: 185\n",
      "training loss: 31678643073366.03\n",
      "validation loss: 21925717307170.793\n",
      "epoch: 186\n",
      "training loss: 31675337072930.438\n",
      "validation loss: 21912122827956.74\n",
      "epoch: 187\n",
      "training loss: 31672454318469.34\n",
      "validation loss: 21898744079724.85\n",
      "epoch: 188\n",
      "training loss: 31669229128857.777\n",
      "validation loss: 21885573870118.797\n",
      "epoch: 189\n",
      "training loss: 31666133066593.348\n",
      "validation loss: 21872609654142.32\n",
      "epoch: 190\n",
      "training loss: 31663083404122.152\n",
      "validation loss: 21859848822229.09\n",
      "epoch: 191\n",
      "training loss: 31660079064679.5\n",
      "validation loss: 21847288505881.105\n",
      "epoch: 192\n",
      "training loss: 31657118997014.027\n",
      "validation loss: 21834925881213.086\n",
      "epoch: 193\n",
      "training loss: 31654202174782.715\n",
      "validation loss: 21822758168169.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 194\n",
      "training loss: 31651327595960.19\n",
      "validation loss: 21810782629757.895\n",
      "epoch: 195\n",
      "training loss: 31648494282262.05\n",
      "validation loss: 21798996571294.2\n",
      "epoch: 196\n",
      "training loss: 31645701278581.895\n",
      "validation loss: 21787397339666.8\n",
      "epoch: 197\n",
      "training loss: 31642947652441.67\n",
      "validation loss: 21775982322611.977\n",
      "epoch: 198\n",
      "training loss: 31640232493455.066\n",
      "validation loss: 21764748948004.58\n",
      "epoch: 199\n",
      "training loss: 31637554912803.605\n",
      "validation loss: 21753694683162.355\n",
      "epoch: 200\n",
      "training loss: 31634914042725.23\n",
      "validation loss: 21742817034163.75\n",
      "epoch: 201\n",
      "training loss: 31632309036014.914\n",
      "validation loss: 21732113545178.867\n",
      "epoch: 202\n",
      "training loss: 31629739065537.203\n",
      "validation loss: 21721581797813.45\n",
      "epoch: 203\n",
      "training loss: 31627203323750.27\n",
      "validation loss: 21711219410465.44\n",
      "epoch: 204\n",
      "training loss: 31624701022241.23\n",
      "validation loss: 21701024037693.984\n",
      "epoch: 205\n",
      "training loss: 31622231391272.574\n",
      "validation loss: 21690993369600.535\n",
      "epoch: 206\n",
      "training loss: 31619793679339.26\n",
      "validation loss: 21681125131221.844\n",
      "epoch: 207\n",
      "training loss: 31617387152736.316\n",
      "validation loss: 21671417081934.586\n",
      "epoch: 208\n",
      "training loss: 31615011095136.758\n",
      "validation loss: 21661867014871.395\n",
      "epoch: 209\n",
      "training loss: 31612664807179.453\n",
      "validation loss: 21652472756347.953\n",
      "epoch: 210\n",
      "training loss: 31610347606066.78\n",
      "validation loss: 21643232165301.13\n",
      "epoch: 211\n",
      "training loss: 31608058825171.86\n",
      "validation loss: 21634143132737.71\n",
      "epoch: 212\n",
      "training loss: 31605797813655.02\n",
      "validation loss: 21625203581193.61\n",
      "epoch: 213\n",
      "training loss: 31603563936089.45\n",
      "validation loss: 21616411464203.336\n",
      "epoch: 214\n",
      "training loss: 31601356572095.63\n",
      "validation loss: 21607764765779.523\n",
      "epoch: 215\n",
      "training loss: 31599175115984.51\n",
      "validation loss: 21599261499902.227\n",
      "epoch: 216\n",
      "training loss: 31597018976409.086\n",
      "validation loss: 21590899710017.9\n",
      "epoch: 217\n",
      "training loss: 31594887576024.164\n",
      "validation loss: 21582677468547.746\n",
      "epoch: 218\n",
      "training loss: 31592780351154.0\n",
      "validation loss: 21574592876405.35\n",
      "epoch: 219\n",
      "training loss: 31590696751466.84\n",
      "validation loss: 21566644062523.32\n",
      "epoch: 220\n",
      "training loss: 31588636239648.582\n",
      "validation loss: 21558829183388.83\n",
      "epoch: 221\n",
      "training loss: 31586598290806.32\n",
      "validation loss: 21551146422588.14\n",
      "epoch: 222\n",
      "training loss: 31584568246861.76\n",
      "validation loss: 21543593921152.13\n",
      "epoch: 223\n",
      "training loss: 31582573894590.53\n",
      "validation loss: 21536012270243.766\n",
      "epoch: 224\n",
      "training loss: 31580600605800.68\n",
      "validation loss: 21528715640850.58\n",
      "epoch: 225\n",
      "training loss: 31578647903581.797\n",
      "validation loss: 21521544126980.24\n",
      "epoch: 226\n",
      "training loss: 31576715322567.496\n",
      "validation loss: 21514496041063.863\n",
      "epoch: 227\n",
      "training loss: 31574802408405.684\n",
      "validation loss: 21507569720168.24\n",
      "epoch: 228\n",
      "training loss: 31572908717497.4\n",
      "validation loss: 21500763525496.27\n",
      "epoch: 229\n",
      "training loss: 31571033816741.734\n",
      "validation loss: 21494075841994.36\n",
      "epoch: 230\n",
      "training loss: 31569177283286.934\n",
      "validation loss: 21487505077967.08\n",
      "epoch: 231\n",
      "training loss: 31567338704287.28\n",
      "validation loss: 21481049664698.78\n",
      "epoch: 232\n",
      "training loss: 31565517676665.797\n",
      "validation loss: 21474708056082.27\n",
      "epoch: 233\n",
      "training loss: 31563713806882.613\n",
      "validation loss: 21468478728254.11\n",
      "epoch: 234\n",
      "training loss: 31561926710708.746\n",
      "validation loss: 21462360179236.63\n",
      "epoch: 235\n",
      "training loss: 31560156013005.277\n",
      "validation loss: 21456350928586.445\n",
      "epoch: 236\n",
      "training loss: 31558401347507.83\n",
      "validation loss: 21450449517049.293\n",
      "epoch: 237\n",
      "training loss: 31556662356616.02\n",
      "validation loss: 21444654506221.156\n",
      "epoch: 238\n",
      "training loss: 31554938691188.062\n",
      "validation loss: 21438964478215.51\n",
      "epoch: 239\n",
      "training loss: 31553230010340.082\n",
      "validation loss: 21433378035336.484\n",
      "epoch: 240\n",
      "training loss: 31551535981250.37\n",
      "validation loss: 21427893799758.016\n",
      "epoch: 241\n",
      "training loss: 31549856278968.07\n",
      "validation loss: 21422510413208.664\n",
      "epoch: 242\n",
      "training loss: 31548190586226.586\n",
      "validation loss: 21417226536662.105\n",
      "epoch: 243\n",
      "training loss: 31546538593261.33\n",
      "validation loss: 21412040850033.152\n",
      "epoch: 244\n",
      "training loss: 31544899997631.77\n",
      "validation loss: 21406952051879.203\n",
      "epoch: 245\n",
      "training loss: 31543274504047.76\n",
      "validation loss: 21401958859106.96\n",
      "epoch: 246\n",
      "training loss: 31541661824199.96\n",
      "validation loss: 21397060006684.43\n",
      "epoch: 247\n",
      "training loss: 31540061676594.25\n",
      "validation loss: 21392254247357.926\n",
      "epoch: 248\n",
      "training loss: 31538473786390.12\n",
      "validation loss: 21387540351374.164\n",
      "epoch: 249\n",
      "training loss: 31536897885242.863\n",
      "validation loss: 21382917106207.19\n",
      "epoch: 250\n",
      "training loss: 31535333711149.52\n",
      "validation loss: 21378383316290.2\n",
      "epoch: 251\n",
      "training loss: 31533781008298.47\n",
      "validation loss: 21373937802751.92\n",
      "epoch: 252\n",
      "training loss: 31532239526922.59\n",
      "validation loss: 21369579403157.81\n",
      "epoch: 253\n",
      "training loss: 31530709023155.918\n",
      "validation loss: 21365306971255.586\n",
      "epoch: 254\n",
      "training loss: 31529189258893.664\n",
      "validation loss: 21361119376725.33\n",
      "epoch: 255\n",
      "training loss: 31527680001655.617\n",
      "validation loss: 21357015504933.863\n",
      "epoch: 256\n",
      "training loss: 31526181024452.7\n",
      "validation loss: 21352994256693.434\n",
      "epoch: 257\n",
      "training loss: 31524692105656.793\n",
      "validation loss: 21349054548024.56\n",
      "epoch: 258\n",
      "training loss: 31523213028873.54\n",
      "validation loss: 21345195309922.953\n",
      "epoch: 259\n",
      "training loss: 31521743582818.223\n",
      "validation loss: 21341415488130.48\n",
      "epoch: 260\n",
      "training loss: 31520283561194.62\n",
      "validation loss: 21337714042910.08\n",
      "epoch: 261\n",
      "training loss: 31518832762576.67\n",
      "validation loss: 21334089948824.47\n",
      "epoch: 262\n",
      "training loss: 31517390990292.945\n",
      "validation loss: 21330542194518.695\n",
      "epoch: 263\n",
      "training loss: 31515958052313.91\n",
      "validation loss: 21327069782506.355\n",
      "epoch: 264\n",
      "training loss: 31514533761141.83\n",
      "validation loss: 21323671728959.42\n",
      "epoch: 265\n",
      "training loss: 31513117933703.277\n",
      "validation loss: 21320347063501.688\n",
      "epoch: 266\n",
      "training loss: 31511710391244.234\n",
      "validation loss: 21317094829005.582\n",
      "epoch: 267\n",
      "training loss: 31510310959227.58\n",
      "validation loss: 21313914081392.457\n",
      "epoch: 268\n",
      "training loss: 31508919467233.168\n",
      "validation loss: 21310803889436.16\n",
      "epoch: 269\n",
      "training loss: 31507535748860.094\n",
      "validation loss: 21307763334569.855\n",
      "epoch: 270\n",
      "training loss: 31506159641631.457\n",
      "validation loss: 21304791510695.992\n",
      "epoch: 271\n",
      "training loss: 31504790986901.21\n",
      "validation loss: 21301887523999.316\n",
      "epoch: 272\n",
      "training loss: 31503429629763.387\n",
      "validation loss: 21299050492762.902\n",
      "epoch: 273\n",
      "training loss: 31502075418963.32\n",
      "validation loss: 21296279547186.984\n",
      "epoch: 274\n",
      "training loss: 31500728206811.094\n",
      "validation loss: 21293573829210.582\n",
      "epoch: 275\n",
      "training loss: 31499387849096.996\n",
      "validation loss: 21290932492335.715\n",
      "epoch: 276\n",
      "training loss: 31498054205008.945\n",
      "validation loss: 21288354701454.016\n",
      "epoch: 277\n",
      "training loss: 31496727137051.953\n",
      "validation loss: 21285839632675.633\n",
      "epoch: 278\n",
      "training loss: 31495406510969.387\n",
      "validation loss: 21283386473159.992\n",
      "epoch: 279\n",
      "training loss: 31494092195666.258\n",
      "validation loss: 21280994420948.18\n",
      "epoch: 280\n",
      "training loss: 31492784063134.152\n",
      "validation loss: 21278662684796.336\n",
      "epoch: 281\n",
      "training loss: 31491481988378.08\n",
      "validation loss: 21276390484009.35\n",
      "epoch: 282\n",
      "training loss: 31490185849344.957\n",
      "validation loss: 21274177048273.758\n",
      "epoch: 283\n",
      "training loss: 31488895526853.89\n",
      "validation loss: 21272021617488.074\n",
      "epoch: 284\n",
      "training loss: 31487610904528.027\n",
      "validation loss: 21269923441588.02\n",
      "epoch: 285\n",
      "training loss: 31486331868728.027\n",
      "validation loss: 21267881780362.137\n",
      "epoch: 286\n",
      "training loss: 31485058308487.18\n",
      "validation loss: 21265895903250.586\n",
      "epoch: 287\n",
      "training loss: 31483790115448.0\n",
      "validation loss: 21263965089114.12\n",
      "epoch: 288\n",
      "training loss: 31482527183800.332\n",
      "validation loss: 21262088625949.434\n",
      "epoch: 289\n",
      "training loss: 31481269410220.953\n",
      "validation loss: 21260265810504.88\n",
      "epoch: 290\n",
      "training loss: 31480016693814.58\n",
      "validation loss: 21258495947701.49\n",
      "epoch: 291\n",
      "training loss: 31478768936056.242\n",
      "validation loss: 21256778349648.055\n",
      "epoch: 292\n",
      "training loss: 31477526040735.09\n",
      "validation loss: 21255112333736.9\n",
      "epoch: 293\n",
      "training loss: 31476287913899.418\n",
      "validation loss: 21253497218419.273\n",
      "epoch: 294\n",
      "training loss: 31475054463802.883\n",
      "validation loss: 21251932312225.465\n",
      "epoch: 295\n",
      "training loss: 31473825600851.42\n",
      "validation loss: 21250416878890.707\n",
      "epoch: 296\n",
      "training loss: 31472601237547.47\n",
      "validation loss: 21248949991685.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 297\n",
      "training loss: 31471381288367.867\n",
      "validation loss: 21247529715557.207\n",
      "epoch: 298\n",
      "training loss: 31470165410393.715\n",
      "validation loss: 21246154967139.15\n",
      "epoch: 299\n",
      "training loss: 31461571606426.246\n",
      "validation loss: 21251497311667.414\n",
      "epoch: 300\n",
      "training loss: 31471745496793.516\n",
      "validation loss: 21278081036346.32\n",
      "epoch: 301\n",
      "training loss: 31470581344297.42\n",
      "validation loss: 21276837247234.32\n",
      "epoch: 302\n",
      "training loss: 31469421708166.77\n",
      "validation loss: 21275643184107.914\n",
      "epoch: 303\n",
      "training loss: 31468266503326.273\n",
      "validation loss: 21274498206218.773\n",
      "epoch: 304\n",
      "training loss: 31467115646704.145\n",
      "validation loss: 21273401681250.457\n",
      "epoch: 305\n",
      "training loss: 31465969057184.27\n",
      "validation loss: 21272352985197.613\n",
      "epoch: 306\n",
      "training loss: 31464826655559.668\n",
      "validation loss: 21271351502247.723\n",
      "epoch: 307\n",
      "training loss: 31463688364486.92\n",
      "validation loss: 21270396624664.668\n",
      "epoch: 308\n",
      "training loss: 31462554108441.824\n",
      "validation loss: 21269487752674.223\n",
      "epoch: 309\n",
      "training loss: 31461423813675.99\n",
      "validation loss: 21268624294351.305\n",
      "epoch: 310\n",
      "training loss: 31460297408174.523\n",
      "validation loss: 21267805665509.01\n",
      "epoch: 311\n",
      "training loss: 31459174821614.754\n",
      "validation loss: 21267031289589.453\n",
      "epoch: 312\n",
      "training loss: 31458055985325.863\n",
      "validation loss: 21266300597556.203\n",
      "epoch: 313\n",
      "training loss: 31456940832249.523\n",
      "validation loss: 21265613027788.547\n",
      "epoch: 314\n",
      "training loss: 31455829296901.49\n",
      "validation loss: 21264968025977.305\n",
      "epoch: 315\n",
      "training loss: 31454721315334.066\n",
      "validation loss: 21264365045022.33\n",
      "epoch: 316\n",
      "training loss: 31453616825099.496\n",
      "validation loss: 21263803544931.555\n",
      "epoch: 317\n",
      "training loss: 31452515765214.23\n",
      "validation loss: 21263282992721.68\n",
      "epoch: 318\n",
      "training loss: 31451418076123.996\n",
      "validation loss: 21262802862320.324\n",
      "epoch: 319\n",
      "training loss: 31450323699669.78\n",
      "validation loss: 21262362634469.77\n",
      "epoch: 320\n",
      "training loss: 31449232579054.555\n",
      "validation loss: 21261961796632.1\n",
      "epoch: 321\n",
      "training loss: 31448144658810.805\n",
      "validation loss: 21261599842895.945\n",
      "epoch: 322\n",
      "training loss: 31447059884768.895\n",
      "validation loss: 21261276273884.465\n",
      "epoch: 323\n",
      "training loss: 31445978204026.074\n",
      "validation loss: 21260990596664.957\n",
      "epoch: 324\n",
      "training loss: 31444899564916.36\n",
      "validation loss: 21260742324659.688\n",
      "epoch: 325\n",
      "training loss: 31443823916981.0\n",
      "validation loss: 21260530977558.168\n",
      "epoch: 326\n",
      "training loss: 31442751210939.727\n",
      "validation loss: 21260356081230.77\n",
      "epoch: 327\n",
      "training loss: 31441681398662.707\n",
      "validation loss: 21260217167643.625\n",
      "epoch: 328\n",
      "training loss: 31440614433143.074\n",
      "validation loss: 21260113774774.836\n",
      "epoch: 329\n",
      "training loss: 31439550268470.207\n",
      "validation loss: 21260045446531.973\n",
      "epoch: 330\n",
      "training loss: 31438488859803.574\n",
      "validation loss: 21260011732670.773\n",
      "epoch: 331\n",
      "training loss: 31437430163347.28\n",
      "validation loss: 21260012188715.156\n",
      "epoch: 332\n",
      "training loss: 31436374136325.125\n",
      "validation loss: 21260046375878.37\n",
      "epoch: 333\n",
      "training loss: 31435320736956.34\n",
      "validation loss: 21260113860985.332\n",
      "epoch: 334\n",
      "training loss: 31434269924431.824\n",
      "validation loss: 21260214216396.18\n",
      "epoch: 335\n",
      "training loss: 31433221658891.055\n",
      "validation loss: 21260347019930.95\n",
      "epoch: 336\n",
      "training loss: 31432175901399.426\n",
      "validation loss: 21260511854795.348\n",
      "epoch: 337\n",
      "training loss: 31431132613926.203\n",
      "validation loss: 21260708309507.715\n",
      "epoch: 338\n",
      "training loss: 31430091759323.0\n",
      "validation loss: 21260935977826.98\n",
      "epoch: 339\n",
      "training loss: 31429053301302.69\n",
      "validation loss: 21261194458681.766\n",
      "epoch: 340\n",
      "training loss: 31428017204418.97\n",
      "validation loss: 21261483356100.508\n",
      "epoch: 341\n",
      "training loss: 31426983434046.25\n",
      "validation loss: 21261802279142.633\n",
      "epoch: 342\n",
      "training loss: 31425951956360.11\n",
      "validation loss: 21262150841830.723\n",
      "epoch: 343\n",
      "training loss: 31424922738318.203\n",
      "validation loss: 21262528663083.758\n",
      "epoch: 344\n",
      "training loss: 31423895747641.625\n",
      "validation loss: 21262935366651.215\n",
      "epoch: 345\n",
      "training loss: 31422870952796.668\n",
      "validation loss: 21263370581048.297\n",
      "epoch: 346\n",
      "training loss: 31421848322977.094\n",
      "validation loss: 21263833939491.984\n",
      "epoch: 347\n",
      "training loss: 31420827828086.727\n",
      "validation loss: 21264325079838.08\n",
      "epoch: 348\n",
      "training loss: 31419809438722.56\n",
      "validation loss: 21264843644519.21\n",
      "epoch: 349\n",
      "training loss: 31418793126158.145\n",
      "validation loss: 21265389280483.652\n"
     ]
    }
   ],
   "source": [
    "batch_sz = [10, 25, 50, 75, 150]\n",
    "\n",
    "means = []\n",
    "\n",
    "for batch in batch_sz:\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=batch, \n",
    "                   activationFn=\"tanh\", lr=.0000001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "    nn.fit(X_std, y)\n",
    "    mae = mean_absolute_error(y, nn.predict(X_std))\n",
    "    \n",
    "    means.append(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6261672.852810156,\n",
       " 6339584.203572352,\n",
       " 6126601.5136576835,\n",
       " 6119698.301655335,\n",
       " 6239534.332500497]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Sizes\n",
    "\n",
    "- 50 or 75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111112\n"
     ]
    }
   ],
   "source": [
    "print(max(y))\n",
    "min(y/100000)\n",
    "y_adjust = y/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 3593.3242861377007\n",
      "validation loss: 3243.4384904546882\n",
      "epoch: 1\n",
      "training loss: 3472.9842821551324\n",
      "validation loss: 3128.083884303558\n",
      "epoch: 2\n",
      "training loss: 3321.1024190496814\n",
      "validation loss: 2987.450597055393\n",
      "epoch: 3\n",
      "training loss: 3112.4269128709057\n",
      "validation loss: 2801.5352202213494\n",
      "epoch: 4\n",
      "training loss: 2814.146487317953\n",
      "validation loss: 2545.7379004303375\n",
      "epoch: 5\n",
      "training loss: 2400.5050524405747\n",
      "validation loss: 2201.116911059948\n",
      "epoch: 6\n",
      "training loss: 1906.8374000418585\n",
      "validation loss: 1794.506939405268\n",
      "epoch: 7\n",
      "training loss: 1481.4261022468902\n",
      "validation loss: 1438.4261970906746\n",
      "epoch: 8\n",
      "training loss: 1247.6589444171545\n",
      "validation loss: 1239.0532526500474\n",
      "epoch: 9\n",
      "training loss: 1148.3906608749255\n",
      "validation loss: 1162.9973066480932\n",
      "epoch: 10\n",
      "training loss: 1095.2721929393372\n",
      "validation loss: 1134.2120298399016\n",
      "epoch: 11\n",
      "training loss: 1055.9543139545021\n",
      "validation loss: 1119.1972580696922\n",
      "epoch: 12\n",
      "training loss: 1022.9378303853504\n",
      "validation loss: 1109.2253570329704\n",
      "epoch: 13\n",
      "training loss: 994.1263655024443\n",
      "validation loss: 1102.2856813902563\n",
      "epoch: 14\n",
      "training loss: 968.6076069879193\n",
      "validation loss: 1097.3465377491602\n",
      "epoch: 15\n",
      "training loss: 945.9214895173736\n",
      "validation loss: 1094.3731166988975\n",
      "epoch: 16\n",
      "training loss: 925.5803854270164\n",
      "validation loss: 1092.6405767959045\n",
      "epoch: 17\n",
      "training loss: 907.4009372591553\n",
      "validation loss: 1092.1176749974077\n",
      "epoch: 18\n",
      "training loss: 890.9763241753091\n",
      "validation loss: 1092.3060210663446\n",
      "epoch: 19\n",
      "training loss: 876.2297720738287\n",
      "validation loss: 1093.057975577832\n",
      "epoch: 20\n",
      "training loss: 862.7956218839464\n",
      "validation loss: 1093.9868085169915\n",
      "epoch: 21\n",
      "training loss: 850.7493555956377\n",
      "validation loss: 1094.9481107968043\n",
      "epoch: 22\n",
      "training loss: 839.6519790234433\n",
      "validation loss: 1095.9604892577936\n",
      "epoch: 23\n",
      "training loss: 829.6641318576972\n",
      "validation loss: 1097.097658076335\n",
      "epoch: 24\n",
      "training loss: 820.4250783783527\n",
      "validation loss: 1097.2774839940014\n",
      "epoch: 25\n",
      "training loss: 812.0599042683551\n",
      "validation loss: 1097.0945134186366\n",
      "epoch: 26\n",
      "training loss: 804.2537009228239\n",
      "validation loss: 1096.6802544973207\n",
      "epoch: 27\n",
      "training loss: 797.003671009581\n",
      "validation loss: 1095.7536974776015\n",
      "epoch: 28\n",
      "training loss: 790.0945011566821\n",
      "validation loss: 1094.5221679567003\n",
      "epoch: 29\n",
      "training loss: 784.0746766966872\n",
      "validation loss: 1092.837621695602\n",
      "epoch: 30\n",
      "training loss: 778.1697721874103\n",
      "validation loss: 1090.670202106704\n",
      "epoch: 31\n",
      "training loss: 772.812676826592\n",
      "validation loss: 1088.264738858258\n",
      "epoch: 32\n",
      "training loss: 767.5230430019839\n",
      "validation loss: 1085.0216713104555\n",
      "epoch: 33\n",
      "training loss: 762.6896776908078\n",
      "validation loss: 1081.9473303831223\n",
      "epoch: 34\n",
      "training loss: 757.8448695885288\n",
      "validation loss: 1078.8822108634297\n",
      "epoch: 35\n",
      "training loss: 753.6208584436616\n",
      "validation loss: 1075.2869144999731\n",
      "epoch: 36\n",
      "training loss: 749.3796783782675\n",
      "validation loss: 1071.3190102352298\n",
      "epoch: 37\n",
      "training loss: 745.2654424036496\n",
      "validation loss: 1067.2477752163613\n",
      "epoch: 38\n",
      "training loss: 741.4503084012332\n",
      "validation loss: 1063.1484026284206\n",
      "epoch: 39\n",
      "training loss: 737.9104104997616\n",
      "validation loss: 1059.4949047289476\n",
      "epoch: 40\n",
      "training loss: 734.3866795061624\n",
      "validation loss: 1055.2802356556458\n",
      "epoch: 41\n",
      "training loss: 731.0091399602989\n",
      "validation loss: 1051.3058206437895\n",
      "epoch: 42\n",
      "training loss: 727.8578912415753\n",
      "validation loss: 1047.5141486414955\n",
      "epoch: 43\n",
      "training loss: 724.6225623381328\n",
      "validation loss: 1042.4730979979277\n",
      "epoch: 44\n",
      "training loss: 721.5075271235827\n",
      "validation loss: 1038.8156996394634\n",
      "epoch: 45\n",
      "training loss: 718.5631368184402\n",
      "validation loss: 1034.7054702936616\n",
      "epoch: 46\n",
      "training loss: 715.7068449585827\n",
      "validation loss: 1030.476794573993\n",
      "epoch: 47\n",
      "training loss: 712.8363726726564\n",
      "validation loss: 1026.8454994519386\n",
      "epoch: 48\n",
      "training loss: 710.1974316789174\n",
      "validation loss: 1022.8417291615219\n",
      "epoch: 49\n",
      "training loss: 707.5838793516575\n",
      "validation loss: 1019.1187072302405\n",
      "epoch: 50\n",
      "training loss: 705.0270025785733\n",
      "validation loss: 1015.316452532808\n",
      "epoch: 51\n",
      "training loss: 702.4960011527314\n",
      "validation loss: 1011.4686269341571\n",
      "epoch: 52\n",
      "training loss: 700.1730978075663\n",
      "validation loss: 1007.4803159119725\n",
      "epoch: 53\n",
      "training loss: 697.624344201143\n",
      "validation loss: 1003.9328338107143\n",
      "epoch: 54\n",
      "training loss: 695.3172030283498\n",
      "validation loss: 1000.3378950867453\n",
      "epoch: 55\n",
      "training loss: 693.3679518461666\n",
      "validation loss: 997.2979524563605\n",
      "epoch: 56\n",
      "training loss: 691.2547923580446\n",
      "validation loss: 993.7284233879119\n",
      "epoch: 57\n",
      "training loss: 689.2416535698884\n",
      "validation loss: 990.7628170158883\n",
      "epoch: 58\n",
      "training loss: 687.3162966457219\n",
      "validation loss: 987.6914749006123\n",
      "epoch: 59\n",
      "training loss: 685.3630603489695\n",
      "validation loss: 984.6914683922491\n",
      "epoch: 60\n",
      "training loss: 683.4388709601516\n",
      "validation loss: 981.7753266098567\n",
      "epoch: 61\n",
      "training loss: 681.6322481746423\n",
      "validation loss: 979.2162486258887\n",
      "epoch: 62\n",
      "training loss: 679.8937066178577\n",
      "validation loss: 975.7817653532934\n",
      "epoch: 63\n",
      "training loss: 678.099353210521\n",
      "validation loss: 973.0599424080236\n",
      "epoch: 64\n",
      "training loss: 676.2855743188845\n",
      "validation loss: 970.0476059369445\n",
      "epoch: 65\n",
      "training loss: 674.5491326382823\n",
      "validation loss: 967.5122018609642\n",
      "epoch: 66\n",
      "training loss: 672.8361571048706\n",
      "validation loss: 964.5744369032899\n",
      "epoch: 67\n",
      "training loss: 671.1161658348741\n",
      "validation loss: 962.0739748981174\n",
      "epoch: 68\n",
      "training loss: 669.3853073936722\n",
      "validation loss: 959.4533870484119\n",
      "epoch: 69\n",
      "training loss: 667.7548091045282\n",
      "validation loss: 956.4973070188234\n",
      "epoch: 70\n",
      "training loss: 666.0969752457337\n",
      "validation loss: 953.8609686842101\n",
      "epoch: 71\n",
      "training loss: 664.462249758047\n",
      "validation loss: 951.2033825459105\n",
      "epoch: 72\n",
      "training loss: 662.7514604012542\n",
      "validation loss: 948.585863731311\n",
      "epoch: 73\n",
      "training loss: 661.155908864945\n",
      "validation loss: 946.2857583628142\n",
      "epoch: 74\n",
      "training loss: 659.5283217167449\n",
      "validation loss: 943.423390258016\n",
      "epoch: 75\n",
      "training loss: 657.9304053436047\n",
      "validation loss: 941.1157480005338\n",
      "epoch: 76\n",
      "training loss: 656.3217539291901\n",
      "validation loss: 938.471924823867\n",
      "epoch: 77\n",
      "training loss: 654.7574781682104\n",
      "validation loss: 936.252963982952\n",
      "epoch: 78\n",
      "training loss: 653.2363106893623\n",
      "validation loss: 934.0823226492032\n",
      "epoch: 79\n",
      "training loss: 651.6921301245176\n",
      "validation loss: 931.2648271271603\n",
      "epoch: 80\n",
      "training loss: 650.143033727674\n",
      "validation loss: 928.9273954662988\n",
      "epoch: 81\n",
      "training loss: 648.5854818196286\n",
      "validation loss: 926.8591764981213\n",
      "epoch: 82\n",
      "training loss: 647.0681491910115\n",
      "validation loss: 924.4514504340991\n",
      "epoch: 83\n",
      "training loss: 645.5187956170763\n",
      "validation loss: 922.2507278504197\n",
      "epoch: 84\n",
      "training loss: 643.9595765392763\n",
      "validation loss: 919.9503439714385\n",
      "epoch: 85\n",
      "training loss: 642.3588875103287\n",
      "validation loss: 917.2908456384185\n",
      "epoch: 86\n",
      "training loss: 640.850815099632\n",
      "validation loss: 915.2101852796698\n",
      "epoch: 87\n",
      "training loss: 639.2289331376031\n",
      "validation loss: 912.88080667818\n",
      "epoch: 88\n",
      "training loss: 637.7824672614113\n",
      "validation loss: 910.8224890692693\n",
      "epoch: 89\n",
      "training loss: 636.1417989820691\n",
      "validation loss: 908.2050252433324\n",
      "epoch: 90\n",
      "training loss: 634.60625624388\n",
      "validation loss: 906.1139839780805\n",
      "epoch: 91\n",
      "training loss: 632.9777719678466\n",
      "validation loss: 903.65851765352\n",
      "epoch: 92\n",
      "training loss: 631.4747279976646\n",
      "validation loss: 901.4506181102558\n",
      "epoch: 93\n",
      "training loss: 629.82659631697\n",
      "validation loss: 899.2765710341248\n",
      "epoch: 94\n",
      "training loss: 628.1828894381069\n",
      "validation loss: 897.1753849544156\n",
      "epoch: 95\n",
      "training loss: 626.6040073961304\n",
      "validation loss: 895.0431836650432\n",
      "epoch: 96\n",
      "training loss: 624.9786309774404\n",
      "validation loss: 892.590773352987\n",
      "epoch: 97\n",
      "training loss: 623.4148557366444\n",
      "validation loss: 890.520848090488\n",
      "epoch: 98\n",
      "training loss: 621.7052322669269\n",
      "validation loss: 888.3100058859851\n",
      "epoch: 99\n",
      "training loss: 620.157152804907\n",
      "validation loss: 886.130609888271\n",
      "epoch: 100\n",
      "training loss: 618.4051306649144\n",
      "validation loss: 883.9544169682018\n",
      "epoch: 101\n",
      "training loss: 616.7642300067826\n",
      "validation loss: 881.8654028409957\n",
      "epoch: 102\n",
      "training loss: 615.1483370543415\n",
      "validation loss: 879.6614282351261\n",
      "epoch: 103\n",
      "training loss: 613.4855815127994\n",
      "validation loss: 877.079427799073\n",
      "epoch: 104\n",
      "training loss: 611.7622393826421\n",
      "validation loss: 875.2309939939943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 105\n",
      "training loss: 610.0219167578557\n",
      "validation loss: 872.8565865409271\n",
      "epoch: 106\n",
      "training loss: 608.3765230858747\n",
      "validation loss: 870.7762322033684\n",
      "epoch: 107\n",
      "training loss: 606.5573070896434\n",
      "validation loss: 868.4386801234995\n",
      "epoch: 108\n",
      "training loss: 604.8790668158139\n",
      "validation loss: 866.4715548882766\n",
      "epoch: 109\n",
      "training loss: 603.0805368382844\n",
      "validation loss: 864.2743417958927\n",
      "epoch: 110\n",
      "training loss: 601.3778873120355\n",
      "validation loss: 861.9888508473127\n",
      "epoch: 111\n",
      "training loss: 599.4704258336416\n",
      "validation loss: 859.8850710276284\n",
      "epoch: 112\n",
      "training loss: 597.720323166152\n",
      "validation loss: 857.7476378254055\n",
      "epoch: 113\n",
      "training loss: 595.9217859732283\n",
      "validation loss: 855.51910769302\n",
      "epoch: 114\n",
      "training loss: 594.1729603736202\n",
      "validation loss: 853.2838346893923\n",
      "epoch: 115\n",
      "training loss: 592.2164070569509\n",
      "validation loss: 851.1001091314213\n",
      "epoch: 116\n",
      "training loss: 590.4016762626591\n",
      "validation loss: 849.0308034165106\n",
      "epoch: 117\n",
      "training loss: 588.5249376533584\n",
      "validation loss: 846.778072181634\n",
      "epoch: 118\n",
      "training loss: 586.6282818543384\n",
      "validation loss: 844.7727621619914\n",
      "epoch: 119\n",
      "training loss: 584.673965917991\n",
      "validation loss: 842.455710979895\n",
      "epoch: 120\n",
      "training loss: 582.8713145195974\n",
      "validation loss: 840.4599655982659\n",
      "epoch: 121\n",
      "training loss: 580.8894563511333\n",
      "validation loss: 838.1225981895225\n",
      "epoch: 122\n",
      "training loss: 578.9389037332307\n",
      "validation loss: 836.1539783523098\n",
      "epoch: 123\n",
      "training loss: 576.9466482387345\n",
      "validation loss: 834.0651885920835\n",
      "epoch: 124\n",
      "training loss: 575.0301851516359\n",
      "validation loss: 831.917029665931\n",
      "epoch: 125\n",
      "training loss: 572.967037322678\n",
      "validation loss: 829.8440956560094\n",
      "epoch: 126\n",
      "training loss: 571.0372084304821\n",
      "validation loss: 827.8831587150404\n",
      "epoch: 127\n",
      "training loss: 568.9890338266267\n",
      "validation loss: 825.5617410104326\n",
      "epoch: 128\n",
      "training loss: 566.9945000726608\n",
      "validation loss: 823.5967362988789\n",
      "epoch: 129\n",
      "training loss: 564.8962638689244\n",
      "validation loss: 821.4595588448918\n",
      "epoch: 130\n",
      "training loss: 562.8798515865934\n",
      "validation loss: 819.458042476623\n",
      "epoch: 131\n",
      "training loss: 560.7916012668321\n",
      "validation loss: 817.5168269711692\n",
      "epoch: 132\n",
      "training loss: 558.6725149562845\n",
      "validation loss: 815.4150893556043\n",
      "epoch: 133\n",
      "training loss: 556.6644132218177\n",
      "validation loss: 813.4409531908778\n",
      "epoch: 134\n",
      "training loss: 554.5195434263298\n",
      "validation loss: 811.3971073335565\n",
      "epoch: 135\n",
      "training loss: 552.4499101573688\n",
      "validation loss: 809.674253243216\n",
      "epoch: 136\n",
      "training loss: 550.3285713200383\n",
      "validation loss: 807.6549732633866\n",
      "epoch: 137\n",
      "training loss: 548.2322334070572\n",
      "validation loss: 805.8422707729304\n",
      "epoch: 138\n",
      "training loss: 546.0590530682658\n",
      "validation loss: 804.0191894353579\n",
      "epoch: 139\n",
      "training loss: 544.0062122985495\n",
      "validation loss: 802.1680149134719\n",
      "epoch: 140\n",
      "training loss: 541.8223503772795\n",
      "validation loss: 800.3170864127926\n",
      "epoch: 141\n",
      "training loss: 539.7058700534054\n",
      "validation loss: 798.5823247910199\n",
      "epoch: 142\n",
      "training loss: 537.5633981005124\n",
      "validation loss: 796.8705267381164\n",
      "epoch: 143\n",
      "training loss: 535.4416615639548\n",
      "validation loss: 795.2240117081487\n",
      "epoch: 144\n",
      "training loss: 533.2676251536363\n",
      "validation loss: 793.6268453567214\n",
      "epoch: 145\n",
      "training loss: 531.1343836896198\n",
      "validation loss: 792.0012666709017\n",
      "epoch: 146\n",
      "training loss: 529.0132338955594\n",
      "validation loss: 790.4279481690304\n",
      "epoch: 147\n",
      "training loss: 526.8605112188612\n",
      "validation loss: 788.9021733319298\n",
      "epoch: 148\n",
      "training loss: 524.7725266443445\n",
      "validation loss: 787.4803544076124\n",
      "epoch: 149\n",
      "training loss: 522.6782446675666\n",
      "validation loss: 786.0036599873006\n",
      "epoch: 150\n",
      "training loss: 520.5128521420017\n",
      "validation loss: 784.6189498404754\n",
      "epoch: 151\n",
      "training loss: 518.4255822965501\n",
      "validation loss: 783.2649120016981\n",
      "epoch: 152\n",
      "training loss: 516.3789642731389\n",
      "validation loss: 781.9297712031205\n",
      "epoch: 153\n",
      "training loss: 514.3384960173988\n",
      "validation loss: 780.6523703800275\n",
      "epoch: 154\n",
      "training loss: 512.3015808475332\n",
      "validation loss: 779.4263681715212\n",
      "epoch: 155\n",
      "training loss: 510.31919785806025\n",
      "validation loss: 778.3000840592654\n",
      "epoch: 156\n",
      "training loss: 508.24686609814125\n",
      "validation loss: 777.1874665456761\n",
      "epoch: 157\n",
      "training loss: 506.3075973305032\n",
      "validation loss: 776.087656301511\n",
      "epoch: 158\n",
      "training loss: 504.3171435129423\n",
      "validation loss: 775.064636709287\n",
      "epoch: 159\n",
      "training loss: 502.402312854343\n",
      "validation loss: 774.0518516752308\n",
      "epoch: 160\n",
      "training loss: 500.5218841619002\n",
      "validation loss: 773.2449163045744\n",
      "epoch: 161\n",
      "training loss: 498.6157639161696\n",
      "validation loss: 772.3559422900014\n",
      "epoch: 162\n",
      "training loss: 496.73757233174706\n",
      "validation loss: 771.5918818243379\n",
      "epoch: 163\n",
      "training loss: 494.88118119434074\n",
      "validation loss: 770.8456433217599\n",
      "epoch: 164\n",
      "training loss: 493.096788357241\n",
      "validation loss: 770.2868888484242\n",
      "epoch: 165\n",
      "training loss: 491.27504960822074\n",
      "validation loss: 769.6155607414552\n",
      "epoch: 166\n",
      "training loss: 489.54810166983805\n",
      "validation loss: 768.968651163869\n",
      "epoch: 167\n",
      "training loss: 487.79038076507135\n",
      "validation loss: 768.4893592836905\n",
      "epoch: 168\n",
      "training loss: 486.1239657175874\n",
      "validation loss: 767.9083834638175\n",
      "epoch: 169\n",
      "training loss: 484.4765122355867\n",
      "validation loss: 767.6610103342675\n",
      "epoch: 170\n",
      "training loss: 482.7709922091659\n",
      "validation loss: 767.1202288131968\n",
      "epoch: 171\n",
      "training loss: 481.1837719708056\n",
      "validation loss: 766.7182826995352\n",
      "epoch: 172\n",
      "training loss: 479.60042746523044\n",
      "validation loss: 766.4948571545289\n",
      "epoch: 173\n",
      "training loss: 478.06681938318775\n",
      "validation loss: 766.2407891224651\n",
      "epoch: 174\n",
      "training loss: 476.5358690733238\n",
      "validation loss: 766.2396668371163\n",
      "epoch: 175\n",
      "training loss: 475.05687362877524\n",
      "validation loss: 766.048718685096\n",
      "epoch: 176\n",
      "training loss: 473.55212812247333\n",
      "validation loss: 765.9925177210562\n",
      "epoch: 177\n",
      "training loss: 472.1122075552356\n",
      "validation loss: 765.9977172479538\n",
      "epoch: 178\n",
      "training loss: 470.75025175872275\n",
      "validation loss: 766.0268070979804\n",
      "epoch: 179\n",
      "training loss: 469.37935055568386\n",
      "validation loss: 766.4415668235455\n",
      "epoch: 180\n",
      "training loss: 468.0094812368155\n",
      "validation loss: 766.3329765708612\n",
      "epoch: 181\n",
      "training loss: 466.7274229053045\n",
      "validation loss: 766.9158701837458\n",
      "epoch: 182\n",
      "training loss: 465.42415106360477\n",
      "validation loss: 766.7260934890094\n",
      "epoch: 183\n",
      "training loss: 464.15701691573986\n",
      "validation loss: 767.356912843073\n",
      "epoch: 184\n",
      "training loss: 462.90782072977026\n",
      "validation loss: 767.7434104962987\n",
      "epoch: 185\n",
      "training loss: 461.71249720958934\n",
      "validation loss: 768.1721784887014\n",
      "epoch: 186\n",
      "training loss: 460.5625187473282\n",
      "validation loss: 768.2545563741892\n",
      "epoch: 187\n",
      "training loss: 459.3754909376176\n",
      "validation loss: 768.7996103312418\n",
      "epoch: 188\n",
      "training loss: 458.24614933420935\n",
      "validation loss: 769.3616467308526\n",
      "epoch: 189\n",
      "training loss: 457.1349858980923\n",
      "validation loss: 769.8097368028556\n",
      "epoch: 190\n",
      "training loss: 456.0476082598109\n",
      "validation loss: 770.264381517038\n",
      "epoch: 191\n",
      "training loss: 454.9976917351847\n",
      "validation loss: 770.8075823115138\n",
      "epoch: 192\n",
      "training loss: 453.96977450008865\n",
      "validation loss: 771.3591658761256\n",
      "epoch: 193\n",
      "training loss: 452.959660390356\n",
      "validation loss: 771.3692398565471\n",
      "epoch: 194\n",
      "training loss: 451.9458766379889\n",
      "validation loss: 772.1420618795266\n",
      "epoch: 195\n",
      "training loss: 450.968942881507\n",
      "validation loss: 772.5634093501528\n",
      "epoch: 196\n",
      "training loss: 450.0318436264245\n",
      "validation loss: 773.256235277348\n",
      "epoch: 197\n",
      "training loss: 449.0909951445072\n",
      "validation loss: 773.681322312571\n",
      "epoch: 198\n",
      "training loss: 448.17798294908124\n",
      "validation loss: 774.3046762590483\n",
      "epoch: 199\n",
      "training loss: 447.2801415034403\n",
      "validation loss: 774.8777271676225\n",
      "epoch: 200\n",
      "training loss: 446.3916731482185\n",
      "validation loss: 775.3610287519707\n",
      "epoch: 201\n",
      "training loss: 445.5355396965712\n",
      "validation loss: 775.8462660873282\n",
      "epoch: 202\n",
      "training loss: 444.68484449015915\n",
      "validation loss: 776.5025766276758\n",
      "epoch: 203\n",
      "training loss: 443.8437768973437\n",
      "validation loss: 776.9959130866761\n",
      "epoch: 204\n",
      "training loss: 443.01669054969557\n",
      "validation loss: 777.4670354347126\n",
      "epoch: 205\n",
      "training loss: 442.21103567470806\n",
      "validation loss: 778.0205593799807\n",
      "epoch: 206\n",
      "training loss: 441.4164291547579\n",
      "validation loss: 778.45488210406\n",
      "epoch: 207\n",
      "training loss: 440.63443417106686\n",
      "validation loss: 779.0421789626166\n",
      "epoch: 208\n",
      "training loss: 439.8617672375068\n",
      "validation loss: 779.4914038245626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 209\n",
      "training loss: 439.09790761299803\n",
      "validation loss: 779.8941260365863\n",
      "epoch: 210\n",
      "training loss: 438.3636307674209\n",
      "validation loss: 780.4071287728415\n",
      "epoch: 211\n",
      "training loss: 437.6332324987839\n",
      "validation loss: 780.7912682285996\n",
      "epoch: 212\n",
      "training loss: 436.91104970700724\n",
      "validation loss: 781.3694581074737\n",
      "epoch: 213\n",
      "training loss: 436.2096665256905\n",
      "validation loss: 781.8577312966954\n",
      "epoch: 214\n",
      "training loss: 435.5110059214165\n",
      "validation loss: 782.2681024416981\n",
      "epoch: 215\n",
      "training loss: 434.8274497848421\n",
      "validation loss: 782.6323004535251\n",
      "epoch: 216\n",
      "training loss: 434.1612922635196\n",
      "validation loss: 783.147653534029\n",
      "epoch: 217\n",
      "training loss: 433.4968733953948\n",
      "validation loss: 783.3950113851547\n",
      "epoch: 218\n",
      "training loss: 432.85125002635516\n",
      "validation loss: 783.8371569467946\n",
      "epoch: 219\n",
      "training loss: 432.20083363360925\n",
      "validation loss: 784.0407966756586\n",
      "epoch: 220\n",
      "training loss: 431.56965320040285\n",
      "validation loss: 784.481826806496\n",
      "epoch: 221\n",
      "training loss: 430.9478858343341\n",
      "validation loss: 784.1841193006155\n",
      "epoch: 222\n",
      "training loss: 430.3022389184092\n",
      "validation loss: 784.9510355746347\n",
      "epoch: 223\n",
      "training loss: 429.67593429593916\n",
      "validation loss: 785.1187075460215\n",
      "epoch: 224\n",
      "training loss: 429.05245717271987\n",
      "validation loss: 785.5786394763742\n",
      "epoch: 225\n",
      "training loss: 428.45682647375133\n",
      "validation loss: 785.8603749968403\n",
      "epoch: 226\n",
      "training loss: 427.8483724803967\n",
      "validation loss: 786.215998588758\n",
      "epoch: 227\n",
      "training loss: 427.25413945043545\n",
      "validation loss: 786.3201492748734\n",
      "epoch: 0\n",
      "training loss: 3708.2194763519783\n",
      "validation loss: 3375.1092962869257\n",
      "epoch: 1\n",
      "training loss: 3702.749066406128\n",
      "validation loss: 3379.9259577356693\n",
      "epoch: 2\n",
      "training loss: 3697.174375223153\n",
      "validation loss: 3384.8862750483577\n",
      "epoch: 3\n",
      "training loss: 3691.4342044322125\n",
      "validation loss: 3390.022184351667\n",
      "epoch: 4\n",
      "training loss: 3685.4675719343113\n",
      "validation loss: 3395.366150493883\n",
      "epoch: 5\n",
      "training loss: 3679.213473939188\n",
      "validation loss: 3400.951049372865\n",
      "epoch: 6\n",
      "training loss: 3672.6107657693606\n",
      "validation loss: 3406.809997749029\n",
      "epoch: 7\n",
      "training loss: 3665.598168530801\n",
      "validation loss: 3412.9761220893533\n",
      "epoch: 8\n",
      "training loss: 3658.1144027651403\n",
      "validation loss: 3419.48226178906\n",
      "epoch: 9\n",
      "training loss: 3650.0984418207827\n",
      "validation loss: 3426.360606873641\n",
      "epoch: 10\n",
      "training loss: 3641.489866953074\n",
      "validation loss: 3433.642275634904\n",
      "epoch: 11\n",
      "training loss: 3632.229293408756\n",
      "validation loss: 3441.3568430239416\n",
      "epoch: 12\n",
      "training loss: 3622.258822605543\n",
      "validation loss: 3449.5318352549593\n",
      "epoch: 13\n",
      "training loss: 3611.5224610074765\n",
      "validation loss: 3458.1922090193907\n",
      "epoch: 14\n",
      "training loss: 3599.966432946082\n",
      "validation loss: 3467.359833689402\n",
      "epoch: 15\n",
      "training loss: 3587.5393046080394\n",
      "validation loss: 3477.052989991207\n",
      "epoch: 16\n",
      "training loss: 3574.1918325842785\n",
      "validation loss: 3487.2858859174617\n",
      "epoch: 17\n",
      "training loss: 3559.8764562881574\n",
      "validation loss: 3498.0681658883295\n",
      "epoch: 18\n",
      "training loss: 3544.546373053615\n",
      "validation loss: 3509.4043469458966\n",
      "epoch: 19\n",
      "training loss: 3528.1541713131232\n",
      "validation loss: 3521.293050294969\n",
      "epoch: 20\n",
      "training loss: 3510.650053166484\n",
      "validation loss: 3533.725803276989\n",
      "epoch: 21\n",
      "training loss: 3491.979753118917\n",
      "validation loss: 3546.6850649402336\n",
      "epoch: 22\n",
      "training loss: 3472.082352844861\n",
      "validation loss: 3560.1409830229472\n",
      "epoch: 23\n",
      "training loss: 3450.8882984284064\n",
      "validation loss: 3574.046236026231\n",
      "epoch: 24\n",
      "training loss: 3428.318038665814\n",
      "validation loss: 3588.328179670774\n",
      "epoch: 25\n",
      "training loss: 3404.281802093074\n",
      "validation loss: 3602.8774516949634\n",
      "epoch: 26\n",
      "training loss: 3378.6810738115496\n",
      "validation loss: 3617.5322691965393\n",
      "epoch: 27\n",
      "training loss: 3351.412240008441\n",
      "validation loss: 3632.0579778994825\n",
      "epoch: 28\n",
      "training loss: 3322.3725237561425\n",
      "validation loss: 3646.1220705620017\n",
      "epoch: 29\n",
      "training loss: 3291.4676489230155\n",
      "validation loss: 3659.265882220603\n",
      "epoch: 30\n",
      "training loss: 3258.619700834297\n",
      "validation loss: 3670.8753232472804\n",
      "epoch: 31\n",
      "training loss: 3223.77276027694\n",
      "validation loss: 3680.154015335806\n",
      "epoch: 32\n",
      "training loss: 3186.893705347861\n",
      "validation loss: 3686.1028678655493\n",
      "epoch: 33\n",
      "training loss: 3147.966648979402\n",
      "validation loss: 3687.5108773738307\n",
      "epoch: 34\n",
      "training loss: 3106.981711979468\n",
      "validation loss: 3682.963968390394\n",
      "epoch: 35\n",
      "training loss: 3063.9213273666883\n",
      "validation loss: 3670.883470500606\n",
      "epoch: 36\n",
      "training loss: 3018.748925744386\n",
      "validation loss: 3649.6134083531856\n",
      "epoch: 37\n",
      "training loss: 2971.405031207884\n",
      "validation loss: 3617.582358678944\n",
      "epoch: 38\n",
      "training loss: 2921.8141856268776\n",
      "validation loss: 3573.560783297909\n",
      "epoch: 39\n",
      "training loss: 2869.9021004711813\n",
      "validation loss: 3517.0015058750632\n",
      "epoch: 40\n",
      "training loss: 2815.615915333014\n",
      "validation loss: 3448.376920457806\n",
      "epoch: 41\n",
      "training loss: 2758.935049598136\n",
      "validation loss: 3369.3321784906584\n",
      "epoch: 42\n",
      "training loss: 2699.8649978933468\n",
      "validation loss: 3282.4379846620545\n",
      "epoch: 43\n",
      "training loss: 2638.428371363632\n",
      "validation loss: 3190.457295500333\n",
      "epoch: 44\n",
      "training loss: 2574.693672157247\n",
      "validation loss: 3095.3631953074887\n",
      "epoch: 45\n",
      "training loss: 2508.8794368400563\n",
      "validation loss: 2997.6702713138748\n",
      "epoch: 46\n",
      "training loss: 2441.520440909035\n",
      "validation loss: 2896.589065269966\n",
      "epoch: 47\n",
      "training loss: 2373.5805856363304\n",
      "validation loss: 2791.072485893156\n",
      "epoch: 48\n",
      "training loss: 2306.3483654157417\n",
      "validation loss: 2681.3861996512\n",
      "epoch: 49\n",
      "training loss: 2241.1217484603367\n",
      "validation loss: 2569.9842274240855\n",
      "epoch: 50\n",
      "training loss: 2178.8319391786504\n",
      "validation loss: 2460.6788910421383\n",
      "epoch: 51\n",
      "training loss: 2119.885901777804\n",
      "validation loss: 2356.963859094337\n",
      "epoch: 52\n",
      "training loss: 2064.3024863819114\n",
      "validation loss: 2260.9873714561786\n",
      "epoch: 53\n",
      "training loss: 2011.9236275635383\n",
      "validation loss: 2173.4941059819043\n",
      "epoch: 54\n",
      "training loss: 1962.5471728321677\n",
      "validation loss: 2094.2527493142825\n",
      "epoch: 55\n",
      "training loss: 1915.9390736711282\n",
      "validation loss: 2022.607437745522\n",
      "epoch: 56\n",
      "training loss: 1871.833590066616\n",
      "validation loss: 1957.7889796333873\n",
      "epoch: 57\n",
      "training loss: 1829.9889263059813\n",
      "validation loss: 1898.9097982737353\n",
      "epoch: 58\n",
      "training loss: 1790.2769179382417\n",
      "validation loss: 1844.910409635016\n",
      "epoch: 59\n",
      "training loss: 1752.7115129044798\n",
      "validation loss: 1794.7434857315056\n",
      "epoch: 60\n",
      "training loss: 1717.3246252137835\n",
      "validation loss: 1747.7425765609405\n",
      "epoch: 61\n",
      "training loss: 1684.0506258411476\n",
      "validation loss: 1703.7167208261876\n",
      "epoch: 62\n",
      "training loss: 1652.7398977321789\n",
      "validation loss: 1662.7027086227592\n",
      "epoch: 63\n",
      "training loss: 1623.2281116632541\n",
      "validation loss: 1624.6964534304523\n",
      "epoch: 64\n",
      "training loss: 1595.3721008138389\n",
      "validation loss: 1589.5738222285672\n",
      "epoch: 65\n",
      "training loss: 1569.051399471962\n",
      "validation loss: 1557.13221277001\n",
      "epoch: 66\n",
      "training loss: 1544.1620212431162\n",
      "validation loss: 1527.142953571107\n",
      "epoch: 67\n",
      "training loss: 1520.6116526976064\n",
      "validation loss: 1499.3819672074126\n",
      "epoch: 68\n",
      "training loss: 1498.3166964377676\n",
      "validation loss: 1473.642853204407\n",
      "epoch: 69\n",
      "training loss: 1477.2004277269914\n",
      "validation loss: 1449.7406147614968\n",
      "epoch: 70\n",
      "training loss: 1457.1917755780337\n",
      "validation loss: 1427.5112177385345\n",
      "epoch: 71\n",
      "training loss: 1438.2244717487238\n",
      "validation loss: 1406.8095822684097\n",
      "epoch: 72\n",
      "training loss: 1420.2364317778931\n",
      "validation loss: 1387.5071825081134\n",
      "epoch: 73\n",
      "training loss: 1403.1692915377073\n",
      "validation loss: 1369.4897408065808\n",
      "epoch: 74\n",
      "training loss: 1386.9680537132763\n",
      "validation loss: 1352.6551887485434\n",
      "epoch: 75\n",
      "training loss: 1371.580816106376\n",
      "validation loss: 1336.9119316283068\n",
      "epoch: 76\n",
      "training loss: 1356.958564167367\n",
      "validation loss: 1322.1773983120008\n",
      "epoch: 77\n",
      "training loss: 1343.0550165300883\n",
      "validation loss: 1308.3768398116522\n",
      "epoch: 78\n",
      "training loss: 1329.826515805095\n",
      "validation loss: 1295.4423363724186\n",
      "epoch: 79\n",
      "training loss: 1317.2319579954942\n",
      "validation loss: 1283.3119751517506\n",
      "epoch: 80\n",
      "training loss: 1305.2327528833846\n",
      "validation loss: 1271.9291647759835\n",
      "epoch: 81\n",
      "training loss: 1293.792804914562\n",
      "validation loss: 1261.242057891452\n",
      "epoch: 82\n",
      "training loss: 1282.87850020338\n",
      "validation loss: 1251.2030581026875\n",
      "epoch: 83\n",
      "training loss: 1272.4586816381309\n",
      "validation loss: 1241.7683935155226\n",
      "epoch: 84\n",
      "training loss: 1262.504592622016\n",
      "validation loss: 1232.897745290131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 85\n",
      "training loss: 1252.9897727184907\n",
      "validation loss: 1224.5539253209267\n",
      "epoch: 86\n",
      "training loss: 1243.8898964036832\n",
      "validation loss: 1216.702600957834\n",
      "epoch: 87\n",
      "training loss: 1235.1825582560232\n",
      "validation loss: 1209.3120651453878\n",
      "epoch: 88\n",
      "training loss: 1226.8470208878146\n",
      "validation loss: 1202.3530471049085\n",
      "epoch: 89\n",
      "training loss: 1218.8639511743509\n",
      "validation loss: 1195.7985531680524\n",
      "epoch: 90\n",
      "training loss: 1211.2151723479626\n",
      "validation loss: 1189.623722593835\n",
      "epoch: 91\n",
      "training loss: 1203.883453871924\n",
      "validation loss: 1183.8056822232795\n",
      "epoch: 92\n",
      "training loss: 1196.8523505923351\n",
      "validation loss: 1178.3233878886372\n",
      "epoch: 93\n",
      "training loss: 1190.1060919115066\n",
      "validation loss: 1173.1574482032577\n",
      "epoch: 94\n",
      "training loss: 1183.629514144442\n",
      "validation loss: 1168.289934564621\n",
      "epoch: 95\n",
      "training loss: 1177.4080260222818\n",
      "validation loss: 1163.7041868730544\n",
      "epoch: 96\n",
      "training loss: 1171.4275976506801\n",
      "validation loss: 1159.3846261629103\n",
      "epoch: 97\n",
      "training loss: 1165.6747653889827\n",
      "validation loss: 1155.31658348473\n",
      "epoch: 98\n",
      "training loss: 1160.1366475478644\n",
      "validation loss: 1151.486150520014\n",
      "epoch: 99\n",
      "training loss: 1154.8009676302702\n",
      "validation loss: 1147.880053256025\n",
      "epoch: 100\n",
      "training loss: 1149.6560828008983\n",
      "validation loss: 1144.4855467964906\n",
      "epoch: 101\n",
      "training loss: 1144.6910154646548\n",
      "validation loss: 1141.2903275617725\n",
      "epoch: 102\n",
      "training loss: 1139.8954855029635\n",
      "validation loss: 1138.2824587338068\n",
      "epoch: 103\n",
      "training loss: 1135.259940113554\n",
      "validation loss: 1135.4503055250473\n",
      "epoch: 104\n",
      "training loss: 1130.7755775627172\n",
      "validation loss: 1132.7824782879727\n",
      "epoch: 105\n",
      "training loss: 1126.434360717303\n",
      "validation loss: 1130.2677832217948\n",
      "epoch: 106\n",
      "training loss: 1122.2290162004012\n",
      "validation loss: 1127.895182101908\n",
      "epoch: 107\n",
      "training loss: 1118.1530156088388\n",
      "validation loss: 1125.6537637272297\n",
      "epoch: 108\n",
      "training loss: 1114.2005365663103\n",
      "validation loss: 1123.5327303793458\n",
      "epoch: 109\n",
      "training loss: 1110.3664034418846\n",
      "validation loss: 1121.5214023288604\n",
      "epoch: 110\n",
      "training loss: 1106.646010113442\n",
      "validation loss: 1119.6092422522786\n",
      "epoch: 111\n",
      "training loss: 1103.035229757973\n",
      "validation loss: 1117.7858994545525\n",
      "epoch: 112\n",
      "training loss: 1099.530318722364\n",
      "validation loss: 1116.0412713338133\n",
      "epoch: 113\n",
      "training loss: 1096.1278225012288\n",
      "validation loss: 1114.3655770315538\n",
      "epoch: 114\n",
      "training loss: 1092.8244913658957\n",
      "validation loss: 1112.7494361927706\n",
      "epoch: 115\n",
      "training loss: 1089.6172112602676\n",
      "validation loss: 1111.183944647877\n",
      "epoch: 116\n",
      "training loss: 1086.50295261564\n",
      "validation loss: 1109.66073885543\n",
      "epoch: 117\n",
      "training loss: 1083.4787364374545\n",
      "validation loss: 1108.1720420885063\n",
      "epoch: 118\n",
      "training loss: 1080.5416141538483\n",
      "validation loss: 1106.7106873508421\n",
      "epoch: 119\n",
      "training loss: 1077.6886559014315\n",
      "validation loss: 1105.270114476239\n",
      "epoch: 120\n",
      "training loss: 1074.916941456225\n",
      "validation loss: 1103.8443413846358\n",
      "epoch: 121\n",
      "training loss: 1072.2235488434799\n",
      "validation loss: 1102.4279117134388\n",
      "epoch: 122\n",
      "training loss: 1069.6055374392354\n",
      "validation loss: 1101.0158228168855\n",
      "epoch: 123\n",
      "training loss: 1067.0599246064323\n",
      "validation loss: 1099.6034393552945\n",
      "epoch: 124\n",
      "training loss: 1064.5836570570616\n",
      "validation loss: 1098.1863983861338\n",
      "epoch: 125\n",
      "training loss: 1062.1735797519223\n",
      "validation loss: 1096.7605120610124\n",
      "epoch: 126\n",
      "training loss: 1059.826405959644\n",
      "validation loss: 1095.3216737773741\n",
      "epoch: 127\n",
      "training loss: 1057.5386920232359\n",
      "validation loss: 1093.8657729894267\n",
      "epoch: 128\n",
      "training loss: 1055.3068195560722\n",
      "validation loss: 1092.3886229305194\n",
      "epoch: 129\n",
      "training loss: 1053.1269864949056\n",
      "validation loss: 1090.8859043544958\n",
      "epoch: 130\n",
      "training loss: 1050.995207029929\n",
      "validation loss: 1089.353127215325\n",
      "epoch: 131\n",
      "training loss: 1048.9073192408166\n",
      "validation loss: 1087.7856111351073\n",
      "epoch: 132\n",
      "training loss: 1046.8589985222807\n",
      "validation loss: 1086.1784847067277\n",
      "epoch: 133\n",
      "training loss: 1044.845774676408\n",
      "validation loss: 1084.5267032395486\n",
      "epoch: 134\n",
      "training loss: 1042.8630508491276\n",
      "validation loss: 1082.8250845213565\n",
      "epoch: 135\n",
      "training loss: 1040.906123177566\n",
      "validation loss: 1081.0683625078339\n",
      "epoch: 136\n",
      "training loss: 1038.970200946049\n",
      "validation loss: 1079.2512594789066\n",
      "epoch: 137\n",
      "training loss: 1037.050428087271\n",
      "validation loss: 1077.3685780006685\n",
      "epoch: 138\n",
      "training loss: 1035.1419079156171\n",
      "validation loss: 1075.4153148633459\n",
      "epoch: 139\n",
      "training loss: 1033.239733984056\n",
      "validation loss: 1073.3867998755243\n",
      "epoch: 140\n",
      "training loss: 1031.33903087605\n",
      "validation loss: 1071.278862804396\n",
      "epoch: 141\n",
      "training loss: 1029.4350095286295\n",
      "validation loss: 1069.088031640494\n",
      "epoch: 142\n",
      "training loss: 1027.5230422284721\n",
      "validation loss: 1066.8117644473614\n",
      "epoch: 143\n",
      "training loss: 1025.598762530279\n",
      "validation loss: 1064.4487149656109\n",
      "epoch: 144\n",
      "training loss: 1023.6581946832526\n",
      "validation loss: 1061.9990284401972\n",
      "epoch: 145\n",
      "training loss: 1021.6979152338785\n",
      "validation loss: 1059.4646583843323\n",
      "epoch: 146\n",
      "training loss: 1019.7152456981254\n",
      "validation loss: 1056.8496868715883\n",
      "epoch: 147\n",
      "training loss: 1017.7084689584116\n",
      "validation loss: 1054.1606205307596\n",
      "epoch: 148\n",
      "training loss: 1015.677052986199\n",
      "validation loss: 1051.4066225209278\n",
      "epoch: 149\n",
      "training loss: 1013.6218539574905\n",
      "validation loss: 1048.5996293675653\n",
      "epoch: 150\n",
      "training loss: 1011.5452585365139\n",
      "validation loss: 1045.754294114128\n",
      "epoch: 151\n",
      "training loss: 1009.4512161156299\n",
      "validation loss: 1042.88769865909\n",
      "epoch: 152\n",
      "training loss: 1007.3451131328543\n",
      "validation loss: 1040.0187937565693\n",
      "epoch: 153\n",
      "training loss: 1005.2334611228247\n",
      "validation loss: 1037.1675587093202\n",
      "epoch: 154\n",
      "training loss: 1003.1234095881215\n",
      "validation loss: 1034.3539233762053\n",
      "epoch: 155\n",
      "training loss: 1001.0221429765126\n",
      "validation loss: 1031.5965543403968\n",
      "epoch: 156\n",
      "training loss: 998.936258957651\n",
      "validation loss: 1028.9116585857537\n",
      "epoch: 157\n",
      "training loss: 996.8712382413268\n",
      "validation loss: 1026.3119818189266\n",
      "epoch: 158\n",
      "training loss: 994.8310988407926\n",
      "validation loss: 1023.8061598489863\n",
      "epoch: 159\n",
      "training loss: 992.8182831046116\n",
      "validation loss: 1021.3985195257331\n",
      "epoch: 160\n",
      "training loss: 990.8337680583315\n",
      "validation loss: 1019.0893371018308\n",
      "epoch: 161\n",
      "training loss: 988.8773400018885\n",
      "validation loss: 1016.8754740747524\n",
      "epoch: 162\n",
      "training loss: 986.9479498100734\n",
      "validation loss: 1014.7512499752258\n",
      "epoch: 163\n",
      "training loss: 985.0440693974913\n",
      "validation loss: 1012.7093921913697\n",
      "epoch: 164\n",
      "training loss: 983.163993320505\n",
      "validation loss: 1010.7419224554907\n",
      "epoch: 165\n",
      "training loss: 981.3060588799639\n",
      "validation loss: 1008.8408838458023\n",
      "epoch: 166\n",
      "training loss: 979.4687826487767\n",
      "validation loss: 1006.9988633050784\n",
      "epoch: 167\n",
      "training loss: 977.6509263872331\n",
      "validation loss: 1005.2093086517609\n",
      "epoch: 168\n",
      "training loss: 975.8515112139568\n",
      "validation loss: 1003.4666686379287\n",
      "epoch: 169\n",
      "training loss: 974.069798550319\n",
      "validation loss: 1001.7663989499113\n",
      "epoch: 170\n",
      "training loss: 972.3052529041142\n",
      "validation loss: 1000.1048792657583\n",
      "epoch: 171\n",
      "training loss: 970.5574973121945\n",
      "validation loss: 998.4792810829746\n",
      "epoch: 172\n",
      "training loss: 968.8262684772723\n",
      "validation loss: 996.8874171328978\n",
      "epoch: 173\n",
      "training loss: 967.1113757607674\n",
      "validation loss: 995.3275937449006\n",
      "epoch: 174\n",
      "training loss: 965.4126662191679\n",
      "validation loss: 993.7984791847358\n",
      "epoch: 175\n",
      "training loss: 963.7299965949594\n",
      "validation loss: 992.2989944467284\n",
      "epoch: 176\n",
      "training loss: 962.0632123685017\n",
      "validation loss: 990.8282282910174\n",
      "epoch: 177\n",
      "training loss: 960.412133466634\n",
      "validation loss: 989.3853752439563\n",
      "epoch: 178\n",
      "training loss: 958.7765458953419\n",
      "validation loss: 987.9696934861439\n",
      "epoch: 179\n",
      "training loss: 957.1561983599696\n",
      "validation loss: 986.580478713891\n",
      "epoch: 180\n",
      "training loss: 955.5508028328553\n",
      "validation loss: 985.2170499011862\n",
      "epoch: 181\n",
      "training loss: 953.960038013621\n",
      "validation loss: 983.8787431868989\n",
      "epoch: 182\n",
      "training loss: 952.3835546904421\n",
      "validation loss: 982.5649106852677\n",
      "epoch: 183\n",
      "training loss: 950.820982134376\n",
      "validation loss: 981.2749217208182\n",
      "epoch: 184\n",
      "training loss: 949.2719348211018\n",
      "validation loss: 980.0081647081774\n",
      "epoch: 185\n",
      "training loss: 947.7360189515229\n",
      "validation loss: 978.7640485515218\n",
      "epoch: 186\n",
      "training loss: 946.212838413682\n",
      "validation loss: 977.5420029780779\n",
      "epoch: 187\n",
      "training loss: 944.701999978143\n",
      "validation loss: 976.3414776245336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 188\n",
      "training loss: 943.2031176389236\n",
      "validation loss: 975.1619399666283\n",
      "epoch: 189\n",
      "training loss: 941.7158160999101\n",
      "validation loss: 974.0028723382093\n",
      "epoch: 190\n",
      "training loss: 940.239733464663\n",
      "validation loss: 972.8637683521166\n",
      "epoch: 191\n",
      "training loss: 938.7745232205189\n",
      "validation loss: 971.7441290384319\n",
      "epoch: 192\n",
      "training loss: 937.319855621869\n",
      "validation loss: 970.6434589801808\n",
      "epoch: 193\n",
      "training loss: 935.8754185781942\n",
      "validation loss: 969.5612626719536\n",
      "epoch: 194\n",
      "training loss: 934.4409181448443\n",
      "validation loss: 968.4970412669127\n",
      "epoch: 195\n",
      "training loss: 933.0160787025185\n",
      "validation loss: 967.4502898210947\n",
      "epoch: 196\n",
      "training loss: 931.6006428976526\n",
      "validation loss: 966.4204950954735\n",
      "epoch: 197\n",
      "training loss: 930.1943714022914\n",
      "validation loss: 965.4071339377188\n",
      "epoch: 198\n",
      "training loss: 928.7970425395305\n",
      "validation loss: 964.4096722368482\n",
      "epoch: 199\n",
      "training loss: 927.4084518098147\n",
      "validation loss: 963.4275644239042\n",
      "epoch: 200\n",
      "training loss: 926.028411344386\n",
      "validation loss: 962.4602534787674\n",
      "epoch: 201\n",
      "training loss: 924.6567493050109\n",
      "validation loss: 961.507171395607\n",
      "epoch: 202\n",
      "training loss: 923.2933092435202\n",
      "validation loss: 960.5677400557458\n",
      "epoch: 203\n",
      "training loss: 921.9379494305247\n",
      "validation loss: 959.6413724556355\n",
      "epoch: 204\n",
      "training loss: 920.5905421596308\n",
      "validation loss: 958.7274742382242\n",
      "epoch: 205\n",
      "training loss: 919.2509730313835\n",
      "validation loss: 957.8254454775691\n",
      "epoch: 206\n",
      "training loss: 917.9191402197957\n",
      "validation loss: 956.9346826685955\n",
      "epoch: 207\n",
      "training loss: 916.5949537235289\n",
      "validation loss: 956.0545808761956\n",
      "epoch: 208\n",
      "training loss: 915.2783346033867\n",
      "validation loss: 955.1845360001681\n",
      "epoch: 209\n",
      "training loss: 913.9692142077258\n",
      "validation loss: 954.32394711486\n",
      "epoch: 210\n",
      "training loss: 912.6675333874905\n",
      "validation loss: 953.4722188446749\n",
      "epoch: 211\n",
      "training loss: 911.3732417028567\n",
      "validation loss: 952.6287637390144\n",
      "epoch: 212\n",
      "training loss: 910.0862966237878\n",
      "validation loss: 951.7930046126622\n",
      "epoch: 213\n",
      "training loss: 908.8066627271727\n",
      "validation loss: 950.9643768202312\n",
      "epoch: 214\n",
      "training loss: 907.5343108935572\n",
      "validation loss: 950.1423304360587\n",
      "epoch: 215\n",
      "training loss: 906.2692175067939\n",
      "validation loss: 949.3263323139076\n",
      "epoch: 216\n",
      "training loss: 905.0113636601843\n",
      "validation loss: 948.5158680039668\n",
      "epoch: 217\n",
      "training loss: 903.7607343728704\n",
      "validation loss: 947.710443507983\n",
      "epoch: 218\n",
      "training loss: 902.5173178203222\n",
      "validation loss: 946.9095868568011\n",
      "epoch: 219\n",
      "training loss: 901.2811045827854\n",
      "validation loss: 946.1128494981275\n",
      "epoch: 220\n",
      "training loss: 900.0520869154583\n",
      "validation loss: 945.319807485892\n",
      "epoch: 221\n",
      "training loss: 898.8302580440097\n",
      "validation loss: 944.5300624660908\n",
      "epoch: 222\n",
      "training loss: 897.6156114887799\n",
      "validation loss: 943.7432424573886\n",
      "epoch: 223\n",
      "training loss: 896.4081404206732\n",
      "validation loss: 942.9590024279894\n",
      "epoch: 224\n",
      "training loss: 895.2078370513461\n",
      "validation loss: 942.1770246732556\n",
      "epoch: 225\n",
      "training loss: 894.0146920598124\n",
      "validation loss: 941.3970190012571\n",
      "epoch: 226\n",
      "training loss: 892.8286940570763\n",
      "validation loss: 940.6187227358135\n",
      "epoch: 227\n",
      "training loss: 891.6498290898277\n",
      "validation loss: 939.8419005485915\n",
      "epoch: 228\n",
      "training loss: 890.4780801836588\n",
      "validation loss: 939.0663441335028\n",
      "epoch: 229\n",
      "training loss: 889.3134269256452\n",
      "validation loss: 938.2918717379085\n",
      "epoch: 230\n",
      "training loss: 888.1558450855329\n",
      "validation loss: 937.5183275660806\n",
      "epoch: 231\n",
      "training loss: 887.0053062741747\n",
      "validation loss: 936.745581070955\n",
      "epoch: 232\n",
      "training loss: 885.8617776372829\n",
      "validation loss: 935.97352615049\n",
      "epoch: 233\n",
      "training loss: 884.7252215820032\n",
      "validation loss: 935.2020802649317\n",
      "epoch: 234\n",
      "training loss: 883.5955955333128\n",
      "validation loss: 934.4311834910687\n",
      "epoch: 235\n",
      "training loss: 882.4728517167482\n",
      "validation loss: 933.660797529094\n",
      "epoch: 236\n",
      "training loss: 881.3569369635378\n",
      "validation loss: 932.8909046771116\n",
      "epoch: 237\n",
      "training loss: 880.2477925338044\n",
      "validation loss: 932.1215067876095\n",
      "epoch: 238\n",
      "training loss: 879.1453539531327\n",
      "validation loss: 931.3526242194297\n",
      "epoch: 239\n",
      "training loss: 878.0495508574585\n",
      "validation loss: 930.5842947979539\n",
      "epoch: 240\n",
      "training loss: 876.9603068409076\n",
      "validation loss: 929.8165727953941\n",
      "epoch: 241\n",
      "training loss: 875.8775393009106\n",
      "validation loss: 929.0495279422984\n",
      "epoch: 242\n",
      "training loss: 874.8011592745943\n",
      "validation loss: 928.2832444806518\n",
      "epoch: 243\n",
      "training loss: 873.7310712601337\n",
      "validation loss: 927.5178202683254\n",
      "epoch: 244\n",
      "training loss: 872.6671730163873\n",
      "validation loss: 926.753365944115\n",
      "epoch: 245\n",
      "training loss: 871.6093553337398\n",
      "validation loss: 925.9900041622374\n",
      "epoch: 246\n",
      "training loss: 870.5575017686194\n",
      "validation loss: 925.2278689049522\n",
      "epoch: 247\n",
      "training loss: 869.5114883336228\n",
      "validation loss: 924.4671048819451\n",
      "epoch: 248\n",
      "training loss: 868.4711831345517\n",
      "validation loss: 923.7078670253227\n",
      "epoch: 249\n",
      "training loss: 867.4364459449292\n",
      "validation loss: 922.9503200894823\n",
      "epoch: 250\n",
      "training loss: 866.4071277076896\n",
      "validation loss: 922.1946383658096\n",
      "epoch: 251\n",
      "training loss: 865.3830699527078\n",
      "validation loss: 921.4410055231681\n",
      "epoch: 252\n",
      "training loss: 864.3641041176512\n",
      "validation loss: 920.6896145864305\n",
      "epoch: 253\n",
      "training loss: 863.3500507582231\n",
      "validation loss: 919.9406680670318\n",
      "epoch: 254\n",
      "training loss: 862.3407186322735\n",
      "validation loss: 919.1943782616222\n",
      "epoch: 255\n",
      "training loss: 861.3359036403969\n",
      "validation loss: 918.450967737528\n",
      "epoch: 256\n",
      "training loss: 860.3353876035504\n",
      "validation loss: 917.7106700268969\n",
      "epoch: 257\n",
      "training loss: 859.3389368558667\n",
      "validation loss: 916.9737305552347\n",
      "epoch: 258\n",
      "training loss: 858.3463006282574\n",
      "validation loss: 916.2404078345953\n",
      "epoch: 259\n",
      "training loss: 857.3572091956032\n",
      "validation loss: 915.5109749571092\n",
      "epoch: 260\n",
      "training loss: 856.3713717574167\n",
      "validation loss: 914.785721430897\n",
      "epoch: 261\n",
      "training loss: 855.3884740189468\n",
      "validation loss: 914.0649554078518\n",
      "epoch: 262\n",
      "training loss: 854.4081754370785\n",
      "validation loss: 913.3490063613626\n",
      "epoch: 263\n",
      "training loss: 853.4301060933421\n",
      "validation loss: 912.6382282818208\n",
      "epoch: 264\n",
      "training loss: 852.4538631555931\n",
      "validation loss: 911.9330034686924\n",
      "epoch: 265\n",
      "training loss: 851.4790068912664\n",
      "validation loss: 911.2337470097831\n",
      "epoch: 266\n",
      "training loss: 850.5050561999723\n",
      "validation loss: 910.5409120506519\n",
      "epoch: 267\n",
      "training loss: 849.531483643547\n",
      "validation loss: 909.854995968949\n",
      "epoch: 268\n",
      "training loss: 848.5577099704614\n",
      "validation loss: 909.1765475781577\n",
      "epoch: 269\n",
      "training loss: 847.5830981629226\n",
      "validation loss: 908.5061754899732\n",
      "epoch: 270\n",
      "training loss: 846.606947085055\n",
      "validation loss: 907.8445577598843\n",
      "epoch: 271\n",
      "training loss: 845.6284848874927\n",
      "validation loss: 907.1924529192171\n",
      "epoch: 272\n",
      "training loss: 844.6468624387742\n",
      "validation loss: 906.5507124479439\n",
      "epoch: 273\n",
      "training loss: 843.6611472216691\n",
      "validation loss: 905.9202946493472\n",
      "epoch: 274\n",
      "training loss: 842.670318371018\n",
      "validation loss: 905.3022797257377\n",
      "epoch: 275\n",
      "training loss: 841.6732638590187\n",
      "validation loss: 904.6978855893537\n",
      "epoch: 276\n",
      "training loss: 840.6687812730509\n",
      "validation loss: 904.1084835274817\n",
      "epoch: 277\n",
      "training loss: 839.6555841895753\n",
      "validation loss: 903.535612216268\n",
      "epoch: 278\n",
      "training loss: 838.6323168102492\n",
      "validation loss: 902.980987676123\n",
      "epoch: 279\n",
      "training loss: 837.5975802274689\n",
      "validation loss: 902.4465055217779\n",
      "epoch: 280\n",
      "training loss: 836.5499742718351\n",
      "validation loss: 901.9342302602644\n",
      "epoch: 281\n",
      "training loss: 835.4881590741597\n",
      "validation loss: 901.4463645099349\n",
      "epoch: 282\n",
      "training loss: 834.4109397886951\n",
      "validation loss: 900.9851891335973\n",
      "epoch: 283\n",
      "training loss: 833.3173757554515\n",
      "validation loss: 900.5529640101787\n",
      "epoch: 284\n",
      "training loss: 832.2069110948125\n",
      "validation loss: 900.151779570366\n",
      "epoch: 285\n",
      "training loss: 831.0795170212118\n",
      "validation loss: 899.7833527880747\n",
      "epoch: 286\n",
      "training loss: 829.9358276155358\n",
      "validation loss: 899.4487696613126\n",
      "epoch: 287\n",
      "training loss: 828.7772424669957\n",
      "validation loss: 899.1481902287484\n",
      "epoch: 288\n",
      "training loss: 827.6059651553761\n",
      "validation loss: 898.8805507182255\n",
      "epoch: 289\n",
      "training loss: 826.4249503050031\n",
      "validation loss: 898.6433160814208\n",
      "epoch: 290\n",
      "training loss: 825.2377465391061\n",
      "validation loss: 898.4323471659062\n",
      "epoch: 291\n",
      "training loss: 824.0482463300148\n",
      "validation loss: 898.2419414096547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 292\n",
      "training loss: 822.8603792818883\n",
      "validation loss: 898.0650793804879\n",
      "epoch: 293\n",
      "training loss: 821.6778026638812\n",
      "validation loss: 897.8938655269886\n",
      "epoch: 294\n",
      "training loss: 820.5036442777064\n",
      "validation loss: 897.7201034483027\n",
      "epoch: 295\n",
      "training loss: 819.3403372809331\n",
      "validation loss: 897.5359114274388\n",
      "epoch: 296\n",
      "training loss: 818.1895611731895\n",
      "validation loss: 897.3342759943686\n",
      "epoch: 297\n",
      "training loss: 817.0522779795991\n",
      "validation loss: 897.1094617347028\n",
      "epoch: 298\n",
      "training loss: 815.9288359012353\n",
      "validation loss: 896.8572348284948\n",
      "epoch: 299\n",
      "training loss: 814.8191072994006\n",
      "validation loss: 896.5749007142167\n",
      "epoch: 300\n",
      "training loss: 813.72263188054\n",
      "validation loss: 896.2611896199446\n",
      "epoch: 301\n",
      "training loss: 812.6387449601624\n",
      "validation loss: 895.9160409686315\n",
      "epoch: 302\n",
      "training loss: 811.5666804607877\n",
      "validation loss: 895.5403393553551\n",
      "epoch: 303\n",
      "training loss: 810.5056461584346\n",
      "validation loss: 895.1356457865801\n",
      "epoch: 304\n",
      "training loss: 809.4548736836907\n",
      "validation loss: 894.7039541161861\n",
      "epoch: 305\n",
      "training loss: 808.4136481486468\n",
      "validation loss: 894.2474888643892\n",
      "epoch: 306\n",
      "training loss: 807.3813227752454\n",
      "validation loss: 893.7685495848951\n",
      "epoch: 307\n",
      "training loss: 806.3573233513735\n",
      "validation loss: 893.2693995478717\n",
      "epoch: 308\n",
      "training loss: 805.3411463571855\n",
      "validation loss: 892.7521924486939\n",
      "epoch: 309\n",
      "training loss: 804.332353568146\n",
      "validation loss: 892.218929319516\n",
      "epoch: 310\n",
      "training loss: 803.3305650424437\n",
      "validation loss: 891.6714379131669\n",
      "epoch: 311\n",
      "training loss: 802.3354517019689\n",
      "validation loss: 891.1113677945584\n",
      "epoch: 312\n",
      "training loss: 801.3467282133253\n",
      "validation loss: 890.5401956818042\n",
      "epoch: 313\n",
      "training loss: 800.3641465350112\n",
      "validation loss: 889.9592369014961\n",
      "epoch: 314\n",
      "training loss: 799.387490279213\n",
      "validation loss: 889.369659990753\n",
      "epoch: 315\n",
      "training loss: 798.4165699057402\n",
      "validation loss: 888.7725024274525\n",
      "epoch: 316\n",
      "training loss: 797.4512186931344\n",
      "validation loss: 888.1686861949756\n",
      "epoch: 317\n",
      "training loss: 796.4912893970634\n",
      "validation loss: 887.5590324146119\n",
      "epoch: 318\n",
      "training loss: 795.5366514942795\n",
      "validation loss: 886.9442746445303\n",
      "epoch: 319\n",
      "training loss: 794.5871889119862\n",
      "validation loss: 886.3250706866104\n",
      "epoch: 320\n",
      "training loss: 793.6427981511554\n",
      "validation loss: 885.7020128945729\n",
      "epoch: 321\n",
      "training loss: 792.703386724258\n",
      "validation loss: 885.0756370656995\n",
      "epoch: 322\n",
      "training loss: 791.7688718406658\n",
      "validation loss: 884.4464300445026\n",
      "epoch: 323\n",
      "training loss: 790.8391792853032\n",
      "validation loss: 883.8148361852137\n",
      "epoch: 324\n",
      "training loss: 789.9142424472847\n",
      "validation loss: 883.1812628214517\n",
      "epoch: 325\n",
      "training loss: 788.9940014649559\n",
      "validation loss: 882.5460848832788\n",
      "epoch: 326\n",
      "training loss: 788.0784024618891\n",
      "validation loss: 881.909648788799\n",
      "epoch: 327\n",
      "training loss: 787.1673968550416\n",
      "validation loss: 881.2722757225322\n",
      "epoch: 328\n",
      "training loss: 786.2609407215889\n",
      "validation loss: 880.6342643978162\n",
      "epoch: 329\n",
      "training loss: 785.3589942150722\n",
      "validation loss: 879.9958933864639\n",
      "epoch: 330\n",
      "training loss: 784.4615210246102\n",
      "validation loss: 879.3574230862828\n",
      "epoch: 331\n",
      "training loss: 783.5684878731989\n",
      "validation loss: 878.7190973860481\n",
      "epoch: 332\n",
      "training loss: 782.6798640527139\n",
      "validation loss: 878.0811450780461\n",
      "epoch: 333\n",
      "training loss: 781.7956209942625\n",
      "validation loss: 877.4437810602697\n",
      "epoch: 334\n",
      "training loss: 780.9157318731545\n",
      "validation loss: 876.8072073635689\n",
      "epoch: 335\n",
      "training loss: 780.0401712480523\n",
      "validation loss: 876.1716140333785\n",
      "epoch: 336\n",
      "training loss: 779.1689147339306\n",
      "validation loss: 875.5371798908566\n",
      "epoch: 337\n",
      "training loss: 778.3019387083802\n",
      "validation loss: 874.9040731942678\n",
      "epoch: 338\n",
      "training loss: 777.4392200506012\n",
      "validation loss: 874.2724522180707\n",
      "epoch: 339\n",
      "training loss: 776.580735912192\n",
      "validation loss: 873.6424657643082\n",
      "epoch: 340\n",
      "training loss: 775.7264635185807\n",
      "validation loss: 873.0142536184987\n",
      "epoch: 341\n",
      "training loss: 774.8763799997\n",
      "validation loss: 872.3879469601842\n",
      "epoch: 342\n",
      "training loss: 774.0304622482932\n",
      "validation loss: 871.76366873655\n",
      "epoch: 343\n",
      "training loss: 773.1886868040501\n",
      "validation loss: 871.1415340060661\n",
      "epoch: 344\n",
      "training loss: 772.3510297616482\n",
      "validation loss: 870.5216502578588\n",
      "epoch: 345\n",
      "training loss: 771.5174667006806\n",
      "validation loss: 869.904117711466\n",
      "epoch: 346\n",
      "training loss: 770.6879726354047\n",
      "validation loss: 869.2890296007657\n",
      "epoch: 347\n",
      "training loss: 769.8625219822366\n",
      "validation loss: 868.6764724451232\n",
      "epoch: 348\n",
      "training loss: 769.0410885429546\n",
      "validation loss: 868.0665263102209\n",
      "epoch: 349\n",
      "training loss: 768.2236455016008\n",
      "validation loss: 867.4592650605272\n",
      "epoch: 350\n",
      "training loss: 767.4101654331613\n",
      "validation loss: 866.8547566049841\n",
      "epoch: 351\n",
      "training loss: 766.6006203221764\n",
      "validation loss: 866.2530631371644\n",
      "epoch: 352\n",
      "training loss: 765.7949815895282\n",
      "validation loss: 865.6542413709024\n",
      "epoch: 353\n",
      "training loss: 764.9932201257428\n",
      "validation loss: 865.0583427721785\n",
      "epoch: 354\n",
      "training loss: 764.195306329253\n",
      "validation loss: 864.4654137878669\n",
      "epoch: 355\n",
      "training loss: 763.4012101481674\n",
      "validation loss: 863.8754960717642\n",
      "epoch: 356\n",
      "training loss: 762.6109011242138\n",
      "validation loss: 863.2886267081543\n",
      "epoch: 357\n",
      "training loss: 761.8243484376417\n",
      "validation loss: 862.7048384329499\n",
      "epoch: 358\n",
      "training loss: 761.0415209520276\n",
      "validation loss: 862.1241598522481\n",
      "epoch: 359\n",
      "training loss: 760.2623872580835\n",
      "validation loss: 861.5466156578923\n",
      "epoch: 360\n",
      "training loss: 759.4869157157634\n",
      "validation loss: 860.9722268393788\n",
      "epoch: 361\n",
      "training loss: 758.7150744941752\n",
      "validation loss: 860.4010108912129\n",
      "epoch: 362\n",
      "training loss: 757.9468316090172\n",
      "validation loss: 859.8329820146118\n",
      "epoch: 363\n",
      "training loss: 757.1821549574868\n",
      "validation loss: 859.268151312324\n",
      "epoch: 364\n",
      "training loss: 756.4210123508018\n",
      "validation loss: 858.7065269752989\n",
      "epoch: 365\n",
      "training loss: 755.6633715446354\n",
      "validation loss: 858.1481144600449\n",
      "epoch: 366\n",
      "training loss: 754.9092002678713\n",
      "validation loss: 857.5929166557269\n",
      "epoch: 367\n",
      "training loss: 754.1584662501258\n",
      "validation loss: 857.0409340404063\n",
      "epoch: 368\n",
      "training loss: 753.4111372484584\n",
      "validation loss: 856.4921648262347\n",
      "epoch: 369\n",
      "training loss: 752.66718107361\n",
      "validation loss: 855.9466050938722\n",
      "epoch: 370\n",
      "training loss: 751.9265656159837\n",
      "validation loss: 855.4042489168124\n",
      "epoch: 371\n",
      "training loss: 751.1892588714534\n",
      "validation loss: 854.8650884766334\n",
      "epoch: 372\n",
      "training loss: 750.4552289669401\n",
      "validation loss: 854.3291141704226\n",
      "epoch: 373\n",
      "training loss: 749.7244441855936\n",
      "validation loss: 853.7963147116865\n",
      "epoch: 374\n",
      "training loss: 748.9968729913442\n",
      "validation loss: 853.2666772260329\n",
      "epoch: 375\n",
      "training loss: 748.2724840525436\n",
      "validation loss: 852.7401873427348\n",
      "epoch: 376\n",
      "training loss: 747.5512462644313\n",
      "validation loss: 852.2168292830851\n",
      "epoch: 377\n",
      "training loss: 746.8331287701781\n",
      "validation loss: 851.6965859461914\n",
      "epoch: 378\n",
      "training loss: 746.1181009803214\n",
      "validation loss: 851.1794389926093\n",
      "epoch: 379\n",
      "training loss: 745.4061325904639\n",
      "validation loss: 850.6653689260022\n",
      "epoch: 380\n",
      "training loss: 744.697193597162\n",
      "validation loss: 850.1543551728415\n",
      "epoch: 381\n",
      "training loss: 743.9912543119992\n",
      "validation loss: 849.64637616004\n",
      "epoch: 382\n",
      "training loss: 743.2882853738763\n",
      "validation loss: 849.1414093903421\n",
      "epoch: 383\n",
      "training loss: 742.588257759589\n",
      "validation loss: 848.6394315152626\n",
      "epoch: 384\n",
      "training loss: 741.8911427927933\n",
      "validation loss: 848.1404184053717\n",
      "epoch: 385\n",
      "training loss: 741.1969121514678\n",
      "validation loss: 847.6443452177472\n",
      "epoch: 386\n",
      "training loss: 740.5055378739952\n",
      "validation loss: 847.1511864604557\n",
      "epoch: 387\n",
      "training loss: 739.8169923639847\n",
      "validation loss: 846.6609160539684\n",
      "epoch: 388\n",
      "training loss: 739.1312483939495\n",
      "validation loss: 846.1735073894663\n",
      "epoch: 389\n",
      "training loss: 738.4482791079546\n",
      "validation loss: 845.6889333840285\n",
      "epoch: 390\n",
      "training loss: 737.7680580233409\n",
      "validation loss: 845.2071665327429\n",
      "epoch: 391\n",
      "training loss: 737.0905590316137\n",
      "validation loss: 844.728178957802\n",
      "epoch: 392\n",
      "training loss: 736.4157563985867\n",
      "validation loss: 844.2519424546824\n",
      "epoch: 393\n",
      "training loss: 735.7436247638626\n",
      "validation loss: 843.7784285355178\n",
      "epoch: 394\n",
      "training loss: 735.0741391397123\n",
      "validation loss: 843.3076084697988\n",
      "epoch: 395\n",
      "training loss: 734.4072749094238\n",
      "validation loss: 842.8394533225353\n",
      "epoch: 396\n",
      "training loss: 733.7430078251776\n",
      "validation loss: 842.3739339900332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 397\n",
      "training loss: 733.0813140054988\n",
      "validation loss: 841.9110212334251\n",
      "epoch: 398\n",
      "training loss: 732.4221699323359\n",
      "validation loss: 841.4506857101162\n",
      "epoch: 399\n",
      "training loss: 731.7655524478163\n",
      "validation loss: 840.9928980032877\n",
      "epoch: 400\n",
      "training loss: 731.1114387507154\n",
      "validation loss: 840.5376286496053\n",
      "epoch: 401\n",
      "training loss: 730.4598063926844\n",
      "validation loss: 840.0848481652764\n",
      "epoch: 402\n",
      "training loss: 729.8106332742724\n",
      "validation loss: 839.6345270705954\n",
      "epoch: 403\n",
      "training loss: 729.1638976407845\n",
      "validation loss: 839.1866359131063\n",
      "epoch: 404\n",
      "training loss: 728.519578078005\n",
      "validation loss: 838.7411452895121\n",
      "epoch: 405\n",
      "training loss: 727.8776535078274\n",
      "validation loss: 838.2980258664501\n",
      "epoch: 406\n",
      "training loss: 727.2381031838146\n",
      "validation loss: 837.857248400247\n",
      "epoch: 407\n",
      "training loss: 726.6009066867266\n",
      "validation loss: 837.4187837557562\n",
      "epoch: 408\n",
      "training loss: 725.9660439200381\n",
      "validation loss: 836.9826029243823\n",
      "epoch: 409\n",
      "training loss: 725.3334951054758\n",
      "validation loss: 836.5486770413709\n",
      "epoch: 410\n",
      "training loss: 724.7032407785922\n",
      "validation loss: 836.116977402458\n",
      "epoch: 411\n",
      "training loss: 724.0752617844005\n",
      "validation loss: 835.6874754799393\n",
      "epoch: 412\n",
      "training loss: 723.4495392730834\n",
      "validation loss: 835.2601429382262\n",
      "epoch: 413\n",
      "training loss: 722.8260546957896\n",
      "validation loss: 834.8349516489432\n",
      "epoch: 414\n",
      "training loss: 722.2047898005268\n",
      "validation loss: 834.4118737056081\n",
      "epoch: 415\n",
      "training loss: 721.5857266281569\n",
      "validation loss: 833.9908814379271\n",
      "epoch: 416\n",
      "training loss: 720.9688475084953\n",
      "validation loss: 833.5719474257287\n",
      "epoch: 417\n",
      "training loss: 720.3541350565146\n",
      "validation loss: 833.1550445125574\n",
      "epoch: 418\n",
      "training loss: 719.7415721686459\n",
      "validation loss: 832.7401458189227\n",
      "epoch: 419\n",
      "training loss: 719.131142019172\n",
      "validation loss: 832.327224755207\n",
      "epoch: 420\n",
      "training loss: 718.5228280566994\n",
      "validation loss: 831.9162550342198\n",
      "epoch: 421\n",
      "training loss: 717.9166140006965\n",
      "validation loss: 831.5072106833793\n",
      "epoch: 422\n",
      "training loss: 717.3124838380832\n",
      "validation loss: 831.1000660564955\n",
      "epoch: 423\n",
      "training loss: 716.7104218198452\n",
      "validation loss: 830.6947958451267\n",
      "epoch: 424\n",
      "training loss: 716.1104124576607\n",
      "validation loss: 830.2913750894694\n",
      "epoch: 425\n",
      "training loss: 715.5124405205052\n",
      "validation loss: 829.8897791887493\n",
      "epoch: 426\n",
      "training loss: 714.9164910312144\n",
      "validation loss: 829.4899839110662\n",
      "epoch: 427\n",
      "training loss: 714.3225492629749\n",
      "validation loss: 829.0919654026552\n",
      "epoch: 428\n",
      "training loss: 713.7306007357158\n",
      "validation loss: 828.6957001965233\n",
      "epoch: 429\n",
      "training loss: 713.1406312123718\n",
      "validation loss: 828.3011652204216\n",
      "epoch: 430\n",
      "training loss: 712.5526266949983\n",
      "validation loss: 827.9083378041241\n",
      "epoch: 431\n",
      "training loss: 711.9665734207119\n",
      "validation loss: 827.5171956859795\n",
      "epoch: 432\n",
      "training loss: 711.3824578574436\n",
      "validation loss: 827.1277170187226\n",
      "epoch: 433\n",
      "training loss: 710.8002666994965\n",
      "validation loss: 826.7398803745322\n",
      "epoch: 434\n",
      "training loss: 710.219986862906\n",
      "validation loss: 826.3536647493328\n",
      "epoch: 435\n",
      "training loss: 709.6416054806102\n",
      "validation loss: 825.9690495663535\n",
      "epoch: 436\n",
      "training loss: 709.0651098974507\n",
      "validation loss: 825.5860146789614\n",
      "epoch: 437\n",
      "training loss: 708.4904876650263\n",
      "validation loss: 825.2045403727976\n",
      "epoch: 438\n",
      "training loss: 707.9177265364369\n",
      "validation loss: 824.8246073672559\n",
      "epoch: 439\n",
      "training loss: 707.3468144609536\n",
      "validation loss: 824.4461968163423\n",
      "epoch: 440\n",
      "training loss: 706.7777395786646\n",
      "validation loss: 824.069290308967\n",
      "epoch: 441\n",
      "training loss: 706.2104902151355\n",
      "validation loss: 823.6938698687146\n",
      "epoch: 442\n",
      "training loss: 705.6450548761334\n",
      "validation loss: 823.3199179531396\n",
      "epoch: 443\n",
      "training loss: 705.0814222424527\n",
      "validation loss: 822.9474174526329\n",
      "epoch: 444\n",
      "training loss: 704.5195811648787\n",
      "validation loss: 822.5763516888967\n",
      "epoch: 445\n",
      "training loss: 703.9595206593149\n",
      "validation loss: 822.2067044130665\n",
      "epoch: 446\n",
      "training loss: 703.4012299020981\n",
      "validation loss: 821.8384598035035\n",
      "epoch: 447\n",
      "training loss: 702.8446982255066\n",
      "validation loss: 821.4716024632768\n",
      "epoch: 448\n",
      "training loss: 702.2899151134727\n",
      "validation loss: 821.106117417357\n",
      "epoch: 449\n",
      "training loss: 701.736870197488\n",
      "validation loss: 820.7419901095269\n",
      "epoch: 450\n",
      "training loss: 701.1855532526982\n",
      "validation loss: 820.3792063990104\n",
      "epoch: 451\n",
      "training loss: 700.6359541941703\n",
      "validation loss: 820.0177525568319\n",
      "epoch: 452\n",
      "training loss: 700.0880630733134\n",
      "validation loss: 819.6576152618994\n",
      "epoch: 453\n",
      "training loss: 699.5418700744353\n",
      "validation loss: 819.2987815968129\n",
      "epoch: 454\n",
      "training loss: 698.9973655114152\n",
      "validation loss: 818.9412390434005\n",
      "epoch: 455\n",
      "training loss: 698.4545398244652\n",
      "validation loss: 818.5849754779786\n",
      "epoch: 456\n",
      "training loss: 697.9133835769695\n",
      "validation loss: 818.2299791663434\n",
      "epoch: 457\n",
      "training loss: 697.3738874523748\n",
      "validation loss: 817.876238758493\n",
      "epoch: 458\n",
      "training loss: 696.8360422511167\n",
      "validation loss: 817.5237432830861\n",
      "epoch: 459\n",
      "training loss: 696.2998388875644\n",
      "validation loss: 817.1724821416468\n",
      "epoch: 460\n",
      "training loss: 695.7652683869703\n",
      "validation loss: 816.8224451025173\n",
      "epoch: 461\n",
      "training loss: 695.2323218824049\n",
      "validation loss: 816.4736222945745\n",
      "epoch: 462\n",
      "training loss: 694.700990611667\n",
      "validation loss: 816.1260042007115\n",
      "epoch: 463\n",
      "training loss: 694.17126591415\n",
      "validation loss: 815.7795816511031\n",
      "epoch: 464\n",
      "training loss: 693.6431392276529\n",
      "validation loss: 815.4343458162559\n",
      "epoch: 465\n",
      "training loss: 693.1166020851124\n",
      "validation loss: 815.0902881998583\n",
      "epoch: 466\n",
      "training loss: 692.591646111244\n",
      "validation loss: 814.7474006314367\n",
      "epoch: 467\n",
      "training loss: 692.0682630190628\n",
      "validation loss: 814.4056752588256\n",
      "epoch: 468\n",
      "training loss: 691.5464446062592\n",
      "validation loss: 814.0651045404561\n",
      "epoch: 469\n",
      "training loss: 691.0261827513982\n",
      "validation loss: 813.7256812374724\n",
      "epoch: 470\n",
      "training loss: 690.5074694099\n",
      "validation loss: 813.3873984056683\n",
      "epoch: 471\n",
      "training loss: 689.9902966097546\n",
      "validation loss: 813.0502493872556\n",
      "epoch: 472\n",
      "training loss: 689.4746564469102\n",
      "validation loss: 812.7142278024514\n",
      "epoch: 473\n",
      "training loss: 688.9605410802629\n",
      "validation loss: 812.3793275408811\n",
      "epoch: 474\n",
      "training loss: 688.4479427261516\n",
      "validation loss: 812.045542752787\n",
      "epoch: 475\n",
      "training loss: 687.9368536522474\n",
      "validation loss: 811.7128678400211\n",
      "epoch: 476\n",
      "training loss: 687.4272661706924\n",
      "validation loss: 811.3812974468062\n",
      "epoch: 477\n",
      "training loss: 686.9191726303006\n",
      "validation loss: 811.050826450226\n",
      "epoch: 478\n",
      "training loss: 686.4125654075983\n",
      "validation loss: 810.721449950414\n",
      "epoch: 479\n",
      "training loss: 685.9074368963952\n",
      "validation loss: 810.3931632603781\n",
      "epoch: 480\n",
      "training loss: 685.4037794955193\n",
      "validation loss: 810.0659618954032\n",
      "epoch: 481\n",
      "training loss: 684.9015855942057\n",
      "validation loss: 809.7398415619407\n",
      "epoch: 482\n",
      "training loss: 684.4008475545029\n",
      "validation loss: 809.414798145872\n",
      "epoch: 483\n",
      "training loss: 683.9015576898397\n",
      "validation loss: 809.0908277000129\n",
      "epoch: 484\n",
      "training loss: 683.4037082386183\n",
      "validation loss: 808.7679264306669\n",
      "epoch: 485\n",
      "training loss: 682.9072913313132\n",
      "validation loss: 808.446090682999\n",
      "epoch: 486\n",
      "training loss: 682.4122989490212\n",
      "validation loss: 808.1253169249252\n",
      "epoch: 487\n",
      "training loss: 681.9187228706406\n",
      "validation loss: 807.805601729122\n",
      "epoch: 488\n",
      "training loss: 681.4265546048018\n",
      "validation loss: 807.4869417526434\n",
      "epoch: 489\n",
      "training loss: 680.9357853011244\n",
      "validation loss: 807.1693337134715\n",
      "epoch: 490\n",
      "training loss: 680.4464056331692\n",
      "validation loss: 806.8527743631148\n",
      "epoch: 491\n",
      "training loss: 679.9584056422289\n",
      "validation loss: 806.5372604540926\n",
      "epoch: 492\n",
      "training loss: 679.4717745263899\n",
      "validation loss: 806.222788700774\n",
      "epoch: 493\n",
      "training loss: 678.9865003523777\n",
      "validation loss: 805.9093557315787\n",
      "epoch: 494\n",
      "training loss: 678.5025696576242\n",
      "validation loss: 805.5969580299627\n",
      "epoch: 495\n",
      "training loss: 678.0199668955063\n",
      "validation loss: 805.2855918609946\n",
      "epoch: 496\n",
      "training loss: 677.5386736567644\n",
      "validation loss: 804.9752531797797\n",
      "epoch: 497\n",
      "training loss: 677.0586675752005\n",
      "validation loss: 804.6659375180096\n",
      "epoch: 498\n",
      "training loss: 676.5799208024514\n",
      "validation loss: 804.3576398466432\n",
      "epoch: 499\n",
      "training loss: 676.1023979404472\n",
      "validation loss: 804.0503544188571\n",
      "epoch: 500\n",
      "training loss: 675.6260534289383\n",
      "validation loss: 803.7440746141498\n",
      "epoch: 501\n",
      "training loss: 675.150828807988\n",
      "validation loss: 803.4387928448774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 502\n",
      "training loss: 674.6766514931987\n",
      "validation loss: 803.1345006750574\n",
      "epoch: 503\n",
      "training loss: 674.2034395336085\n",
      "validation loss: 802.8311894709667\n",
      "epoch: 504\n",
      "training loss: 673.7311214666777\n",
      "validation loss: 802.5288521462626\n",
      "epoch: 505\n",
      "training loss: 673.2596816827155\n",
      "validation loss: 802.2274866342734\n",
      "epoch: 506\n",
      "training loss: 672.789221316986\n",
      "validation loss: 801.9271007610512\n",
      "epoch: 507\n",
      "training loss: 672.3199715501862\n",
      "validation loss: 801.627715224889\n",
      "epoch: 508\n",
      "training loss: 671.8521899098421\n",
      "validation loss: 801.3293590648162\n",
      "epoch: 509\n",
      "training loss: 671.3860110636325\n",
      "validation loss: 801.0320578573389\n",
      "epoch: 510\n",
      "training loss: 670.9214072960523\n",
      "validation loss: 800.7358247502292\n",
      "epoch: 511\n",
      "training loss: 670.4582569159608\n",
      "validation loss: 800.4406607619574\n",
      "epoch: 512\n",
      "training loss: 669.9964169831671\n",
      "validation loss: 800.1465600217928\n",
      "epoch: 513\n",
      "training loss: 669.5357640541487\n",
      "validation loss: 799.8535146549574\n",
      "epoch: 514\n",
      "training loss: 669.0762231947944\n",
      "validation loss: 799.5615181066536\n",
      "epoch: 515\n",
      "training loss: 668.6178012043877\n",
      "validation loss: 799.270567437178\n",
      "epoch: 516\n",
      "training loss: 668.1606070767671\n",
      "validation loss: 798.9806645737642\n",
      "epoch: 517\n",
      "training loss: 667.7048171510556\n",
      "validation loss: 798.6918154768831\n",
      "epoch: 518\n",
      "training loss: 667.2505832653294\n",
      "validation loss: 798.4040267407528\n",
      "epoch: 519\n",
      "training loss: 666.7979625235321\n",
      "validation loss: 798.1173018140635\n",
      "epoch: 520\n",
      "training loss: 666.3469248615745\n",
      "validation loss: 797.8316398055665\n",
      "epoch: 521\n",
      "training loss: 665.8974008745375\n",
      "validation loss: 797.5470368380174\n",
      "epoch: 522\n",
      "training loss: 665.4493183206871\n",
      "validation loss: 797.2634879020418\n",
      "epoch: 523\n",
      "training loss: 665.0026165543088\n",
      "validation loss: 796.9809880223557\n",
      "epoch: 524\n",
      "training loss: 664.5572483692818\n",
      "validation loss: 796.6995327217298\n",
      "epoch: 525\n",
      "training loss: 664.1131775789707\n",
      "validation loss: 796.4191181014268\n",
      "epoch: 526\n",
      "training loss: 663.6703760077542\n",
      "validation loss: 796.139740773233\n",
      "epoch: 527\n",
      "training loss: 663.228820976979\n",
      "validation loss: 795.861397755958\n",
      "epoch: 528\n",
      "training loss: 662.7884934070091\n",
      "validation loss: 795.5840863781583\n",
      "epoch: 529\n",
      "training loss: 662.3493763935634\n",
      "validation loss: 795.3078041976948\n",
      "epoch: 530\n",
      "training loss: 661.9114540796024\n",
      "validation loss: 795.0325489373992\n",
      "epoch: 531\n",
      "training loss: 661.4747106580752\n",
      "validation loss: 794.7583184329329\n",
      "epoch: 532\n",
      "training loss: 661.0391293483658\n",
      "validation loss: 794.4851105884974\n",
      "epoch: 533\n",
      "training loss: 660.604691169355\n",
      "validation loss: 794.2129233362931\n",
      "epoch: 534\n",
      "training loss: 660.1713732682705\n",
      "validation loss: 793.9417545957049\n",
      "epoch: 535\n",
      "training loss: 659.7391464298016\n",
      "validation loss: 793.6716022278049\n",
      "epoch: 536\n",
      "training loss: 659.3079711337347\n",
      "validation loss: 793.4024639797332\n",
      "epoch: 537\n",
      "training loss: 658.8777910658847\n",
      "validation loss: 793.1343374118848\n",
      "epoch: 538\n",
      "training loss: 658.4485222140227\n",
      "validation loss: 792.8672197993637\n",
      "epoch: 539\n",
      "training loss: 658.0200346965759\n",
      "validation loss: 792.6011080016849\n",
      "epoch: 540\n",
      "training loss: 657.5921247817768\n",
      "validation loss: 792.3359983164214\n",
      "epoch: 541\n",
      "training loss: 657.1644842820522\n",
      "validation loss: 792.0718864272445\n",
      "epoch: 542\n",
      "training loss: 656.7367238498875\n",
      "validation loss: 791.8087678875222\n",
      "epoch: 543\n",
      "training loss: 656.3086414191737\n",
      "validation loss: 791.5466404415025\n",
      "epoch: 544\n",
      "training loss: 655.880887281588\n",
      "validation loss: 791.2855101591398\n",
      "epoch: 545\n",
      "training loss: 655.455033431828\n",
      "validation loss: 791.0253977754036\n",
      "epoch: 546\n",
      "training loss: 655.0320045699762\n",
      "validation loss: 790.7663294172581\n",
      "epoch: 547\n",
      "training loss: 654.6115056813626\n",
      "validation loss: 790.508316236923\n",
      "epoch: 548\n",
      "training loss: 654.1929855752344\n",
      "validation loss: 790.2513524774059\n",
      "epoch: 549\n",
      "training loss: 653.7760785854329\n",
      "validation loss: 789.9954260085799\n",
      "epoch: 550\n",
      "training loss: 653.3605800932086\n",
      "validation loss: 789.7405247938599\n",
      "epoch: 551\n",
      "training loss: 652.9463748246957\n",
      "validation loss: 789.4866386537415\n",
      "epoch: 552\n",
      "training loss: 652.5333943362351\n",
      "validation loss: 789.2337592748984\n",
      "epoch: 553\n",
      "training loss: 652.1215954059819\n",
      "validation loss: 788.981879830184\n",
      "epoch: 554\n",
      "training loss: 651.7109490387031\n",
      "validation loss: 788.7309946035285\n",
      "epoch: 555\n",
      "training loss: 651.3014346532013\n",
      "validation loss: 788.4810986963014\n",
      "epoch: 556\n",
      "training loss: 650.8930368661065\n",
      "validation loss: 788.2321878107348\n",
      "epoch: 557\n",
      "training loss: 650.4857436317637\n",
      "validation loss: 787.9842580919806\n",
      "epoch: 558\n",
      "training loss: 650.0795451221113\n",
      "validation loss: 787.7373060122736\n",
      "epoch: 559\n",
      "training loss: 649.6744330273991\n",
      "validation loss: 787.4913282849965\n",
      "epoch: 560\n",
      "training loss: 649.2704001054482\n",
      "validation loss: 787.2463218001081\n",
      "epoch: 561\n",
      "training loss: 648.867439882751\n",
      "validation loss: 787.002283575046\n",
      "epoch: 562\n",
      "training loss: 648.4655464511992\n",
      "validation loss: 786.7592107169955\n",
      "epoch: 563\n",
      "training loss: 648.0647143266989\n",
      "validation loss: 786.5171003936575\n",
      "epoch: 564\n",
      "training loss: 647.6649383488332\n",
      "validation loss: 786.2759498104556\n",
      "epoch: 565\n",
      "training loss: 647.2662136083552\n",
      "validation loss: 786.0357561926866\n",
      "epoch: 566\n",
      "training loss: 646.8685353939356\n",
      "validation loss: 785.7965167715213\n",
      "epoch: 567\n",
      "training loss: 646.4718991524742\n",
      "validation loss: 785.5582287730173\n",
      "epoch: 568\n",
      "training loss: 646.0763004591236\n",
      "validation loss: 785.3208894095258\n",
      "epoch: 569\n",
      "training loss: 645.6817349943778\n",
      "validation loss: 785.0844958729997\n",
      "epoch: 570\n",
      "training loss: 645.2881985263647\n",
      "validation loss: 784.8490453298274\n",
      "epoch: 571\n",
      "training loss: 644.895686897023\n",
      "validation loss: 784.6145349168933\n",
      "epoch: 572\n",
      "training loss: 644.5041960112129\n",
      "validation loss: 784.380961738627\n",
      "epoch: 573\n",
      "training loss: 644.1137218280596\n",
      "validation loss: 784.1483228648507\n",
      "epoch: 574\n",
      "training loss: 643.724260354019\n",
      "validation loss: 783.9166153292713\n",
      "epoch: 575\n",
      "training loss: 643.3358076372779\n",
      "validation loss: 783.6858361284953\n",
      "epoch: 576\n",
      "training loss: 642.9483597632003\n",
      "validation loss: 783.4559822214611\n",
      "epoch: 577\n",
      "training loss: 642.5619128505939\n",
      "validation loss: 783.2270505292108\n",
      "epoch: 578\n",
      "training loss: 642.1764630486313\n",
      "validation loss: 782.9990379349338\n",
      "epoch: 579\n",
      "training loss: 641.7920065342914\n",
      "validation loss: 782.7719412842252\n",
      "epoch: 580\n",
      "training loss: 641.4085395102186\n",
      "validation loss: 782.5457573855205\n",
      "epoch: 581\n",
      "training loss: 641.0260582029179\n",
      "validation loss: 782.32048301066\n",
      "epoch: 582\n",
      "training loss: 640.6445588612274\n",
      "validation loss: 782.0961148955708\n",
      "epoch: 583\n",
      "training loss: 640.2640377550107\n",
      "validation loss: 781.8726497410223\n",
      "epoch: 584\n",
      "training loss: 639.8844911740337\n",
      "validation loss: 781.6500842134543\n",
      "epoch: 585\n",
      "training loss: 639.5059154269944\n",
      "validation loss: 781.4284149458505\n",
      "epoch: 586\n",
      "training loss: 639.128306840677\n",
      "validation loss: 781.2076385386509\n",
      "epoch: 587\n",
      "training loss: 638.7516617592103\n",
      "validation loss: 780.987751560693\n",
      "epoch: 588\n",
      "training loss: 638.3759765434154\n",
      "validation loss: 780.76875055017\n",
      "epoch: 589\n",
      "training loss: 638.0012475702282\n",
      "validation loss: 780.5506320156076\n",
      "epoch: 590\n",
      "training loss: 637.6274712321837\n",
      "validation loss: 780.3333924368436\n",
      "epoch: 591\n",
      "training loss: 637.2546439369561\n",
      "validation loss: 780.1170282660186\n",
      "epoch: 592\n",
      "training loss: 636.882762106944\n",
      "validation loss: 779.9015359285656\n",
      "epoch: 593\n",
      "training loss: 636.5118221788982\n",
      "validation loss: 779.6869118242014\n",
      "epoch: 594\n",
      "training loss: 636.1418206035826\n",
      "validation loss: 779.473152327913\n",
      "epoch: 595\n",
      "training loss: 635.7727538454669\n",
      "validation loss: 779.2602537909441\n",
      "epoch: 596\n",
      "training loss: 635.4046183824478\n",
      "validation loss: 779.0482125417748\n",
      "epoch: 597\n",
      "training loss: 635.037410705592\n",
      "validation loss: 778.8370248870958\n",
      "epoch: 598\n",
      "training loss: 634.671127318903\n",
      "validation loss: 778.6266871127766\n",
      "epoch: 599\n",
      "training loss: 634.3057647391053\n",
      "validation loss: 778.417195484825\n",
      "epoch: 600\n",
      "training loss: 633.941319495449\n",
      "validation loss: 778.2085462503411\n",
      "epoch: 601\n",
      "training loss: 633.577788129526\n",
      "validation loss: 778.0007356384612\n",
      "epoch: 602\n",
      "training loss: 633.2151671951042\n",
      "validation loss: 777.7937598612908\n",
      "epoch: 603\n",
      "training loss: 632.8534532579727\n",
      "validation loss: 777.5876151148342\n",
      "epoch: 604\n",
      "training loss: 632.4926428957993\n",
      "validation loss: 777.382297579906\n",
      "epoch: 605\n",
      "training loss: 632.1327326979992\n",
      "validation loss: 777.1778034230379\n",
      "epoch: 606\n",
      "training loss: 631.7737192656126\n",
      "validation loss: 776.9741287973739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 607\n",
      "training loss: 631.4155992111937\n",
      "validation loss: 776.7712698435506\n",
      "epoch: 608\n",
      "training loss: 631.058369158705\n",
      "validation loss: 776.5692226905702\n",
      "epoch: 609\n",
      "training loss: 630.7020257434224\n",
      "validation loss: 776.367983456658\n",
      "epoch: 610\n",
      "training loss: 630.3465656118452\n",
      "validation loss: 776.167548250108\n",
      "epoch: 611\n",
      "training loss: 629.9919854216148\n",
      "validation loss: 775.967913170117\n",
      "epoch: 612\n",
      "training loss: 629.6382818414376\n",
      "validation loss: 775.7690743076035\n",
      "epoch: 613\n",
      "training loss: 629.2854515510151\n",
      "validation loss: 775.5710277460131\n",
      "epoch: 614\n",
      "training loss: 628.9334912409781\n",
      "validation loss: 775.3737695621094\n",
      "epoch: 615\n",
      "training loss: 628.5823976128282\n",
      "validation loss: 775.1772958267528\n",
      "epoch: 616\n",
      "training loss: 628.2321673788808\n",
      "validation loss: 774.9816026056602\n",
      "epoch: 617\n",
      "training loss: 627.882797262215\n",
      "validation loss: 774.7866859601545\n",
      "epoch: 618\n",
      "training loss: 627.5342839966263\n",
      "validation loss: 774.5925419478951\n",
      "epoch: 619\n",
      "training loss: 627.1866243265837\n",
      "validation loss: 774.3991666235947\n",
      "epoch: 620\n",
      "training loss: 626.83981500719\n",
      "validation loss: 774.2065560397202\n",
      "epoch: 621\n",
      "training loss: 626.4938528041474\n",
      "validation loss: 774.0147062471774\n",
      "epoch: 622\n",
      "training loss: 626.1487344937219\n",
      "validation loss: 773.8236132959796\n",
      "epoch: 623\n",
      "training loss: 625.8044568627174\n",
      "validation loss: 773.633273235902\n",
      "epoch: 624\n",
      "training loss: 625.4610167084463\n",
      "validation loss: 773.4436821171171\n",
      "epoch: 625\n",
      "training loss: 625.1184108387079\n",
      "validation loss: 773.2548359908162\n",
      "epoch: 626\n",
      "training loss: 624.7766360717673\n",
      "validation loss: 773.0667309098143\n",
      "epoch: 627\n",
      "training loss: 624.4356892363371\n",
      "validation loss: 772.8793629291387\n",
      "epoch: 628\n",
      "training loss: 624.095567171563\n",
      "validation loss: 772.6927281066013\n",
      "epoch: 629\n",
      "training loss: 623.7562667270109\n",
      "validation loss: 772.5068225033572\n",
      "epoch: 630\n",
      "training loss: 623.417784762657\n",
      "validation loss: 772.3216421844431\n",
      "epoch: 631\n",
      "training loss: 623.0801181488821\n",
      "validation loss: 772.1371832193058\n",
      "epoch: 632\n",
      "training loss: 622.7432637664646\n",
      "validation loss: 771.9534416823109\n",
      "epoch: 633\n",
      "training loss: 622.4072185065808\n",
      "validation loss: 771.7704136532375\n",
      "epoch: 634\n",
      "training loss: 622.0719792708041\n",
      "validation loss: 771.5880952177587\n",
      "epoch: 635\n",
      "training loss: 621.7375429711099\n",
      "validation loss: 771.4064824679083\n",
      "epoch: 636\n",
      "training loss: 621.4039065298799\n",
      "validation loss: 771.2255715025286\n",
      "epoch: 637\n",
      "training loss: 621.0710668799125\n",
      "validation loss: 771.0453584277102\n",
      "epoch: 638\n",
      "training loss: 620.7390209644323\n",
      "validation loss: 770.8658393572123\n",
      "epoch: 639\n",
      "training loss: 620.4077657371057\n",
      "validation loss: 770.6870104128735\n",
      "epoch: 640\n",
      "training loss: 620.0772981620566\n",
      "validation loss: 770.5088677250066\n",
      "epoch: 641\n",
      "training loss: 619.7476152138862\n",
      "validation loss: 770.331407432782\n",
      "epoch: 642\n",
      "training loss: 619.4187138776947\n",
      "validation loss: 770.1546256845987\n",
      "epoch: 643\n",
      "training loss: 619.0905911491063\n",
      "validation loss: 769.978518638441\n",
      "epoch: 644\n",
      "training loss: 618.763244034297\n",
      "validation loss: 769.8030824622253\n",
      "epoch: 645\n",
      "training loss: 618.4366695500232\n",
      "validation loss: 769.6283133341344\n",
      "epoch: 646\n",
      "training loss: 618.1108647236558\n",
      "validation loss: 769.4542074429417\n",
      "epoch: 647\n",
      "training loss: 617.7858265932147\n",
      "validation loss: 769.2807609883229\n",
      "epoch: 648\n",
      "training loss: 617.4615522074059\n",
      "validation loss: 769.1079701811569\n",
      "epoch: 649\n",
      "training loss: 617.1380386256628\n",
      "validation loss: 768.9358312438205\n",
      "epoch: 650\n",
      "training loss: 616.8152829181869\n",
      "validation loss: 768.7643404104663\n",
      "epoch: 651\n",
      "training loss: 616.4932821659943\n",
      "validation loss: 768.5934939272988\n",
      "epoch: 652\n",
      "training loss: 616.1720334609611\n",
      "validation loss: 768.4232880528358\n",
      "epoch: 653\n",
      "training loss: 615.8515339058722\n",
      "validation loss: 768.2537190581623\n",
      "epoch: 654\n",
      "training loss: 615.5317806144725\n",
      "validation loss: 768.0847832271781\n",
      "epoch: 655\n",
      "training loss: 615.21277071152\n",
      "validation loss: 767.9164768568336\n",
      "epoch: 656\n",
      "training loss: 614.8945013328382\n",
      "validation loss: 767.7487962573618\n",
      "epoch: 657\n",
      "training loss: 614.5769696253733\n",
      "validation loss: 767.5817377524985\n",
      "epoch: 658\n",
      "training loss: 614.2601727472507\n",
      "validation loss: 767.415297679699\n",
      "epoch: 659\n",
      "training loss: 613.9441078678327\n",
      "validation loss: 767.2494723903446\n",
      "epoch: 660\n",
      "training loss: 613.6287721677787\n",
      "validation loss: 767.0842582499456\n",
      "epoch: 661\n",
      "training loss: 613.3141628391047\n",
      "validation loss: 766.9196516383352\n",
      "epoch: 662\n",
      "training loss: 613.0002770852446\n",
      "validation loss: 766.7556489498585\n",
      "epoch: 663\n",
      "training loss: 612.6871121211121\n",
      "validation loss: 766.5922465935558\n",
      "epoch: 664\n",
      "training loss: 612.3746651731616\n",
      "validation loss: 766.4294409933407\n",
      "epoch: 665\n",
      "training loss: 612.0629334794517\n",
      "validation loss: 766.2672285881699\n",
      "epoch: 666\n",
      "training loss: 611.7519142897069\n",
      "validation loss: 766.1056058322132\n",
      "epoch: 667\n",
      "training loss: 611.4416048653815\n",
      "validation loss: 765.9445691950134\n",
      "epoch: 668\n",
      "training loss: 611.1320024797211\n",
      "validation loss: 765.784115161645\n",
      "epoch: 669\n",
      "training loss: 610.8231044178253\n",
      "validation loss: 765.6242402328678\n",
      "epoch: 670\n",
      "training loss: 610.5149079767101\n",
      "validation loss: 765.4649409252744\n",
      "epoch: 671\n",
      "training loss: 610.2074104653692\n",
      "validation loss: 765.3062137714371\n",
      "epoch: 672\n",
      "training loss: 609.9006092048369\n",
      "validation loss: 765.1480553200482\n",
      "epoch: 673\n",
      "training loss: 609.5945015282467\n",
      "validation loss: 764.990462136059\n",
      "epoch: 674\n",
      "training loss: 609.2890847808936\n",
      "validation loss: 764.8334308008132\n",
      "epoch: 675\n",
      "training loss: 608.9843563202927\n",
      "validation loss: 764.6769579121795\n",
      "epoch: 676\n",
      "training loss: 608.680313516238\n",
      "validation loss: 764.5210400846798\n",
      "epoch: 677\n",
      "training loss: 608.3769537508609\n",
      "validation loss: 764.3656739496138\n",
      "epoch: 678\n",
      "training loss: 608.0742744186879\n",
      "validation loss: 764.2108561551843\n",
      "epoch: 679\n",
      "training loss: 607.7722729266952\n",
      "validation loss: 764.0565833666157\n",
      "epoch: 680\n",
      "training loss: 607.4709466943658\n",
      "validation loss: 763.902852266273\n",
      "epoch: 681\n",
      "training loss: 607.1702931537426\n",
      "validation loss: 763.7496595537777\n",
      "epoch: 682\n",
      "training loss: 606.8703097494816\n",
      "validation loss: 763.597001946122\n",
      "epoch: 683\n",
      "training loss: 606.570993938904\n",
      "validation loss: 763.444876177778\n",
      "epoch: 684\n",
      "training loss: 606.2723431920466\n",
      "validation loss: 763.2932790008107\n",
      "epoch: 685\n",
      "training loss: 605.9743549917105\n",
      "validation loss: 763.1422071849834\n",
      "epoch: 686\n",
      "training loss: 605.6770268335094\n",
      "validation loss: 762.9916575178635\n",
      "epoch: 687\n",
      "training loss: 605.3803562259144\n",
      "validation loss: 762.841626804927\n",
      "epoch: 688\n",
      "training loss: 605.0843406902985\n",
      "validation loss: 762.6921118696603\n",
      "epoch: 689\n",
      "training loss: 604.7889777609796\n",
      "validation loss: 762.54310955366\n",
      "epoch: 690\n",
      "training loss: 604.494264985259\n",
      "validation loss: 762.394616716732\n",
      "epoch: 691\n",
      "training loss: 604.2001999234601\n",
      "validation loss: 762.2466302369872\n",
      "epoch: 692\n",
      "training loss: 603.9067801489641\n",
      "validation loss: 762.0991470109365\n",
      "epoch: 693\n",
      "training loss: 603.614003248242\n",
      "validation loss: 761.9521639535835\n",
      "epoch: 694\n",
      "training loss: 603.3218668208852\n",
      "validation loss: 761.8056779985161\n",
      "epoch: 695\n",
      "training loss: 603.0303684796321\n",
      "validation loss: 761.659686097995\n",
      "epoch: 696\n",
      "training loss: 602.7395058503928\n",
      "validation loss: 761.5141852230404\n",
      "epoch: 697\n",
      "training loss: 602.4492765722682\n",
      "validation loss: 761.369172363518\n",
      "epoch: 698\n",
      "training loss: 602.1596782975683\n",
      "validation loss: 761.2246445282211\n",
      "epoch: 699\n",
      "training loss: 601.8707086918249\n",
      "validation loss: 761.0805987449529\n",
      "epoch: 700\n",
      "training loss: 601.582365433801\n",
      "validation loss: 760.9370320606047\n",
      "epoch: 701\n",
      "training loss: 601.2946462154947\n",
      "validation loss: 760.7939415412319\n",
      "epoch: 702\n",
      "training loss: 601.0075487421421\n",
      "validation loss: 760.6513242721312\n",
      "epoch: 703\n",
      "training loss: 600.7210707322113\n",
      "validation loss: 760.5091773579096\n",
      "epoch: 704\n",
      "training loss: 600.4352099173951\n",
      "validation loss: 760.367497922557\n",
      "epoch: 705\n",
      "training loss: 600.149964042598\n",
      "validation loss: 760.2262831095146\n",
      "epoch: 706\n",
      "training loss: 599.8653308659162\n",
      "validation loss: 760.0855300817377\n",
      "epoch: 707\n",
      "training loss: 599.5813081586166\n",
      "validation loss: 759.9452360217624\n",
      "epoch: 708\n",
      "training loss: 599.297893705106\n",
      "validation loss: 759.8053981317639\n",
      "epoch: 709\n",
      "training loss: 599.0150853028985\n",
      "validation loss: 759.6660136336171\n",
      "epoch: 710\n",
      "training loss: 598.7328807625757\n",
      "validation loss: 759.5270797689521\n",
      "epoch: 711\n",
      "training loss: 598.4512779077431\n",
      "validation loss: 759.3885937992085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 712\n",
      "training loss: 598.1702745749782\n",
      "validation loss: 759.2505530056869\n",
      "epoch: 713\n",
      "training loss: 597.8898686137765\n",
      "validation loss: 759.1129546895986\n",
      "epoch: 714\n",
      "training loss: 597.6100578864902\n",
      "validation loss: 758.9757961721125\n",
      "epoch: 715\n",
      "training loss: 597.330840268262\n",
      "validation loss: 758.8390747944003\n",
      "epoch: 716\n",
      "training loss: 597.0522136469541\n",
      "validation loss: 758.7027879176784\n",
      "epoch: 717\n",
      "training loss: 596.7741759230704\n",
      "validation loss: 758.5669329232485\n",
      "epoch: 718\n",
      "training loss: 596.4967250096765\n",
      "validation loss: 758.4315072125361\n",
      "epoch: 719\n",
      "training loss: 596.2198588323116\n",
      "validation loss: 758.2965082071246\n",
      "epoch: 720\n",
      "training loss: 595.9435753288974\n",
      "validation loss: 758.161933348791\n",
      "epoch: 721\n",
      "training loss: 595.667872449643\n",
      "validation loss: 758.0277800995356\n",
      "epoch: 722\n",
      "training loss: 595.3927481569441\n",
      "validation loss: 757.8940459416127\n",
      "epoch: 723\n",
      "training loss: 595.1182004252775\n",
      "validation loss: 757.7607283775559\n",
      "epoch: 724\n",
      "training loss: 594.8442272410938\n",
      "validation loss: 757.6278249302042\n",
      "epoch: 725\n",
      "training loss: 594.5708266027044\n",
      "validation loss: 757.4953331427237\n",
      "epoch: 726\n",
      "training loss: 594.297996520166\n",
      "validation loss: 757.3632505786289\n",
      "epoch: 727\n",
      "training loss: 594.0257350151613\n",
      "validation loss: 757.2315748218005\n",
      "epoch: 728\n",
      "training loss: 593.7540401208769\n",
      "validation loss: 757.1003034765013\n",
      "epoch: 729\n",
      "training loss: 593.4829098818783\n",
      "validation loss: 756.969434167391\n",
      "epoch: 730\n",
      "training loss: 593.2123423539832\n",
      "validation loss: 756.8389645395362\n",
      "epoch: 731\n",
      "training loss: 592.9423356041308\n",
      "validation loss: 756.7088922584215\n",
      "epoch: 732\n",
      "training loss: 592.6728877102511\n",
      "validation loss: 756.5792150099553\n",
      "epoch: 733\n",
      "training loss: 592.4039967611307\n",
      "validation loss: 756.4499305004755\n",
      "epoch: 734\n",
      "training loss: 592.1356608562785\n",
      "validation loss: 756.32103645675\n",
      "epoch: 735\n",
      "training loss: 591.8678781057896\n",
      "validation loss: 756.1925306259787\n",
      "epoch: 736\n",
      "training loss: 591.600646630207\n",
      "validation loss: 756.0644107757894\n",
      "epoch: 737\n",
      "training loss: 591.3339645603849\n",
      "validation loss: 755.9366746942327\n",
      "epoch: 738\n",
      "training loss: 591.0678300373487\n",
      "validation loss: 755.8093201897743\n",
      "epoch: 739\n",
      "training loss: 590.802241212157\n",
      "validation loss: 755.6823450912842\n",
      "epoch: 740\n",
      "training loss: 590.5371962457614\n",
      "validation loss: 755.5557472480225\n",
      "epoch: 741\n",
      "training loss: 590.2726933088675\n",
      "validation loss: 755.4295245296245\n",
      "epoch: 742\n",
      "training loss: 590.008730581795\n",
      "validation loss: 755.303674826079\n",
      "epoch: 743\n",
      "training loss: 589.7453062543392\n",
      "validation loss: 755.1781960477075\n",
      "epoch: 744\n",
      "training loss: 589.4824185256318\n",
      "validation loss: 755.0530861251375\n",
      "epoch: 745\n",
      "training loss: 589.2200656040035\n",
      "validation loss: 754.928343009274\n",
      "epoch: 746\n",
      "training loss: 588.9582457068454\n",
      "validation loss: 754.8039646712666\n",
      "epoch: 747\n",
      "training loss: 588.696957060473\n",
      "validation loss: 754.6799491024732\n",
      "epoch: 748\n",
      "training loss: 588.4361978999898\n",
      "validation loss: 754.5562943144208\n",
      "epoch: 749\n",
      "training loss: 588.1759664691531\n",
      "validation loss: 754.4329983387621\n",
      "epoch: 750\n",
      "training loss: 587.9162610202396\n",
      "validation loss: 754.3100592272291\n",
      "epoch: 751\n",
      "training loss: 587.657079813913\n",
      "validation loss: 754.1874750515806\n",
      "epoch: 752\n",
      "training loss: 587.3984211190918\n",
      "validation loss: 754.0652439035483\n",
      "epoch: 753\n",
      "training loss: 587.1402832128206\n",
      "validation loss: 753.9433638947778\n",
      "epoch: 754\n",
      "training loss: 586.8826643801395\n",
      "validation loss: 753.8218331567645\n",
      "epoch: 755\n",
      "training loss: 586.6255629139581\n",
      "validation loss: 753.7006498407869\n",
      "epoch: 756\n",
      "training loss: 586.3689771149286\n",
      "validation loss: 753.5798121178348\n",
      "epoch: 757\n",
      "training loss: 586.1129052913219\n",
      "validation loss: 753.4593181785315\n",
      "epoch: 758\n",
      "training loss: 585.8573457589043\n",
      "validation loss: 753.3391662330562\n",
      "epoch: 759\n",
      "training loss: 585.6022968408172\n",
      "validation loss: 753.2193545110558\n",
      "epoch: 760\n",
      "training loss: 585.3477568674557\n",
      "validation loss: 753.0998812615571\n",
      "epoch: 761\n",
      "training loss: 585.0937241763528\n",
      "validation loss: 752.980744752871\n",
      "epoch: 762\n",
      "training loss: 584.8401971120614\n",
      "validation loss: 752.861943272495\n",
      "epoch: 763\n",
      "training loss: 584.5871740260416\n",
      "validation loss: 752.7434751270081\n",
      "epoch: 764\n",
      "training loss: 584.3346532765461\n",
      "validation loss: 752.6253386419633\n",
      "epoch: 765\n",
      "training loss: 584.0826332285119\n",
      "validation loss: 752.5075321617745\n",
      "epoch: 766\n",
      "training loss: 583.8311122534492\n",
      "validation loss: 752.3900540495986\n",
      "epoch: 767\n",
      "training loss: 583.5800887293359\n",
      "validation loss: 752.2729026872133\n",
      "epoch: 768\n",
      "training loss: 583.329561040512\n",
      "validation loss: 752.1560764748901\n",
      "epoch: 769\n",
      "training loss: 583.079527577577\n",
      "validation loss: 752.0395738312626\n",
      "epoch: 770\n",
      "training loss: 582.8299867372882\n",
      "validation loss: 751.92339319319\n",
      "epoch: 771\n",
      "training loss: 582.580936922463\n",
      "validation loss: 751.8075330156173\n",
      "epoch: 772\n",
      "training loss: 582.3323765418804\n",
      "validation loss: 751.6919917714291\n",
      "epoch: 773\n",
      "training loss: 582.0843040101878\n",
      "validation loss: 751.5767679513017\n",
      "epoch: 774\n",
      "training loss: 581.8367177478073\n",
      "validation loss: 751.4618600635471\n",
      "epoch: 775\n",
      "training loss: 581.5896161808457\n",
      "validation loss: 751.3472666339569\n",
      "epoch: 776\n",
      "training loss: 581.3429977410061\n",
      "validation loss: 751.2329862056397\n",
      "epoch: 777\n",
      "training loss: 581.0968608655016\n",
      "validation loss: 751.1190173388543\n",
      "epoch: 778\n",
      "training loss: 580.8512039969715\n",
      "validation loss: 751.0053586108411\n",
      "epoch: 779\n",
      "training loss: 580.6060255834002\n",
      "validation loss: 750.8920086156475\n",
      "epoch: 780\n",
      "training loss: 580.3613240780365\n",
      "validation loss: 750.7789659639507\n",
      "epoch: 781\n",
      "training loss: 580.1170979393178\n",
      "validation loss: 750.6662292828769\n",
      "epoch: 782\n",
      "training loss: 579.8733456307943\n",
      "validation loss: 750.5537972158183\n",
      "epoch: 783\n",
      "training loss: 579.6300656210574\n",
      "validation loss: 750.4416684222447\n",
      "epoch: 784\n",
      "training loss: 579.3872563836679\n",
      "validation loss: 750.329841577513\n",
      "epoch: 785\n",
      "training loss: 579.14491639709\n",
      "validation loss: 750.2183153726752\n",
      "epoch: 786\n",
      "training loss: 578.9030441446235\n",
      "validation loss: 750.1070885142817\n",
      "epoch: 787\n",
      "training loss: 578.6616381143425\n",
      "validation loss: 749.9961597241839\n",
      "epoch: 788\n",
      "training loss: 578.4206967990326\n",
      "validation loss: 749.8855277393325\n",
      "epoch: 789\n",
      "training loss: 578.1802186961327\n",
      "validation loss: 749.7751913115756\n",
      "epoch: 790\n",
      "training loss: 577.9402023076792\n",
      "validation loss: 749.6651492074528\n",
      "epoch: 791\n",
      "training loss: 577.7006461402507\n",
      "validation loss: 749.5554002079901\n",
      "epoch: 792\n",
      "training loss: 577.461548704916\n",
      "validation loss: 749.4459431084903\n",
      "epoch: 793\n",
      "training loss: 577.2229085171851\n",
      "validation loss: 749.3367767183241\n",
      "epoch: 794\n",
      "training loss: 576.9847240969609\n",
      "validation loss: 749.2278998607195\n",
      "epoch: 795\n",
      "training loss: 576.746993968494\n",
      "validation loss: 749.1193113725491\n",
      "epoch: 796\n",
      "training loss: 576.5097166603389\n",
      "validation loss: 749.0110101041175\n",
      "epoch: 797\n",
      "training loss: 576.272890705313\n",
      "validation loss: 748.9029949189486\n",
      "epoch: 798\n",
      "training loss: 576.0365146404584\n",
      "validation loss: 748.7952646935709\n",
      "epoch: 799\n",
      "training loss: 575.8005870070028\n",
      "validation loss: 748.6878183173035\n",
      "epoch: 800\n",
      "training loss: 575.5651063503256\n",
      "validation loss: 748.5806546920414\n",
      "epoch: 801\n",
      "training loss: 575.3300712199242\n",
      "validation loss: 748.4737727320421\n",
      "epoch: 802\n",
      "training loss: 575.0954801693825\n",
      "validation loss: 748.3671713637111\n",
      "epoch: 803\n",
      "training loss: 574.8613317563417\n",
      "validation loss: 748.2608495253875\n",
      "epoch: 804\n",
      "training loss: 574.6276245424707\n",
      "validation loss: 748.1548061671314\n",
      "epoch: 805\n",
      "training loss: 574.3943570934416\n",
      "validation loss: 748.0490402505136\n",
      "epoch: 806\n",
      "training loss: 574.161527978904\n",
      "validation loss: 747.9435507484013\n",
      "epoch: 807\n",
      "training loss: 573.9291357724625\n",
      "validation loss: 747.83833664475\n",
      "epoch: 808\n",
      "training loss: 573.697179051653\n",
      "validation loss: 747.7333969343933\n",
      "epoch: 809\n",
      "training loss: 573.4656563979245\n",
      "validation loss: 747.6287306228372\n",
      "epoch: 810\n",
      "training loss: 573.234566396618\n",
      "validation loss: 747.5243367260522\n",
      "epoch: 811\n",
      "training loss: 573.0039076369488\n",
      "validation loss: 747.4202142702711\n",
      "epoch: 812\n",
      "training loss: 572.7736787119894\n",
      "validation loss: 747.3163622917857\n",
      "epoch: 813\n",
      "training loss: 572.5438782186532\n",
      "validation loss: 747.2127798367472\n",
      "epoch: 814\n",
      "training loss: 572.3145047576782\n",
      "validation loss: 747.109465960968\n",
      "epoch: 815\n",
      "training loss: 572.0855569336122\n",
      "validation loss: 747.0064197297272\n",
      "epoch: 816\n",
      "training loss: 571.8570333547991\n",
      "validation loss: 746.9036402175758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 817\n",
      "training loss: 571.6289326333633\n",
      "validation loss: 746.8011265081483\n",
      "epoch: 818\n",
      "training loss: 571.401253385197\n",
      "validation loss: 746.6988776939731\n",
      "epoch: 819\n",
      "training loss: 571.1739942299449\n",
      "validation loss: 746.5968928762902\n",
      "epoch: 820\n",
      "training loss: 570.9471537909906\n",
      "validation loss: 746.4951711648665\n",
      "epoch: 821\n",
      "training loss: 570.7207306954417\n",
      "validation loss: 746.3937116778199\n",
      "epoch: 822\n",
      "training loss: 570.4947235741153\n",
      "validation loss: 746.2925135414423\n",
      "epoch: 823\n",
      "training loss: 570.2691310615212\n",
      "validation loss: 746.1915758900286\n",
      "epoch: 824\n",
      "training loss: 570.0439517958459\n",
      "validation loss: 746.0908978657084\n",
      "epoch: 825\n",
      "training loss: 569.8191844189341\n",
      "validation loss: 745.9904786182802\n",
      "epoch: 826\n",
      "training loss: 569.5948275762695\n",
      "validation loss: 745.8903173050523\n",
      "epoch: 827\n",
      "training loss: 569.370879916954\n",
      "validation loss: 745.7904130906842\n",
      "epoch: 828\n",
      "training loss: 569.1473400936853\n",
      "validation loss: 745.6907651470336\n",
      "epoch: 829\n",
      "training loss: 568.9242067627324\n",
      "validation loss: 745.5913726530098\n",
      "epoch: 830\n",
      "training loss: 568.7014785839083\n",
      "validation loss: 745.4922347944265\n",
      "epoch: 831\n",
      "training loss: 568.4791542205427\n",
      "validation loss: 745.3933507638639\n",
      "epoch: 832\n",
      "training loss: 568.2572323394495\n",
      "validation loss: 745.2947197605316\n",
      "epoch: 833\n",
      "training loss: 568.0357116108937\n",
      "validation loss: 745.1963409901397\n",
      "epoch: 834\n",
      "training loss: 567.8145907085561\n",
      "validation loss: 745.0982136647722\n",
      "epoch: 835\n",
      "training loss: 567.5938683094948\n",
      "validation loss: 745.0003370027673\n",
      "epoch: 836\n",
      "training loss: 567.3735430941042\n",
      "validation loss: 744.9027102285999\n",
      "epoch: 837\n",
      "training loss: 567.1536137460737\n",
      "validation loss: 744.8053325727749\n",
      "epoch: 838\n",
      "training loss: 566.9340789523428\n",
      "validation loss: 744.7082032717213\n",
      "epoch: 839\n",
      "training loss: 566.7149374030564\n",
      "validation loss: 744.6113215676937\n",
      "epoch: 840\n",
      "training loss: 566.496187791519\n",
      "validation loss: 744.5146867086809\n",
      "epoch: 841\n",
      "training loss: 566.2778288141496\n",
      "validation loss: 744.4182979483186\n",
      "epoch: 842\n",
      "training loss: 566.0598591704365\n",
      "validation loss: 744.3221545458102\n",
      "epoch: 843\n",
      "training loss: 565.8422775628966\n",
      "validation loss: 744.2262557658534\n",
      "epoch: 844\n",
      "training loss: 565.6250826970355\n",
      "validation loss: 744.1306008785723\n",
      "epoch: 845\n",
      "training loss: 565.408273281316\n",
      "validation loss: 744.0351891594576\n",
      "epoch: 846\n",
      "training loss: 565.1918480271312\n",
      "validation loss: 743.9400198893125\n",
      "epoch: 847\n",
      "training loss: 564.9758056487865\n",
      "validation loss: 743.8450923542052\n",
      "epoch: 848\n",
      "training loss: 564.7601448634938\n",
      "validation loss: 743.7504058454278\n",
      "epoch: 849\n",
      "training loss: 564.5448643913783\n",
      "validation loss: 743.6559596594601\n",
      "epoch: 850\n",
      "training loss: 564.3299629555003\n",
      "validation loss: 743.5617530979414\n",
      "epoch: 851\n",
      "training loss: 564.115439281895\n",
      "validation loss: 743.4677854676432\n",
      "epoch: 852\n",
      "training loss: 563.901292099633\n",
      "validation loss: 743.3740560804517\n",
      "epoch: 853\n",
      "training loss: 563.6875201409014\n",
      "validation loss: 743.2805642533477\n",
      "epoch: 854\n",
      "training loss: 563.4741221411115\n",
      "validation loss: 743.1873093083937\n",
      "epoch: 855\n",
      "training loss: 563.2610968390293\n",
      "validation loss: 743.0942905727172\n",
      "epoch: 856\n",
      "training loss: 563.0484429769349\n",
      "validation loss: 743.0015073784983\n",
      "epoch: 857\n",
      "training loss: 562.8361593008076\n",
      "validation loss: 742.9089590629516\n",
      "epoch: 858\n",
      "training loss: 562.6242445605379\n",
      "validation loss: 742.8166449683033\n",
      "epoch: 859\n",
      "training loss: 562.4126975101657\n",
      "validation loss: 742.7245644417668\n",
      "epoch: 860\n",
      "training loss: 562.2015169081399\n",
      "validation loss: 742.632716835504\n",
      "epoch: 861\n",
      "training loss: 561.9907015175991\n",
      "validation loss: 742.5411015065807\n",
      "epoch: 862\n",
      "training loss: 561.7802501066675\n",
      "validation loss: 742.4497178169087\n",
      "epoch: 863\n",
      "training loss: 561.5701614487598\n",
      "validation loss: 742.3585651331729\n",
      "epoch: 864\n",
      "training loss: 561.3604343228897\n",
      "validation loss: 742.2676428267436\n",
      "epoch: 865\n",
      "training loss: 561.1510675139741\n",
      "validation loss: 742.1769502735732\n",
      "epoch: 866\n",
      "training loss: 560.9420598131246\n",
      "validation loss: 742.086486854075\n",
      "epoch: 867\n",
      "training loss: 560.7334100179186\n",
      "validation loss: 741.9962519529857\n",
      "epoch: 868\n",
      "training loss: 560.52511693264\n",
      "validation loss: 741.9062449592124\n",
      "epoch: 869\n",
      "training loss: 560.317179368484\n",
      "validation loss: 741.816465265665\n",
      "epoch: 870\n",
      "training loss: 560.1095961437138\n",
      "validation loss: 741.7269122690795\n",
      "epoch: 871\n",
      "training loss: 559.9023660837693\n",
      "validation loss: 741.6375853698335\n",
      "epoch: 872\n",
      "training loss: 559.6954880213178\n",
      "validation loss: 741.5484839717577\n",
      "epoch: 873\n",
      "training loss: 559.4889607962473\n",
      "validation loss: 741.4596074819534\n",
      "epoch: 874\n",
      "training loss: 559.2827832555998\n",
      "validation loss: 741.3709553106171\n",
      "epoch: 875\n",
      "training loss: 559.0769542534491\n",
      "validation loss: 741.2825268708799\n",
      "epoch: 876\n",
      "training loss: 558.8714726507258\n",
      "validation loss: 741.1943215786732\n",
      "epoch: 877\n",
      "training loss: 558.6663373149952\n",
      "validation loss: 741.1063388526164\n",
      "epoch: 878\n",
      "training loss: 558.4615471202006\n",
      "validation loss: 741.0185781139454\n",
      "epoch: 879\n",
      "training loss: 558.2571009463778\n",
      "validation loss: 740.9310387864753\n",
      "epoch: 880\n",
      "training loss: 558.0529976793558\n",
      "validation loss: 740.843720296609\n",
      "epoch: 881\n",
      "training loss: 557.8492362104528\n",
      "validation loss: 740.7566220733888\n",
      "epoch: 882\n",
      "training loss: 557.6458154361826\n",
      "validation loss: 740.6697435485962\n",
      "epoch: 883\n",
      "training loss: 557.4427342579799\n",
      "validation loss: 740.5830841568932\n",
      "epoch: 884\n",
      "training loss: 557.2399915819559\n",
      "validation loss: 740.4966433360053\n",
      "epoch: 885\n",
      "training loss: 557.0375863186941\n",
      "validation loss: 740.4104205269424\n",
      "epoch: 886\n",
      "training loss: 556.8355173830879\n",
      "validation loss: 740.3244151742447\n",
      "epoch: 887\n",
      "training loss: 556.6337836942315\n",
      "validation loss: 740.238626726252\n",
      "epoch: 888\n",
      "training loss: 556.4323841753584\n",
      "validation loss: 740.153054635381\n",
      "epoch: 889\n",
      "training loss: 556.2313177538322\n",
      "validation loss: 740.067698358401\n",
      "epoch: 890\n",
      "training loss: 556.0305833611839\n",
      "validation loss: 739.9825573566967\n",
      "epoch: 891\n",
      "training loss: 555.8301799331919\n",
      "validation loss: 739.8976310965064\n",
      "epoch: 892\n",
      "training loss: 555.6301064099963\n",
      "validation loss: 739.8129190491175\n",
      "epoch: 893\n",
      "training loss: 555.4303617362393\n",
      "validation loss: 739.7284206910142\n",
      "epoch: 894\n",
      "training loss: 555.2309448612235\n",
      "validation loss: 739.6441355039607\n",
      "epoch: 895\n",
      "training loss: 555.0318547390744\n",
      "validation loss: 739.5600629750079\n",
      "epoch: 896\n",
      "training loss: 554.8330903288979\n",
      "validation loss: 739.4762025964187\n",
      "epoch: 897\n",
      "training loss: 554.6346505949228\n",
      "validation loss: 739.3925538654983\n",
      "epoch: 898\n",
      "training loss: 554.4365345066121\n",
      "validation loss: 739.3091162843256\n",
      "epoch: 899\n",
      "training loss: 554.2387410387418\n",
      "validation loss: 739.2258893593792\n",
      "epoch: 900\n",
      "training loss: 554.0412691714298\n",
      "validation loss: 739.1428726010597\n",
      "epoch: 901\n",
      "training loss: 553.8441178901139\n",
      "validation loss: 739.0600655231032\n",
      "epoch: 902\n",
      "training loss: 553.647286185471\n",
      "validation loss: 738.9774676418951\n",
      "epoch: 903\n",
      "training loss: 553.4507730532739\n",
      "validation loss: 738.8950784756845\n",
      "epoch: 904\n",
      "training loss: 553.2545774941889\n",
      "validation loss: 738.8128975437145\n",
      "epoch: 905\n",
      "training loss: 553.0586985135101\n",
      "validation loss: 738.7309243652774\n",
      "epoch: 906\n",
      "training loss: 552.8631351208425\n",
      "validation loss: 738.6491584587092\n",
      "epoch: 907\n",
      "training loss: 552.6678863297345\n",
      "validation loss: 738.5675993403461\n",
      "epoch: 908\n",
      "training loss: 552.4729511572741\n",
      "validation loss: 738.4862465234581\n",
      "epoch: 909\n",
      "training loss: 552.2783286236557\n",
      "validation loss: 738.4050995171873\n",
      "epoch: 910\n",
      "training loss: 552.0840177517341\n",
      "validation loss: 738.3241578255071\n",
      "epoch: 911\n",
      "training loss: 551.8900175665772\n",
      "validation loss: 738.2434209462366\n",
      "epoch: 912\n",
      "training loss: 551.6963270950301\n",
      "validation loss: 738.1628883701227\n",
      "epoch: 913\n",
      "training loss: 551.502945365307\n",
      "validation loss: 738.0825595800202\n",
      "epoch: 914\n",
      "training loss: 551.3098714066185\n",
      "validation loss: 738.002434050187\n",
      "epoch: 915\n",
      "training loss: 551.1171042488482\n",
      "validation loss: 737.9225112457088\n",
      "epoch: 916\n",
      "training loss: 550.9246429222853\n",
      "validation loss: 737.8427906220727\n",
      "epoch: 917\n",
      "training loss: 550.7324864574177\n",
      "validation loss: 737.763271624892\n",
      "epoch: 918\n",
      "training loss: 550.5406338847905\n",
      "validation loss: 737.683953689795\n",
      "epoch: 919\n",
      "training loss: 550.3490842349288\n",
      "validation loss: 737.6048362424677\n",
      "epoch: 920\n",
      "training loss: 550.157836538323\n",
      "validation loss: 737.525918698855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 921\n",
      "training loss: 549.9668898254699\n",
      "validation loss: 737.4472004655041\n",
      "epoch: 922\n",
      "training loss: 549.7762431269678\n",
      "validation loss: 737.3686809400423\n",
      "epoch: 923\n",
      "training loss: 549.5858954736512\n",
      "validation loss: 737.2903595117676\n",
      "epoch: 924\n",
      "training loss: 549.395845896763\n",
      "validation loss: 737.2122355623369\n",
      "epoch: 925\n",
      "training loss: 549.2060934281492\n",
      "validation loss: 737.13430846653\n",
      "epoch: 926\n",
      "training loss: 549.0166371004697\n",
      "validation loss: 737.0565775930648\n",
      "epoch: 927\n",
      "training loss: 548.8274759474167\n",
      "validation loss: 736.9790423054467\n",
      "epoch: 928\n",
      "training loss: 548.6386090039309\n",
      "validation loss: 736.9017019628244\n",
      "epoch: 929\n",
      "training loss: 548.4500353064082\n",
      "validation loss: 736.8245559208362\n",
      "epoch: 930\n",
      "training loss: 548.2617538928934\n",
      "validation loss: 736.7476035324272\n",
      "epoch: 931\n",
      "training loss: 548.0737638032521\n",
      "validation loss: 736.6708441486184\n",
      "epoch: 932\n",
      "training loss: 547.8860640793216\n",
      "validation loss: 736.5942771192165\n",
      "epoch: 933\n",
      "training loss: 547.6986537650323\n",
      "validation loss: 736.5179017934481\n",
      "epoch: 934\n",
      "training loss: 547.5115319065051\n",
      "validation loss: 736.4417175205158\n",
      "epoch: 935\n",
      "training loss: 547.3246975521158\n",
      "validation loss: 736.3657236500597\n",
      "epoch: 936\n",
      "training loss: 547.1381497525359\n",
      "validation loss: 736.2899195325297\n",
      "epoch: 937\n",
      "training loss: 546.9518875607401\n",
      "validation loss: 736.2143045194566\n",
      "epoch: 938\n",
      "training loss: 546.7659100319919\n",
      "validation loss: 736.1388779636294\n",
      "epoch: 939\n",
      "training loss: 546.5802162237987\n",
      "validation loss: 736.0636392191736\n",
      "epoch: 940\n",
      "training loss: 546.3948051958469\n",
      "validation loss: 735.9885876415373\n",
      "epoch: 941\n",
      "training loss: 546.2096760099113\n",
      "validation loss: 735.9137225873826\n",
      "epoch: 942\n",
      "training loss: 546.0248277297453\n",
      "validation loss: 735.8390434143948\n",
      "epoch: 943\n",
      "training loss: 545.8402594209514\n",
      "validation loss: 735.7645494810063\n",
      "epoch: 944\n",
      "training loss: 545.6559701508319\n",
      "validation loss: 735.6902401460441\n",
      "epoch: 945\n",
      "training loss: 545.4719589882257\n",
      "validation loss: 735.6161147683072\n",
      "epoch: 946\n",
      "training loss: 545.2882250033267\n",
      "validation loss: 735.5421727060746\n",
      "epoch: 947\n",
      "training loss: 545.1047672674885\n",
      "validation loss: 735.4684133165565\n",
      "epoch: 948\n",
      "training loss: 544.921584853014\n",
      "validation loss: 735.394835955289\n",
      "epoch: 949\n",
      "training loss: 544.7386768329313\n",
      "validation loss: 735.3214399754748\n",
      "epoch: 950\n",
      "training loss: 544.5560422807555\n",
      "validation loss: 735.248224727285\n",
      "epoch: 951\n",
      "training loss: 544.3736802702371\n",
      "validation loss: 735.175189557115\n",
      "epoch: 952\n",
      "training loss: 544.1915898750954\n",
      "validation loss: 735.1023338068075\n",
      "epoch: 953\n",
      "training loss: 544.0097701687389\n",
      "validation loss: 735.0296568128418\n",
      "epoch: 954\n",
      "training loss: 543.828220223969\n",
      "validation loss: 734.9571579054978\n",
      "epoch: 955\n",
      "training loss: 543.6469391126702\n",
      "validation loss: 734.8848364079952\n",
      "epoch: 956\n",
      "training loss: 543.4659259054822\n",
      "validation loss: 734.812691635615\n",
      "epoch: 957\n",
      "training loss: 543.2851796714568\n",
      "validation loss: 734.7407228948092\n",
      "epoch: 958\n",
      "training loss: 543.1046994776959\n",
      "validation loss: 734.6689294822977\n",
      "epoch: 959\n",
      "training loss: 542.9244843889727\n",
      "validation loss: 734.5973106841676\n",
      "epoch: 960\n",
      "training loss: 542.7445334673328\n",
      "validation loss: 734.5258657749741\n",
      "epoch: 961\n",
      "training loss: 542.5648457716773\n",
      "validation loss: 734.4545940168553\n",
      "epoch: 962\n",
      "training loss: 542.3854203573259\n",
      "validation loss: 734.3834946586676\n",
      "epoch: 963\n",
      "training loss: 542.2062562755622\n",
      "validation loss: 734.3125669351576\n",
      "epoch: 964\n",
      "training loss: 542.0273525731602\n",
      "validation loss: 734.2418100661797\n",
      "epoch: 965\n",
      "training loss: 541.8487082918948\n",
      "validation loss: 734.1712232559813\n",
      "epoch: 966\n",
      "training loss: 541.6703224680393\n",
      "validation loss: 734.1008056925713\n",
      "epoch: 967\n",
      "training loss: 541.4921941318512\n",
      "validation loss: 734.0305565472041\n",
      "epoch: 968\n",
      "training loss: 541.3143223070556\n",
      "validation loss: 733.960474974004\n",
      "epoch: 969\n",
      "training loss: 541.1367060103291\n",
      "validation loss: 733.8905601097703\n",
      "epoch: 970\n",
      "training loss: 540.9593442507977\n",
      "validation loss: 733.8208110740076\n",
      "epoch: 971\n",
      "training loss: 540.782236029558\n",
      "validation loss: 733.7512269692345\n",
      "epoch: 972\n",
      "training loss: 540.6053803392412\n",
      "validation loss: 733.6818068816366\n",
      "epoch: 973\n",
      "training loss: 540.428776163639\n",
      "validation loss: 733.6125498821388\n",
      "epoch: 974\n",
      "training loss: 540.2524224774168\n",
      "validation loss: 733.543455027983\n",
      "epoch: 975\n",
      "training loss: 540.0763182459502\n",
      "validation loss: 733.4745213649242\n",
      "epoch: 976\n",
      "training loss: 539.900462425321\n",
      "validation loss: 733.4057479301538\n",
      "epoch: 977\n",
      "training loss: 539.7248539625231\n",
      "validation loss: 733.3371337560967\n",
      "epoch: 978\n",
      "training loss: 539.549491795937\n",
      "validation loss: 733.2686778752274\n",
      "epoch: 979\n",
      "training loss: 539.3743748561394\n",
      "validation loss: 733.2003793260816\n",
      "epoch: 980\n",
      "training loss: 539.1995020671282\n",
      "validation loss: 733.1322371606379\n",
      "epoch: 981\n",
      "training loss: 539.0248723480497\n",
      "validation loss: 733.0642504532644\n",
      "epoch: 982\n",
      "training loss: 538.8504846155292\n",
      "validation loss: 732.9964183114116\n",
      "epoch: 983\n",
      "training loss: 538.6763377867022\n",
      "validation loss: 732.9287398882316\n",
      "epoch: 984\n",
      "training loss: 538.5024307830562\n",
      "validation loss: 732.8612143972625\n",
      "epoch: 985\n",
      "training loss: 538.3287625351744\n",
      "validation loss: 732.7938411292696\n",
      "epoch: 986\n",
      "training loss: 538.1553319884603\n",
      "validation loss: 732.7266194712472\n",
      "epoch: 987\n",
      "training loss: 537.9821381098893\n",
      "validation loss: 732.6595489274671\n",
      "epoch: 988\n",
      "training loss: 537.8091798957795\n",
      "validation loss: 732.5926291423027\n",
      "epoch: 989\n",
      "training loss: 537.636456380502\n",
      "validation loss: 732.5258599243509\n",
      "epoch: 990\n",
      "training loss: 537.46396664596\n",
      "validation loss: 732.4592412711322\n",
      "epoch: 991\n",
      "training loss: 537.2917098315463\n",
      "validation loss: 732.3927733933785\n",
      "epoch: 992\n",
      "training loss: 537.1196851441595\n",
      "validation loss: 732.3264567376211\n",
      "epoch: 993\n",
      "training loss: 536.9478918677225\n",
      "validation loss: 732.2602920055248\n",
      "epoch: 994\n",
      "training loss: 536.7763293715268\n",
      "validation loss: 732.1942801681837\n",
      "epoch: 995\n",
      "training loss: 536.6049971166269\n",
      "validation loss: 732.1284224734804\n",
      "epoch: 996\n",
      "training loss: 536.4338946594846\n",
      "validation loss: 732.0627204446362\n",
      "epoch: 997\n",
      "training loss: 536.2630216521065\n",
      "validation loss: 731.9971758682992\n",
      "epoch: 998\n",
      "training loss: 536.0923778380604\n",
      "validation loss: 731.9317907709732\n",
      "epoch: 999\n",
      "training loss: 535.9219630440092\n",
      "validation loss: 731.8665673832525\n",
      "epoch: 1000\n",
      "training loss: 535.7517771667252\n",
      "validation loss: 731.8015080921823\n",
      "epoch: 1001\n",
      "training loss: 535.5818201559359\n",
      "validation loss: 731.7366153830234\n",
      "epoch: 1002\n",
      "training loss: 535.4120919937386\n",
      "validation loss: 731.671891772645\n",
      "epoch: 1003\n",
      "training loss: 535.2425926716514\n",
      "validation loss: 731.6073397375962\n",
      "epoch: 1004\n",
      "training loss: 535.0733221665954\n",
      "validation loss: 731.542961640455\n",
      "epoch: 1005\n",
      "training loss: 534.904280417184\n",
      "validation loss: 731.4787596582819\n",
      "epoch: 1006\n",
      "training loss: 534.7354673016204\n",
      "validation loss: 731.4147357168475\n",
      "epoch: 1007\n",
      "training loss: 534.5668826183005\n",
      "validation loss: 731.3508914337888\n",
      "epoch: 1008\n",
      "training loss: 534.3985260699072\n",
      "validation loss: 731.2872280730815\n",
      "epoch: 1009\n",
      "training loss: 534.2303972514322\n",
      "validation loss: 731.2237465122805\n",
      "epoch: 1010\n",
      "training loss: 534.0624956422167\n",
      "validation loss: 731.1604472230338\n",
      "epoch: 1011\n",
      "training loss: 533.8948206018191\n",
      "validation loss: 731.0973302645149\n",
      "epoch: 1012\n",
      "training loss: 533.7273713693014\n",
      "validation loss: 731.0343952887478\n",
      "epoch: 1013\n",
      "training loss: 533.5601470654191\n",
      "validation loss: 730.9716415563366\n",
      "epoch: 1014\n",
      "training loss: 533.393146697173\n",
      "validation loss: 730.9090679609063\n",
      "epoch: 1015\n",
      "training loss: 533.226369164234\n",
      "validation loss: 730.846673060537\n",
      "epoch: 1016\n",
      "training loss: 533.0598132668649\n",
      "validation loss: 730.7844551146494\n",
      "epoch: 1017\n",
      "training loss: 532.8934777151053\n",
      "validation loss: 730.7224121250596\n",
      "epoch: 1018\n",
      "training loss: 532.72736113916\n",
      "validation loss: 730.6605418802524\n",
      "epoch: 1019\n",
      "training loss: 532.5614621010883\n",
      "validation loss: 730.5988420022678\n",
      "epoch: 1020\n",
      "training loss: 532.39577910805\n",
      "validation loss: 730.5373099958825\n",
      "epoch: 1021\n",
      "training loss: 532.2303106274869\n",
      "validation loss: 730.4759432999856\n",
      "epoch: 1022\n",
      "training loss: 532.0650551046843\n",
      "validation loss: 730.4147393411465\n",
      "epoch: 1023\n",
      "training loss: 531.9000109831674\n",
      "validation loss: 730.3536955892936\n",
      "epoch: 1024\n",
      "training loss: 531.7351767282757\n",
      "validation loss: 730.2928096151555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1025\n",
      "training loss: 531.5705508540415\n",
      "validation loss: 730.2320791486275\n",
      "epoch: 1026\n",
      "training loss: 531.4061319530884\n",
      "validation loss: 730.171502136451\n",
      "epoch: 1027\n",
      "training loss: 531.2419187287055\n",
      "validation loss: 730.1110767966231\n",
      "epoch: 1028\n",
      "training loss: 531.0779100274841\n",
      "validation loss: 730.0508016657521\n",
      "epoch: 1029\n",
      "training loss: 530.9141048700455\n",
      "validation loss: 729.9906756343237\n",
      "epoch: 1030\n",
      "training loss: 530.7505024764813\n",
      "validation loss: 729.930697963737\n",
      "epoch: 1031\n",
      "training loss: 530.5871022824101\n",
      "validation loss: 729.8708682782501\n",
      "epoch: 1032\n",
      "training loss: 530.4239039412151\n",
      "validation loss: 729.8111865250197\n",
      "epoch: 1033\n",
      "training loss: 530.2609073083071\n",
      "validation loss: 729.7516528964787\n",
      "epoch: 1034\n",
      "training loss: 530.0981124043078\n",
      "validation loss: 729.6922677115417\n",
      "epoch: 1035\n",
      "training loss: 529.9355193558481\n",
      "validation loss: 729.6330312554489\n",
      "epoch: 1036\n",
      "training loss: 529.7731283149658\n",
      "validation loss: 729.5739435820147\n",
      "epoch: 1037\n",
      "training loss: 529.6109393603875\n",
      "validation loss: 729.5150042858811\n",
      "epoch: 1038\n",
      "training loss: 529.4489523856971\n",
      "validation loss: 729.4562122551614\n",
      "epoch: 1039\n",
      "training loss: 529.2871669799396\n",
      "validation loss: 729.3975654157673\n",
      "epoch: 1040\n",
      "training loss: 529.125582305286\n",
      "validation loss: 729.3390604772153\n",
      "epoch: 1041\n",
      "training loss: 528.9641969739613\n",
      "validation loss: 729.2806926858594\n",
      "epoch: 1042\n",
      "training loss: 528.8030089230034\n",
      "validation loss: 729.2224555858037\n",
      "epoch: 1043\n",
      "training loss: 528.6420152809968\n",
      "validation loss: 729.1643407810526\n",
      "epoch: 1044\n",
      "training loss: 528.4812122160172\n",
      "validation loss: 729.1063376856572\n",
      "epoch: 1045\n",
      "training loss: 528.3205947487307\n",
      "validation loss: 729.0484332423886\n",
      "epoch: 1046\n",
      "training loss: 528.160156508612\n",
      "validation loss: 728.9906115851943\n",
      "epoch: 1047\n",
      "training loss: 527.9998894040336\n",
      "validation loss: 728.9328536166581\n",
      "epoch: 1048\n",
      "training loss: 527.8397831678088\n",
      "validation loss: 728.8751364693408\n",
      "epoch: 1049\n",
      "training loss: 527.6798247283538\n",
      "validation loss: 728.8174328208836\n",
      "epoch: 1050\n",
      "training loss: 527.5199973441834\n",
      "validation loss: 728.7597100413176\n",
      "epoch: 1051\n",
      "training loss: 527.3602794317452\n",
      "validation loss: 728.701929177044\n",
      "epoch: 1052\n",
      "training loss: 527.2006430312982\n",
      "validation loss: 728.6440438409783\n",
      "epoch: 1053\n",
      "training loss: 527.0410519391589\n",
      "validation loss: 728.5859992264374\n",
      "epoch: 1054\n",
      "training loss: 526.8814597979807\n",
      "validation loss: 728.527731778185\n",
      "epoch: 1055\n",
      "training loss: 526.7218091125401\n",
      "validation loss: 728.4691706803535\n",
      "epoch: 1056\n",
      "training loss: 526.5620336274164\n",
      "validation loss: 728.410243430132\n",
      "epoch: 1057\n",
      "training loss: 526.4020689846365\n",
      "validation loss: 728.3508893119844\n",
      "epoch: 1058\n",
      "training loss: 526.241878599643\n",
      "validation loss: 728.2910854207499\n",
      "epoch: 1059\n",
      "training loss: 526.0814966981684\n",
      "validation loss: 728.2308859883292\n",
      "epoch: 1060\n",
      "training loss: 525.9210680092999\n",
      "validation loss: 728.1704605583866\n",
      "epoch: 1061\n",
      "training loss: 525.7608348188971\n",
      "validation loss: 728.1100936745237\n",
      "epoch: 1062\n",
      "training loss: 525.601046514026\n",
      "validation loss: 728.0501142841686\n",
      "epoch: 1063\n",
      "training loss: 525.4418574991148\n",
      "validation loss: 727.9907876711368\n",
      "epoch: 1064\n",
      "training loss: 525.2833008145386\n",
      "validation loss: 727.9322520069646\n",
      "epoch: 1065\n",
      "training loss: 525.1253310122238\n",
      "validation loss: 727.874530120467\n",
      "epoch: 1066\n",
      "training loss: 524.9678755719766\n",
      "validation loss: 727.8175766304959\n",
      "epoch: 1067\n",
      "training loss: 524.8108642784155\n",
      "validation loss: 727.7613191608328\n",
      "epoch: 1068\n",
      "training loss: 524.6542393366602\n",
      "validation loss: 727.7056819336237\n",
      "epoch: 1069\n",
      "training loss: 524.4979561714412\n",
      "validation loss: 727.6505959782061\n",
      "epoch: 1070\n",
      "training loss: 524.3419811922157\n",
      "validation loss: 727.5960021527369\n",
      "epoch: 1071\n",
      "training loss: 524.1862891595646\n",
      "validation loss: 727.541851009301\n",
      "epoch: 1072\n",
      "training loss: 524.0308609679687\n",
      "validation loss: 727.4881015443083\n",
      "epoch: 1073\n",
      "training loss: 523.8756819700839\n",
      "validation loss: 727.4347197316034\n",
      "epoch: 1074\n",
      "training loss: 523.7207407619255\n",
      "validation loss: 727.3816771802311\n",
      "epoch: 1075\n",
      "training loss: 523.5660283144986\n",
      "validation loss: 727.328950014725\n",
      "epoch: 1076\n",
      "training loss: 523.411537353545\n",
      "validation loss: 727.2765179786066\n",
      "epoch: 1077\n",
      "training loss: 523.2572619137766\n",
      "validation loss: 727.2243637292769\n",
      "epoch: 1078\n",
      "training loss: 523.1031970152893\n",
      "validation loss: 727.172472286441\n",
      "epoch: 1079\n",
      "training loss: 522.949338425769\n",
      "validation loss: 727.1208305998559\n",
      "epoch: 1080\n",
      "training loss: 522.7956824833099\n",
      "validation loss: 727.0694272084594\n",
      "epoch: 1081\n",
      "training loss: 522.6422259623681\n",
      "validation loss: 727.018251969092\n",
      "epoch: 1082\n",
      "training loss: 522.4889659706408\n",
      "validation loss: 726.9672958382306\n",
      "epoch: 1083\n",
      "training loss: 522.3358998682372\n",
      "validation loss: 726.9165506942354\n",
      "epoch: 1084\n",
      "training loss: 522.1830252029755\n",
      "validation loss: 726.8660091907267\n",
      "epoch: 1085\n",
      "training loss: 522.0303396573289\n",
      "validation loss: 726.8156646340449\n",
      "epoch: 1086\n",
      "training loss: 521.8778410037215\n",
      "validation loss: 726.7655108794722\n",
      "epoch: 1087\n",
      "training loss: 521.7255270657064\n",
      "validation loss: 726.7155422421968\n",
      "epoch: 1088\n",
      "training loss: 521.5733956831401\n",
      "validation loss: 726.6657534199709\n",
      "epoch: 1089\n",
      "training loss: 521.4214446799103\n",
      "validation loss: 726.6161394251656\n",
      "epoch: 1090\n",
      "training loss: 521.2696718330967\n",
      "validation loss: 726.566695524517\n",
      "epoch: 1091\n",
      "training loss: 521.1180748427362\n",
      "validation loss: 726.51741718537\n",
      "epoch: 1092\n",
      "training loss: 520.966651301644\n",
      "validation loss: 726.4683000276821\n",
      "epoch: 1093\n",
      "training loss: 520.815398665058\n",
      "validation loss: 726.4193397815146\n",
      "epoch: 1094\n",
      "training loss: 520.6643142203152\n",
      "validation loss: 726.3705322502609\n",
      "epoch: 1095\n",
      "training loss: 520.5133950573686\n",
      "validation loss: 726.3218732804658\n",
      "epoch: 1096\n",
      "training loss: 520.3626380418615\n",
      "validation loss: 726.2733587398792\n",
      "epoch: 1097\n",
      "training loss: 520.21203979378\n",
      "validation loss: 726.2249845063535\n",
      "epoch: 1098\n",
      "training loss: 520.061596676608\n",
      "validation loss: 726.1767464714616\n",
      "epoch: 1099\n",
      "training loss: 519.9113048045248\n",
      "validation loss: 726.1286405642389\n",
      "epoch: 1100\n",
      "training loss: 519.7611600786498\n",
      "validation loss: 726.0806628022403\n",
      "epoch: 1101\n",
      "training loss: 519.6111582675102\n",
      "validation loss: 726.0328093789377\n",
      "epoch: 1102\n",
      "training loss: 519.4612951512172\n",
      "validation loss: 725.9850767979161\n",
      "epoch: 1103\n",
      "training loss: 519.3115667518077\n",
      "validation loss: 725.9374620644845\n",
      "epoch: 1104\n",
      "training loss: 519.1619696709705\n",
      "validation loss: 725.8899629427032\n",
      "epoch: 1105\n",
      "training loss: 519.0125015463607\n",
      "validation loss: 725.8425782783974\n",
      "epoch: 1106\n",
      "training loss: 518.8631616132499\n",
      "validation loss: 725.795308374118\n",
      "epoch: 1107\n",
      "training loss: 518.7139513153654\n",
      "validation loss: 725.7481553789662\n",
      "epoch: 1108\n",
      "training loss: 518.5648748509269\n",
      "validation loss: 725.7011236267816\n",
      "epoch: 1109\n",
      "training loss: 518.4159394853502\n",
      "validation loss: 725.6542198288037\n",
      "epoch: 1110\n",
      "training loss: 518.2671554462942\n",
      "validation loss: 725.6074530173571\n",
      "epoch: 1111\n",
      "training loss: 518.1185352786936\n",
      "validation loss: 725.5608341638825\n",
      "epoch: 1112\n",
      "training loss: 517.9700926874278\n",
      "validation loss: 725.5143754661117\n",
      "epoch: 1113\n",
      "training loss: 517.8218410839169\n",
      "validation loss: 725.468089398896\n",
      "epoch: 1114\n",
      "training loss: 517.6737921818782\n",
      "validation loss: 725.4219877084047\n",
      "epoch: 1115\n",
      "training loss: 517.5259549748613\n",
      "validation loss: 725.3760805538645\n",
      "epoch: 1116\n",
      "training loss: 517.3783352777639\n",
      "validation loss: 725.3303759489148\n",
      "epoch: 1117\n",
      "training loss: 517.2309358154434\n",
      "validation loss: 725.284879554438\n",
      "epoch: 1118\n",
      "training loss: 517.0837566963553\n",
      "validation loss: 725.2395947774296\n",
      "epoch: 1119\n",
      "training loss: 516.9367960640822\n",
      "validation loss: 725.1945230741294\n",
      "epoch: 1120\n",
      "training loss: 516.7900507552914\n",
      "validation loss: 725.149664347288\n",
      "epoch: 1121\n",
      "training loss: 516.6435168615282\n",
      "validation loss: 725.1050173509839\n",
      "epoch: 1122\n",
      "training loss: 516.4971901558308\n",
      "validation loss: 725.0605800504644\n",
      "epoch: 1123\n",
      "training loss: 516.3510663869449\n",
      "validation loss: 725.0163499143966\n",
      "epoch: 1124\n",
      "training loss: 516.2051414639641\n",
      "validation loss: 724.9723241370006\n",
      "epoch: 1125\n",
      "training loss: 516.0594115594445\n",
      "validation loss: 724.9284997981939\n",
      "epoch: 1126\n",
      "training loss: 515.9138731564753\n",
      "validation loss: 724.8848739737961\n",
      "epoch: 1127\n",
      "training loss: 515.76852305981\n",
      "validation loss: 724.8414438079185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1128\n",
      "training loss: 515.6233583855669\n",
      "validation loss: 724.7982065579782\n",
      "epoch: 1129\n",
      "training loss: 515.4783765393428\n",
      "validation loss: 724.755159620557\n",
      "epoch: 1130\n",
      "training loss: 515.3335751890625\n",
      "validation loss: 724.7123005442536\n",
      "epoch: 1131\n",
      "training loss: 515.1889522364394\n",
      "validation loss: 724.6696270339287\n",
      "epoch: 1132\n",
      "training loss: 515.0445057892586\n",
      "validation loss: 724.6271369494308\n",
      "epoch: 1133\n",
      "training loss: 514.900234135662\n",
      "validation loss: 724.5848283008987\n",
      "epoch: 1134\n",
      "training loss: 514.7561357209452\n",
      "validation loss: 724.5426992420563\n",
      "epoch: 1135\n",
      "training loss: 514.6122091270001\n",
      "validation loss: 724.5007480624369\n",
      "epoch: 1136\n",
      "training loss: 514.4684530543212\n",
      "validation loss: 724.4589731791364\n",
      "epoch: 1137\n",
      "training loss: 514.3248663063732\n",
      "validation loss: 724.4173731284919\n",
      "epoch: 1138\n",
      "training loss: 514.1814477760857\n",
      "validation loss: 724.3759465579221\n",
      "epoch: 1139\n",
      "training loss: 514.0381964342168\n",
      "validation loss: 724.3346922180784\n",
      "epoch: 1140\n",
      "training loss: 513.8951113193416\n",
      "validation loss: 724.2936089553837\n",
      "epoch: 1141\n",
      "training loss: 513.7521915292476\n",
      "validation loss: 724.2526957050069\n",
      "epoch: 1142\n",
      "training loss: 513.6094362135334\n",
      "validation loss: 724.2119514842858\n",
      "epoch: 1143\n",
      "training loss: 513.4668445672365\n",
      "validation loss: 724.1713753865932\n",
      "epoch: 1144\n",
      "training loss: 513.3244158253423\n",
      "validation loss: 724.1309665756406\n",
      "epoch: 1145\n",
      "training loss: 513.182149258039\n",
      "validation loss: 724.0907242801966\n",
      "epoch: 1146\n",
      "training loss: 513.040044166615\n",
      "validation loss: 724.0506477892031\n",
      "epoch: 1147\n",
      "training loss: 512.8980998798971\n",
      "validation loss: 724.0107364472642\n",
      "epoch: 1148\n",
      "training loss: 512.7563157511554\n",
      "validation loss: 723.9709896504847\n",
      "epoch: 1149\n",
      "training loss: 512.6146911554029\n",
      "validation loss: 723.9314068426369\n",
      "epoch: 1150\n",
      "training loss: 512.4732254870349\n",
      "validation loss: 723.8919875116328\n",
      "epoch: 1151\n",
      "training loss: 512.3319181577589\n",
      "validation loss: 723.8527311862821\n",
      "epoch: 1152\n",
      "training loss: 512.1907685947713\n",
      "validation loss: 723.8136374333114\n",
      "epoch: 1153\n",
      "training loss: 512.0497762391492\n",
      "validation loss: 723.7747058546339\n",
      "epoch: 1154\n",
      "training loss: 511.9089405444245\n",
      "validation loss: 723.7359360848463\n",
      "epoch: 1155\n",
      "training loss: 511.76826097531557\n",
      "validation loss: 723.6973277889356\n",
      "epoch: 1156\n",
      "training loss: 511.6277370065937\n",
      "validation loss: 723.6588806601892\n",
      "epoch: 1157\n",
      "training loss: 511.4873681220681\n",
      "validation loss: 723.6205944182857\n",
      "epoch: 1158\n",
      "training loss: 511.3471538136687\n",
      "validation loss: 723.5824688075581\n",
      "epoch: 1159\n",
      "training loss: 511.20709358061714\n",
      "validation loss: 723.5445035954172\n",
      "epoch: 1160\n",
      "training loss: 511.0671869286705\n",
      "validation loss: 723.5066985709249\n",
      "epoch: 1161\n",
      "training loss: 510.9274333694287\n",
      "validation loss: 723.4690535435103\n",
      "epoch: 1162\n",
      "training loss: 510.7878324196956\n",
      "validation loss: 723.4315683418146\n",
      "epoch: 1163\n",
      "training loss: 510.64838360088635\n",
      "validation loss: 723.394242812661\n",
      "epoch: 1164\n",
      "training loss: 510.50908643847083\n",
      "validation loss: 723.3570768201461\n",
      "epoch: 1165\n",
      "training loss: 510.3699404614515\n",
      "validation loss: 723.3200702448391\n",
      "epoch: 1166\n",
      "training loss: 510.23094520186334\n",
      "validation loss: 723.2832229830908\n",
      "epoch: 1167\n",
      "training loss: 510.09210019429634\n",
      "validation loss: 723.2465349464458\n",
      "epoch: 1168\n",
      "training loss: 509.9534049754309\n",
      "validation loss: 723.2100060611511\n",
      "epoch: 1169\n",
      "training loss: 509.8148590835827\n",
      "validation loss: 723.1736362677689\n",
      "epoch: 1170\n",
      "training loss: 509.676462058254\n",
      "validation loss: 723.1374255208758\n",
      "epoch: 1171\n",
      "training loss: 509.5382134396829\n",
      "validation loss: 723.1013737888636\n",
      "epoch: 1172\n",
      "training loss: 509.40011276839033\n",
      "validation loss: 723.0654810538324\n",
      "epoch: 1173\n",
      "training loss: 509.26215958471624\n",
      "validation loss: 723.029747311578\n",
      "epoch: 1174\n",
      "training loss: 509.12435342834243\n",
      "validation loss: 722.9941725716762\n",
      "epoch: 1175\n",
      "training loss: 508.9866938377958\n",
      "validation loss: 722.9587568576695\n",
      "epoch: 1176\n",
      "training loss: 508.8491803499285\n",
      "validation loss: 722.9235002073526\n",
      "epoch: 1177\n",
      "training loss: 508.7118124993646\n",
      "validation loss: 722.8884026731719\n",
      "epoch: 1178\n",
      "training loss: 508.5745898179131\n",
      "validation loss: 722.8534643227342\n",
      "epoch: 1179\n",
      "training loss: 508.4375118339345\n",
      "validation loss: 722.8186852394427\n",
      "epoch: 1180\n",
      "training loss: 508.30057807165576\n",
      "validation loss: 722.78406552326\n",
      "epoch: 1181\n",
      "training loss: 508.163788050424\n",
      "validation loss: 722.7496052916172\n",
      "epoch: 1182\n",
      "training loss: 508.0271412838887\n",
      "validation loss: 722.7153046804788\n",
      "epoch: 1183\n",
      "training loss: 507.8906372790989\n",
      "validation loss: 722.6811638455777\n",
      "epoch: 1184\n",
      "training loss: 507.7542755355046\n",
      "validation loss: 722.6471829638415\n",
      "epoch: 1185\n",
      "training loss: 507.6180555438422\n",
      "validation loss: 722.6133622350328\n",
      "epoch: 1186\n",
      "training loss: 507.4819767848891\n",
      "validation loss: 722.5797018836237\n",
      "epoch: 1187\n",
      "training loss: 507.34603872806207\n",
      "validation loss: 722.5462021609424\n",
      "epoch: 1188\n",
      "training loss: 507.2102408298343\n",
      "validation loss: 722.5128633476219\n",
      "epoch: 1189\n",
      "training loss: 507.07458253193954\n",
      "validation loss: 722.4796857563949\n",
      "epoch: 1190\n",
      "training loss: 506.93906325932625\n",
      "validation loss: 722.4466697352857\n",
      "epoch: 1191\n",
      "training loss: 506.80368241781457\n",
      "validation loss: 722.413815671258\n",
      "epoch: 1192\n",
      "training loss: 506.66843939140443\n",
      "validation loss: 722.3811239943875\n",
      "epoch: 1193\n",
      "training loss: 506.5333335391643\n",
      "validation loss: 722.3485951826455\n",
      "epoch: 1194\n",
      "training loss: 506.39836419162367\n",
      "validation loss: 722.316229767396\n",
      "epoch: 1195\n",
      "training loss: 506.26353064656513\n",
      "validation loss: 722.2840283397228\n",
      "epoch: 1196\n",
      "training loss: 506.12883216409466\n",
      "validation loss: 722.2519915577383\n",
      "epoch: 1197\n",
      "training loss: 505.99426796083634\n",
      "validation loss: 722.2201201550507\n",
      "epoch: 1198\n",
      "training loss: 505.85983720305654\n",
      "validation loss: 722.188414950608\n",
      "epoch: 1199\n",
      "training loss: 505.725538998477\n",
      "validation loss: 722.1568768601827\n",
      "epoch: 1200\n",
      "training loss: 505.5913723864641\n",
      "validation loss: 722.1255069098315\n",
      "epoch: 1201\n",
      "training loss: 505.45733632620346\n",
      "validation loss: 722.0943062517314\n",
      "epoch: 1202\n",
      "training loss: 505.32342968234445\n",
      "validation loss: 722.0632761829094\n",
      "epoch: 1203\n",
      "training loss: 505.1896512074539\n",
      "validation loss: 722.0324181674956\n",
      "epoch: 1204\n",
      "training loss: 505.0559995204039\n",
      "validation loss: 722.0017338633168\n",
      "epoch: 1205\n",
      "training loss: 504.92247307953795\n",
      "validation loss: 721.971225153856\n",
      "epoch: 1206\n",
      "training loss: 504.7890701490672\n",
      "validation loss: 721.9408941868992\n",
      "epoch: 1207\n",
      "training loss: 504.6557887565952\n",
      "validation loss: 721.9107434215822\n",
      "epoch: 1208\n",
      "training loss: 504.5226266389004\n",
      "validation loss: 721.88077568607\n",
      "epoch: 1209\n",
      "training loss: 504.38958117199115\n",
      "validation loss: 721.8509942488072\n",
      "epoch: 1210\n",
      "training loss: 504.25664927982893\n",
      "validation loss: 721.8214029072566\n",
      "epoch: 1211\n",
      "training loss: 504.12382731374254\n",
      "validation loss: 721.7920060993782\n",
      "epoch: 1212\n",
      "training loss: 503.99111089098255\n",
      "validation loss: 721.7628090449917\n",
      "epoch: 1213\n",
      "training loss: 503.85849467543824\n",
      "validation loss: 721.7338179268246\n",
      "epoch: 1214\n",
      "training loss: 503.72597207511694\n",
      "validation loss: 721.705040124871\n",
      "epoch: 1215\n",
      "training loss: 503.5935348177015\n",
      "validation loss: 721.6764845232084\n",
      "epoch: 1216\n",
      "training loss: 503.46117234410576\n",
      "validation loss: 721.6481619165064\n",
      "epoch: 1217\n",
      "training loss: 503.32887092485316\n",
      "validation loss: 721.6200855553752\n",
      "epoch: 1218\n",
      "training loss: 503.19661234540456\n",
      "validation loss: 721.5922718873368\n",
      "epoch: 1219\n",
      "training loss: 503.0643719067168\n",
      "validation loss: 721.5647415762038\n",
      "epoch: 1220\n",
      "training loss: 502.932115315461\n",
      "validation loss: 721.5375209201742\n",
      "epoch: 1221\n",
      "training loss: 502.7997937424474\n",
      "validation loss: 721.5106438399293\n",
      "epoch: 1222\n",
      "training loss: 502.6673358320793\n",
      "validation loss: 721.4841546660887\n",
      "epoch: 1223\n",
      "training loss: 502.53463469582675\n",
      "validation loss: 721.45811198054\n",
      "epoch: 1224\n",
      "training loss: 502.4015271786345\n",
      "validation loss: 721.4325936021622\n",
      "epoch: 1225\n",
      "training loss: 502.2677638781138\n",
      "validation loss: 721.4077019211965\n",
      "epoch: 1226\n",
      "training loss: 502.1329802992742\n",
      "validation loss: 721.3835655366697\n",
      "epoch: 1227\n",
      "training loss: 501.9967311564004\n",
      "validation loss: 721.3603233819713\n",
      "epoch: 1228\n",
      "training loss: 501.85877927324856\n",
      "validation loss: 721.3380566251702\n",
      "epoch: 1229\n",
      "training loss: 501.7198000669493\n",
      "validation loss: 721.316630723016\n",
      "epoch: 1230\n",
      "training loss: 501.5815965587909\n",
      "validation loss: 721.2955714288856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1231\n",
      "training loss: 501.4455287990887\n",
      "validation loss: 721.2743280800228\n",
      "epoch: 1232\n",
      "training loss: 501.3115007041842\n",
      "validation loss: 721.2526603008246\n",
      "epoch: 1233\n",
      "training loss: 501.1788759978969\n",
      "validation loss: 721.2305559057318\n",
      "epoch: 1234\n",
      "training loss: 501.04715804799514\n",
      "validation loss: 721.2080509446406\n",
      "epoch: 1235\n",
      "training loss: 500.9160492662\n",
      "validation loss: 721.18518782132\n",
      "epoch: 1236\n",
      "training loss: 500.7853771454259\n",
      "validation loss: 721.1620138977052\n",
      "epoch: 1237\n",
      "training loss: 500.65503859803533\n",
      "validation loss: 721.1385792643849\n",
      "epoch: 1238\n",
      "training loss: 500.5249691008912\n",
      "validation loss: 721.1149331166301\n",
      "epoch: 1239\n",
      "training loss: 500.39512628010016\n",
      "validation loss: 721.091121064657\n",
      "epoch: 1240\n",
      "training loss: 500.2654810025494\n",
      "validation loss: 721.0671838339174\n",
      "epoch: 1241\n",
      "training loss: 500.1360123679535\n",
      "validation loss: 721.0431569966007\n",
      "epoch: 1242\n",
      "training loss: 500.0067047907413\n",
      "validation loss: 721.0190712890696\n",
      "epoch: 1243\n",
      "training loss: 499.87754624821974\n",
      "validation loss: 720.9949531957683\n",
      "epoch: 1244\n",
      "training loss: 499.748527209846\n",
      "validation loss: 720.9708256135999\n",
      "epoch: 1245\n",
      "training loss: 499.619639984641\n",
      "validation loss: 720.9467085033153\n",
      "epoch: 1246\n",
      "training loss: 499.49087833971925\n",
      "validation loss: 720.922619488275\n",
      "epoch: 1247\n",
      "training loss: 499.3622373043512\n",
      "validation loss: 720.8985743880645\n",
      "epoch: 1248\n",
      "training loss: 499.23371310570525\n",
      "validation loss: 720.8745876851906\n",
      "epoch: 1249\n",
      "training loss: 499.10530319645596\n",
      "validation loss: 720.8506729243013\n",
      "epoch: 1250\n",
      "training loss: 498.97700633719205\n",
      "validation loss: 720.8268430393875\n",
      "epoch: 1251\n",
      "training loss: 498.8488226924145\n",
      "validation loss: 720.8031105983509\n",
      "epoch: 1252\n",
      "training loss: 498.7207538925469\n",
      "validation loss: 720.7794879491503\n",
      "epoch: 1253\n",
      "training loss: 498.5928030117239\n",
      "validation loss: 720.7559872506771\n",
      "epoch: 1254\n",
      "training loss: 498.4649744185982\n",
      "validation loss: 720.732620377643\n",
      "epoch: 1255\n",
      "training loss: 498.3372734793494\n",
      "validation loss: 720.70939870371\n",
      "epoch: 1256\n",
      "training loss: 498.2097061270125\n",
      "validation loss: 720.6863327891632\n",
      "epoch: 1257\n",
      "training loss: 498.0822783498135\n",
      "validation loss: 720.6634320227215\n",
      "epoch: 1258\n",
      "training loss: 497.95499567879773\n",
      "validation loss: 720.6407042827049\n",
      "epoch: 1259\n",
      "training loss: 497.8278627591588\n",
      "validation loss: 720.6181556829305\n",
      "epoch: 1260\n",
      "training loss: 497.7008830677277\n",
      "validation loss: 720.5957904511175\n",
      "epoch: 1261\n",
      "training loss: 497.5740588005142\n",
      "validation loss: 720.573610957623\n",
      "epoch: 1262\n",
      "training loss: 497.4473909148917\n",
      "validation loss: 720.551617880509\n",
      "epoch: 1263\n",
      "training loss: 497.3208792843162\n",
      "validation loss: 720.5298104693529\n",
      "epoch: 1264\n",
      "training loss: 497.1945229144446\n",
      "validation loss: 720.5081868602414\n",
      "epoch: 1265\n",
      "training loss: 497.06832017508054\n",
      "validation loss: 720.4867443972381\n",
      "epoch: 1266\n",
      "training loss: 496.9422690156795\n",
      "validation loss: 720.4654799267166\n",
      "epoch: 1267\n",
      "training loss: 496.8163671466136\n",
      "validation loss: 720.4443900447171\n",
      "epoch: 1268\n",
      "training loss: 496.69061218014\n",
      "validation loss: 720.4234712898905\n",
      "epoch: 1269\n",
      "training loss: 496.5650017326811\n",
      "validation loss: 720.4027202835233\n",
      "epoch: 1270\n",
      "training loss: 496.4395334940017\n",
      "validation loss: 720.3821338233771\n",
      "epoch: 1271\n",
      "training loss: 496.3142052702114\n",
      "validation loss: 720.3617089402558\n",
      "epoch: 1272\n",
      "training loss: 496.1890150072819\n",
      "validation loss: 720.3414429263415\n",
      "epoch: 1273\n",
      "training loss: 496.06396080080555\n",
      "validation loss: 720.3213333433532\n",
      "epoch: 1274\n",
      "training loss: 495.9390408965176\n",
      "validation loss: 720.3013780170938\n",
      "epoch: 1275\n",
      "training loss: 495.814253684973\n",
      "validation loss: 720.281575023449\n",
      "epoch: 1276\n",
      "training loss: 495.6895976928035\n",
      "validation loss: 720.2619226695289\n",
      "epoch: 1277\n",
      "training loss: 495.56507157223143\n",
      "validation loss: 720.2424194725338\n",
      "epoch: 1278\n",
      "training loss: 495.4406740899504\n",
      "validation loss: 720.2230641380836\n",
      "epoch: 1279\n",
      "training loss: 495.31640411608083\n",
      "validation loss: 720.2038555390994\n",
      "epoch: 1280\n",
      "training loss: 495.1922606136254\n",
      "validation loss: 720.1847926959007\n",
      "epoch: 1281\n",
      "training loss: 495.06824262865854\n",
      "validation loss: 720.1658747578576\n",
      "epoch: 1282\n",
      "training loss: 494.9443492813576\n",
      "validation loss: 720.1471009867466\n",
      "epoch: 1283\n",
      "training loss: 494.82057975790343\n",
      "validation loss: 720.1284707418231\n",
      "epoch: 1284\n",
      "training loss: 494.69693330322855\n",
      "validation loss: 720.109983466546\n",
      "epoch: 1285\n",
      "training loss: 494.57340921456296\n",
      "validation loss: 720.0916386768499\n",
      "epoch: 1286\n",
      "training loss: 494.4500068357154\n",
      "validation loss: 720.0734359508306\n",
      "epoch: 1287\n",
      "training loss: 494.3267255520169\n",
      "validation loss: 720.0553749197111\n",
      "epoch: 1288\n",
      "training loss: 494.20356478586143\n",
      "validation loss: 720.0374552599541\n",
      "epoch: 1289\n",
      "training loss: 494.08052399277346\n",
      "validation loss: 720.0196766863902\n",
      "epoch: 1290\n",
      "training loss: 493.95760265794235\n",
      "validation loss: 720.0020389462434\n",
      "epoch: 1291\n",
      "training loss: 493.83480029316763\n",
      "validation loss: 719.9845418139503\n",
      "epoch: 1292\n",
      "training loss: 493.712116434164\n",
      "validation loss: 719.9671850866732\n",
      "epoch: 1293\n",
      "training loss: 493.58955063818155\n",
      "validation loss: 719.9499685804196\n",
      "epoch: 1294\n",
      "training loss: 493.46710248190175\n",
      "validation loss: 719.9328921266955\n",
      "epoch: 1295\n",
      "training loss: 493.3447715595761\n",
      "validation loss: 719.9159555696212\n",
      "epoch: 1296\n",
      "training loss: 493.22255748137354\n",
      "validation loss: 719.8991587634539\n",
      "epoch: 1297\n",
      "training loss: 493.1004598719153\n",
      "validation loss: 719.8825015704612\n",
      "epoch: 1298\n",
      "training loss: 492.9784783689688\n",
      "validation loss: 719.8659838591035\n",
      "epoch: 1299\n",
      "training loss: 492.85661262228604\n",
      "validation loss: 719.8496055024823\n",
      "epoch: 1300\n",
      "training loss: 492.7348622925641\n",
      "validation loss: 719.8333663770221\n",
      "epoch: 1301\n",
      "training loss: 492.61322705051674\n",
      "validation loss: 719.817266361351\n",
      "epoch: 1302\n",
      "training loss: 492.4917065760413\n",
      "validation loss: 719.8013053353606\n",
      "epoch: 1303\n",
      "training loss: 492.37030055747033\n",
      "validation loss: 719.7854831794142\n",
      "epoch: 1304\n",
      "training loss: 492.24900869089964\n",
      "validation loss: 719.7697997736869\n",
      "epoch: 1305\n",
      "training loss: 492.12783067958\n",
      "validation loss: 719.7542549976192\n",
      "epoch: 1306\n",
      "training loss: 492.00676623336943\n",
      "validation loss: 719.738848729468\n",
      "epoch: 1307\n",
      "training loss: 491.8858150682374\n",
      "validation loss: 719.7235808459425\n",
      "epoch: 1308\n",
      "training loss: 491.76497690581334\n",
      "validation loss: 719.708451221912\n",
      "epoch: 1309\n",
      "training loss: 491.6442514729794\n",
      "validation loss: 719.6934597301729\n",
      "epoch: 1310\n",
      "training loss: 491.5236385014971\n",
      "validation loss: 719.6786062412749\n",
      "epoch: 1311\n",
      "training loss: 491.4031377276676\n",
      "validation loss: 719.6638906233848\n",
      "epoch: 1312\n",
      "training loss: 491.28274889202225\n",
      "validation loss: 719.6493127421942\n",
      "epoch: 1313\n",
      "training loss: 491.16247173903736\n",
      "validation loss: 719.6348724608564\n",
      "epoch: 1314\n",
      "training loss: 491.04230601687476\n",
      "validation loss: 719.6205696399502\n",
      "epoch: 1315\n",
      "training loss: 490.9222514771411\n",
      "validation loss: 719.606404137469\n",
      "epoch: 1316\n",
      "training loss: 490.8023078746686\n",
      "validation loss: 719.592375808825\n",
      "epoch: 1317\n",
      "training loss: 490.6824749673107\n",
      "validation loss: 719.5784845068714\n",
      "epoch: 1318\n",
      "training loss: 490.5627525157542\n",
      "validation loss: 719.5647300819353\n",
      "epoch: 1319\n",
      "training loss: 490.4431402833452\n",
      "validation loss: 719.5511123818624\n",
      "epoch: 1320\n",
      "training loss: 490.32363803592744\n",
      "validation loss: 719.5376312520659\n",
      "epoch: 1321\n",
      "training loss: 490.2042455416917\n",
      "validation loss: 719.5242865355871\n",
      "epoch: 1322\n",
      "training loss: 490.0849625710362\n",
      "validation loss: 719.5110780731538\n",
      "epoch: 1323\n",
      "training loss: 489.96578889643536\n",
      "validation loss: 719.4980057032462\n",
      "epoch: 1324\n",
      "training loss: 489.84672429231694\n",
      "validation loss: 719.4850692621641\n",
      "epoch: 1325\n",
      "training loss: 489.72776853494713\n",
      "validation loss: 719.4722685840919\n",
      "epoch: 1326\n",
      "training loss: 489.6089214023226\n",
      "validation loss: 719.459603501169\n",
      "epoch: 1327\n",
      "training loss: 489.49018267406757\n",
      "validation loss: 719.447073843554\n",
      "epoch: 1328\n",
      "training loss: 489.3715521313379\n",
      "validation loss: 719.4346794394933\n",
      "epoch: 1329\n",
      "training loss: 489.253029556728\n",
      "validation loss: 719.4224201153825\n",
      "epoch: 1330\n",
      "training loss: 489.1346147341843\n",
      "validation loss: 719.4102956958313\n",
      "epoch: 1331\n",
      "training loss: 489.01630744892157\n",
      "validation loss: 719.3983060037225\n",
      "epoch: 1332\n",
      "training loss: 488.89810748734203\n",
      "validation loss: 719.3864508602693\n",
      "epoch: 1333\n",
      "training loss: 488.7800146369585\n",
      "validation loss: 719.374730085071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1334\n",
      "training loss: 488.66202868631956\n",
      "validation loss: 719.3631434961652\n",
      "epoch: 1335\n",
      "training loss: 488.54414942493713\n",
      "validation loss: 719.351690910078\n",
      "epoch: 1336\n",
      "training loss: 488.42637664321575\n",
      "validation loss: 719.3403721418691\n",
      "epoch: 1337\n",
      "training loss: 488.3087101323833\n",
      "validation loss: 719.3291870051765\n",
      "epoch: 1338\n",
      "training loss: 488.19114968442227\n",
      "validation loss: 719.3181353122562\n",
      "epoch: 1339\n",
      "training loss: 488.0736950920035\n",
      "validation loss: 719.3072168740208\n",
      "epoch: 1340\n",
      "training loss: 487.9563461484183\n",
      "validation loss: 719.2964315000729\n",
      "epoch: 1341\n",
      "training loss: 487.8391026475114\n",
      "validation loss: 719.2857789987381\n",
      "epoch: 1342\n",
      "training loss: 487.721964383615\n",
      "validation loss: 719.2752591770925\n",
      "epoch: 1343\n",
      "training loss: 487.60493115147966\n",
      "validation loss: 719.2648718409885\n",
      "epoch: 1344\n",
      "training loss: 487.48800274620635\n",
      "validation loss: 719.2546167950787\n",
      "epoch: 1345\n",
      "training loss: 487.37117896317676\n",
      "validation loss: 719.2444938428355\n",
      "epoch: 1346\n",
      "training loss: 487.2544595979812\n",
      "validation loss: 719.2345027865678\n",
      "epoch: 1347\n",
      "training loss: 487.1378444463455\n",
      "validation loss: 719.2246434274363\n",
      "epoch: 1348\n",
      "training loss: 487.02133330405474\n",
      "validation loss: 719.2149155654652\n",
      "epoch: 1349\n",
      "training loss: 486.90492596687443\n",
      "validation loss: 719.2053189995513\n",
      "epoch: 1350\n",
      "training loss: 486.7886222304688\n",
      "validation loss: 719.1958535274707\n",
      "epoch: 1351\n",
      "training loss: 486.6724218903144\n",
      "validation loss: 719.1865189458829\n",
      "epoch: 1352\n",
      "training loss: 486.5563247416106\n",
      "validation loss: 719.1773150503319\n",
      "epoch: 1353\n",
      "training loss: 486.44033057918415\n",
      "validation loss: 719.1682416352459\n",
      "epoch: 1354\n",
      "training loss: 486.3244391973894\n",
      "validation loss: 719.1592984939346\n",
      "epoch: 1355\n",
      "training loss: 486.20865039000165\n",
      "validation loss: 719.1504854185803\n",
      "epoch: 1356\n",
      "training loss: 486.09296395010443\n",
      "validation loss: 719.1418022002339\n",
      "epoch: 1357\n",
      "training loss: 485.9773796699688\n",
      "validation loss: 719.1332486288022\n",
      "epoch: 1358\n",
      "training loss: 485.86189734092517\n",
      "validation loss: 719.1248244930363\n",
      "epoch: 1359\n",
      "training loss: 485.7465167532245\n",
      "validation loss: 719.1165295805159\n",
      "epoch: 1360\n",
      "training loss: 485.63123769589174\n",
      "validation loss: 719.1083636776353\n",
      "epoch: 1361\n",
      "training loss: 485.5160599565654\n",
      "validation loss: 719.1003265695823\n",
      "epoch: 1362\n",
      "training loss: 485.40098332132703\n",
      "validation loss: 719.0924180403204\n",
      "epoch: 1363\n",
      "training loss: 485.2860075745155\n",
      "validation loss: 719.0846378725661\n",
      "epoch: 1364\n",
      "training loss: 485.17113249852747\n",
      "validation loss: 719.0769858477671\n",
      "epoch: 1365\n",
      "training loss: 485.05635787360063\n",
      "validation loss: 719.0694617460796\n",
      "epoch: 1366\n",
      "training loss: 484.94168347757875\n",
      "validation loss: 719.0620653463442\n",
      "epoch: 1367\n",
      "training loss: 484.82710908565696\n",
      "validation loss: 719.0547964260608\n",
      "epoch: 1368\n",
      "training loss: 484.7126344701052\n",
      "validation loss: 719.0476547613694\n",
      "epoch: 1369\n",
      "training loss: 484.59825939996705\n",
      "validation loss: 719.0406401270258\n",
      "epoch: 1370\n",
      "training loss: 484.4839836407319\n",
      "validation loss: 719.0337522963841\n",
      "epoch: 1371\n",
      "training loss: 484.36980695397835\n",
      "validation loss: 719.0269910413817\n",
      "epoch: 1372\n",
      "training loss: 484.25572909698616\n",
      "validation loss: 719.0203561325285\n",
      "epoch: 1373\n",
      "training loss: 484.1417498223114\n",
      "validation loss: 719.0138473389056\n",
      "epoch: 1374\n",
      "training loss: 484.027868877326\n",
      "validation loss: 719.0074644281693\n",
      "epoch: 1375\n",
      "training loss: 483.9140860037145\n",
      "validation loss: 719.0012071665699\n",
      "epoch: 1376\n",
      "training loss: 483.80040093692537\n",
      "validation loss: 718.9950753189848\n",
      "epoch: 1377\n",
      "training loss: 483.68681340557555\n",
      "validation loss: 718.9890686489687\n",
      "epoch: 1378\n",
      "training loss: 483.5733231308016\n",
      "validation loss: 718.9831869188299\n",
      "epoch: 1379\n",
      "training loss: 483.45992982555566\n",
      "validation loss: 718.9774298897358\n",
      "epoch: 1380\n",
      "training loss: 483.34663319384265\n",
      "validation loss: 718.9717973218538\n",
      "epoch: 1381\n",
      "training loss: 483.2334329298971\n",
      "validation loss: 718.9662889745406\n",
      "epoch: 1382\n",
      "training loss: 483.1203287172958\n",
      "validation loss: 718.9609046065847\n",
      "epoch: 1383\n",
      "training loss: 483.0073202280076\n",
      "validation loss: 718.9556439765237\n",
      "epoch: 1384\n",
      "training loss: 482.8944071213823\n",
      "validation loss: 718.9505068430418\n",
      "epoch: 1385\n",
      "training loss: 482.7815890430814\n",
      "validation loss: 718.9454929654785\n",
      "epoch: 1386\n",
      "training loss: 482.6688656239618\n",
      "validation loss: 718.9406021044629\n",
      "epoch: 1387\n",
      "training loss: 482.55623647892264\n",
      "validation loss: 718.9358340227086\n",
      "epoch: 1388\n",
      "training loss: 482.4437012057418\n",
      "validation loss: 718.9311884860037\n",
      "epoch: 1389\n",
      "training loss: 482.3312593839283\n",
      "validation loss: 718.9266652644359\n",
      "epoch: 1390\n",
      "training loss: 482.2189105736402\n",
      "validation loss: 718.9222641339106\n",
      "epoch: 1391\n",
      "training loss: 482.10665431472574\n",
      "validation loss: 718.917984878019\n",
      "epoch: 1392\n",
      "training loss: 481.99449012597756\n",
      "validation loss: 718.9138272903357\n",
      "epoch: 1393\n",
      "training loss: 481.8824175047089\n",
      "validation loss: 718.9097911772343\n",
      "epoch: 1394\n",
      "training loss: 481.7704359268105\n",
      "validation loss: 718.9058763613301\n",
      "epoch: 1395\n",
      "training loss: 481.6585448474804\n",
      "validation loss: 718.9020826856713\n",
      "epoch: 1396\n",
      "training loss: 481.5467437028889\n",
      "validation loss: 718.8984100188281\n",
      "epoch: 1397\n",
      "training loss: 481.43503191309964\n",
      "validation loss: 718.8948582610419\n",
      "epoch: 1398\n",
      "training loss: 481.3234088866559\n",
      "validation loss: 718.8914273516181\n",
      "epoch: 1399\n",
      "training loss: 481.2118740273276\n",
      "validation loss: 718.8881172777556\n",
      "epoch: 1400\n",
      "training loss: 481.10042674360227\n",
      "validation loss: 718.8849280850114\n",
      "epoch: 1401\n",
      "training loss: 480.9890664615944\n",
      "validation loss: 718.8818598895792\n",
      "epoch: 1402\n",
      "training loss: 480.87779264209195\n",
      "validation loss: 718.878912892508\n",
      "epoch: 1403\n",
      "training loss: 480.7666048024589\n",
      "validation loss: 718.8760873959146\n",
      "epoch: 1404\n",
      "training loss: 480.65550254400097\n",
      "validation loss: 718.8733838210663\n",
      "epoch: 1405\n",
      "training loss: 480.544485585123\n",
      "validation loss: 718.8708027279896\n",
      "epoch: 1406\n",
      "training loss: 480.4335538001065\n",
      "validation loss: 718.8683448359039\n",
      "epoch: 1407\n",
      "training loss: 480.32270726251494\n",
      "validation loss: 718.8660110433246\n",
      "epoch: 1408\n",
      "training loss: 480.21194629108766\n",
      "validation loss: 718.8638024460712\n",
      "epoch: 1409\n",
      "training loss: 480.101271494487\n",
      "validation loss: 718.8617203507539\n",
      "epoch: 1410\n",
      "training loss: 479.9906838096039\n",
      "validation loss: 718.8597662806108\n",
      "epoch: 1411\n",
      "training loss: 479.8801845265712\n",
      "validation loss: 718.8579419700226\n",
      "epoch: 1412\n",
      "training loss: 479.7697752927154\n",
      "validation loss: 718.8562493438477\n",
      "epoch: 1413\n",
      "training loss: 479.6594580880136\n",
      "validation loss: 718.8546904781657\n",
      "epoch: 1414\n",
      "training loss: 479.5492351668098\n",
      "validation loss: 718.8532675403674\n",
      "epoch: 1415\n",
      "training loss: 479.43910896483965\n",
      "validation loss: 718.8519827088909\n",
      "epoch: 1416\n",
      "training loss: 479.3290819766077\n",
      "validation loss: 718.8508380761458\n",
      "epoch: 1417\n",
      "training loss: 479.219156614632\n",
      "validation loss: 718.8498355417935\n",
      "epoch: 1418\n",
      "training loss: 479.1093350671682\n",
      "validation loss: 718.8489767066827\n",
      "epoch: 1419\n",
      "training loss: 478.99961917293496\n",
      "validation loss: 718.8482627794117\n",
      "epoch: 1420\n",
      "training loss: 478.89001032907527\n",
      "validation loss: 718.8476945069207\n",
      "epoch: 1421\n",
      "training loss: 478.7805094425359\n",
      "validation loss: 718.8472721375314\n",
      "epoch: 1422\n",
      "training loss: 478.6711169269905\n",
      "validation loss: 718.8469954200417\n",
      "epoch: 1423\n",
      "training loss: 478.56183273974375\n",
      "validation loss: 718.8468636370377\n",
      "epoch: 1424\n",
      "training loss: 478.45265644769603\n",
      "validation loss: 718.8468756658625\n",
      "epoch: 1425\n",
      "training loss: 478.34358730934883\n",
      "validation loss: 718.8470300577112\n",
      "epoch: 1426\n",
      "training loss: 478.2346243606761\n",
      "validation loss: 718.8473251245335\n",
      "epoch: 1427\n",
      "training loss: 478.12576649548356\n",
      "validation loss: 718.8477590244963\n",
      "epoch: 1428\n",
      "training loss: 478.0170125343676\n",
      "validation loss: 718.8483298390779\n",
      "epoch: 1429\n",
      "training loss: 477.90836127964275\n",
      "validation loss: 718.8490356375944\n",
      "epoch: 1430\n",
      "training loss: 477.79981155607953\n",
      "validation loss: 718.8498745274997\n",
      "epoch: 1431\n",
      "training loss: 477.6913622388647\n",
      "validation loss: 718.8508446907415\n",
      "epoch: 1432\n",
      "training loss: 477.58301227095757\n",
      "validation loss: 718.8519444077243\n",
      "epoch: 1433\n",
      "training loss: 477.4747606722147\n",
      "validation loss: 718.8531720710473\n",
      "epoch: 1434\n",
      "training loss: 477.36660654248584\n",
      "validation loss: 718.8545261913258\n",
      "epoch: 1435\n",
      "training loss: 477.25854906054957\n",
      "validation loss: 718.8560053972557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1436\n",
      "training loss: 477.15058748037114\n",
      "validation loss: 718.8576084317368\n",
      "epoch: 1437\n",
      "training loss: 477.0427211257931\n",
      "validation loss: 718.859334145512\n",
      "epoch: 1438\n",
      "training loss: 476.9349493844562\n",
      "validation loss: 718.8611814894152\n",
      "epoch: 1439\n",
      "training loss: 476.827271701495\n",
      "validation loss: 718.863149506006\n",
      "epoch: 1440\n",
      "training loss: 476.71968757336373\n",
      "validation loss: 718.8652373211195\n",
      "epoch: 1441\n",
      "training loss: 476.6121965420037\n",
      "validation loss: 718.8674441356725\n",
      "epoch: 1442\n",
      "training loss: 476.50479818947326\n",
      "validation loss: 718.8697692179339\n",
      "epoch: 1443\n",
      "training loss: 476.3974921330865\n",
      "validation loss: 718.8722118963547\n",
      "epoch: 1444\n",
      "training loss: 476.29027802107197\n",
      "validation loss: 718.8747715530073\n",
      "epoch: 1445\n",
      "training loss: 476.1831555287306\n",
      "validation loss: 718.8774476176361\n",
      "epoch: 1446\n",
      "training loss: 476.07612435505763\n",
      "validation loss: 718.8802395622889\n",
      "epoch: 1447\n",
      "training loss: 475.96918421979194\n",
      "validation loss: 718.8831468965014\n",
      "epoch: 1448\n",
      "training loss: 475.86233486084114\n",
      "validation loss: 718.8861691629869\n",
      "epoch: 1449\n",
      "training loss: 475.7555760320474\n",
      "validation loss: 718.8893059337859\n",
      "epoch: 1450\n",
      "training loss: 475.64890750124516\n",
      "validation loss: 718.8925568068364\n",
      "epoch: 1451\n",
      "training loss: 475.5423290485804\n",
      "validation loss: 718.8959214029188\n",
      "epoch: 1452\n",
      "training loss: 475.43584046505134\n",
      "validation loss: 718.8993993629404\n",
      "epoch: 1453\n",
      "training loss: 475.32944155124414\n",
      "validation loss: 718.9029903455219\n",
      "epoch: 1454\n",
      "training loss: 475.22313211623594\n",
      "validation loss: 718.9066940248559\n",
      "epoch: 1455\n",
      "training loss: 475.1169119766427\n",
      "validation loss: 718.910510088811\n",
      "epoch: 1456\n",
      "training loss: 475.01078095579055\n",
      "validation loss: 718.9144382372468\n",
      "epoch: 1457\n",
      "training loss: 474.9047388829952\n",
      "validation loss: 718.9184781805297\n",
      "epoch: 1458\n",
      "training loss: 474.79878559293275\n",
      "validation loss: 718.9226296382163\n",
      "epoch: 1459\n",
      "training loss: 474.6929209250912\n",
      "validation loss: 718.9268923378952\n",
      "epoch: 1460\n",
      "training loss: 474.58714472328904\n",
      "validation loss: 718.9312660141634\n",
      "epoch: 1461\n",
      "training loss: 474.4814568352537\n",
      "validation loss: 718.9357504077259\n",
      "epoch: 1462\n",
      "training loss: 474.3758571122516\n",
      "validation loss: 718.9403452646034\n",
      "epoch: 1463\n",
      "training loss: 474.2703454087611\n",
      "validation loss: 718.9450503354376\n",
      "epoch: 1464\n",
      "training loss: 474.1649215821849\n",
      "validation loss: 718.949865374881\n",
      "epoch: 1465\n",
      "training loss: 474.0595854925952\n",
      "validation loss: 718.9547901410616\n",
      "epoch: 1466\n",
      "training loss: 473.9543370025078\n",
      "validation loss: 718.9598243951152\n",
      "epoch: 1467\n",
      "training loss: 473.84917597668147\n",
      "validation loss: 718.9649679007754\n",
      "epoch: 1468\n",
      "training loss: 473.7441022819387\n",
      "validation loss: 718.9702204240168\n",
      "epoch: 1469\n",
      "training loss: 473.6391157870067\n",
      "validation loss: 718.9755817327401\n",
      "epoch: 1470\n",
      "training loss: 473.5342163623745\n",
      "validation loss: 718.9810515965019\n",
      "epoch: 1471\n",
      "training loss: 473.429403880165\n",
      "validation loss: 718.986629786273\n",
      "epoch: 1472\n",
      "training loss: 473.3246782140198\n",
      "validation loss: 718.9923160742321\n",
      "epoch: 1473\n",
      "training loss: 473.22003923899587\n",
      "validation loss: 718.998110233582\n",
      "epoch: 1474\n",
      "training loss: 473.1154868314721\n",
      "validation loss: 719.004012038391\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a8a1959581b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                    \u001b[0mactivationFn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.0000001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    max_epoch=2000, momentum=0.9, early_stopping=True)\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_adjust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_adjust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d85a0294894f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# initalize weights on first iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# M = number of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mbest_v_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d85a0294894f>\u001b[0m in \u001b[0;36minitialize_weights\u001b[0;34m(self, M)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;31m#w_i = np.random.normal(size=(input_size, output_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mw_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mw_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# initialize bias to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "actives = [\"relu\", \"tanh\", \"leaky_relu\"]\n",
    "\n",
    "means = []\n",
    "\n",
    "for active in actives:\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "                   activationFn=active, lr=.0000001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "    nn.fit(X_std, y_adjust)\n",
    "    mae = mean_absolute_error(y_adjust, nn.predict(X_std))\n",
    "    \n",
    "    means.append(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1700428.8677688513, 1503911.4032920278]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_means = []\n",
    "for m in means:\n",
    "    new_means.append(m*100000)\n",
    "new_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh looks slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 3706.2885191182563\n",
      "validation loss: 3378.948067945886\n",
      "epoch: 1\n",
      "training loss: 3702.40387178552\n",
      "validation loss: 3380.2344524640284\n",
      "epoch: 2\n",
      "training loss: 3698.368373558642\n",
      "validation loss: 3381.6717667815524\n",
      "epoch: 3\n",
      "training loss: 3694.1383174756274\n",
      "validation loss: 3383.2630555302703\n",
      "epoch: 4\n",
      "training loss: 3689.6682921870843\n",
      "validation loss: 3385.012112852429\n",
      "epoch: 5\n",
      "training loss: 3684.910803386941\n",
      "validation loss: 3386.9233591771317\n",
      "epoch: 6\n",
      "training loss: 3679.815941782296\n",
      "validation loss: 3389.001710172531\n",
      "epoch: 7\n",
      "training loss: 3674.3311078427687\n",
      "validation loss: 3391.2524338554704\n",
      "epoch: 8\n",
      "training loss: 3668.4008061493264\n",
      "validation loss: 3393.68099250713\n",
      "epoch: 9\n",
      "training loss: 3661.9665243167824\n",
      "validation loss: 3396.292866934064\n",
      "epoch: 10\n",
      "training loss: 3654.9667126301015\n",
      "validation loss: 3399.0933617416235\n",
      "epoch: 11\n",
      "training loss: 3647.3368798987267\n",
      "validation loss: 3402.08739158479\n",
      "epoch: 12\n",
      "training loss: 3639.009817514784\n",
      "validation loss: 3405.279249633789\n",
      "epoch: 54\n",
      "training loss: 1908.6527459942847\n",
      "validation loss: 1916.9068390244884\n",
      "epoch: 55\n",
      "training loss: 1865.7843663332824\n",
      "validation loss: 1861.0484236972407\n",
      "epoch: 56\n",
      "training loss: 1825.2750207903973\n",
      "validation loss: 1809.853524188129\n",
      "epoch: 57\n",
      "training loss: 1786.9784127300975\n",
      "validation loss: 1762.7860407801982\n",
      "epoch: 58\n",
      "training loss: 1750.7586991107003\n",
      "validation loss: 1719.3929490342857\n",
      "epoch: 59\n",
      "training loss: 1716.4903426798326\n",
      "validation loss: 1679.2886095720867\n",
      "epoch: 60\n",
      "training loss: 1684.0574906113627\n",
      "validation loss: 1642.1422739768377\n",
      "epoch: 61\n",
      "training loss: 1653.3529716939415\n",
      "validation loss: 1607.6686269849245\n",
      "epoch: 62\n",
      "training loss: 1624.2772068553213\n",
      "validation loss: 1575.6203138470034\n",
      "epoch: 63\n",
      "training loss: 1596.7372261163268\n",
      "validation loss: 1545.7817782577845\n",
      "epoch: 64\n",
      "training loss: 1570.6458573938776\n",
      "validation loss: 1517.9641080842516\n",
      "epoch: 65\n",
      "training loss: 1545.921082823714\n",
      "validation loss: 1492.0007421202058\n",
      "epoch: 66\n",
      "training loss: 1522.485533882887\n",
      "validation loss: 1467.7439276574369\n",
      "epoch: 67\n",
      "training loss: 1500.2660938750612\n",
      "validation loss: 1445.061820572712\n",
      "epoch: 68\n",
      "training loss: 1479.1935811694575\n",
      "validation loss: 1423.8361210279215\n",
      "epoch: 69\n",
      "training loss: 1459.202492787897\n",
      "validation loss: 1403.9601457782676\n",
      "epoch: 70\n",
      "training loss: 1440.230793385414\n",
      "validation loss: 1385.3372506735886\n",
      "epoch: 71\n",
      "training loss: 1422.2197388979803\n",
      "validation loss: 1367.8795312236591\n",
      "epoch: 72\n",
      "training loss: 1405.1137272161636\n",
      "validation loss: 1351.5067429699423\n",
      "epoch: 73\n",
      "training loss: 1388.860170375199\n",
      "validation loss: 1336.1453957597823\n",
      "epoch: 74\n",
      "training loss: 1373.409384082038\n",
      "validation loss: 1321.7279864285429\n",
      "epoch: 75\n",
      "training loss: 1358.7144910162447\n",
      "validation loss: 1308.192342810227\n",
      "epoch: 76\n",
      "training loss: 1344.7313342924674\n",
      "validation loss: 1295.481058557243\n",
      "epoch: 77\n",
      "training loss: 1331.418396829795\n",
      "validation loss: 1283.5410032002846\n",
      "epoch: 78\n",
      "training loss: 1318.7367213318782\n",
      "validation loss: 1272.3228955411546\n",
      "epoch: 79\n",
      "training loss: 1306.6498245547214\n",
      "validation loss: 1261.7809312105164\n",
      "epoch: 80\n",
      "training loss: 1295.1235991723083\n",
      "validation loss: 1251.872457370024\n",
      "epoch: 81\n",
      "training loss: 1284.1261975552134\n",
      "validation loss: 1242.557689263993\n",
      "epoch: 82\n",
      "training loss: 1273.6278945760948\n",
      "validation loss: 1233.7994645502515\n",
      "epoch: 83\n",
      "training loss: 1263.6009308977812\n",
      "validation loss: 1225.563031803509\n",
      "epoch: 84\n",
      "training loss: 1254.01934303777\n",
      "validation loss: 1217.8158691136855\n",
      "epoch: 85\n",
      "training loss: 1244.8587903387447\n",
      "validation loss: 1210.527527518177\n",
      "epoch: 86\n",
      "training loss: 1236.0963905550634\n",
      "validation loss: 1203.6694928103516\n",
      "epoch: 87\n",
      "training loss: 1227.7105746749196\n",
      "validation loss: 1197.2150589247874\n",
      "epoch: 88\n",
      "training loss: 1219.6809682560254\n",
      "validation loss: 1191.1392071008243\n",
      "epoch: 89\n",
      "training loss: 1211.9883015885202\n",
      "validation loss: 1185.4184870773063\n",
      "epoch: 90\n",
      "training loss: 1204.614344847551\n",
      "validation loss: 1180.030898710528\n",
      "epoch: 91\n",
      "training loss: 1197.541858507362\n",
      "validation loss: 1174.9557737449659\n",
      "epoch: 92\n",
      "training loss: 1190.7545482078017\n",
      "validation loss: 1170.1736582239894\n",
      "epoch: 93\n",
      "training loss: 1184.237021390973\n",
      "validation loss: 1165.6661975599939\n",
      "epoch: 94\n",
      "training loss: 1177.974754152048\n",
      "validation loss: 1161.4160287465395\n",
      "epoch: 95\n",
      "training loss: 1171.9540762247452\n",
      "validation loss: 1157.406684772844\n",
      "epoch: 96\n",
      "training loss: 1166.1621692760132\n",
      "validation loss: 1153.622513033276\n",
      "epoch: 97\n",
      "training loss: 1160.5870646108315\n",
      "validation loss: 1150.048605296115\n",
      "epoch: 98\n",
      "training loss: 1155.2176283390952\n",
      "validation loss: 1146.6707352696112\n",
      "epoch: 99\n",
      "training loss: 1150.04352845947\n",
      "validation loss: 1143.4753008626317\n",
      "epoch: 100\n",
      "training loss: 1145.0551831523533\n",
      "validation loss: 1140.4492699223133\n",
      "epoch: 101\n",
      "training loss: 1140.2436923755392\n",
      "validation loss: 1137.5801295032486\n",
      "epoch: 102\n",
      "training loss: 1135.60075681407\n",
      "validation loss: 1134.8558394867455\n",
      "epoch: 103\n",
      "training loss: 1131.118589786543\n",
      "validation loss: 1132.264791716149\n",
      "epoch: 104\n",
      "training loss: 1126.7898286571085\n",
      "validation loss: 1129.7957757907204\n",
      "epoch: 105\n",
      "training loss: 1122.607452401999\n",
      "validation loss: 1127.4379523392608\n",
      "epoch: 106\n",
      "training loss: 1118.5647111829162\n",
      "validation loss: 1125.1808341111146\n",
      "epoch: 107\n",
      "training loss: 1114.6550722257543\n",
      "validation loss: 1123.0142747183704\n",
      "epoch: 108\n",
      "training loss: 1110.8721842640366\n",
      "validation loss: 1120.9284644302886\n",
      "epoch: 109\n",
      "training loss: 1107.209860634646\n",
      "validation loss: 1118.913932093707\n",
      "epoch: 110\n",
      "training loss: 1103.6620791626904\n",
      "validation loss: 1116.9615520332106\n",
      "epoch: 111\n",
      "training loss: 1100.2229955041264\n",
      "validation loss: 1115.0625546591662\n",
      "epoch: 112\n",
      "training loss: 1096.8869657391522\n",
      "validation loss: 1113.2085394571568\n",
      "epoch: 113\n",
      "training loss: 1093.6485736889574\n",
      "validation loss: 1111.391489017302\n",
      "epoch: 114\n",
      "training loss: 1090.5026585325056\n",
      "validation loss: 1109.60378275336\n",
      "epoch: 115\n",
      "training loss: 1087.444338676436\n",
      "validation loss: 1107.838208936774\n",
      "epoch: 116\n",
      "training loss: 1084.469028364116\n",
      "validation loss: 1106.0879736295065\n",
      "epoch: 117\n",
      "training loss: 1081.572444148937\n",
      "validation loss: 1104.3467050697886\n",
      "epoch: 118\n",
      "training loss: 1078.7505991139803\n",
      "validation loss: 1102.608452101482\n",
      "epoch: 119\n",
      "training loss: 1075.9997836404177\n",
      "validation loss: 1100.8676754102669\n",
      "epoch: 120\n",
      "training loss: 1073.3165326453934\n",
      "validation loss: 1099.1192307035421\n",
      "epoch: 121\n",
      "training loss: 1070.6975805065704\n",
      "validation loss: 1097.358343583483\n",
      "epoch: 122\n",
      "training loss: 1068.1398062604962\n",
      "validation loss: 1095.580576704107\n",
      "epoch: 123\n",
      "training loss: 1065.6401729174586\n",
      "validation loss: 1093.7817908056672\n",
      "epoch: 124\n",
      "training loss: 1063.1956656429375\n",
      "validation loss: 1091.9581022643376\n",
      "epoch: 125\n",
      "training loss: 1060.8032339002857\n",
      "validation loss: 1090.1058407365726\n",
      "epoch: 126\n",
      "training loss: 1058.4597422972251\n",
      "validation loss: 1088.221511180658\n",
      "epoch: 127\n",
      "training loss: 1056.1619338035007\n",
      "validation loss: 1086.3017649152077\n",
      "epoch: 128\n",
      "training loss: 1053.9064072740134\n",
      "validation loss: 1084.343384408562\n",
      "epoch: 129\n",
      "training loss: 1051.6896089915656\n",
      "validation loss: 1082.3432862363582\n",
      "epoch: 130\n",
      "training loss: 1049.50783565412\n",
      "validation loss: 1080.2985461931898\n",
      "epoch: 131\n",
      "training loss: 1047.3572447031147\n",
      "validation loss: 1078.2064499998773\n",
      "epoch: 132\n",
      "training loss: 1045.2338680520852\n",
      "validation loss: 1076.0645724834126\n",
      "epoch: 133\n",
      "training loss: 1043.1336272552744\n",
      "validation loss: 1073.8708875481561\n",
      "epoch: 134\n",
      "training loss: 1041.0523509758418\n",
      "validation loss: 1071.6239106756254\n",
      "epoch: 135\n",
      "training loss: 1038.9857979935364\n",
      "validation loss: 1069.3228749975603\n",
      "epoch: 136\n",
      "training loss: 1036.929689678538\n",
      "validation loss: 1066.9679410337556\n",
      "epoch: 137\n",
      "training loss: 1034.8797541502502\n",
      "validation loss: 1064.560438757598\n",
      "epoch: 138\n",
      "training loss: 1032.8317817474267\n",
      "validation loss: 1062.1031384598405\n",
      "epoch: 139\n",
      "training loss: 1030.7816905340278\n",
      "validation loss: 1059.6005436102644\n",
      "epoch: 140\n",
      "training loss: 1028.7256016579706\n",
      "validation loss: 1057.0591943157963\n",
      "epoch: 141\n",
      "training loss: 1026.6599256842333\n",
      "validation loss: 1054.4879638905752\n",
      "epoch: 142\n",
      "training loss: 1024.581461207693\n",
      "validation loss: 1051.898323450624\n",
      "epoch: 143\n",
      "training loss: 1022.4875059904749\n",
      "validation loss: 1049.3045405007438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 144\n",
      "training loss: 1020.3759788813904\n",
      "validation loss: 1046.723767806126\n",
      "epoch: 145\n",
      "training loss: 1018.2455480615587\n",
      "validation loss: 1044.1759697878579\n",
      "epoch: 146\n",
      "training loss: 1016.0957577814913\n",
      "validation loss: 1041.6836276534934\n",
      "epoch: 147\n",
      "training loss: 1013.9271418232432\n",
      "validation loss: 1039.271165078571\n",
      "epoch: 148\n",
      "training loss: 1011.7413078164542\n",
      "validation loss: 1036.9640480580597\n",
      "epoch: 149\n",
      "training loss: 1009.5409731041458\n",
      "validation loss: 1034.787540192247\n",
      "epoch: 150\n",
      "training loss: 1007.3299315169799\n",
      "validation loss: 1032.7651412022694\n",
      "epoch: 151\n",
      "training loss: 1005.1129330472588\n",
      "validation loss: 1030.9168008756164\n",
      "epoch: 152\n",
      "training loss: 1002.8954667566746\n",
      "validation loss: 1029.2570750980683\n",
      "epoch: 153\n",
      "training loss: 1000.6834518525991\n",
      "validation loss: 1027.7934587340596\n",
      "epoch: 154\n",
      "training loss: 998.482860965561\n",
      "validation loss: 1026.5251683231186\n",
      "epoch: 155\n",
      "training loss: 996.2993187111939\n",
      "validation loss: 1025.4426316149045\n",
      "epoch: 156\n",
      "training loss: 994.137731117146\n",
      "validation loss: 1024.5278567716102\n",
      "epoch: 157\n",
      "training loss: 992.0020013828024\n",
      "validation loss: 1023.7557088103704\n",
      "epoch: 158\n",
      "training loss: 989.8948723775688\n",
      "validation loss: 1023.0959474553779\n",
      "epoch: 159\n",
      "training loss: 987.8179098056236\n",
      "validation loss: 1022.5157294250931\n",
      "epoch: 160\n",
      "training loss: 985.7716106179923\n",
      "validation loss: 1021.9821971706115\n",
      "epoch: 161\n",
      "training loss: 983.7555987187053\n",
      "validation loss: 1021.4647887762617\n",
      "epoch: 162\n",
      "training loss: 981.7688605289413\n",
      "validation loss: 1020.9369990739042\n",
      "epoch: 163\n",
      "training loss: 979.8099769462798\n",
      "validation loss: 1020.3774617252557\n",
      "epoch: 164\n",
      "training loss: 977.877321197463\n",
      "validation loss: 1019.7703601065721\n",
      "epoch: 165\n",
      "training loss: 975.9692077787378\n",
      "validation loss: 1019.1052767741817\n",
      "epoch: 166\n",
      "training loss: 974.0839910467951\n",
      "validation loss: 1018.3766432505989\n",
      "epoch: 167\n",
      "training loss: 972.2201206050723\n",
      "validation loss: 1017.5829585373408\n",
      "epoch: 168\n",
      "training loss: 970.3761643507718\n",
      "validation loss: 1016.7259209511282\n",
      "epoch: 169\n",
      "training loss: 968.5508101881587\n",
      "validation loss: 1015.8095799802148\n",
      "epoch: 170\n",
      "training loss: 966.7428555792432\n",
      "validation loss: 1014.8395755530721\n",
      "epoch: 171\n",
      "training loss: 964.9511916009787\n",
      "validation loss: 1013.8224987824675\n",
      "epoch: 172\n",
      "training loss: 963.17478581515\n",
      "validation loss: 1012.7653837140962\n",
      "epoch: 173\n",
      "training loss: 961.4126663964811\n",
      "validation loss: 1011.6753237386893\n",
      "epoch: 174\n",
      "training loss: 959.6639086774412\n",
      "validation loss: 1010.5591975481401\n",
      "epoch: 175\n",
      "training loss: 957.9276244777116\n",
      "validation loss: 1009.423485859342\n",
      "epoch: 176\n",
      "training loss: 956.2029541657795\n",
      "validation loss: 1008.2741597897204\n",
      "epoch: 177\n",
      "training loss: 954.4890612269966\n",
      "validation loss: 1007.1166233302815\n",
      "epoch: 178\n",
      "training loss: 952.785129089282\n",
      "validation loss: 1005.9556948257912\n",
      "epoch: 179\n",
      "training loss: 951.090360015351\n",
      "validation loss: 1004.7956150925211\n",
      "epoch: 180\n",
      "training loss: 949.403975963426\n",
      "validation loss: 1003.6400724047401\n",
      "epoch: 181\n",
      "training loss: 947.7252214177529\n",
      "validation loss: 1002.4922368707299\n",
      "epoch: 182\n",
      "training loss: 946.0533682765908\n",
      "validation loss: 1001.3547986271932\n",
      "epoch: 183\n",
      "training loss: 944.3877229436387\n",
      "validation loss: 1000.2300058105218\n",
      "epoch: 184\n",
      "training loss: 942.7276357845977\n",
      "validation loss: 999.1196994591944\n",
      "epoch: 185\n",
      "training loss: 941.072513067204\n",
      "validation loss: 998.0253434299754\n",
      "epoch: 186\n",
      "training loss: 939.4218313814774\n",
      "validation loss: 996.9480481481537\n",
      "epoch: 187\n",
      "training loss: 937.7751543167843\n",
      "validation loss: 995.8885876387423\n",
      "epoch: 188\n",
      "training loss: 936.1321508367242\n",
      "validation loss: 994.8474098799342\n",
      "epoch: 189\n",
      "training loss: 934.4926143368191\n",
      "validation loss: 993.8246411535299\n",
      "epoch: 190\n",
      "training loss: 932.856480813189\n",
      "validation loss: 992.8200857938828\n",
      "epoch: 191\n",
      "training loss: 931.2238439730655\n",
      "validation loss: 991.8332235787436\n",
      "epoch: 192\n",
      "training loss: 929.5949645950581\n",
      "validation loss: 990.86320792856\n",
      "epoch: 193\n",
      "training loss: 927.9702711721967\n",
      "validation loss: 989.9088689717128\n",
      "epoch: 194\n",
      "training loss: 926.3503490571517\n",
      "validation loss: 988.968726181173\n",
      "epoch: 195\n",
      "training loss: 924.7359161755265\n",
      "validation loss: 988.0410153941484\n",
      "epoch: 196\n",
      "training loss: 923.1277849746839\n",
      "validation loss: 987.1237342586976\n",
      "epoch: 197\n",
      "training loss: 921.5268125190744\n",
      "validation loss: 986.2147082593707\n",
      "epoch: 198\n",
      "training loss: 919.9338431394857\n",
      "validation loss: 985.3116764449964\n",
      "epoch: 199\n",
      "training loss: 918.3496501586466\n",
      "validation loss: 984.4123921818705\n",
      "epoch: 200\n",
      "training loss: 916.7748842376217\n",
      "validation loss: 983.5147304643125\n",
      "epoch: 201\n",
      "training loss: 915.2100352993845\n",
      "validation loss: 982.6167905718569\n",
      "epoch: 202\n",
      "training loss: 913.6554127272045\n",
      "validation loss: 981.7169821303302\n",
      "epoch: 203\n",
      "training loss: 912.111145111596\n",
      "validation loss: 980.8140843921836\n",
      "epoch: 204\n",
      "training loss: 910.5771971826376\n",
      "validation loss: 979.9072725318917\n",
      "epoch: 205\n",
      "training loss: 909.0533987958813\n",
      "validation loss: 978.9961099801043\n",
      "epoch: 206\n",
      "training loss: 907.5394797332411\n",
      "validation loss: 978.0805109647172\n",
      "epoch: 207\n",
      "training loss: 906.0351047566094\n",
      "validation loss: 977.1606812820897\n",
      "epoch: 208\n",
      "training loss: 904.5399051478802\n",
      "validation loss: 976.2370471782182\n",
      "epoch: 209\n",
      "training loss: 903.0535048451651\n",
      "validation loss: 975.31018199613\n",
      "epoch: 210\n",
      "training loss: 901.5755405599323\n",
      "validation loss: 974.3807384008634\n",
      "epoch: 211\n",
      "training loss: 900.1056759483832\n",
      "validation loss: 973.4493913001136\n",
      "epoch: 212\n",
      "training loss: 898.6436103510181\n",
      "validation loss: 972.5167938311153\n",
      "epoch: 213\n",
      "training loss: 897.1890829755441\n",
      "validation loss: 971.583546546251\n",
      "epoch: 214\n",
      "training loss: 895.7418736229768\n",
      "validation loss: 970.6501784378391\n",
      "epoch: 215\n",
      "training loss: 894.3018010755732\n",
      "validation loss: 969.717137669609\n",
      "epoch: 216\n",
      "training loss: 892.8687201092849\n",
      "validation loss: 968.7847896654389\n",
      "epoch: 217\n",
      "training loss: 891.4425178543138\n",
      "validation loss: 967.8534203503326\n",
      "epoch: 218\n",
      "training loss: 890.023109987457\n",
      "validation loss: 966.923242674441\n",
      "epoch: 219\n",
      "training loss: 888.6104370439859\n",
      "validation loss: 965.9944049537244\n",
      "epoch: 220\n",
      "training loss: 887.2044609967454\n",
      "validation loss: 965.066999951651\n",
      "epoch: 221\n",
      "training loss: 885.8051621600214\n",
      "validation loss: 964.1410739637112\n",
      "epoch: 222\n",
      "training loss: 884.4125364232\n",
      "validation loss: 963.2166354350994\n",
      "epoch: 223\n",
      "training loss: 883.0265927921616\n",
      "validation loss: 962.2936628424985\n",
      "epoch: 224\n",
      "training loss: 881.6473512049955\n",
      "validation loss: 961.372111712462\n",
      "epoch: 225\n",
      "training loss: 880.2748405861896\n",
      "validation loss: 960.4519207433218\n",
      "epoch: 226\n",
      "training loss: 878.9090971055737\n",
      "validation loss: 959.533017056697\n",
      "epoch: 227\n",
      "training loss: 877.5501626123717\n",
      "validation loss: 958.6153206386783\n",
      "epoch: 228\n",
      "training loss: 876.1980832193896\n",
      "validation loss: 957.698748047756\n",
      "epoch: 229\n",
      "training loss: 874.852908016941\n",
      "validation loss: 956.7832154725521\n",
      "epoch: 230\n",
      "training loss: 873.5146879003589\n",
      "validation loss: 955.8686412216696\n",
      "epoch: 231\n",
      "training loss: 872.1834744987599\n",
      "validation loss: 954.9549477234438\n",
      "epoch: 232\n",
      "training loss: 870.8593191961232\n",
      "validation loss: 954.0420631068276\n",
      "epoch: 233\n",
      "training loss: 869.5422722386909\n",
      "validation loss: 953.1299224273232\n",
      "epoch: 234\n",
      "training loss: 868.2323819251387\n",
      "validation loss: 952.2184685943243\n",
      "epoch: 235\n",
      "training loss: 866.9296938778613\n",
      "validation loss: 951.3076530489233\n",
      "epoch: 236\n",
      "training loss: 865.6342503950082\n",
      "validation loss: 950.397436234263\n",
      "epoch: 237\n",
      "training loss: 864.3460898835758\n",
      "validation loss: 949.4877878940639\n",
      "epoch: 238\n",
      "training loss: 863.0652463739359\n",
      "validation loss: 948.578687229039\n",
      "epoch: 239\n",
      "training loss: 861.7917491157485\n",
      "validation loss: 947.6701229356377\n",
      "epoch: 240\n",
      "training loss: 860.5256222543732\n",
      "validation loss: 946.7620931469322\n",
      "epoch: 241\n",
      "training loss: 859.2668845858176\n",
      "validation loss: 945.8546052915465\n",
      "epoch: 242\n",
      "training loss: 858.0155493870893\n",
      "validation loss: 944.9476758833149\n",
      "epoch: 243\n",
      "training loss: 856.7716243176824\n",
      "validation loss: 944.041330251818\n",
      "epoch: 244\n",
      "training loss: 855.5351113869659\n",
      "validation loss: 943.1356022220755\n",
      "epoch: 245\n",
      "training loss: 854.3060069814838\n",
      "validation loss: 942.2305337503155\n",
      "epoch: 246\n",
      "training loss: 853.084301945698\n",
      "validation loss: 941.3261745219046\n",
      "epoch: 247\n",
      "training loss: 851.869981709484\n",
      "validation loss: 940.4225815170204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 248\n",
      "training loss: 850.6630264557144\n",
      "validation loss: 939.5198185494172\n",
      "epoch: 249\n",
      "training loss: 849.4634113214764\n",
      "validation loss: 938.6179557835874\n",
      "epoch: 250\n",
      "training loss: 848.2711066268424\n",
      "validation loss: 937.7170692356235\n",
      "epoch: 251\n",
      "training loss: 847.086078125564\n",
      "validation loss: 936.8172402631308\n",
      "epoch: 252\n",
      "training loss: 845.9082872725747\n",
      "validation loss: 935.9185550494984\n",
      "epoch: 253\n",
      "training loss: 844.7376915036997\n",
      "validation loss: 935.0211040877466\n",
      "epoch: 254\n",
      "training loss: 843.574244523475\n",
      "validation loss: 934.1249816689102\n",
      "epoch: 255\n",
      "training loss: 842.4178965974535\n",
      "validation loss: 933.2302853796007\n",
      "epoch: 256\n",
      "training loss: 841.2685948457903\n",
      "validation loss: 932.3371156128853\n",
      "epoch: 257\n",
      "training loss: 840.1262835352992\n",
      "validation loss: 931.4455750960591\n",
      "epoch: 258\n",
      "training loss: 838.9909043675059\n",
      "validation loss: 930.5557684382222\n",
      "epoch: 259\n",
      "training loss: 837.8623967605441\n",
      "validation loss: 929.6678016998623\n",
      "epoch: 260\n",
      "training loss: 836.740698123022\n",
      "validation loss: 928.781781985921\n",
      "epoch: 261\n",
      "training loss: 835.6257441182548\n",
      "validation loss: 927.8978170631175\n",
      "epoch: 262\n",
      "training loss: 834.5174689175199\n",
      "validation loss: 927.0160150016499\n",
      "epoch: 263\n",
      "training loss: 833.4158054412192\n",
      "validation loss: 926.1364838408423\n",
      "epoch: 264\n",
      "training loss: 832.3206855870843\n",
      "validation loss: 925.2593312778247\n",
      "epoch: 265\n",
      "training loss: 831.2320404447721\n",
      "validation loss: 924.3846643779971\n",
      "epoch: 266\n",
      "training loss: 830.1498004964205\n",
      "validation loss: 923.5125893058039\n",
      "epoch: 267\n",
      "training loss: 829.0738958029345\n",
      "validation loss: 922.6432110742277\n",
      "epoch: 268\n",
      "training loss: 828.0042561759554\n",
      "validation loss: 921.7766333114085\n",
      "epoch: 269\n",
      "training loss: 826.9408113356459\n",
      "validation loss: 920.9129580428869\n",
      "epoch: 270\n",
      "training loss: 825.8834910545584\n",
      "validation loss: 920.0522854880967\n",
      "epoch: 271\n",
      "training loss: 824.8322252879949\n",
      "validation loss: 919.1947138699628\n",
      "epoch: 272\n",
      "training loss: 823.7869442913693\n",
      "validation loss: 918.3403392366523\n",
      "epoch: 273\n",
      "training loss: 822.7475787251625\n",
      "validation loss: 917.4892552947817\n",
      "epoch: 274\n",
      "training loss: 821.7140597481347\n",
      "validation loss: 916.641553253587\n",
      "epoch: 275\n",
      "training loss: 820.6863190994911\n",
      "validation loss: 915.7973216797743\n",
      "epoch: 276\n",
      "training loss: 819.6642891707347\n",
      "validation loss: 914.956646362927\n",
      "epoch: 277\n",
      "training loss: 818.6479030679438\n",
      "validation loss: 914.1196101914813\n",
      "epoch: 278\n",
      "training loss: 817.6370946652098\n",
      "validation loss: 913.2862930393807\n",
      "epoch: 279\n",
      "training loss: 816.6317986499573\n",
      "validation loss: 912.4567716635561\n",
      "epoch: 280\n",
      "training loss: 815.6319505608541\n",
      "validation loss: 911.6311196124308\n",
      "epoch: 281\n",
      "training loss: 814.6374868189706\n",
      "validation loss: 910.8094071456064\n",
      "epoch: 282\n",
      "training loss: 813.6483447528415\n",
      "validation loss: 909.9917011648802\n",
      "epoch: 283\n",
      "training loss: 812.6644626180206\n",
      "validation loss: 909.1780651566721\n",
      "epoch: 284\n",
      "training loss: 811.6857796117073\n",
      "validation loss: 908.3685591458919\n",
      "epoch: 285\n",
      "training loss: 810.7122358829645\n",
      "validation loss: 907.5632396611793\n",
      "epoch: 286\n",
      "training loss: 809.743772539031\n",
      "validation loss: 906.7621597114054\n",
      "epoch: 287\n",
      "training loss: 808.7803316481845\n",
      "validation loss: 905.965368773213\n",
      "epoch: 288\n",
      "training loss: 807.8218562395862\n",
      "validation loss: 905.1729127893244\n",
      "epoch: 289\n",
      "training loss: 806.8682903005052\n",
      "validation loss: 904.3848341772691\n",
      "epoch: 290\n",
      "training loss: 805.9195787712923\n",
      "validation loss: 903.6011718481199\n",
      "epoch: 291\n",
      "training loss: 804.9756675384501\n",
      "validation loss: 902.8219612347845\n",
      "epoch: 292\n",
      "training loss: 804.0365034261101\n",
      "validation loss: 902.047234329343\n",
      "epoch: 293\n",
      "training loss: 803.1020341862142\n",
      "validation loss: 901.2770197288917\n",
      "epoch: 294\n",
      "training loss: 802.172208487656\n",
      "validation loss: 900.5113426893212\n",
      "epoch: 295\n",
      "training loss: 801.2469759046229\n",
      "validation loss: 899.7502251864348\n",
      "epoch: 296\n",
      "training loss: 800.3262869043352\n",
      "validation loss: 898.9936859837909\n",
      "epoch: 297\n",
      "training loss: 799.4100928343618\n",
      "validation loss: 898.2417407066462\n",
      "epoch: 298\n",
      "training loss: 798.4983459096367\n",
      "validation loss: 897.494401921359\n",
      "epoch: 299\n",
      "training loss: 797.5909991992908\n",
      "validation loss: 896.7516792196228\n",
      "epoch: 300\n",
      "training loss: 796.6880066133488\n",
      "validation loss: 896.0135793068795\n",
      "epoch: 301\n",
      "training loss: 795.7893228893282\n",
      "validation loss: 895.280106094292\n",
      "epoch: 302\n",
      "training loss: 794.894903578728\n",
      "validation loss: 894.5512607936457\n",
      "epoch: 303\n",
      "training loss: 794.0047050333698\n",
      "validation loss: 893.8270420145817\n",
      "epoch: 304\n",
      "training loss: 793.1186843915218\n",
      "validation loss: 893.1074458635728\n",
      "epoch: 305\n",
      "training loss: 792.2367995637194\n",
      "validation loss: 892.3924660440932\n",
      "epoch: 306\n",
      "training loss: 791.3590092181716\n",
      "validation loss: 891.6820939574524\n",
      "epoch: 307\n",
      "training loss: 790.4852727656427\n",
      "validation loss: 890.976318803817\n",
      "epoch: 308\n",
      "training loss: 789.615550343687\n",
      "validation loss: 890.2751276829715\n",
      "epoch: 309\n",
      "training loss: 788.7498028001296\n",
      "validation loss: 889.5785056944219\n",
      "epoch: 310\n",
      "training loss: 787.8879916756952\n",
      "validation loss: 888.8864360364986\n",
      "epoch: 311\n",
      "training loss: 787.030079185709\n",
      "validation loss: 888.1989001041562\n",
      "epoch: 312\n",
      "training loss: 786.176028200835\n",
      "validation loss: 887.5158775852397\n",
      "epoch: 313\n",
      "training loss: 785.325802226845\n",
      "validation loss: 886.8373465550332\n",
      "epoch: 314\n",
      "training loss: 784.4793653834712\n",
      "validation loss: 886.1632835689755\n",
      "epoch: 315\n",
      "training loss: 783.6366823824502\n",
      "validation loss: 885.4936637534942\n",
      "epoch: 316\n",
      "training loss: 782.7977185049258\n",
      "validation loss: 884.8284608949721\n",
      "epoch: 317\n",
      "training loss: 781.9624395784463\n",
      "validation loss: 884.1676475269333\n",
      "epoch: 318\n",
      "training loss: 781.1308119538604\n",
      "validation loss: 883.5111950155881\n",
      "epoch: 319\n",
      "training loss: 780.3028024824722\n",
      "validation loss: 882.8590736439401\n",
      "epoch: 320\n",
      "training loss: 779.478378493854\n",
      "validation loss: 882.2112526946838\n",
      "epoch: 321\n",
      "training loss: 778.6575077747617\n",
      "validation loss: 881.5677005321473\n",
      "epoch: 322\n",
      "training loss: 777.8401585495676\n",
      "validation loss: 880.9283846835118\n",
      "epoch: 323\n",
      "training loss: 777.0262994626196\n",
      "validation loss: 880.2932719195007\n",
      "epoch: 324\n",
      "training loss: 776.2158995628431\n",
      "validation loss: 879.6623283346513\n",
      "epoch: 325\n",
      "training loss: 775.4089282908159\n",
      "validation loss: 879.0355194271623\n",
      "epoch: 326\n",
      "training loss: 774.6053554683966\n",
      "validation loss: 878.4128101781766\n",
      "epoch: 327\n",
      "training loss: 773.8051512908461\n",
      "validation loss: 877.7941651302119\n",
      "epoch: 328\n",
      "training loss: 773.0082863212109\n",
      "validation loss: 877.1795484642865\n",
      "epoch: 329\n",
      "training loss: 772.2147314865899\n",
      "validation loss: 876.5689240751647\n",
      "epoch: 330\n",
      "training loss: 771.4244580757701\n",
      "validation loss: 875.9622556440347\n",
      "epoch: 331\n",
      "training loss: 770.6374377376279\n",
      "validation loss: 875.3595067078849\n",
      "epoch: 332\n",
      "training loss: 769.8536424796351\n",
      "validation loss: 874.7606407248073\n",
      "epoch: 333\n",
      "training loss: 769.0730446658118\n",
      "validation loss: 874.1656211345196\n",
      "epoch: 334\n",
      "training loss: 768.295617013511\n",
      "validation loss: 873.5744114134227\n",
      "epoch: 335\n",
      "training loss: 767.5213325884994\n",
      "validation loss: 872.9869751235992\n",
      "epoch: 336\n",
      "training loss: 766.7501647979315\n",
      "validation loss: 872.4032759551953\n",
      "epoch: 337\n",
      "training loss: 765.9820873809363\n",
      "validation loss: 871.8232777616153\n",
      "epoch: 338\n",
      "training loss: 765.2170743967257\n",
      "validation loss: 871.2469445868779\n",
      "epoch: 339\n",
      "training loss: 764.4551002103018\n",
      "validation loss: 870.674240684279\n",
      "epoch: 340\n",
      "training loss: 763.6961394760577\n",
      "validation loss: 870.1051305251373\n",
      "epoch: 341\n",
      "training loss: 762.9401671198292\n",
      "validation loss: 869.5395787959227\n",
      "epoch: 342\n",
      "training loss: 762.1871583202563\n",
      "validation loss: 868.9775503813358\n",
      "epoch: 343\n",
      "training loss: 761.4370884907427\n",
      "validation loss: 868.4190103300805\n",
      "epoch: 344\n",
      "training loss: 760.6899332638118\n",
      "validation loss: 867.8639237990578\n",
      "epoch: 345\n",
      "training loss: 759.9456684803455\n",
      "validation loss: 867.3122559706914\n",
      "epoch: 346\n",
      "training loss: 759.2042701869925\n",
      "validation loss: 866.7639719372585\n",
      "epoch: 347\n",
      "training loss: 758.4657146459061\n",
      "validation loss: 866.2190365458179\n",
      "epoch: 348\n",
      "training loss: 757.7299783617277\n",
      "validation loss: 865.6774141982704\n",
      "epoch: 349\n",
      "training loss: 756.9970381309786\n",
      "validation loss: 865.1390686041937\n",
      "epoch: 350\n",
      "training loss: 756.2668711181291\n",
      "validation loss: 864.6039624907667\n",
      "epoch: 351\n",
      "training loss: 755.5394549595786\n",
      "validation loss: 864.0720572857472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 352\n",
      "training loss: 754.8147678905203\n",
      "validation loss: 863.5433128072916\n",
      "epoch: 353\n",
      "training loss: 754.0927888792028\n",
      "validation loss: 863.0176870178044\n",
      "epoch: 354\n",
      "training loss: 753.3734977388384\n",
      "validation loss: 862.4951359239576\n",
      "epoch: 355\n",
      "training loss: 752.6568751720436\n",
      "validation loss: 861.9756137218698\n",
      "epoch: 356\n",
      "training loss: 751.9429026925518\n",
      "validation loss: 861.4590732797163\n",
      "epoch: 357\n",
      "training loss: 751.2315623727725\n",
      "validation loss: 860.9454670029293\n",
      "epoch: 358\n",
      "training loss: 750.5228363909257\n",
      "validation loss: 860.4347480329409\n",
      "epoch: 359\n",
      "training loss: 749.8167063973167\n",
      "validation loss: 859.9268716077598\n",
      "epoch: 360\n",
      "training loss: 749.1131527735471\n",
      "validation loss: 859.4217963115105\n",
      "epoch: 361\n",
      "training loss: 748.4121539029293\n",
      "validation loss: 858.9194849239681\n",
      "epoch: 362\n",
      "training loss: 747.7136855965533\n",
      "validation loss: 858.4199046851709\n",
      "epoch: 363\n",
      "training loss: 747.017720843407\n",
      "validation loss: 857.9230269854706\n",
      "epoch: 364\n",
      "training loss: 746.3242301133329\n",
      "validation loss: 857.4288266930814\n",
      "epoch: 365\n",
      "training loss: 745.633182571634\n",
      "validation loss: 856.9372814510724\n",
      "epoch: 366\n",
      "training loss: 744.9445487313584\n",
      "validation loss: 856.4483712708438\n",
      "epoch: 367\n",
      "training loss: 744.2583050742638\n",
      "validation loss: 855.962078619788\n",
      "epoch: 368\n",
      "training loss: 743.5744405530658\n",
      "validation loss: 855.4783889492884\n",
      "epoch: 369\n",
      "training loss: 742.8929631229829\n",
      "validation loss: 854.9972912334588\n",
      "epoch: 370\n",
      "training loss: 742.2139019889235\n",
      "validation loss: 854.5187777005248\n",
      "epoch: 371\n",
      "training loss: 741.5373008834057\n",
      "validation loss: 854.0428419449612\n",
      "epoch: 372\n",
      "training loss: 740.8632027273172\n",
      "validation loss: 853.5694755117984\n",
      "epoch: 373\n",
      "training loss: 740.1916337425881\n",
      "validation loss: 853.0986644916833\n",
      "epoch: 374\n",
      "training loss: 739.522596323107\n",
      "validation loss: 852.6303880799823\n",
      "epoch: 375\n",
      "training loss: 738.8560723728717\n",
      "validation loss: 852.1646196335046\n",
      "epoch: 376\n",
      "training loss: 738.1920316676683\n",
      "validation loss: 851.701329063829\n",
      "epoch: 377\n",
      "training loss: 737.5304395407486\n",
      "validation loss: 851.2404851165826\n",
      "epoch: 378\n",
      "training loss: 736.8712616448595\n",
      "validation loss: 850.7820568556065\n",
      "epoch: 379\n",
      "training loss: 736.2144661031277\n",
      "validation loss: 850.3260143797879\n",
      "epoch: 380\n",
      "training loss: 735.5600240826739\n",
      "validation loss: 849.8723290504302\n",
      "epoch: 381\n",
      "training loss: 734.9079096280557\n",
      "validation loss: 849.4209734762354\n",
      "epoch: 382\n",
      "training loss: 734.2580992356752\n",
      "validation loss: 848.9719214050612\n",
      "epoch: 383\n",
      "training loss: 733.6105713949809\n",
      "validation loss: 848.5251475951737\n",
      "epoch: 384\n",
      "training loss: 732.9653061832206\n",
      "validation loss: 848.0806276954366\n",
      "epoch: 385\n",
      "training loss: 732.3222849358812\n",
      "validation loss: 847.6383381432964\n",
      "epoch: 386\n",
      "training loss: 731.6814899890614\n",
      "validation loss: 847.198256080949\n",
      "epoch: 387\n",
      "training loss: 731.0429044818535\n",
      "validation loss: 846.7603592872126\n",
      "epoch: 388\n",
      "training loss: 730.4065122059438\n",
      "validation loss: 846.3246261220992\n",
      "epoch: 389\n",
      "training loss: 729.7722974913277\n",
      "validation loss: 845.8910354813707\n",
      "epoch: 390\n",
      "training loss: 729.1402451192522\n",
      "validation loss: 845.4595667588582\n",
      "epoch: 391\n",
      "training loss: 728.5103402555911\n",
      "validation loss: 845.0301998148317\n",
      "epoch: 392\n",
      "training loss: 727.8825683995265\n",
      "validation loss: 844.6029149490896\n",
      "epoch: 393\n",
      "training loss: 727.2569153437346\n",
      "validation loss: 844.1776928777684\n",
      "epoch: 394\n",
      "training loss: 726.6333671432433\n",
      "validation loss: 843.7545147131084\n",
      "epoch: 395\n",
      "training loss: 726.0119100908644\n",
      "validation loss: 843.3333619455875\n",
      "epoch: 396\n",
      "training loss: 725.3925306976339\n",
      "validation loss: 842.9142164279775\n",
      "epoch: 397\n",
      "training loss: 724.7752156770954\n",
      "validation loss: 842.4970603609721\n",
      "epoch: 398\n",
      "training loss: 724.1599519325415\n",
      "validation loss: 842.0818762801205\n",
      "epoch: 399\n",
      "training loss: 723.5467265465585\n",
      "validation loss: 841.6686470438476\n",
      "epoch: 400\n",
      "training loss: 722.9355267723614\n",
      "validation loss: 841.2573558224042\n",
      "epoch: 401\n",
      "training loss: 722.3263400265399\n",
      "validation loss: 840.8479860876029\n",
      "epoch: 402\n",
      "training loss: 721.7191538829151\n",
      "validation loss: 840.4405216032408\n",
      "epoch: 403\n",
      "training loss: 721.1139560672832\n",
      "validation loss: 840.0349464161201\n",
      "epoch: 404\n",
      "training loss: 720.5107344528622\n",
      "validation loss: 839.6312448475978\n",
      "epoch: 405\n",
      "training loss: 719.9094770563064\n",
      "validation loss: 839.2294014856034\n",
      "epoch: 406\n",
      "training loss: 719.3101720341804\n",
      "validation loss: 838.829401177078\n",
      "epoch: 407\n",
      "training loss: 718.7128076798028\n",
      "validation loss: 838.4312290207997\n",
      "epoch: 408\n",
      "training loss: 718.1173724203932\n",
      "validation loss: 838.0348703605491\n",
      "epoch: 409\n",
      "training loss: 717.5238548144675\n",
      "validation loss: 837.6403107785953\n",
      "epoch: 410\n",
      "training loss: 716.9322435494403\n",
      "validation loss: 837.2475360894736\n",
      "epoch: 411\n",
      "training loss: 716.3425274393921\n",
      "validation loss: 836.8565323340281\n",
      "epoch: 412\n",
      "training loss: 715.7546954229822\n",
      "validation loss: 836.4672857737079\n",
      "epoch: 413\n",
      "training loss: 715.1687365614765\n",
      "validation loss: 836.0797828850888\n",
      "epoch: 414\n",
      "training loss: 714.5846400368755\n",
      "validation loss: 835.6940103546107\n",
      "epoch: 415\n",
      "training loss: 714.0023951501267\n",
      "validation loss: 835.3099550735171\n",
      "epoch: 416\n",
      "training loss: 713.4219913194077\n",
      "validation loss: 834.9276041329792\n",
      "epoch: 417\n",
      "training loss: 712.8434180784693\n",
      "validation loss: 834.5469448193946\n",
      "epoch: 418\n",
      "training loss: 712.266665075034\n",
      "validation loss: 834.1679646098506\n",
      "epoch: 419\n",
      "training loss: 711.6917220692368\n",
      "validation loss: 833.7906511677404\n",
      "epoch: 420\n",
      "training loss: 711.1185789321094\n",
      "validation loss: 833.4149923385266\n",
      "epoch: 421\n",
      "training loss: 710.5472256440964\n",
      "validation loss: 833.0409761456384\n",
      "epoch: 422\n",
      "training loss: 709.9776522936068\n",
      "validation loss: 832.6685907864983\n",
      "epoch: 423\n",
      "training loss: 709.40984907559\n",
      "validation loss: 832.2978246286734\n",
      "epoch: 424\n",
      "training loss: 708.8438062901407\n",
      "validation loss: 831.9286662061387\n",
      "epoch: 425\n",
      "training loss: 708.2795143411264\n",
      "validation loss: 831.5611042156534\n",
      "epoch: 426\n",
      "training loss: 707.7169637348366\n",
      "validation loss: 831.1951275132396\n",
      "epoch: 427\n",
      "training loss: 707.1561450786546\n",
      "validation loss: 830.8307251107632\n",
      "epoch: 428\n",
      "training loss: 706.5970490797463\n",
      "validation loss: 830.4678861726073\n",
      "epoch: 429\n",
      "training loss: 706.0396665437704\n",
      "validation loss: 830.1066000124398\n",
      "epoch: 430\n",
      "training loss: 705.483988373605\n",
      "validation loss: 829.746856090067\n",
      "epoch: 431\n",
      "training loss: 704.9300055680941\n",
      "validation loss: 829.3886440083722\n",
      "epoch: 432\n",
      "training loss: 704.377709220808\n",
      "validation loss: 829.0319535103355\n",
      "epoch: 433\n",
      "training loss: 703.8270905188243\n",
      "validation loss: 828.6767744761313\n",
      "epoch: 434\n",
      "training loss: 703.2781407415232\n",
      "validation loss: 828.3230969203033\n",
      "epoch: 435\n",
      "training loss: 702.7308512594004\n",
      "validation loss: 827.9709109890105\n",
      "epoch: 436\n",
      "training loss: 702.1852135328974\n",
      "validation loss: 827.6202069573462\n",
      "epoch: 437\n",
      "training loss: 701.6412191112448\n",
      "validation loss: 827.270975226725\n",
      "epoch: 438\n",
      "training loss: 701.0988596313268\n",
      "validation loss: 826.9232063223369\n",
      "epoch: 439\n",
      "training loss: 700.5581268165589\n",
      "validation loss: 826.5768908906671\n",
      "epoch: 440\n",
      "training loss: 700.0190124757813\n",
      "validation loss: 826.2320196970791\n",
      "epoch: 441\n",
      "training loss: 699.4815085021718\n",
      "validation loss: 825.8885836234589\n",
      "epoch: 442\n",
      "training loss: 698.9456068721712\n",
      "validation loss: 825.5465736659203\n",
      "epoch: 443\n",
      "training loss: 698.411299644428\n",
      "validation loss: 825.2059809325688\n",
      "epoch: 444\n",
      "training loss: 697.878578958757\n",
      "validation loss: 824.8667966413219\n",
      "epoch: 445\n",
      "training loss: 697.3474370351155\n",
      "validation loss: 824.5290121177875\n",
      "epoch: 446\n",
      "training loss: 696.8178661725941\n",
      "validation loss: 824.1926187931944\n",
      "epoch: 447\n",
      "training loss: 696.2898587484246\n",
      "validation loss: 823.8576082023774\n",
      "epoch: 448\n",
      "training loss: 695.7634072170034\n",
      "validation loss: 823.5239719818154\n",
      "epoch: 449\n",
      "training loss: 695.2385041089284\n",
      "validation loss: 823.1917018677175\n",
      "epoch: 450\n",
      "training loss: 694.7151420300556\n",
      "validation loss: 822.8607896941622\n",
      "epoch: 451\n",
      "training loss: 694.1933136605661\n",
      "validation loss: 822.5312273912824\n",
      "epoch: 452\n",
      "training loss: 693.6730117540501\n",
      "validation loss: 822.2030069834998\n",
      "epoch: 453\n",
      "training loss: 693.1542291366054\n",
      "validation loss: 821.8761205878037\n",
      "epoch: 454\n",
      "training loss: 692.6369587059503\n",
      "validation loss: 821.5505604120762\n",
      "epoch: 455\n",
      "training loss: 692.1211934305475\n",
      "validation loss: 821.2263187534618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 456\n",
      "training loss: 691.6069263487448\n",
      "validation loss: 820.9033879967783\n",
      "epoch: 457\n",
      "training loss: 691.0941505679253\n",
      "validation loss: 820.5817606129709\n",
      "epoch: 458\n",
      "training loss: 690.5828592636717\n",
      "validation loss: 820.2614291576084\n",
      "epoch: 459\n",
      "training loss: 690.0730456789389\n",
      "validation loss: 819.942386269417\n",
      "epoch: 460\n",
      "training loss: 689.5647031232409\n",
      "validation loss: 819.6246246688542\n",
      "epoch: 461\n",
      "training loss: 689.0578249718453\n",
      "validation loss: 819.3081371567197\n",
      "epoch: 462\n",
      "training loss: 688.5524046649765\n",
      "validation loss: 818.9929166128065\n",
      "epoch: 463\n",
      "training loss: 688.0484357070281\n",
      "validation loss: 818.6789559945832\n",
      "epoch: 464\n",
      "training loss: 687.5459116657819\n",
      "validation loss: 818.3662483359147\n",
      "epoch: 465\n",
      "training loss: 687.0448261716346\n",
      "validation loss: 818.054786745815\n",
      "epoch: 466\n",
      "training loss: 686.5451729168278\n",
      "validation loss: 817.7445644072349\n",
      "epoch: 467\n",
      "training loss: 686.0469456546859\n",
      "validation loss: 817.4355745758795\n",
      "epoch: 468\n",
      "training loss: 685.5501381988548\n",
      "validation loss: 817.1278105790594\n",
      "epoch: 469\n",
      "training loss: 685.0547444225458\n",
      "validation loss: 816.8212658145687\n",
      "epoch: 470\n",
      "training loss: 684.5607582577808\n",
      "validation loss: 816.5159337495955\n",
      "epoch: 471\n",
      "training loss: 684.0681736946397\n",
      "validation loss: 816.2118079196587\n",
      "epoch: 472\n",
      "training loss: 683.5769847805091\n",
      "validation loss: 815.9088819275721\n",
      "epoch: 473\n",
      "training loss: 683.0871856193305\n",
      "validation loss: 815.6071494424373\n",
      "epoch: 474\n",
      "training loss: 682.5987703708487\n",
      "validation loss: 815.3066041986573\n",
      "epoch: 475\n",
      "training loss: 682.1117332498609\n",
      "validation loss: 815.0072399949803\n",
      "epoch: 476\n",
      "training loss: 681.6260685254639\n",
      "validation loss: 814.7090506935637\n",
      "epoch: 477\n",
      "training loss: 681.1417705203004\n",
      "validation loss: 814.4120302190626\n",
      "epoch: 478\n",
      "training loss: 680.6588336098059\n",
      "validation loss: 814.1161725577404\n",
      "epoch: 479\n",
      "training loss: 680.1772522214529\n",
      "validation loss: 813.8214717566009\n",
      "epoch: 480\n",
      "training loss: 679.6970208339953\n",
      "validation loss: 813.5279219225413\n",
      "epoch: 481\n",
      "training loss: 679.2181339767126\n",
      "validation loss: 813.2355172215266\n",
      "epoch: 482\n",
      "training loss: 678.7405862286521\n",
      "validation loss: 812.9442518777828\n",
      "epoch: 483\n",
      "training loss: 678.2643722178749\n",
      "validation loss: 812.654120173009\n",
      "epoch: 484\n",
      "training loss: 677.7894866206989\n",
      "validation loss: 812.3651164456112\n",
      "epoch: 485\n",
      "training loss: 677.315924160945\n",
      "validation loss: 812.0772350899501\n",
      "epoch: 486\n",
      "training loss: 676.8436796091858\n",
      "validation loss: 811.7904705556128\n",
      "epoch: 487\n",
      "training loss: 676.3727477819954\n",
      "validation loss: 811.5048173466975\n",
      "epoch: 488\n",
      "training loss: 675.9031235412037\n",
      "validation loss: 811.2202700211171\n",
      "epoch: 489\n",
      "training loss: 675.4348017931551\n",
      "validation loss: 810.9368231899235\n",
      "epoch: 490\n",
      "training loss: 674.9677774879701\n",
      "validation loss: 810.6544715166448\n",
      "epoch: 491\n",
      "training loss: 674.5020456188151\n",
      "validation loss: 810.3732097166417\n",
      "epoch: 492\n",
      "training loss: 674.0376012211766\n",
      "validation loss: 810.09303255648\n",
      "epoch: 493\n",
      "training loss: 673.5744393721432\n",
      "validation loss: 809.8139348533217\n",
      "epoch: 494\n",
      "training loss: 673.1125551896962\n",
      "validation loss: 809.5359114743294\n",
      "epoch: 495\n",
      "training loss: 672.6519438320066\n",
      "validation loss: 809.2589573360891\n",
      "epoch: 496\n",
      "training loss: 672.1926004967441\n",
      "validation loss: 808.9830674040478\n",
      "epoch: 497\n",
      "training loss: 671.7345204203916\n",
      "validation loss: 808.7082366919678\n",
      "epoch: 498\n",
      "training loss: 671.2776988775752\n",
      "validation loss: 808.4344602613942\n",
      "epoch: 499\n",
      "training loss: 670.8221311803998\n",
      "validation loss: 808.1617332211375\n",
      "epoch: 500\n",
      "training loss: 670.3678126777996\n",
      "validation loss: 807.8900507267717\n",
      "epoch: 501\n",
      "training loss: 669.9147387548983\n",
      "validation loss: 807.6194079801425\n",
      "epoch: 502\n",
      "training loss: 669.4629048323807\n",
      "validation loss: 807.3498002288894\n",
      "epoch: 503\n",
      "training loss: 669.0123063658784\n",
      "validation loss: 807.0812227659809\n",
      "epoch: 504\n",
      "training loss: 668.5629388453654\n",
      "validation loss: 806.8136709292552\n",
      "epoch: 505\n",
      "training loss: 668.1147977945685\n",
      "validation loss: 806.5471401009738\n",
      "epoch: 506\n",
      "training loss: 667.667878770388\n",
      "validation loss: 806.281625707382\n",
      "epoch: 507\n",
      "training loss: 667.2221773623328\n",
      "validation loss: 806.0171232182719\n",
      "epoch: 508\n",
      "training loss: 666.7776891919669\n",
      "validation loss: 805.7536281465565\n",
      "epoch: 509\n",
      "training loss: 666.3344099123692\n",
      "validation loss: 805.4911360478391\n",
      "epoch: 510\n",
      "training loss: 665.8923352076049\n",
      "validation loss: 805.2296425199887\n",
      "epoch: 511\n",
      "training loss: 665.4514607922094\n",
      "validation loss: 804.9691432027108\n",
      "epoch: 512\n",
      "training loss: 665.0117824106843\n",
      "validation loss: 804.7096337771161\n",
      "epoch: 513\n",
      "training loss: 664.5732958370047\n",
      "validation loss: 804.4511099652835\n",
      "epoch: 514\n",
      "training loss: 664.1359968741373\n",
      "validation loss: 804.1935675298148\n",
      "epoch: 515\n",
      "training loss: 663.6998813535697\n",
      "validation loss: 803.937002273377\n",
      "epoch: 516\n",
      "training loss: 663.2649451348509\n",
      "validation loss: 803.6814100382348\n",
      "epoch: 517\n",
      "training loss: 662.8311841051391\n",
      "validation loss: 803.4267867057646\n",
      "epoch: 518\n",
      "training loss: 662.3985941787607\n",
      "validation loss: 803.1731281959503\n",
      "epoch: 519\n",
      "training loss: 661.967171296777\n",
      "validation loss: 802.9204304668615\n",
      "epoch: 520\n",
      "training loss: 661.5369114265577\n",
      "validation loss: 802.668689514103\n",
      "epoch: 521\n",
      "training loss: 661.107810561363\n",
      "validation loss: 802.4179013702426\n",
      "epoch: 522\n",
      "training loss: 660.6798647199303\n",
      "validation loss: 802.1680621042087\n",
      "epoch: 523\n",
      "training loss: 660.2530699460652\n",
      "validation loss: 801.9191678206549\n",
      "epoch: 524\n",
      "training loss: 659.8274223082389\n",
      "validation loss: 801.6712146592951\n",
      "epoch: 525\n",
      "training loss: 659.4029178991857\n",
      "validation loss: 801.4241987941992\n",
      "epoch: 526\n",
      "training loss: 658.9795528355035\n",
      "validation loss: 801.1781164330533\n",
      "epoch: 527\n",
      "training loss: 658.5573232572546\n",
      "validation loss: 800.9329638163778\n",
      "epoch: 528\n",
      "training loss: 658.1362253275653\n",
      "validation loss: 800.688737216707\n",
      "epoch: 529\n",
      "training loss: 657.7162552322229\n",
      "validation loss: 800.4454329377209\n",
      "epoch: 530\n",
      "training loss: 657.2974091792687\n",
      "validation loss: 800.2030473133364\n",
      "epoch: 531\n",
      "training loss: 656.879683398585\n",
      "validation loss: 799.961576706749\n",
      "epoch: 532\n",
      "training loss: 656.4630741414751\n",
      "validation loss: 799.7210175094289\n",
      "epoch: 533\n",
      "training loss: 656.0475776802332\n",
      "validation loss: 799.481366140068\n",
      "epoch: 534\n",
      "training loss: 655.6331903077034\n",
      "validation loss: 799.242619043478\n",
      "epoch: 535\n",
      "training loss: 655.219908336825\n",
      "validation loss: 799.0047726894394\n",
      "epoch: 536\n",
      "training loss: 654.8077281001622\n",
      "validation loss: 798.7678235715007\n",
      "epoch: 537\n",
      "training loss: 654.396645949414\n",
      "validation loss: 798.5317682057271\n",
      "epoch: 538\n",
      "training loss: 653.9866582549088\n",
      "validation loss: 798.2966031294015\n",
      "epoch: 539\n",
      "training loss: 653.5777614050694\n",
      "validation loss: 798.0623248996751\n",
      "epoch: 540\n",
      "training loss: 653.1699518058591\n",
      "validation loss: 797.8289300921724\n",
      "epoch: 541\n",
      "training loss: 652.7632258801998\n",
      "validation loss: 797.5964152995493\n",
      "epoch: 542\n",
      "training loss: 652.3575800673593\n",
      "validation loss: 797.3647771300085\n",
      "epoch: 543\n",
      "training loss: 651.9530108223147\n",
      "validation loss: 797.1340122057743\n",
      "epoch: 544\n",
      "training loss: 651.549514615079\n",
      "validation loss: 796.9041171615321\n",
      "epoch: 545\n",
      "training loss: 651.1470879300061\n",
      "validation loss: 796.6750886428379\n",
      "epoch: 546\n",
      "training loss: 650.7457272650622\n",
      "validation loss: 796.446923304502\n",
      "epoch: 547\n",
      "training loss: 650.3454291310744\n",
      "validation loss: 796.2196178089615\n",
      "epoch: 548\n",
      "training loss: 649.9461900509596\n",
      "validation loss: 795.9931688246434\n",
      "epoch: 549\n",
      "training loss: 649.5480065589368\n",
      "validation loss: 795.7675730243402\n",
      "epoch: 550\n",
      "training loss: 649.1508751997349\n",
      "validation loss: 795.5428270836046\n",
      "epoch: 551\n",
      "training loss: 648.7547925278012\n",
      "validation loss: 795.3189276791853\n",
      "epoch: 552\n",
      "training loss: 648.3597551065278\n",
      "validation loss: 795.0958714875202\n",
      "epoch: 553\n",
      "training loss: 647.9657595075042\n",
      "validation loss: 794.8736551833068\n",
      "epoch: 554\n",
      "training loss: 647.5728023098138\n",
      "validation loss: 794.6522754381757\n",
      "epoch: 555\n",
      "training loss: 647.180880099389\n",
      "validation loss: 794.4317289194797\n",
      "epoch: 556\n",
      "training loss: 646.7899894684415\n",
      "validation loss: 794.2120122892293\n",
      "epoch: 557\n",
      "training loss: 646.4001270149797\n",
      "validation loss: 793.9931222031838\n",
      "epoch: 558\n",
      "training loss: 646.0112893424298\n",
      "validation loss: 793.7750553101205\n",
      "epoch: 559\n",
      "training loss: 645.6234730593659\n",
      "validation loss: 793.5578082512809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 560\n",
      "training loss: 645.2366747793578\n",
      "validation loss: 793.3413776600072\n",
      "epoch: 561\n",
      "training loss: 644.8508911209365\n",
      "validation loss: 793.1257601615579\n",
      "epoch: 562\n",
      "training loss: 644.4661187076691\n",
      "validation loss: 792.9109523730838\n",
      "epoch: 563\n",
      "training loss: 644.0823541683366\n",
      "validation loss: 792.6969509037561\n",
      "epoch: 564\n",
      "training loss: 643.6995941371885\n",
      "validation loss: 792.4837523549994\n",
      "epoch: 565\n",
      "training loss: 643.3178352542624\n",
      "validation loss: 792.2713533208042\n",
      "epoch: 566\n",
      "training loss: 642.9370741657291\n",
      "validation loss: 792.0597503880736\n",
      "epoch: 567\n",
      "training loss: 642.5573075242423\n",
      "validation loss: 791.8489401369587\n",
      "epoch: 568\n",
      "training loss: 642.1785319892551\n",
      "validation loss: 791.6389191411471\n",
      "epoch: 569\n",
      "training loss: 641.8007442272778\n",
      "validation loss: 791.4296839680587\n",
      "epoch: 570\n",
      "training loss: 641.423940912048\n",
      "validation loss: 791.2212311789272\n",
      "epoch: 571\n",
      "training loss: 641.0481187245922\n",
      "validation loss: 791.0135573287414\n",
      "epoch: 572\n",
      "training loss: 640.6732743531662\n",
      "validation loss: 790.8066589660411\n",
      "epoch: 573\n",
      "training loss: 640.299404493065\n",
      "validation loss: 790.6005326325693\n",
      "epoch: 574\n",
      "training loss: 639.9265058463042\n",
      "validation loss: 790.3951748627933\n",
      "epoch: 575\n",
      "training loss: 639.5545751211829\n",
      "validation loss: 790.1905821833228\n",
      "epoch: 576\n",
      "training loss: 639.1836090317408\n",
      "validation loss: 789.986751112256\n",
      "epoch: 577\n",
      "training loss: 638.8136042971319\n",
      "validation loss: 789.7836781584848\n",
      "epoch: 578\n",
      "training loss: 638.4445576409357\n",
      "validation loss: 789.5813598210068\n",
      "epoch: 579\n",
      "training loss: 638.0764657904347\n",
      "validation loss: 789.3797925882748\n",
      "epoch: 580\n",
      "training loss: 637.7093254758763\n",
      "validation loss: 789.1789729376212\n",
      "epoch: 581\n",
      "training loss: 637.3431334297492\n",
      "validation loss: 788.9788973347864\n",
      "epoch: 582\n",
      "training loss: 636.977886386088\n",
      "validation loss: 788.7795622335735\n",
      "epoch: 583\n",
      "training loss: 636.613581079825\n",
      "validation loss: 788.580964075644\n",
      "epoch: 584\n",
      "training loss: 636.2502142462021\n",
      "validation loss: 788.3830992904691\n",
      "epoch: 585\n",
      "training loss: 635.8877826202464\n",
      "validation loss: 788.1859642954299\n",
      "epoch: 586\n",
      "training loss: 635.5262829363196\n",
      "validation loss: 787.9895554960748\n",
      "epoch: 587\n",
      "training loss: 635.1657119277387\n",
      "validation loss: 787.7938692865157\n",
      "epoch: 588\n",
      "training loss: 634.8060663264665\n",
      "validation loss: 787.5989020499618\n",
      "epoch: 589\n",
      "training loss: 634.4473428628706\n",
      "validation loss: 787.404650159372\n",
      "epoch: 590\n",
      "training loss: 634.089538265543\n",
      "validation loss: 787.2111099782134\n",
      "epoch: 591\n",
      "training loss: 633.7326492611772\n",
      "validation loss: 787.018277861311\n",
      "epoch: 592\n",
      "training loss: 633.376672574496\n",
      "validation loss: 786.8261501557763\n",
      "epoch: 593\n",
      "training loss: 633.0216049282221\n",
      "validation loss: 786.6347232019943\n",
      "epoch: 594\n",
      "training loss: 632.6674430430878\n",
      "validation loss: 786.4439933346632\n",
      "epoch: 595\n",
      "training loss: 632.3141836378763\n",
      "validation loss: 786.2539568838689\n",
      "epoch: 596\n",
      "training loss: 631.9618234294928\n",
      "validation loss: 786.0646101761863\n",
      "epoch: 597\n",
      "training loss: 631.6103591330547\n",
      "validation loss: 785.875949535797\n",
      "epoch: 598\n",
      "training loss: 631.2597874620054\n",
      "validation loss: 785.6879712856139\n",
      "epoch: 599\n",
      "training loss: 630.9101051282406\n",
      "validation loss: 785.500671748407\n",
      "epoch: 600\n",
      "training loss: 630.5613088422491\n",
      "validation loss: 785.314047247922\n",
      "epoch: 601\n",
      "training loss: 630.2133953132635\n",
      "validation loss: 785.1280941099866\n",
      "epoch: 602\n",
      "training loss: 629.8663612494199\n",
      "validation loss: 784.9428086636034\n",
      "epoch: 603\n",
      "training loss: 629.5202033579236\n",
      "validation loss: 784.7581872420177\n",
      "epoch: 604\n",
      "training loss: 629.1749183452209\n",
      "validation loss: 784.5742261837662\n",
      "epoch: 605\n",
      "training loss: 628.8305029171765\n",
      "validation loss: 784.3909218336971\n",
      "epoch: 606\n",
      "training loss: 628.4869537792514\n",
      "validation loss: 784.2082705439622\n",
      "epoch: 607\n",
      "training loss: 628.1442676366856\n",
      "validation loss: 784.026268674979\n",
      "epoch: 608\n",
      "training loss: 627.8024411946838\n",
      "validation loss: 783.8449125963634\n",
      "epoch: 609\n",
      "training loss: 627.4614711586003\n",
      "validation loss: 783.6641986878287\n",
      "epoch: 610\n",
      "training loss: 627.1213542341276\n",
      "validation loss: 783.4841233400513\n",
      "epoch: 611\n",
      "training loss: 626.7820871274838\n",
      "validation loss: 783.304682955507\n",
      "epoch: 612\n",
      "training loss: 626.4436665456034\n",
      "validation loss: 783.1258739492713\n",
      "epoch: 613\n",
      "training loss: 626.1060891963273\n",
      "validation loss: 782.9476927497889\n",
      "epoch: 614\n",
      "training loss: 625.7693517885923\n",
      "validation loss: 782.7701357996104\n",
      "epoch: 615\n",
      "training loss: 625.4334510326245\n",
      "validation loss: 782.5931995560989\n",
      "epoch: 616\n",
      "training loss: 625.0983836401293\n",
      "validation loss: 782.4168804921029\n",
      "epoch: 617\n",
      "training loss: 624.764146324485\n",
      "validation loss: 782.2411750966025\n",
      "epoch: 618\n",
      "training loss: 624.4307358009353\n",
      "validation loss: 782.0660798753246\n",
      "epoch: 619\n",
      "training loss: 624.0981487867828\n",
      "validation loss: 781.8915913513276\n",
      "epoch: 620\n",
      "training loss: 623.7663820015831\n",
      "validation loss: 781.7177060655642\n",
      "epoch: 621\n",
      "training loss: 623.4354321673407\n",
      "validation loss: 781.5444205774135\n",
      "epoch: 622\n",
      "training loss: 623.1052960087038\n",
      "validation loss: 781.3717314651884\n",
      "epoch: 623\n",
      "training loss: 622.7759702531612\n",
      "validation loss: 781.1996353266201\n",
      "epoch: 624\n",
      "training loss: 622.4474516312417\n",
      "validation loss: 781.0281287793177\n",
      "epoch: 625\n",
      "training loss: 622.1197368767116\n",
      "validation loss: 780.8572084612081\n",
      "epoch: 626\n",
      "training loss: 621.7928227267761\n",
      "validation loss: 780.6868710309507\n",
      "epoch: 627\n",
      "training loss: 621.4667059222813\n",
      "validation loss: 780.5171131683359\n",
      "epoch: 628\n",
      "training loss: 621.141383207919\n",
      "validation loss: 780.3479315746606\n",
      "epoch: 629\n",
      "training loss: 620.81685133243\n",
      "validation loss: 780.1793229730877\n",
      "epoch: 630\n",
      "training loss: 620.4931070488141\n",
      "validation loss: 780.0112841089868\n",
      "epoch: 631\n",
      "training loss: 620.1701471145371\n",
      "validation loss: 779.843811750257\n",
      "epoch: 632\n",
      "training loss: 619.8479682917441\n",
      "validation loss: 779.6769026876362\n",
      "epoch: 633\n",
      "training loss: 619.5265673474719\n",
      "validation loss: 779.5105537349906\n",
      "epoch: 634\n",
      "training loss: 619.2059410538644\n",
      "validation loss: 779.344761729592\n",
      "epoch: 635\n",
      "training loss: 618.886086188391\n",
      "validation loss: 779.1795235323805\n",
      "epoch: 636\n",
      "training loss: 618.5669995340659\n",
      "validation loss: 779.0148360282107\n",
      "epoch: 637\n",
      "training loss: 618.2486778796691\n",
      "validation loss: 778.8506961260862\n",
      "epoch: 638\n",
      "training loss: 617.9311180199702\n",
      "validation loss: 778.6871007593783\n",
      "epoch: 639\n",
      "training loss: 617.614316755953\n",
      "validation loss: 778.5240468860325\n",
      "epoch: 640\n",
      "training loss: 617.2982708950406\n",
      "validation loss: 778.3615314887608\n",
      "epoch: 641\n",
      "training loss: 616.9829772513232\n",
      "validation loss: 778.1995515752218\n",
      "epoch: 642\n",
      "training loss: 616.6684326457845\n",
      "validation loss: 778.0381041781835\n",
      "epoch: 643\n",
      "training loss: 616.3546339065291\n",
      "validation loss: 777.8771863556766\n",
      "epoch: 644\n",
      "training loss: 616.0415778690086\n",
      "validation loss: 777.7167951911302\n",
      "epoch: 645\n",
      "training loss: 615.7292613762465\n",
      "validation loss: 777.5569277934957\n",
      "epoch: 646\n",
      "training loss: 615.4176812790625\n",
      "validation loss: 777.3975812973546\n",
      "epoch: 647\n",
      "training loss: 615.106834436292\n",
      "validation loss: 777.2387528630109\n",
      "epoch: 648\n",
      "training loss: 614.7967177150038\n",
      "validation loss: 777.08043967657\n",
      "epoch: 649\n",
      "training loss: 614.4873279907121\n",
      "validation loss: 776.9226389500002\n",
      "epoch: 650\n",
      "training loss: 614.1786621475858\n",
      "validation loss: 776.7653479211782\n",
      "epoch: 651\n",
      "training loss: 613.8707170786492\n",
      "validation loss: 776.6085638539198\n",
      "epoch: 652\n",
      "training loss: 613.5634896859775\n",
      "validation loss: 776.452284037993\n",
      "epoch: 653\n",
      "training loss: 613.2569768808847\n",
      "validation loss: 776.2965057891146\n",
      "epoch: 654\n",
      "training loss: 612.9511755841033\n",
      "validation loss: 776.1412264489312\n",
      "epoch: 655\n",
      "training loss: 612.6460827259536\n",
      "validation loss: 775.9864433849822\n",
      "epoch: 656\n",
      "training loss: 612.3416952465075\n",
      "validation loss: 775.8321539906482\n",
      "epoch: 657\n",
      "training loss: 612.0380100957375\n",
      "validation loss: 775.6783556850838\n",
      "epoch: 658\n",
      "training loss: 611.7350242336591\n",
      "validation loss: 775.5250459131331\n",
      "epoch: 659\n",
      "training loss: 611.4327346304617\n",
      "validation loss: 775.372222145234\n",
      "epoch: 660\n",
      "training loss: 611.1311382666278\n",
      "validation loss: 775.2198818773048\n",
      "epoch: 661\n",
      "training loss: 610.8302321330444\n",
      "validation loss: 775.068022630622\n",
      "epoch: 662\n",
      "training loss: 610.5300132311023\n",
      "validation loss: 774.9166419516854\n",
      "epoch: 663\n",
      "training loss: 610.230478572786\n",
      "validation loss: 774.7657374120703\n",
      "epoch: 664\n",
      "training loss: 609.9316251807552\n",
      "validation loss: 774.6153066082748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 665\n",
      "training loss: 609.633450088417\n",
      "validation loss: 774.4653471615554\n",
      "epoch: 666\n",
      "training loss: 609.3359503399905\n",
      "validation loss: 774.3158567177596\n",
      "epoch: 667\n",
      "training loss: 609.0391229905641\n",
      "validation loss: 774.1668329471505\n",
      "epoch: 668\n",
      "training loss: 608.742965106148\n",
      "validation loss: 774.0182735442295\n",
      "epoch: 669\n",
      "training loss: 608.4474737637196\n",
      "validation loss: 773.8701762275559\n",
      "epoch: 670\n",
      "training loss: 608.1526460512669\n",
      "validation loss: 773.7225387395655\n",
      "epoch: 671\n",
      "training loss: 607.8584790678266\n",
      "validation loss: 773.5753588463883\n",
      "epoch: 672\n",
      "training loss: 607.564969923522\n",
      "validation loss: 773.4286343376674\n",
      "epoch: 673\n",
      "training loss: 607.2721157395974\n",
      "validation loss: 773.2823630263796\n",
      "epoch: 674\n",
      "training loss: 606.9799136484538\n",
      "validation loss: 773.1365427486573\n",
      "epoch: 675\n",
      "training loss: 606.6883607936841\n",
      "validation loss: 772.9911713636144\n",
      "epoch: 676\n",
      "training loss: 606.397454330108\n",
      "validation loss: 772.8462467531734\n",
      "epoch: 677\n",
      "training loss: 606.1071914238103\n",
      "validation loss: 772.7017668218978\n",
      "epoch: 678\n",
      "training loss: 605.8175692521792\n",
      "validation loss: 772.5577294968252\n",
      "epoch: 679\n",
      "training loss: 605.5285850039471\n",
      "validation loss: 772.4141327273071\n",
      "epoch: 680\n",
      "training loss: 605.2402358792343\n",
      "validation loss: 772.2709744848469\n",
      "epoch: 681\n",
      "training loss: 604.952519089594\n",
      "validation loss: 772.1282527629447\n",
      "epoch: 682\n",
      "training loss: 604.6654318580605\n",
      "validation loss: 771.9859655769438\n",
      "epoch: 683\n",
      "training loss: 604.3789714192\n",
      "validation loss: 771.8441109638762\n",
      "epoch: 684\n",
      "training loss: 604.0931350191614\n",
      "validation loss: 771.7026869823143\n",
      "epoch: 685\n",
      "training loss: 603.8079199157323\n",
      "validation loss: 771.5616917122179\n",
      "epoch: 686\n",
      "training loss: 603.5233233783945\n",
      "validation loss: 771.4211232547894\n",
      "epoch: 687\n",
      "training loss: 603.2393426883809\n",
      "validation loss: 771.2809797323207\n",
      "epoch: 688\n",
      "training loss: 602.9559751387362\n",
      "validation loss: 771.1412592880464\n",
      "epoch: 689\n",
      "training loss: 602.6732180343741\n",
      "validation loss: 771.0019600859943\n",
      "epoch: 690\n",
      "training loss: 602.3910686921408\n",
      "validation loss: 770.8630803108342\n",
      "epoch: 691\n",
      "training loss: 602.1095244408729\n",
      "validation loss: 770.7246181677273\n",
      "epoch: 692\n",
      "training loss: 601.828582621461\n",
      "validation loss: 770.5865718821727\n",
      "epoch: 693\n",
      "training loss: 601.5482405869088\n",
      "validation loss: 770.4489396998564\n",
      "epoch: 694\n",
      "training loss: 601.2684957023964\n",
      "validation loss: 770.3117198864926\n",
      "epoch: 695\n",
      "training loss: 600.9893453453392\n",
      "validation loss: 770.1749107276717\n",
      "epoch: 696\n",
      "training loss: 600.7107869054494\n",
      "validation loss: 770.0385105287004\n",
      "epoch: 697\n",
      "training loss: 600.432817784796\n",
      "validation loss: 769.9025176144461\n",
      "epoch: 698\n",
      "training loss: 600.1554353978667\n",
      "validation loss: 769.7669303291777\n",
      "epoch: 699\n",
      "training loss: 599.8786371716244\n",
      "validation loss: 769.6317470364072\n",
      "epoch: 700\n",
      "training loss: 599.6024205455708\n",
      "validation loss: 769.4969661187314\n",
      "epoch: 701\n",
      "training loss: 599.3267829718056\n",
      "validation loss: 769.3625859776747\n",
      "epoch: 702\n",
      "training loss: 599.0517219150856\n",
      "validation loss: 769.2286050335309\n",
      "epoch: 703\n",
      "training loss: 598.7772348528885\n",
      "validation loss: 769.0950217252083\n",
      "epoch: 704\n",
      "training loss: 598.5033192754729\n",
      "validation loss: 768.9618345100746\n",
      "epoch: 705\n",
      "training loss: 598.2299726859427\n",
      "validation loss: 768.8290418638048\n",
      "epoch: 706\n",
      "training loss: 597.9571926003108\n",
      "validation loss: 768.6966422802307\n",
      "epoch: 707\n",
      "training loss: 597.6849765475653\n",
      "validation loss: 768.5646342711939\n",
      "epoch: 708\n",
      "training loss: 597.413322069736\n",
      "validation loss: 768.4330163663985\n",
      "epoch: 709\n",
      "training loss: 597.1422267219643\n",
      "validation loss: 768.3017871132711\n",
      "epoch: 710\n",
      "training loss: 596.8716880725736\n",
      "validation loss: 768.1709450768183\n",
      "epoch: 711\n",
      "training loss: 596.6017037031405\n",
      "validation loss: 768.0404888394913\n",
      "epoch: 712\n",
      "training loss: 596.3322712085695\n",
      "validation loss: 767.9104170010507\n",
      "epoch: 713\n",
      "training loss: 596.0633881971667\n",
      "validation loss: 767.7807281784316\n",
      "epoch: 714\n",
      "training loss: 595.7950522907157\n",
      "validation loss: 767.6514210056152\n",
      "epoch: 715\n",
      "training loss: 595.527261124553\n",
      "validation loss: 767.5224941334972\n",
      "epoch: 716\n",
      "training loss: 595.2600123476429\n",
      "validation loss: 767.3939462297585\n",
      "epoch: 717\n",
      "training loss: 594.9933036226519\n",
      "validation loss: 767.265775978734\n",
      "epoch: 718\n",
      "training loss: 594.7271326260195\n",
      "validation loss: 767.137982081283\n",
      "epoch: 719\n",
      "training loss: 594.4614970480275\n",
      "validation loss: 767.0105632546528\n",
      "epoch: 720\n",
      "training loss: 594.1963945928643\n",
      "validation loss: 766.8835182323437\n",
      "epoch: 721\n",
      "training loss: 593.9318229786836\n",
      "validation loss: 766.756845763966\n",
      "epoch: 722\n",
      "training loss: 593.667779937656\n",
      "validation loss: 766.6305446150927\n",
      "epoch: 723\n",
      "training loss: 593.404263216014\n",
      "validation loss: 766.5046135671056\n",
      "epoch: 724\n",
      "training loss: 593.141270574086\n",
      "validation loss: 766.3790514170321\n",
      "epoch: 725\n",
      "training loss: 592.8787997863208\n",
      "validation loss: 766.2538569773754\n",
      "epoch: 726\n",
      "training loss: 592.6168486412995\n",
      "validation loss: 766.129029075932\n",
      "epoch: 727\n",
      "training loss: 592.3554149417347\n",
      "validation loss: 766.004566555602\n",
      "epoch: 728\n",
      "training loss: 592.0944965044565\n",
      "validation loss: 765.8804682741842\n",
      "epoch: 729\n",
      "training loss: 591.8340911603835\n",
      "validation loss: 765.7567331041637\n",
      "epoch: 730\n",
      "training loss: 591.5741967544782\n",
      "validation loss: 765.6333599324856\n",
      "epoch: 731\n",
      "training loss: 591.3148111456879\n",
      "validation loss: 765.5103476603169\n",
      "epoch: 732\n",
      "training loss: 591.0559322068697\n",
      "validation loss: 765.3876952028003\n",
      "epoch: 733\n",
      "training loss: 590.7975578247008\n",
      "validation loss: 765.2654014887926\n",
      "epoch: 734\n",
      "training loss: 590.539685899575\n",
      "validation loss: 765.1434654606003\n",
      "epoch: 735\n",
      "training loss: 590.2823143454839\n",
      "validation loss: 765.0218860737008\n",
      "epoch: 736\n",
      "training loss: 590.0254410898872\n",
      "validation loss: 764.9006622964613\n",
      "epoch: 737\n",
      "training loss: 589.7690640735719\n",
      "validation loss: 764.7797931098501\n",
      "epoch: 738\n",
      "training loss: 589.5131812505015\n",
      "validation loss: 764.6592775071458\n",
      "epoch: 739\n",
      "training loss: 589.2577905876576\n",
      "validation loss: 764.5391144936445\n",
      "epoch: 740\n",
      "training loss: 589.0028900648773\n",
      "validation loss: 764.4193030863678\n",
      "epoch: 741\n",
      "training loss: 588.7484776746842\n",
      "validation loss: 764.2998423137724\n",
      "epoch: 742\n",
      "training loss: 588.4945514221195\n",
      "validation loss: 764.1807312154657\n",
      "epoch: 743\n",
      "training loss: 588.241109324573\n",
      "validation loss: 764.061968841924\n",
      "epoch: 744\n",
      "training loss: 587.988149411616\n",
      "validation loss: 763.943554254222\n",
      "epoch: 745\n",
      "training loss: 587.7356697248366\n",
      "validation loss: 763.8254865237683\n",
      "epoch: 746\n",
      "training loss: 587.4836683176826\n",
      "validation loss: 763.7077647320529\n",
      "epoch: 747\n",
      "training loss: 587.2321432553092\n",
      "validation loss: 763.5903879704032\n",
      "epoch: 748\n",
      "training loss: 586.9810926144343\n",
      "validation loss: 763.4733553397559\n",
      "epoch: 749\n",
      "training loss: 586.7305144832039\n",
      "validation loss: 763.3566659504357\n",
      "epoch: 750\n",
      "training loss: 586.4804069610647\n",
      "validation loss: 763.2403189219509\n",
      "epoch: 751\n",
      "training loss: 586.2307681586482\n",
      "validation loss: 763.1243133827982\n",
      "epoch: 752\n",
      "training loss: 585.9815961976626\n",
      "validation loss: 763.0086484702813\n",
      "epoch: 753\n",
      "training loss: 585.7328892107963\n",
      "validation loss: 762.893323330341\n",
      "epoch: 754\n",
      "training loss: 585.484645341629\n",
      "validation loss: 762.7783371173931\n",
      "epoch: 755\n",
      "training loss: 585.236862744554\n",
      "validation loss: 762.6636889941824\n",
      "epoch: 756\n",
      "training loss: 584.9895395847064\n",
      "validation loss: 762.5493781316401\n",
      "epoch: 757\n",
      "training loss: 584.742674037902\n",
      "validation loss: 762.4354037087529\n",
      "epoch: 758\n",
      "training loss: 584.4962642905808\n",
      "validation loss: 762.3217649124396\n",
      "epoch: 759\n",
      "training loss: 584.2503085397589\n",
      "validation loss: 762.208460937432\n",
      "epoch: 760\n",
      "training loss: 584.004804992984\n",
      "validation loss: 762.0954909861645\n",
      "epoch: 761\n",
      "training loss: 583.7597518682973\n",
      "validation loss: 761.9828542686655\n",
      "epoch: 762\n",
      "training loss: 583.5151473941984\n",
      "validation loss: 761.8705500024563\n",
      "epoch: 763\n",
      "training loss: 583.2709898096139\n",
      "validation loss: 761.758577412448\n",
      "epoch: 764\n",
      "training loss: 583.0272773638669\n",
      "validation loss: 761.6469357308476\n",
      "epoch: 765\n",
      "training loss: 582.7840083166504\n",
      "validation loss: 761.5356241970608\n",
      "epoch: 766\n",
      "training loss: 582.5411809379999\n",
      "validation loss: 761.4246420575977\n",
      "epoch: 767\n",
      "training loss: 582.2987935082677\n",
      "validation loss: 761.3139885659839\n",
      "epoch: 768\n",
      "training loss: 582.0568443180965\n",
      "validation loss: 761.2036629826658\n",
      "epoch: 769\n",
      "training loss: 581.8153316683929\n",
      "validation loss: 761.0936645749247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 770\n",
      "training loss: 581.5742538703005\n",
      "validation loss: 760.9839926167855\n",
      "epoch: 771\n",
      "training loss: 581.333609245172\n",
      "validation loss: 760.8746463889318\n",
      "epoch: 772\n",
      "training loss: 581.09339612454\n",
      "validation loss: 760.7656251786207\n",
      "epoch: 773\n",
      "training loss: 580.8536128500871\n",
      "validation loss: 760.656928279601\n",
      "epoch: 774\n",
      "training loss: 580.6142577736139\n",
      "validation loss: 760.548554992034\n",
      "epoch: 775\n",
      "training loss: 580.375329257008\n",
      "validation loss: 760.4405046224181\n",
      "epoch: 776\n",
      "training loss: 580.1368256722096\n",
      "validation loss: 760.3327764835173\n",
      "epoch: 777\n",
      "training loss: 579.8987454011776\n",
      "validation loss: 760.2253698943007\n",
      "epoch: 778\n",
      "training loss: 579.6610868358564\n",
      "validation loss: 760.1182841798802\n",
      "epoch: 779\n",
      "training loss: 579.4238483781407\n",
      "validation loss: 760.0115186714669\n",
      "epoch: 780\n",
      "training loss: 579.1870284398418\n",
      "validation loss: 759.9050727063296\n",
      "epoch: 781\n",
      "training loss: 578.9506254426543\n",
      "validation loss: 759.7989456277704\n",
      "epoch: 782\n",
      "training loss: 578.7146378181253\n",
      "validation loss: 759.6931367851131\n",
      "epoch: 783\n",
      "training loss: 578.4790640076247\n",
      "validation loss: 759.5876455337068\n",
      "epoch: 784\n",
      "training loss: 578.24390246232\n",
      "validation loss: 759.482471234951\n",
      "epoch: 785\n",
      "training loss: 578.0091516431523\n",
      "validation loss: 759.3776132563406\n",
      "epoch: 786\n",
      "training loss: 577.774810020821\n",
      "validation loss: 759.2730709715364\n",
      "epoch: 787\n",
      "training loss: 577.5408760757706\n",
      "validation loss: 759.1688437604648\n",
      "epoch: 788\n",
      "training loss: 577.307348298188\n",
      "validation loss: 759.0649310094486\n",
      "epoch: 789\n",
      "training loss: 577.0742251880055\n",
      "validation loss: 758.9613321113742\n",
      "epoch: 790\n",
      "training loss: 576.8415052549161\n",
      "validation loss: 758.8580464659009\n",
      "epoch: 791\n",
      "training loss: 576.6091870183989\n",
      "validation loss: 758.7550734797127\n",
      "epoch: 792\n",
      "training loss: 576.3772690077573\n",
      "validation loss: 758.6524125668252\n",
      "epoch: 793\n",
      "training loss: 576.1457497621741\n",
      "validation loss: 758.5500631489452\n",
      "epoch: 794\n",
      "training loss: 575.9146278307825\n",
      "validation loss: 758.4480246558944\n",
      "epoch: 795\n",
      "training loss: 575.6839017727547\n",
      "validation loss: 758.3462965261015\n",
      "epoch: 796\n",
      "training loss: 575.4535701574143\n",
      "validation loss: 758.2448782071685\n",
      "epoch: 797\n",
      "training loss: 575.2236315643696\n",
      "validation loss: 758.1437691565224\n",
      "epoch: 798\n",
      "training loss: 574.9940845836744\n",
      "validation loss: 758.0429688421522\n",
      "epoch: 799\n",
      "training loss: 574.7649278160136\n",
      "validation loss: 757.9424767434434\n",
      "epoch: 800\n",
      "training loss: 574.536159872921\n",
      "validation loss: 757.8422923521175\n",
      "epoch: 801\n",
      "training loss: 574.3077793770267\n",
      "validation loss: 757.7424151732791\n",
      "epoch: 802\n",
      "training loss: 574.0797849623384\n",
      "validation loss: 757.6428447265806\n",
      "epoch: 803\n",
      "training loss: 573.8521752745561\n",
      "validation loss: 757.5435805475097\n",
      "epoch: 804\n",
      "training loss: 573.6249489714218\n",
      "validation loss: 757.4446221888012\n",
      "epoch: 805\n",
      "training loss: 573.3981047231043\n",
      "validation loss: 757.3459692219819\n",
      "epoch: 806\n",
      "training loss: 573.1716412126173\n",
      "validation loss: 757.2476212390445\n",
      "epoch: 807\n",
      "training loss: 572.9455571362711\n",
      "validation loss: 757.1495778542559\n",
      "epoch: 808\n",
      "training loss: 572.7198512041532\n",
      "validation loss: 757.0518387060887\n",
      "epoch: 809\n",
      "training loss: 572.4945221406335\n",
      "validation loss: 756.9544034592789\n",
      "epoch: 810\n",
      "training loss: 572.2695686848929\n",
      "validation loss: 756.8572718069922\n",
      "epoch: 811\n",
      "training loss: 572.044989591461\n",
      "validation loss: 756.7604434730843\n",
      "epoch: 812\n",
      "training loss: 571.8207836307612\n",
      "validation loss: 756.6639182144396\n",
      "epoch: 813\n",
      "training loss: 571.5969495896501\n",
      "validation loss: 756.567695823354\n",
      "epoch: 814\n",
      "training loss: 571.3734862719365\n",
      "validation loss: 756.4717761299311\n",
      "epoch: 815\n",
      "training loss: 571.1503924988709\n",
      "validation loss: 756.3761590044519\n",
      "epoch: 816\n",
      "training loss: 570.9276671095857\n",
      "validation loss: 756.2808443596581\n",
      "epoch: 817\n",
      "training loss: 570.7053089614708\n",
      "validation loss: 756.185832152894\n",
      "epoch: 818\n",
      "training loss: 570.4833169304645\n",
      "validation loss: 756.0911223880306\n",
      "epoch: 819\n",
      "training loss: 570.2616899112421\n",
      "validation loss: 755.9967151170847\n",
      "epoch: 820\n",
      "training loss: 570.0404268172762\n",
      "validation loss: 755.9026104414406\n",
      "epoch: 821\n",
      "training loss: 569.8195265807536\n",
      "validation loss: 755.8088085125605\n",
      "epoch: 822\n",
      "training loss: 569.5989881523225\n",
      "validation loss: 755.715309532061\n",
      "epoch: 823\n",
      "training loss: 569.3788105006506\n",
      "validation loss: 755.6221137510321\n",
      "epoch: 824\n",
      "training loss: 569.1589926117783\n",
      "validation loss: 755.5292214684482\n",
      "epoch: 825\n",
      "training loss: 568.9395334882435\n",
      "validation loss: 755.4366330285407\n",
      "epoch: 826\n",
      "training loss: 568.7204321479721\n",
      "validation loss: 755.3443488169819\n",
      "epoch: 827\n",
      "training loss: 568.5016876229209\n",
      "validation loss: 755.2523692557563\n",
      "epoch: 828\n",
      "training loss: 568.283298957475\n",
      "validation loss: 755.1606947966036\n",
      "epoch: 829\n",
      "training loss: 568.0652652066028\n",
      "validation loss: 755.0693259129523\n",
      "epoch: 830\n",
      "training loss: 567.8475854337869\n",
      "validation loss: 754.9782630903029\n",
      "epoch: 831\n",
      "training loss: 567.6302587087588\n",
      "validation loss: 754.8875068150749\n",
      "epoch: 832\n",
      "training loss: 567.4132841050746\n",
      "validation loss: 754.7970575619995\n",
      "epoch: 833\n",
      "training loss: 567.1966606975841\n",
      "validation loss: 754.7069157802155\n",
      "epoch: 834\n",
      "training loss: 566.9803875598608\n",
      "validation loss: 754.6170818783139\n",
      "epoch: 835\n",
      "training loss: 566.7644637616603\n",
      "validation loss: 754.5275562086605\n",
      "epoch: 836\n",
      "training loss: 566.5488883664975\n",
      "validation loss: 754.4383390514153\n",
      "epoch: 837\n",
      "training loss: 566.3336604294201\n",
      "validation loss: 754.3494305987296\n",
      "epoch: 838\n",
      "training loss: 566.1187789950676\n",
      "validation loss: 754.2608309396661\n",
      "epoch: 839\n",
      "training loss: 565.90424309609\n",
      "validation loss: 754.172540046405\n",
      "epoch: 840\n",
      "training loss: 565.6900517519879\n",
      "validation loss: 754.0845577622962\n",
      "epoch: 841\n",
      "training loss: 565.4762039684205\n",
      "validation loss: 753.996883792276\n",
      "epoch: 842\n",
      "training loss: 565.262698737005\n",
      "validation loss: 753.9095176960899\n",
      "epoch: 843\n",
      "training loss: 565.0495350355998\n",
      "validation loss: 753.8224588846482\n",
      "epoch: 844\n",
      "training loss: 564.8367118290496\n",
      "validation loss: 753.7357066197166\n",
      "epoch: 845\n",
      "training loss: 564.6242280703403\n",
      "validation loss: 753.6492600169782\n",
      "epoch: 846\n",
      "training loss: 564.4120827020948\n",
      "validation loss: 753.5631180523661\n",
      "epoch: 847\n",
      "training loss: 564.2002746583287\n",
      "validation loss: 753.4772795713982\n",
      "epoch: 848\n",
      "training loss: 563.9888028663795\n",
      "validation loss: 753.3917433011416\n",
      "epoch: 849\n",
      "training loss: 563.7776662489176\n",
      "validation loss: 753.3065078643087\n",
      "epoch: 850\n",
      "training loss: 563.5668637259606\n",
      "validation loss: 753.2215717949463\n",
      "epoch: 851\n",
      "training loss: 563.3563942168129\n",
      "validation loss: 753.1369335551196\n",
      "epoch: 852\n",
      "training loss: 563.1462566418757\n",
      "validation loss: 753.0525915520278\n",
      "epoch: 853\n",
      "training loss: 562.9364499242822\n",
      "validation loss: 752.9685441549964\n",
      "epoch: 854\n",
      "training loss: 562.7269729913272\n",
      "validation loss: 752.8847897118686\n",
      "epoch: 855\n",
      "training loss: 562.5178247756778\n",
      "validation loss: 752.8013265643851\n",
      "epoch: 856\n",
      "training loss: 562.3090042163635\n",
      "validation loss: 752.7181530622364\n",
      "epoch: 857\n",
      "training loss: 562.1005102595535\n",
      "validation loss: 752.6352675755548\n",
      "epoch: 858\n",
      "training loss: 561.8923418591347\n",
      "validation loss: 752.5526685057093\n",
      "epoch: 859\n",
      "training loss: 561.6844979771139\n",
      "validation loss: 752.4703542943354\n",
      "epoch: 860\n",
      "training loss: 561.4769775838648\n",
      "validation loss: 752.3883234306136\n",
      "epoch: 861\n",
      "training loss: 561.269779658246\n",
      "validation loss: 752.3065744568565\n",
      "epoch: 862\n",
      "training loss: 561.0629031876119\n",
      "validation loss: 752.2251059725115\n",
      "epoch: 863\n",
      "training loss: 560.8563471677388\n",
      "validation loss: 752.1439166367205\n",
      "epoch: 864\n",
      "training loss: 560.6501106026869\n",
      "validation loss: 752.0630051695939\n",
      "epoch: 865\n",
      "training loss: 560.4441925046153\n",
      "validation loss: 751.9823703523638\n",
      "epoch: 866\n",
      "training loss: 560.2385918935629\n",
      "validation loss: 751.9020110265913\n",
      "epoch: 867\n",
      "training loss: 560.033307797208\n",
      "validation loss: 751.8219260925869\n",
      "epoch: 868\n",
      "training loss: 559.8283392506183\n",
      "validation loss: 751.7421145072051\n",
      "epoch: 869\n",
      "training loss: 559.6236852959954\n",
      "validation loss: 751.6625752811497\n",
      "epoch: 870\n",
      "training loss: 559.41934498242\n",
      "validation loss: 751.5833074759225\n",
      "epoch: 871\n",
      "training loss: 559.2153173656044\n",
      "validation loss: 751.504310200529\n",
      "epoch: 872\n",
      "training loss: 559.0116015076528\n",
      "validation loss: 751.4255826080288\n",
      "epoch: 873\n",
      "training loss: 558.8081964768315\n",
      "validation loss: 751.347123892029\n",
      "epoch: 874\n",
      "training loss: 558.6051013473506\n",
      "validation loss: 751.2689332831714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 875\n",
      "training loss: 558.4023151991573\n",
      "validation loss: 751.1910100456794\n",
      "epoch: 876\n",
      "training loss: 558.1998371177424\n",
      "validation loss: 751.1133534740068\n",
      "epoch: 877\n",
      "training loss: 557.9976661939559\n",
      "validation loss: 751.0359628896167\n",
      "epoch: 878\n",
      "training loss: 557.7958015238364\n",
      "validation loss: 750.9588376379191\n",
      "epoch: 879\n",
      "training loss: 557.5942422084498\n",
      "validation loss: 750.8819770853852\n",
      "epoch: 880\n",
      "training loss: 557.3929873537381\n",
      "validation loss: 750.8053806168424\n",
      "epoch: 881\n",
      "training loss: 557.1920360703774\n",
      "validation loss: 750.7290476329636\n",
      "epoch: 882\n",
      "training loss: 556.9913874736446\n",
      "validation loss: 750.6529775479453\n",
      "epoch: 883\n",
      "training loss: 556.7910406832926\n",
      "validation loss: 750.577169787377\n",
      "epoch: 884\n",
      "training loss: 556.5909948234306\n",
      "validation loss: 750.5016237862916\n",
      "epoch: 885\n",
      "training loss: 556.3912490224145\n",
      "validation loss: 750.4263389873953\n",
      "epoch: 886\n",
      "training loss: 556.1918024127394\n",
      "validation loss: 750.3513148394663\n",
      "epoch: 887\n",
      "training loss: 555.9926541309402\n",
      "validation loss: 750.2765507959115\n",
      "epoch: 888\n",
      "training loss: 555.7938033174968\n",
      "validation loss: 750.2020463134735\n",
      "epoch: 889\n",
      "training loss: 555.5952491167419\n",
      "validation loss: 750.127800851077\n",
      "epoch: 890\n",
      "training loss: 555.3969906767759\n",
      "validation loss: 750.0538138688044\n",
      "epoch: 891\n",
      "training loss: 555.1990271493823\n",
      "validation loss: 749.9800848269884\n",
      "epoch: 892\n",
      "training loss: 555.0013576899494\n",
      "validation loss: 749.9066131854157\n",
      "epoch: 893\n",
      "training loss: 554.8039814573916\n",
      "validation loss: 749.8333984026284\n",
      "epoch: 894\n",
      "training loss: 554.6068976140772\n",
      "validation loss: 749.7604399353148\n",
      "epoch: 895\n",
      "training loss: 554.4101053257548\n",
      "validation loss: 749.6877372377829\n",
      "epoch: 896\n",
      "training loss: 554.2136037614856\n",
      "validation loss: 749.6152897615075\n",
      "epoch: 897\n",
      "training loss: 554.0173920935739\n",
      "validation loss: 749.5430969547401\n",
      "epoch: 898\n",
      "training loss: 553.8214694975027\n",
      "validation loss: 749.4711582621818\n",
      "epoch: 899\n",
      "training loss: 553.6258351518676\n",
      "validation loss: 749.3994731247047\n",
      "epoch: 900\n",
      "training loss: 553.4304882383159\n",
      "validation loss: 749.3280409791222\n",
      "epoch: 901\n",
      "training loss: 553.235427941483\n",
      "validation loss: 749.2568612579992\n",
      "epoch: 902\n",
      "training loss: 553.0406534489322\n",
      "validation loss: 749.1859333894984\n",
      "epoch: 903\n",
      "training loss: 552.8461639510954\n",
      "validation loss: 749.1152567972591\n",
      "epoch: 904\n",
      "training loss: 552.651958641213\n",
      "validation loss: 749.0448309003032\n",
      "epoch: 905\n",
      "training loss: 552.4580367152768\n",
      "validation loss: 748.9746551129645\n",
      "epoch: 906\n",
      "training loss: 552.2643973719714\n",
      "validation loss: 748.9047288448404\n",
      "epoch: 907\n",
      "training loss: 552.0710398126178\n",
      "validation loss: 748.8350515007619\n",
      "epoch: 908\n",
      "training loss: 551.8779632411159\n",
      "validation loss: 748.7656224807765\n",
      "epoch: 909\n",
      "training loss: 551.6851668638891\n",
      "validation loss: 748.6964411801489\n",
      "epoch: 910\n",
      "training loss: 551.4926498898277\n",
      "validation loss: 748.6275069893669\n",
      "epoch: 911\n",
      "training loss: 551.3004115302333\n",
      "validation loss: 748.5588192941647\n",
      "epoch: 912\n",
      "training loss: 551.108450998764\n",
      "validation loss: 748.4903774755467\n",
      "epoch: 913\n",
      "training loss: 550.9167675113783\n",
      "validation loss: 748.4221809098251\n",
      "epoch: 914\n",
      "training loss: 550.7253602862805\n",
      "validation loss: 748.3542289686584\n",
      "epoch: 915\n",
      "training loss: 550.5342285438666\n",
      "validation loss: 748.2865210190969\n",
      "epoch: 916\n",
      "training loss: 550.3433715066683\n",
      "validation loss: 748.2190564236341\n",
      "epoch: 917\n",
      "training loss: 550.1527883993006\n",
      "validation loss: 748.1518345402591\n",
      "epoch: 918\n",
      "training loss: 549.9624784484064\n",
      "validation loss: 748.0848547225138\n",
      "epoch: 919\n",
      "training loss: 549.7724408826039\n",
      "validation loss: 748.0181163195552\n",
      "epoch: 920\n",
      "training loss: 549.5826749324334\n",
      "validation loss: 747.9516186762169\n",
      "epoch: 921\n",
      "training loss: 549.3931798303041\n",
      "validation loss: 747.8853611330767\n",
      "epoch: 922\n",
      "training loss: 549.2039548104444\n",
      "validation loss: 747.8193430265276\n",
      "epoch: 923\n",
      "training loss: 549.0149991088492\n",
      "validation loss: 747.7535636888502\n",
      "epoch: 924\n",
      "training loss: 548.8263119632322\n",
      "validation loss: 747.6880224482916\n",
      "epoch: 925\n",
      "training loss: 548.6378926129767\n",
      "validation loss: 747.6227186291469\n",
      "epoch: 926\n",
      "training loss: 548.4497402990897\n",
      "validation loss: 747.5576515518478\n",
      "epoch: 927\n",
      "training loss: 548.2618542641591\n",
      "validation loss: 747.4928205330557\n",
      "epoch: 928\n",
      "training loss: 548.0742337523101\n",
      "validation loss: 747.4282248857623\n",
      "epoch: 929\n",
      "training loss: 547.8868780091689\n",
      "validation loss: 747.3638639193998\n",
      "epoch: 930\n",
      "training loss: 547.6997862818266\n",
      "validation loss: 747.299736939957\n",
      "epoch: 931\n",
      "training loss: 547.5129578188087\n",
      "validation loss: 747.2358432501103\n",
      "epoch: 932\n",
      "training loss: 547.3263918700509\n",
      "validation loss: 747.1721821493634\n",
      "epoch: 933\n",
      "training loss: 547.1400876868775\n",
      "validation loss: 747.1087529342033\n",
      "epoch: 934\n",
      "training loss: 546.9540445219893\n",
      "validation loss: 747.0455548982708\n",
      "epoch: 935\n",
      "training loss: 546.7682616294585\n",
      "validation loss: 746.9825873325493\n",
      "epoch: 936\n",
      "training loss: 546.582738264731\n",
      "validation loss: 746.9198495255732\n",
      "epoch: 937\n",
      "training loss: 546.3974736846407\n",
      "validation loss: 746.8573407636583\n",
      "epoch: 938\n",
      "training loss: 546.2124671474328\n",
      "validation loss: 746.7950603311566\n",
      "epoch: 939\n",
      "training loss: 546.0277179128033\n",
      "validation loss: 746.7330075107386\n",
      "epoch: 940\n",
      "training loss: 545.8432252419473\n",
      "validation loss: 746.6711815837054\n",
      "epoch: 941\n",
      "training loss: 545.6589883976294\n",
      "validation loss: 746.6095818303305\n",
      "epoch: 942\n",
      "training loss: 545.4750066442664\n",
      "validation loss: 746.5482075302385\n",
      "epoch: 943\n",
      "training loss: 545.2912792480314\n",
      "validation loss: 746.4870579628218\n",
      "epoch: 944\n",
      "training loss: 545.1078054769783\n",
      "validation loss: 746.426132407692\n",
      "epoch: 945\n",
      "training loss: 544.9245846011888\n",
      "validation loss: 746.3654301451794\n",
      "epoch: 946\n",
      "training loss: 544.7416158929434\n",
      "validation loss: 746.3049504568692\n",
      "epoch: 947\n",
      "training loss: 544.5588986269167\n",
      "validation loss: 746.2446926261879\n",
      "epoch: 948\n",
      "training loss: 544.3764320804013\n",
      "validation loss: 746.1846559390309\n",
      "epoch: 949\n",
      "training loss: 544.1942155335569\n",
      "validation loss: 746.1248396844386\n",
      "epoch: 950\n",
      "training loss: 544.0122482696878\n",
      "validation loss: 746.0652431553132\n",
      "epoch: 951\n",
      "training loss: 543.8305295755481\n",
      "validation loss: 746.0058656491815\n",
      "epoch: 952\n",
      "training loss: 543.6490587416736\n",
      "validation loss: 745.946706468994\n",
      "epoch: 953\n",
      "training loss: 543.4678350627363\n",
      "validation loss: 745.887764923959\n",
      "epoch: 954\n",
      "training loss: 543.2868578379247\n",
      "validation loss: 745.829040330405\n",
      "epoch: 955\n",
      "training loss: 543.1061263713385\n",
      "validation loss: 745.7705320126581\n",
      "epoch: 956\n",
      "training loss: 542.9256399723977\n",
      "validation loss: 745.7122393039331\n",
      "epoch: 957\n",
      "training loss: 542.7453979562589\n",
      "validation loss: 745.6541615472161\n",
      "epoch: 958\n",
      "training loss: 542.5653996442279\n",
      "validation loss: 745.596298096132\n",
      "epoch: 959\n",
      "training loss: 542.3856443641635\n",
      "validation loss: 745.5386483157696\n",
      "epoch: 960\n",
      "training loss: 542.2061314508569\n",
      "validation loss: 745.4812115834578\n",
      "epoch: 961\n",
      "training loss: 542.0268602463792\n",
      "validation loss: 745.4239872894619\n",
      "epoch: 962\n",
      "training loss: 541.847830100379\n",
      "validation loss: 745.3669748375817\n",
      "epoch: 963\n",
      "training loss: 541.6690403703201\n",
      "validation loss: 745.3101736456288\n",
      "epoch: 964\n",
      "training loss: 541.4904904216409\n",
      "validation loss: 745.2535831457577\n",
      "epoch: 965\n",
      "training loss: 541.3121796278244\n",
      "validation loss: 745.1972027846285\n",
      "epoch: 966\n",
      "training loss: 541.1341073703641\n",
      "validation loss: 745.14103202338\n",
      "epoch: 967\n",
      "training loss: 540.9562730386098\n",
      "validation loss: 745.0850703373939\n",
      "epoch: 968\n",
      "training loss: 540.7786760294905\n",
      "validation loss: 745.0293172158351\n",
      "epoch: 969\n",
      "training loss: 540.6013157470999\n",
      "validation loss: 744.9737721609572\n",
      "epoch: 970\n",
      "training loss: 540.4241916021446\n",
      "validation loss: 744.9184346871656\n",
      "epoch: 971\n",
      "training loss: 540.2473030112541\n",
      "validation loss: 744.8633043198445\n",
      "epoch: 972\n",
      "training loss: 540.0706493961583\n",
      "validation loss: 744.8083805939519\n",
      "epoch: 973\n",
      "training loss: 539.8942301827382\n",
      "validation loss: 744.7536630523975\n",
      "epoch: 974\n",
      "training loss: 539.7180447999701\n",
      "validation loss: 744.6991512442307\n",
      "epoch: 975\n",
      "training loss: 539.5420926787742\n",
      "validation loss: 744.6448447226626\n",
      "epoch: 976\n",
      "training loss: 539.366373250794\n",
      "validation loss: 744.5907430429611\n",
      "epoch: 977\n",
      "training loss: 539.19088594713\n",
      "validation loss: 744.5368457602605\n",
      "epoch: 978\n",
      "training loss: 539.015630197052\n",
      "validation loss: 744.483152427324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 979\n",
      "training loss: 538.8406054267205\n",
      "validation loss: 744.429662592312\n",
      "epoch: 980\n",
      "training loss: 538.6658110579398\n",
      "validation loss: 744.3763757965972\n",
      "epoch: 981\n",
      "training loss: 538.4912465069698\n",
      "validation loss: 744.3232915726703\n",
      "epoch: 982\n",
      "training loss: 538.3169111834186\n",
      "validation loss: 744.2704094421825\n",
      "epoch: 983\n",
      "training loss: 538.1428044892341\n",
      "validation loss: 744.2177289141567\n",
      "epoch: 984\n",
      "training loss: 537.9689258178096\n",
      "validation loss: 744.1652494833984\n",
      "epoch: 985\n",
      "training loss: 537.7952745532148\n",
      "validation loss: 744.1129706291375\n",
      "epoch: 986\n",
      "training loss: 537.6218500695587\n",
      "validation loss: 744.0608918139117\n",
      "epoch: 987\n",
      "training loss: 537.4486517304882\n",
      "validation loss: 744.0090124827097\n",
      "epoch: 988\n",
      "training loss: 537.2756788888203\n",
      "validation loss: 743.9573320623801\n",
      "epoch: 989\n",
      "training loss: 537.1029308863094\n",
      "validation loss: 743.905849961308\n",
      "epoch: 990\n",
      "training loss: 536.9304070535421\n",
      "validation loss: 743.854565569357\n",
      "epoch: 991\n",
      "training loss: 536.7581067099594\n",
      "validation loss: 743.8034782580759\n",
      "epoch: 992\n",
      "training loss: 536.586029163999\n",
      "validation loss: 743.7525873811594\n",
      "epoch: 993\n",
      "training loss: 536.4141737133593\n",
      "validation loss: 743.7018922751644\n",
      "epoch: 994\n",
      "training loss: 536.2425396453846\n",
      "validation loss: 743.651392260471\n",
      "epoch: 995\n",
      "training loss: 536.071126237575\n",
      "validation loss: 743.6010866424898\n",
      "epoch: 996\n",
      "training loss: 535.8999327582326\n",
      "validation loss: 743.5509747131159\n",
      "epoch: 997\n",
      "training loss: 535.728958467255\n",
      "validation loss: 743.5010557524314\n",
      "epoch: 998\n",
      "training loss: 535.5582026170958\n",
      "validation loss: 743.4513290306644\n",
      "epoch: 999\n",
      "training loss: 535.3876644539176\n",
      "validation loss: 743.4017938104156\n",
      "epoch: 1000\n",
      "training loss: 535.2173432189662\n",
      "validation loss: 743.3524493491661\n",
      "epoch: 1001\n",
      "training loss: 535.0472381502029\n",
      "validation loss: 743.3032949020801\n",
      "epoch: 1002\n",
      "training loss: 534.8773484842333\n",
      "validation loss: 743.2543297251141\n",
      "epoch: 1003\n",
      "training loss: 534.7076734585746\n",
      "validation loss: 743.2055530784552\n",
      "epoch: 1004\n",
      "training loss: 534.5382123143077\n",
      "validation loss: 743.156964230283\n",
      "epoch: 1005\n",
      "training loss: 534.3689642991499\n",
      "validation loss: 743.1085624608625\n",
      "epoch: 1006\n",
      "training loss: 534.1999286709892\n",
      "validation loss: 743.0603470669497\n",
      "epoch: 1007\n",
      "training loss: 534.0311047018971\n",
      "validation loss: 743.0123173664747\n",
      "epoch: 1008\n",
      "training loss: 533.8624916826324\n",
      "validation loss: 742.9644727034452\n",
      "epoch: 1009\n",
      "training loss: 533.6940889276071\n",
      "validation loss: 742.9168124529817\n",
      "epoch: 1010\n",
      "training loss: 533.525895780271\n",
      "validation loss: 742.8693360263574\n",
      "epoch: 1011\n",
      "training loss: 533.3579116188123\n",
      "validation loss: 742.8220428758739\n",
      "epoch: 1012\n",
      "training loss: 533.1901358620353\n",
      "validation loss: 742.7749324993697\n",
      "epoch: 1013\n",
      "training loss: 533.0225679752123\n",
      "validation loss: 742.7280044440967\n",
      "epoch: 1014\n",
      "training loss: 532.855207475659\n",
      "validation loss: 742.6812583096819\n",
      "epoch: 1015\n",
      "training loss: 532.688053937723\n",
      "validation loss: 742.6346937498528\n",
      "epoch: 1016\n",
      "training loss: 532.521106996838\n",
      "validation loss: 742.588310472598\n",
      "epoch: 1017\n",
      "training loss: 532.3543663522756\n",
      "validation loss: 742.5421082384561\n",
      "epoch: 1018\n",
      "training loss: 532.1878317682286\n",
      "validation loss: 742.4960868566707\n",
      "epoch: 1019\n",
      "training loss: 532.0215030729089\n",
      "validation loss: 742.4502461790371\n",
      "epoch: 1020\n",
      "training loss: 531.8553801554169\n",
      "validation loss: 742.4045860913843\n",
      "epoch: 1021\n",
      "training loss: 531.6894629602707\n",
      "validation loss: 742.3591065027947\n",
      "epoch: 1022\n",
      "training loss: 531.5237514796266\n",
      "validation loss: 742.313807332827\n",
      "epoch: 1023\n",
      "training loss: 531.3582457434042\n",
      "validation loss: 742.2686884972007\n",
      "epoch: 1024\n",
      "training loss: 531.1929458076944\n",
      "validation loss: 742.2237498925507\n",
      "epoch: 1025\n",
      "training loss: 531.0278517419879\n",
      "validation loss: 742.1789913809898\n",
      "epoch: 1026\n",
      "training loss: 530.8629636158623\n",
      "validation loss: 742.1344127752875\n",
      "epoch: 1027\n",
      "training loss: 530.6982814858253\n",
      "validation loss: 742.0900138254677\n",
      "epoch: 1028\n",
      "training loss: 530.5338053829901\n",
      "validation loss: 742.0457942075523\n",
      "epoch: 1029\n",
      "training loss: 530.3695353021859\n",
      "validation loss: 742.0017535150401\n",
      "epoch: 1030\n",
      "training loss: 530.2054711929702\n",
      "validation loss: 741.9578912535093\n",
      "epoch: 1031\n",
      "training loss: 530.0416129528513\n",
      "validation loss: 741.9142068385127\n",
      "epoch: 1032\n",
      "training loss: 529.8779604228432\n",
      "validation loss: 741.8706995967099\n",
      "epoch: 1033\n",
      "training loss: 529.7145133853201\n",
      "validation loss: 741.8273687699774\n",
      "epoch: 1034\n",
      "training loss: 529.5512715639805\n",
      "validation loss: 741.7842135220747\n",
      "epoch: 1035\n",
      "training loss: 529.3882346256418\n",
      "validation loss: 741.7412329473342\n",
      "epoch: 1036\n",
      "training loss: 529.2254021835071\n",
      "validation loss: 741.6984260807909\n",
      "epoch: 1037\n",
      "training loss: 529.0627738015357\n",
      "validation loss: 741.6557919091598\n",
      "epoch: 1038\n",
      "training loss: 528.9003489995451\n",
      "validation loss: 741.6133293821156\n",
      "epoch: 1039\n",
      "training loss: 528.7381272587119\n",
      "validation loss: 741.5710374233975\n",
      "epoch: 1040\n",
      "training loss: 528.5761080271881\n",
      "validation loss: 741.5289149413496\n",
      "epoch: 1041\n",
      "training loss: 528.4142907256041\n",
      "validation loss: 741.486960838611\n",
      "epoch: 1042\n",
      "training loss: 528.2526747522851\n",
      "validation loss: 741.4451740207527\n",
      "epoch: 1043\n",
      "training loss: 528.0912594880647\n",
      "validation loss: 741.4035534037563\n",
      "epoch: 1044\n",
      "training loss: 527.9300443006255\n",
      "validation loss: 741.3620979202916\n",
      "epoch: 1045\n",
      "training loss: 527.7690285483303\n",
      "validation loss: 741.320806524813\n",
      "epoch: 1046\n",
      "training loss: 527.6082115835436\n",
      "validation loss: 741.2796781975374\n",
      "epoch: 1047\n",
      "training loss: 527.4475927554563\n",
      "validation loss: 741.2387119473918\n",
      "epoch: 1048\n",
      "training loss: 527.287171412453\n",
      "validation loss: 741.1979068140403\n",
      "epoch: 1049\n",
      "training loss: 527.1269469040559\n",
      "validation loss: 741.1572618691074\n",
      "epoch: 1050\n",
      "training loss: 526.9669185824955\n",
      "validation loss: 741.1167762167169\n",
      "epoch: 1051\n",
      "training loss: 526.8070858039539\n",
      "validation loss: 741.0764489934537\n",
      "epoch: 1052\n",
      "training loss: 526.647447929525\n",
      "validation loss: 741.0362793678665\n",
      "epoch: 1053\n",
      "training loss: 526.4880043259366\n",
      "validation loss: 740.9962665395933\n",
      "epoch: 1054\n",
      "training loss: 526.3287543660709\n",
      "validation loss: 740.9564097382056\n",
      "epoch: 1055\n",
      "training loss: 526.169697429319\n",
      "validation loss: 740.9167082218419\n",
      "epoch: 1056\n",
      "training loss: 526.0108329018026\n",
      "validation loss: 740.877161275693\n",
      "epoch: 1057\n",
      "training loss: 525.8521601764851\n",
      "validation loss: 740.8377682103944\n",
      "epoch: 1058\n",
      "training loss: 525.6936786531965\n",
      "validation loss: 740.7985283603672\n",
      "epoch: 1059\n",
      "training loss: 525.5353877385926\n",
      "validation loss: 740.7594410821483\n",
      "epoch: 1060\n",
      "training loss: 525.3772868460618\n",
      "validation loss: 740.720505752731\n",
      "epoch: 1061\n",
      "training loss: 525.2193753955937\n",
      "validation loss: 740.6817217679468\n",
      "epoch: 1062\n",
      "training loss: 525.0616528136203\n",
      "validation loss: 740.6430885408995\n",
      "epoch: 1063\n",
      "training loss: 524.9041185328377\n",
      "validation loss: 740.6046055004667\n",
      "epoch: 1064\n",
      "training loss: 524.7467719920171\n",
      "validation loss: 740.566272089877\n",
      "epoch: 1065\n",
      "training loss: 524.5896126358066\n",
      "validation loss: 740.5280877653688\n",
      "epoch: 1066\n",
      "training loss: 524.4326399145322\n",
      "validation loss: 740.4900519949355\n",
      "epoch: 1067\n",
      "training loss: 524.275853284\n",
      "validation loss: 740.452164257156\n",
      "epoch: 1068\n",
      "training loss: 524.1192522053001\n",
      "validation loss: 740.4144240401122\n",
      "epoch: 1069\n",
      "training loss: 523.9628361446167\n",
      "validation loss: 740.3768308403929\n",
      "epoch: 1070\n",
      "training loss: 523.8066045730437\n",
      "validation loss: 740.339384162178\n",
      "epoch: 1071\n",
      "training loss: 523.6505569664087\n",
      "validation loss: 740.3020835164057\n",
      "epoch: 1072\n",
      "training loss: 523.4946928051032\n",
      "validation loss: 740.264928420014\n",
      "epoch: 1073\n",
      "training loss: 523.339011573922\n",
      "validation loss: 740.2279183952554\n",
      "epoch: 1074\n",
      "training loss: 523.1835127619106\n",
      "validation loss: 740.1910529690809\n",
      "epoch: 1075\n",
      "training loss: 523.0281958622197\n",
      "validation loss: 740.1543316725874\n",
      "epoch: 1076\n",
      "training loss: 522.873060371969\n",
      "validation loss: 740.1177540405259\n",
      "epoch: 1077\n",
      "training loss: 522.7181057921182\n",
      "validation loss: 740.0813196108667\n",
      "epoch: 1078\n",
      "training loss: 522.563331627344\n",
      "validation loss: 740.0450279244146\n",
      "epoch: 1079\n",
      "training loss: 522.4087373859268\n",
      "validation loss: 740.0088785244745\n",
      "epoch: 1080\n",
      "training loss: 522.2543225796417\n",
      "validation loss: 739.9728709565599\n",
      "epoch: 1081\n",
      "training loss: 522.1000867236564\n",
      "validation loss: 739.9370047681415\n",
      "epoch: 1082\n",
      "training loss: 521.9460293364367\n",
      "validation loss: 739.9012795084361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1083\n",
      "training loss: 521.7921499396549\n",
      "validation loss: 739.8656947282249\n",
      "epoch: 1084\n",
      "training loss: 521.6384480581056\n",
      "validation loss: 739.8302499797062\n"
     ]
    }
   ],
   "source": [
    "lrstypes = [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]\n",
    "means = []\n",
    "\n",
    "for l in lrstypes:\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "                       activationFn=\"tanh\", lr=.0000001, lr_type=l, \n",
    "                       max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "    nn.fit(X_std, y_adjust)\n",
    "    mae = mean_absolute_error(y_adjust, nn.predict(X_std))*100000\n",
    "    means.append(mae)\n",
    "\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error of Housing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $6125120829.72\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y, nn.predict(X_std))\n",
    "print('Mean absolute error: $%0.2f'%(mae*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to results to those in nn_tuning_example.ipynb.  Goal: Get MAE Under $1000 with our NN.  Then, we know our NN is working well and can use it on the dataset for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # create vector of ones...\n",
    "        ones = np.ones(shape=len(X_train))[..., None]\n",
    "        #...and add to feature matrix\n",
    "        X = np.concatenate((ones, X_train), 1)\n",
    "        #calculate coefficients using closed-form solution\n",
    "        self.coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        ones = np.ones(shape=len(X_test))[..., None]\n",
    "        X_test = np.concatenate((ones, X_test), 1)\n",
    "        y_hat = X_test.dot(self.coeffs)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $2312733180.23\n"
     ]
    }
   ],
   "source": [
    "lr = LR()\n",
    "lr.fit(X_std, y)\n",
    "mae = mean_absolute_error(y, lr.predict(X_std))\n",
    "print('Mean absolute error: $%0.2f'%(mae*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2312733180.23\n",
    "6215120.875882364\n",
    "6125120829.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6492759.066518886, \n",
    "6097805.846057779, \n",
    "6198530.949583186, \n",
    "6158860.9105913825, \n",
    "7111780.459290973, \n",
    "6958060.4762852825, \n",
    "6050248.423598237, \n",
    "6215120.875882364, \n",
    "6676444.162470134, \n",
    "6168218.017111915, \n",
    "6106309.475293339, \n",
    "6073935.760516125\n",
    "\n",
    "6261672.852810156,\n",
    "6339584.203572352,\n",
    "6126601.5136576835,\n",
    "6119698.301655335,\n",
    "6239534.332500497"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
