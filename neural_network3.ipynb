{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 401 Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "russia_df = pd.read_csv(\"russian_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3047.1000000000004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(russia_df)*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(russia_df.index)\n",
    "random.shuffle(ind)\n",
    "\n",
    "# we will use a test set of 10% of the data\n",
    "test_ind = ind[:3050]\n",
    "train_ind = ind[3050:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[train_ind])\n",
    "X_test_std = scaler.transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1).iloc[test_ind])\n",
    "y_train = russia_df[\"price_doc\"].iloc[train_ind]\n",
    "y_test = russia_df[\"price_doc\"].iloc[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "X_std = scaler2.fit_transform(russia_df.drop([\"timestamp\", \"price_doc\"], axis=1))\n",
    "y = russia_df[\"price_doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(y).plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y)\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then decided to use the log of y as the results are more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(y, y_pred):\n",
    "    return np.mean(np.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(y, y_pred):\n",
    "    return np.mean(np.absolute(y-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # create vector of ones...\n",
    "        ones = np.ones(shape=len(X_train))[..., None]\n",
    "        #...and add to feature matrix\n",
    "        X = np.concatenate((ones, X_train), 1)\n",
    "        #calculate coefficients using closed-form solution\n",
    "        self.coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        ones = np.ones(shape=len(X_test))[..., None]\n",
    "        X_test = np.concatenate((ones, X_test), 1)\n",
    "        y_hat = X_test.dot(self.coeffs)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $8968729.22\n"
     ]
    }
   ],
   "source": [
    "lr = LR()\n",
    "lr.fit(X_train_std, y_train_log)\n",
    "mae = get_mae(y_test, np.exp(lr.predict(X_test_std)))\n",
    "print('Mean absolute error: $%0.2f'%(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.4042960337829707e+17\n"
     ]
    }
   ],
   "source": [
    "mse = get_mse(y_test, np.exp(lr.predict(X_test_std)))\n",
    "print('MSE:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our baseline from the linear regression, it is time to develop a neural network implementations to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers=None, nodes=None, nnodes=None, \n",
    "                 activations=[], activationFn=\"relu\", batchSize=50, \n",
    "                 lr=.001, lr_type=\"constant\", power_t=.5,\n",
    "                 annealing_rate=.999, max_epoch=200, momentum=.9, \n",
    "                 tol=0.0001, alpha=.0001, shuffle=False, \n",
    "                 early_stopping=False, num_epochs_stop=50, verbose=True):\n",
    "        \n",
    "        if layers != None:\n",
    "            self.layers = layers # total number of hidden layers\n",
    "        else:\n",
    "            self.layers = len(nodes)\n",
    "\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        self.nodes = nodes\n",
    "        if nodes != None:\n",
    "            self.nodes.insert(0, batchSize)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        self.nnodes = nnodes\n",
    "        if nnodes != None:\n",
    "            self.nodes = []\n",
    "            self.nodes.append(batchSize)\n",
    "            for i in range(layers):\n",
    "                self.nodes.append(nnodes)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        self.activations = activations\n",
    "        self.activationFn = activationFn\n",
    "        if activationFn != \"\":\n",
    "            self.activations = [activationFn] * self.layers\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.power_t = power_t\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.mu = momentum\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if early_stopping == False:\n",
    "            self.num_epochs_stop = max_epoch\n",
    "        else:\n",
    "            self.num_epochs_stop = num_epochs_stop\n",
    "    \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        self.epochs = 0\n",
    "                \n",
    "    def validateHyperParams(self):\n",
    "        \n",
    "        if self.layers != (len(self.nodes) - 2):\n",
    "            raise ValueError(\"layers must be equal to the number of hidden layers, got %s.\" % self.layers)\n",
    "        if self.nnodes != None and self.nnodes <= 0:\n",
    "            raise ValueError(\"nnodes must be > 0, got %s.\" % self.nnodes)\n",
    "        if self.lr <= 0 or self.lr > 1:\n",
    "            raise ValueError(\"lr must be in (0, 1], got %s.\" % self.lr)\n",
    "            \n",
    "        if self.lr_type not in [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]:\n",
    "            raise ValueError(\"lr_type is not valid\" % self.lr_type\n",
    "                            + \"\\nAvailable lr types: constant, invscaling, adaptive\")\n",
    "            \n",
    "        if self.max_epoch <= 0:\n",
    "            raise ValueError(\"max_iter must be > 0, got %s.\" % self.max_epoch)\n",
    "               \n",
    "        activation_functions = list(ACTIVATIONS.keys())\n",
    "        if self.activationFn != \"\":\n",
    "            if self.activationFn not in activation_functions:\n",
    "                raise ValueError(\"%s is not an activation function\" % self.activationFn\n",
    "                                + \"\\nAvailable activation functions: relu, leaky_relu, sigmoid, tanh\")\n",
    "    \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            \n",
    "            # Xavier (Glorot) Initialization\n",
    "            if self.activationFn == \"tanh\":\n",
    "                target_variance = 2 / (input_size + output_size)\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # He Initialization\n",
    "            elif self.activationFn == \"relu\":\n",
    "                target_variance = 2 / input_size\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # Random Uniform\n",
    "            else:\n",
    "                w_i = np.random.uniform(-1/np.sqrt(input_size), 1/np.sqrt(input_size))\n",
    "                #w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "    \n",
    "    # returns the weight term for L2 regularization\n",
    "    def get_weight_term(self):\n",
    "        weight_term = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_term = np.sum(self.weights[i] ** 2)\n",
    "        return weight_term\n",
    "        \n",
    "    def forward_pass(self, X_batch, y_batch):\n",
    "        \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(self.layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = 0\n",
    "        # only need predictions once we have fit the data\n",
    "        if isinstance(y_batch, np.ndarray): \n",
    "            l2_loss = squared_loss(y_pred, y_batch) # l2\n",
    "            weight_term = self.get_weight_term()\n",
    "            l2_loss += self.alpha * weight_term # l2 regularization\n",
    "            self.layer_values[self.layers + 1] = l2_loss\n",
    "        \n",
    "        return l2_loss, y_pred\n",
    "    \n",
    "    \n",
    "    def backward_pass(self, y_pred, y_batch):\n",
    "        \n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(y_pred, y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (self.layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        w_t = np.delete(w_t, w_t.shape[1] - 1, 1) # take out the bias\n",
    "        J = np.dot(J, w_t)\n",
    "        zeros = [0] * len(J)\n",
    "        zeros = np.reshape(zeros, (len(J), 1))\n",
    "        J = np.append(J, zeros, axis=1)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(self.layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "            \n",
    "            # jacobian w.r.t. inputs\n",
    "            w_t = self.weights[i - 1].T\n",
    "            w_t = np.delete(w_t, w_t.shape[1] - 1, 1)\n",
    "            J = np.dot(J, w_t)\n",
    "            zeros = [0] * len(J)\n",
    "            zeros = np.reshape(zeros, (len(J), 1))\n",
    "            J = np.append(J, zeros, axis=1)\n",
    "            \n",
    "            \n",
    "        # initialize velocity to 0\n",
    "        if self.epochs == 0 and self.iters == 0:\n",
    "            self.velocity = []\n",
    "            for i in range(len(J_weights)):\n",
    "                n_rows = J_weights[i].shape[0]\n",
    "                n_cols = J_weights[i].shape[1]\n",
    "                vel_i = np.zeros((n_rows, n_cols))\n",
    "                self.velocity.append(vel_i)\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.velocity[i] = self.mu * self.velocity[i] - self.lr * J_weights[i]\n",
    "            self.weights[i] += self.velocity[i]\n",
    "      \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.validateHyperParams()\n",
    "        # convert to numpy arrays\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.to_numpy()\n",
    "            \n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.to_numpy()\n",
    "            \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_train)\n",
    "        ones = np.reshape(ones, (len(X_train), 1))\n",
    "        X_train = np.append(X_train, ones, axis=1)\n",
    "        \n",
    "        # save 10% for validation\n",
    "        val_rows = round(len(X_train) * .1)\n",
    "        X_val = X_train[:val_rows, :]\n",
    "        y_val = y_train[:val_rows]\n",
    "        \n",
    "        X_train = X_train[val_rows:, :]\n",
    "        y_train = y_train[val_rows:]\n",
    "        \n",
    "        # initalize weights on first iteration\n",
    "        M = X_train.shape[1] # M = number of features\n",
    "        self.weights = self.initialize_weights(M)\n",
    "        \n",
    "        best_v_loss = np.inf\n",
    "        n_epoch_no_change = 0 \n",
    "        while (self.epochs < self.max_epoch and n_epoch_no_change <= self.num_epochs_stop):\n",
    "            # ONE EPOCH \n",
    "            last_idx = 0\n",
    "            if self.shuffle == True: # shuffle data after every epoch, if specified \n",
    "                np.random.shuffle(X_train)\n",
    "            while (last_idx < len(X_train)):\n",
    "                first_idx = self.iters * self.batchSize\n",
    "                remaining_rows = len(X_train) - first_idx\n",
    "                last_idx = first_idx + min(self.batchSize, remaining_rows)\n",
    "                X_batch = X_train[first_idx: last_idx, :]\n",
    "                y_batch = y_train[first_idx: last_idx]\n",
    "\n",
    "                loss, y_pred = self.forward_pass(X_batch, y_batch)\n",
    "                self.backward_pass(y_pred, y_batch)\n",
    "                self.iters += 1\n",
    "            \n",
    "            # trainig and validation loss after one epoch\n",
    "            t_loss, y_pred = self.forward_pass(X_train, y_train)\n",
    "            v_loss, y_pred = self.forward_pass(X_val, y_val)\n",
    "            if self.verbose:\n",
    "                print(\"epoch:\", self.epochs)\n",
    "                print(\"training loss:\", t_loss)\n",
    "                print(\"validation loss:\", v_loss)\n",
    "            \n",
    "            self.iters = 0 # start over, next epoch\n",
    "            self.epochs += 1\n",
    "            \n",
    "            # decrease the learning rate by one of three methods, if specified\n",
    "            if self.lr_type == \"invscaling\":\n",
    "                self.lr = self.lr/pow(self.epochs, self.power_t)\n",
    "            elif self.lr_type == \"annealing\":\n",
    "                self.lr = self.lr * self.annealing_rate\n",
    "            elif self.lr_type == \"adaptive\":\n",
    "                if n_epoch_no_change >= 2: \n",
    "                    self.lr = self.lr/5\n",
    "                \n",
    "            # stops when validation loss doesn't improve for num_epochs_stop\n",
    "            if best_v_loss - v_loss < self.tol: \n",
    "                n_epoch_no_change += 1\n",
    "            else:\n",
    "                n_epoch_no_change = 0\n",
    "            # update best_v_loss\n",
    "            if v_loss < best_v_loss:\n",
    "                best_v_loss = v_loss\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # convert to numpy array\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.to_numpy()\n",
    "        \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_test)\n",
    "        ones = np.reshape(ones, (len(X_test), 1))\n",
    "        X_test = np.append(X_test, ones, axis=1)\n",
    "        \n",
    "        loss, y_pred = self.forward_pass(X_test, None)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 100\n",
      "Batch Size: 75\n",
      "Nodes: 100\n",
      "Batch Size: 150\n"
     ]
    }
   ],
   "source": [
    "nodes = [100] # use to specify a number of hidden nodes per layer\n",
    "batches = [75, 150, 1000, 3000]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for nod in nodes:\n",
    "    for batch in batches:\n",
    "        print(\"Nodes:\", nod)\n",
    "        print(\"Batch Size:\", batch)\n",
    "        nn = NeuralNetwork(layers=3, nnodes=nod, batchSize=batch, \n",
    "                   activationFn=\"tanh\", lr=0.00001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "        nn.fit(X_std, y_log)\n",
    "        \n",
    "        mae = get_mae(y_log, nn.predict(X_std))\n",
    "        means.append(mae)\n",
    "        mse = get_mse(y_log, nn.predict(X_std))\n",
    "        mses.append(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that a neural network with ____ nodes in each layer and with a batch size of ____ is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3719213974654842,\n",
       "  0.40464122313609907,\n",
       "  15.507711550791655,\n",
       "  15.496401756124515,\n",
       "  0.37059927448713376,\n",
       "  0.40061924091584505],\n",
       " [0.30738081309640997,\n",
       "  0.3448503254331879,\n",
       "  240.89535707146248,\n",
       "  240.68193902850905,\n",
       "  0.3019580281590337,\n",
       "  0.3398968652326514])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1\n",
      "0.32878571475904267\n",
      "0.2653659683032532\n",
      "Number of layers: 2\n",
      "0.33078621934338914\n",
      "0.2630391625102294\n",
      "Number of layers: 3\n",
      "0.32166828746298876\n",
      "0.25342771172128625\n",
      "Number of layers: 4\n",
      "0.3290997733116515\n",
      "0.259221440482325\n",
      "Number of layers: 5\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "mses = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(\"Number of layers:\", i)\n",
    "    nn = NeuralNetwork(layers=i, nnodes=75, batchSize=75, \n",
    "               activationFn=\"tanh\", lr=0.00001, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that a neural network with 3 hidden layers is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: tanh\n"
     ]
    }
   ],
   "source": [
    "actives = [\"tanh\", \"sigmoid\"]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for active in actives:\n",
    "    print(\"Activation Function:\", active)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "               activationFn=active, lr=0.000001, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation Function: relu\n",
    "1.839555048410389\n",
    "5.905572357965957\n",
    "Activation Function: tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a nerual network with ____ as the activation function is the most optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-06\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.000001, 0.0000001]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for l in lrs:\n",
    "    print(\"Learning Rate:\", l)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=75, batchSize=75, \n",
    "               activationFn=\"tanh\", lr=l, lr_type=\"constant\", \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    print(\"MAE:\", mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning Rate: 0.0001\n",
    "MAE: 0.28629193636873423\n",
    "MSE: 0.22753831460589047\n",
    "Learning Rate: 1e-05\n",
    "MAE: 0.3239843514618037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that a neural netwrok with a learning rate of 0.0001 is the most optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrstypes = [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]\n",
    "\n",
    "means = []\n",
    "mses = []\n",
    "\n",
    "for ltype in lrstypes: \n",
    "    print(\"Learning Type:\", ltype)\n",
    "    nn = NeuralNetwork(layers=3, nnodes=, batchSize=, \n",
    "               activationFn=\"tanh\", lr= , lr_type=ltype, \n",
    "               max_epoch=2000, momentum=0.9, early_stopping=True, verbose=False)\n",
    "    nn.fit(X_std, y_log)\n",
    "\n",
    "    mae = get_mae(y_log, nn.predict(X_std))\n",
    "    means.append(mae)\n",
    "    mse = get_mse(y_log, nn.predict(X_std))\n",
    "    mses.append(mse)\n",
    "\n",
    "print(means)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 0.31640061621858545\n",
      "validation loss: 0.31174246673268996\n",
      "epoch: 1\n",
      "training loss: 0.23330194974307583\n",
      "validation loss: 0.23142053046161115\n",
      "epoch: 2\n",
      "training loss: 0.20453296240880303\n",
      "validation loss: 0.20482535379596287\n",
      "epoch: 3\n",
      "training loss: 0.18811828663685037\n",
      "validation loss: 0.19018127301090265\n",
      "epoch: 4\n",
      "training loss: 0.1770208290746311\n",
      "validation loss: 0.18051436473513122\n",
      "epoch: 5\n",
      "training loss: 0.16875642624335274\n",
      "validation loss: 0.17353289096921565\n",
      "epoch: 6\n",
      "training loss: 0.16272728572000741\n",
      "validation loss: 0.16815113657275743\n",
      "epoch: 7\n",
      "training loss: 0.15793767179144694\n",
      "validation loss: 0.1638431735767969\n",
      "epoch: 8\n",
      "training loss: 0.15402401716859293\n",
      "validation loss: 0.16029298629960953\n",
      "epoch: 9\n",
      "training loss: 0.15075569835725044\n",
      "validation loss: 0.15730316600952318\n",
      "epoch: 10\n",
      "training loss: 0.1479780965556555\n",
      "validation loss: 0.15474223524941846\n",
      "epoch: 11\n",
      "training loss: 0.14558296926875935\n",
      "validation loss: 0.15251845133346612\n",
      "epoch: 12\n",
      "training loss: 0.14349134637187058\n",
      "validation loss: 0.15056545429779755\n",
      "epoch: 13\n",
      "training loss: 0.141642025197764\n",
      "validation loss: 0.1488338497989312\n",
      "epoch: 14\n",
      "training loss: 0.1399798869224356\n",
      "validation loss: 0.14728601437313543\n",
      "epoch: 15\n",
      "training loss: 0.13845256996701352\n",
      "validation loss: 0.14589271652198613\n",
      "epoch: 16\n",
      "training loss: 0.137094405333812\n",
      "validation loss: 0.14463029307341108\n",
      "epoch: 17\n",
      "training loss: 0.1358960733782321\n",
      "validation loss: 0.14347974431030663\n",
      "epoch: 18\n",
      "training loss: 0.13480783285135986\n",
      "validation loss: 0.14242661459027256\n",
      "epoch: 19\n",
      "training loss: 0.1338119149037991\n",
      "validation loss: 0.14145834352534767\n",
      "epoch: 20\n",
      "training loss: 0.1328957686704646\n",
      "validation loss: 0.14056451044196352\n",
      "epoch: 21\n",
      "training loss: 0.13204934277344973\n",
      "validation loss: 0.13973643325658006\n",
      "epoch: 22\n",
      "training loss: 0.13126443713070537\n",
      "validation loss: 0.13896684125609723\n",
      "epoch: 23\n",
      "training loss: 0.1305344138749868\n",
      "validation loss: 0.1382496157554452\n",
      "epoch: 24\n",
      "training loss: 0.12985386102865268\n",
      "validation loss: 0.1375795443630118\n",
      "epoch: 25\n",
      "training loss: 0.12921809715410354\n",
      "validation loss: 0.13695207598376707\n",
      "epoch: 26\n",
      "training loss: 0.12862281171473822\n",
      "validation loss: 0.13636314340590341\n",
      "epoch: 27\n",
      "training loss: 0.12806403936951066\n",
      "validation loss: 0.1358091029072262\n",
      "epoch: 28\n",
      "training loss: 0.12753824953440202\n",
      "validation loss: 0.13528672904184375\n",
      "epoch: 29\n",
      "training loss: 0.1270423600334411\n",
      "validation loss: 0.13479319093456485\n",
      "epoch: 30\n",
      "training loss: 0.12657368617002002\n",
      "validation loss: 0.13432600482491566\n",
      "epoch: 31\n",
      "training loss: 0.1261298765601988\n",
      "validation loss: 0.13388298242773605\n",
      "epoch: 32\n",
      "training loss: 0.12570885734467632\n",
      "validation loss: 0.1334621854741863\n",
      "epoch: 33\n",
      "training loss: 0.1253087879741625\n",
      "validation loss: 0.1330618882873141\n",
      "epoch: 34\n",
      "training loss: 0.12492802657418488\n",
      "validation loss: 0.1326805473603163\n",
      "epoch: 35\n",
      "training loss: 0.12456510238603558\n",
      "validation loss: 0.13231677648634207\n",
      "epoch: 36\n",
      "training loss: 0.12421869333294373\n",
      "validation loss: 0.13196932621122148\n",
      "epoch: 37\n",
      "training loss: 0.1238876073697683\n",
      "validation loss: 0.13163706667298353\n",
      "epoch: 38\n",
      "training loss: 0.12357076672668273\n",
      "validation loss: 0.13131897312950966\n",
      "epoch: 39\n",
      "training loss: 0.12326719444951094\n",
      "validation loss: 0.13101411365005858\n",
      "epoch: 40\n",
      "training loss: 0.12297600281694193\n",
      "validation loss: 0.13072163857168376\n",
      "epoch: 41\n",
      "training loss: 0.12269638332219493\n",
      "validation loss: 0.13044077141187144\n",
      "epoch: 42\n",
      "training loss: 0.12242759797455162\n",
      "validation loss: 0.13017080099465217\n",
      "epoch: 43\n",
      "training loss: 0.12216897172246878\n",
      "validation loss: 0.1299110745964168\n",
      "epoch: 44\n",
      "training loss: 0.12191988583415879\n",
      "validation loss: 0.12966099195472236\n",
      "epoch: 45\n",
      "training loss: 0.12167977209829761\n",
      "validation loss: 0.12942000001192794\n",
      "epoch: 46\n",
      "training loss: 0.12144810772924641\n",
      "validation loss: 0.12918758828785484\n",
      "epoch: 47\n",
      "training loss: 0.12122441087912852\n",
      "validation loss: 0.12896328479342833\n",
      "epoch: 48\n",
      "training loss: 0.12100823667405959\n",
      "validation loss: 0.12874665241153074\n",
      "epoch: 49\n",
      "training loss: 0.12079917370433575\n",
      "validation loss: 0.1285372856828938\n",
      "epoch: 50\n",
      "training loss: 0.12059684090885629\n",
      "validation loss: 0.12833480794435545\n",
      "epoch: 51\n",
      "training loss: 0.12040088480284235\n",
      "validation loss: 0.12813886877464967\n",
      "epoch: 52\n",
      "training loss: 0.12021097700528936\n",
      "validation loss: 0.1279491417094089\n",
      "epoch: 53\n",
      "training loss: 0.12002681202880067\n",
      "validation loss: 0.12776532219250245\n",
      "epoch: 54\n",
      "training loss: 0.11984810529968319\n",
      "validation loss: 0.12758712573540207\n",
      "epoch: 55\n",
      "training loss: 0.11967459138061244\n",
      "validation loss: 0.1274142862601215\n",
      "epoch: 56\n",
      "training loss: 0.11950602237192266\n",
      "validation loss: 0.1272465546045398\n",
      "epoch: 57\n",
      "training loss: 0.11934216647076523\n",
      "validation loss: 0.12708369717169538\n",
      "epoch: 58\n",
      "training loss: 0.1191828066700905\n",
      "validation loss: 0.12692549470700565\n",
      "epoch: 59\n",
      "training loss: 0.11902773958172948\n",
      "validation loss: 0.12677174118939571\n",
      "epoch: 60\n",
      "training loss: 0.11887677436983436\n",
      "validation loss: 0.12662224282406187\n",
      "epoch: 61\n",
      "training loss: 0.11872973178264468\n",
      "validation loss: 0.1264768171260975\n",
      "epoch: 62\n",
      "training loss: 0.11858644327201237\n",
      "validation loss: 0.12633529208550764\n",
      "epoch: 63\n",
      "training loss: 0.1184467501913874\n",
      "validation loss: 0.1261975054052654\n",
      "epoch: 64\n",
      "training loss: 0.11831050306406257\n",
      "validation loss: 0.12606330380504405\n",
      "epoch: 65\n",
      "training loss: 0.11817756091442962\n",
      "validation loss: 0.12593254238411847\n",
      "epoch: 66\n",
      "training loss: 0.11804779065582789\n",
      "validation loss: 0.12580508403768156\n",
      "epoch: 67\n",
      "training loss: 0.11792106652929554\n",
      "validation loss: 0.12568079892149098\n",
      "epoch: 68\n",
      "training loss: 0.11779726958817226\n",
      "validation loss: 0.12555956396035436\n",
      "epoch: 69\n",
      "training loss: 0.11767628722407135\n",
      "validation loss: 0.12544126239649803\n",
      "epoch: 70\n",
      "training loss: 0.1175580127302516\n",
      "validation loss: 0.1253257833743518\n",
      "epoch: 71\n",
      "training loss: 0.11744234489888657\n",
      "validation loss: 0.12521302155873842\n",
      "epoch: 72\n",
      "training loss: 0.11732918764917098\n",
      "validation loss: 0.1251028767838822\n",
      "epoch: 73\n",
      "training loss: 0.11721844968363154\n",
      "validation loss: 0.12499525373107354\n",
      "epoch: 74\n",
      "training loss: 0.11711004417044553\n",
      "validation loss: 0.12489006163323632\n",
      "epoch: 75\n",
      "training loss: 0.11700388845003314\n",
      "validation loss: 0.12478721400507323\n",
      "epoch: 76\n",
      "training loss: 0.11689990376470542\n",
      "validation loss: 0.12468662839789994\n",
      "epoch: 77\n",
      "training loss: 0.1167980150107428\n",
      "validation loss: 0.12458822617874185\n",
      "epoch: 78\n",
      "training loss: 0.11669815051297606\n",
      "validation loss: 0.12449193233373947\n",
      "epoch: 79\n",
      "training loss: 0.11660024182275691\n",
      "validation loss: 0.12439767529637756\n",
      "epoch: 80\n",
      "training loss: 0.11650422354113224\n",
      "validation loss: 0.12430538680147106\n",
      "epoch: 81\n",
      "training loss: 0.11641003317001654\n",
      "validation loss: 0.12421500176611351\n",
      "epoch: 82\n",
      "training loss: 0.11631761099504781\n",
      "validation loss: 0.12412645819878333\n",
      "epoch: 83\n",
      "training loss: 0.11622690000432742\n",
      "validation loss: 0.12403969713727334\n",
      "epoch: 84\n",
      "training loss: 0.11613784584690197\n",
      "validation loss: 0.12395466261475034\n",
      "epoch: 85\n",
      "training loss: 0.11605039683293843\n",
      "validation loss: 0.12387130165068555\n",
      "epoch: 86\n",
      "training loss: 0.11596450397321284\n",
      "validation loss: 0.12378956425927103\n",
      "epoch: 87\n",
      "training loss: 0.11588012104802328\n",
      "validation loss: 0.1237094034621202\n",
      "epoch: 88\n",
      "training loss: 0.11579720468488278\n",
      "validation loss: 0.12363077528497421\n",
      "epoch: 89\n",
      "training loss: 0.11571571441180951\n",
      "validation loss: 0.12355363871118918\n",
      "epoch: 90\n",
      "training loss: 0.11563561264252838\n",
      "validation loss: 0.12347795556062595\n",
      "epoch: 91\n",
      "training loss: 0.11555686454754738\n",
      "validation loss: 0.12340369026486127\n",
      "epoch: 92\n",
      "training loss: 0.11547943777737858\n",
      "validation loss: 0.12333080952184336\n",
      "epoch: 93\n",
      "training loss: 0.11540330203389672\n",
      "validation loss: 0.12325928183613691\n",
      "epoch: 94\n",
      "training loss: 0.1153284285275076\n",
      "validation loss: 0.12318907698076559\n",
      "epoch: 95\n",
      "training loss: 0.11525478939682401\n",
      "validation loss: 0.12312016544399376\n",
      "epoch: 96\n",
      "training loss: 0.1151823571860059\n",
      "validation loss: 0.12305251793760401\n",
      "epoch: 97\n",
      "training loss: 0.1151111044622732\n",
      "validation loss: 0.1229861050347144\n",
      "epoch: 98\n",
      "training loss: 0.11504100361686631\n",
      "validation loss: 0.12292089697651479\n",
      "epoch: 99\n",
      "training loss: 0.11497202684437385\n",
      "validation loss: 0.12285686364927127\n",
      "epoch: 100\n",
      "training loss: 0.11490414625744176\n",
      "validation loss: 0.12279397469965929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 101\n",
      "training loss: 0.11483733407733453\n",
      "validation loss: 0.12273219973790686\n",
      "epoch: 102\n",
      "training loss: 0.11477156284471367\n",
      "validation loss: 0.12267150857635879\n",
      "epoch: 103\n",
      "training loss: 0.11470680561089973\n",
      "validation loss: 0.1226118714612921\n",
      "epoch: 104\n",
      "training loss: 0.11464303608845665\n",
      "validation loss: 0.12255325927109843\n",
      "epoch: 105\n",
      "training loss: 0.11458022875508775\n",
      "validation loss: 0.1224956436685239\n",
      "epoch: 106\n",
      "training loss: 0.11451835891441697\n",
      "validation loss: 0.12243899720548362\n",
      "epoch: 107\n",
      "training loss: 0.11445740272175337\n",
      "validation loss: 0.1223832933854411\n",
      "epoch: 108\n",
      "training loss: 0.11439733718398087\n",
      "validation loss: 0.12232850669122446\n",
      "epoch: 109\n",
      "training loss: 0.11433814014184542\n",
      "validation loss: 0.122274612586646\n",
      "epoch: 110\n",
      "training loss: 0.114279790241279\n",
      "validation loss: 0.12222158749947956\n",
      "epoch: 111\n",
      "training loss: 0.11422226689867632\n",
      "validation loss: 0.1221694087920027\n",
      "epoch: 112\n",
      "training loss: 0.11416555026353759\n",
      "validation loss: 0.12211805472388529\n",
      "epoch: 113\n",
      "training loss: 0.11410962118071849\n",
      "validation loss: 0.12206750441093075\n",
      "epoch: 114\n",
      "training loss: 0.11405446115366916\n",
      "validation loss: 0.12201773778213776\n",
      "epoch: 115\n",
      "training loss: 0.11400005230944735\n",
      "validation loss: 0.12196873553674815\n",
      "epoch: 116\n",
      "training loss: 0.11394637736589623\n",
      "validation loss: 0.12192047910235726\n",
      "epoch: 117\n",
      "training loss: 0.11389341960112231\n",
      "validation loss: 0.12187295059474229\n",
      "epoch: 118\n",
      "training loss: 0.11384116282525733\n",
      "validation loss: 0.12182613277977046\n",
      "epoch: 119\n",
      "training loss: 0.1137895913543981\n",
      "validation loss: 0.12178000903755394\n",
      "epoch: 120\n",
      "training loss: 0.11373868998657424\n",
      "validation loss: 0.1217345633288865\n",
      "epoch: 121\n",
      "training loss: 0.11368844397957506\n",
      "validation loss: 0.12168978016391976\n",
      "epoch: 122\n",
      "training loss: 0.11363883903046323\n",
      "validation loss: 0.12164564457298395\n",
      "epoch: 123\n",
      "training loss: 0.11358986125661115\n",
      "validation loss: 0.12160214207943397\n",
      "epoch: 124\n",
      "training loss: 0.11354149717810617\n",
      "validation loss: 0.1215592586743896\n",
      "epoch: 125\n",
      "training loss: 0.11349373370138433\n",
      "validation loss: 0.12151698079323392\n",
      "epoch: 126\n",
      "training loss: 0.11344655810396742\n",
      "validation loss: 0.12147529529373885\n",
      "epoch: 127\n",
      "training loss: 0.11339995802019005\n",
      "validation loss: 0.12143418943569162\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers=3, nnodes=100, batchSize=75, \n",
    "                       activationFn=\"tanh\", lr=0.0001, lr_type=\"constant\", \n",
    "                       max_epoch=2000, momentum=0.9, early_stopping=True)\n",
    "nn.fit(X_train_std, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $1774245.96\n",
      "Root mean squared error: $9190356278705.72\n"
     ]
    }
   ],
   "source": [
    "pred = nn.predict(X_test_std)\n",
    "mse = get_mse(y_test, np.exp(pred))\n",
    "mae = get_mae(y_test, np.exp(pred))\n",
    "print('Mean absolute error: $%0.2f'%(mae))\n",
    "print('Root mean squared error: $%0.2f'%(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $0.29\n",
      "Root mean squared error: $0.22\n"
     ]
    }
   ],
   "source": [
    "mse = get_mse(y_test_log, pred)\n",
    "mae = get_mae(y_test_log, pred)\n",
    "print('Mean absolute error: $%0.2f'%(mae))\n",
    "print('Root mean squared error: $%0.2f'%(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error: $0.29\n",
    "Root mean squared error: $0.47"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
