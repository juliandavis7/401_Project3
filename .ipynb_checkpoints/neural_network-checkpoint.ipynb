{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"housing/boston_fixed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"MEDV\",axis=1)\n",
    "y = df[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, activationFn=\"ReLU\"):\n",
    "        \n",
    "        self.batchSize = 50\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[0], nodes[1], nodes[2]]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.ACTIVATIONS = {'relu': inplace_relu, \n",
    "                            'tanh': inplace_softmax}\n",
    "        \n",
    "        self.DERIVATIVES = {'relu': inplace_relu_derivative,\n",
    "                            'tanh': inplace_tanh_derivative}\n",
    "        \n",
    "    def squared_loss(y_true, y_pred):\n",
    "        \"\"\"Compute the squared loss for regression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like or label indicator matrix\n",
    "            Ground truth (correct) values.\n",
    "        y_pred : array-like or label indicator matrix\n",
    "            Predicted values, as returned by a regression estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The degree to which the samples are correctly predicted.\n",
    "        \"\"\"\n",
    "        return ((y_true - y_pred) ** 2).mean() / 2\n",
    "    \n",
    "    \n",
    "    def d_L(self, y_pred, y_true):\n",
    "        \"\"\" Compute the squared loss for regression.\n",
    "        \"\"\"\n",
    "        return (y_pred - y_true)/self.batchSize\n",
    "    \n",
    "    def inplace_relu(X):\n",
    "        \"\"\"Compute the rectified linear unit function inplace.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        \"\"\"\n",
    "        np.maximum(X, 0, out=X)\n",
    "        \n",
    "    def inplace_relu_derivative(Z, delta):\n",
    "        \"\"\"Apply the derivative of the relu function.\n",
    "        It exploits the fact that the derivative is a simple function of the output\n",
    "        value from rectified linear units activation function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The data which was output from the rectified linear units activation\n",
    "            function during the forward pass.\n",
    "        delta : {array-like}, shape (n_samples, n_features)\n",
    "             The backpropagated error signal to be modified inplace.\n",
    "        \"\"\"\n",
    "        delta[Z == 0] = 0\n",
    "    \n",
    "    def inplace_tanh(X):\n",
    "        \"\"\"Compute the hyperbolic tan function inplace.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        \"\"\"\n",
    "        np.tanh(X, out=X)\n",
    "        \n",
    "    def inplace_tanh_derivative(Z, delta):\n",
    "        \"\"\"Apply the derivative of the hyperbolic tanh function.\n",
    "        It exploits the fact that the derivative is a simple function of the output\n",
    "        value from hyperbolic tangent.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The data which was output from the hyperbolic tangent activation\n",
    "            function during the forward pass.\n",
    "        delta : {array-like}, shape (n_samples, n_features)\n",
    "             The backpropagated error signal to be modified inplace.\n",
    "        \"\"\"\n",
    "        delta *= (1 - Z ** 2)\n",
    "    \n",
    "    # x = B * M (batch of data points)\n",
    "    def L(self, x, y):\n",
    "        avgLoss = 0\n",
    "        for i in range(self.batchSize):\n",
    "            avgLoss += L2Loss(x[i], y[i])\n",
    "        return avgLoss\n",
    "    \n",
    "    def activationFunction(self, x, a=.1):\n",
    "        val = 0\n",
    "        if self.activationFn == \"ReLU\":\n",
    "            val = max(0, x)\n",
    "        elif self.activationFn == \"leakyReLU\":\n",
    "            if x > 0: val = x\n",
    "            else: val = a * x\n",
    "        return val\n",
    "            \n",
    "    def d_activationFunction(self, x, a=.1):\n",
    "        val = 0\n",
    "        if self.activationFn == \"ReLU\":\n",
    "            if x > 0: val = 1\n",
    "            else: val = 0\n",
    "        elif self.activationFn == \"leakyReLU\":\n",
    "            if x > 0: val = 1\n",
    "            else: val = a\n",
    "        return val\n",
    "    \n",
    "    \n",
    "    # x = B * M (batch of data points)\n",
    "    # apply activation function element wise to x\n",
    "    def applyActivationFn(self, x):\n",
    "        activationFnVec = np.vectorize(self.activationFunction)\n",
    "        return activationFnVec(x)\n",
    "        \n",
    "    # loss over batch\n",
    "    # x = B * 1 = vector of predictions\n",
    "    # y = B * 1 = vector of labals\n",
    "    # output L(x) has size 1\n",
    "    def jacobianLossLayer(self, x, y):\n",
    "        J = {}\n",
    "        B = len(x)\n",
    "        for i in range(B):\n",
    "            J[i] = d_L(x, y, B)\n",
    "        return pd.series(J)\n",
    "    \n",
    "    # x has size B * M\n",
    "    # output σ(x) has size B * M\n",
    "    \n",
    "    def jacobianActivationLayer(self, x, J):\n",
    "        new_x = x.applymap(d_activationFunction).copy()\n",
    "        J = J * new_x\n",
    "        return J\n",
    "    \n",
    "    def jacobianDenseLayerInput(self, x):\n",
    "        pass\n",
    "    \n",
    "    def jacobianDenseLayerWeights(self, x):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, Xtrain, ytrain):\n",
    "        df_train = Xtrain.copy()\n",
    "        response = ytrain.name\n",
    "        df_train[response] = ytrain\n",
    "        \n",
    "        df_train_batch = df_train.sample(self.batchSize) # get batch\n",
    "        X = df_train_batch.drop(response, axis=1)\n",
    "        X[\"ones\"] = 1 # add ones for bias\n",
    "        y = df_train_batch[response]\n",
    "        \n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        \n",
    "        M = X.shape[1]\n",
    "        N = nodes[1]\n",
    "        \n",
    "        # weights = M * N\n",
    "        w1 = np.random.normal(size=(M, N)) # initalize weights\n",
    "        w1 = np.round(w1, 2)\n",
    "        w1[M-1:] = 0 # initialize biases to 0\n",
    "        \n",
    "        h1 = X.dot(w1) # first hidden layer\n",
    "        \n",
    "        h1_activation = self.applyActivationFn(h1) # hidden layer\n",
    "        \n",
    "        w2 = np.random.normal(size=N) # initialize weights\n",
    "        w2 = np.round(w2, 2)\n",
    "        w2[N - 1] = 0 # initialize bias to 0\n",
    "        \n",
    "        z = h1.dot(w2) # output layer\n",
    "        \n",
    "        return z\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 1\n",
      "nodes: [50, 4, 1]\n",
      "activations: [None, 'ReLU', None]\n",
      "activationFn: ReLU\n",
      "dimensitons: (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 817.07153389, -413.37169749, -215.75419819, -412.94675718,\n",
       "        109.30279391])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 1\n",
    "nodes = [50, 4, 1]\n",
    "activations = [None, \"ReLU\", None]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "print(\"layers:\", NN.layers)\n",
    "print(\"nodes:\", NN.nodes)\n",
    "print(\"activations:\", NN.activations)\n",
    "print(\"activationFn:\", NN.activationFn)\n",
    "\n",
    "z = NN.fit(X, y)\n",
    "print(\"dimensitons:\", h1.shape)\n",
    "z[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
