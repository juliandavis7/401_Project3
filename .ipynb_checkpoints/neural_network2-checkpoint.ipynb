{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"housing/boston_fixed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"MEDV\",axis=1)\n",
    "y = df[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital Class for 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, batchSize=50, activationFn=\"relu\", lr=.1):\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[0], nodes[1], nodes[2]]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        \n",
    "    def initialize_weights(self, M):\n",
    "        self.weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            self.weights.append(w_i)\n",
    "        \n",
    "    def forward_pass(self, X_train, y_train):\n",
    "        \n",
    "        # add ones for bias\n",
    "        X_train[\"ones\"] = 1 \n",
    "        \n",
    "        # get batch\n",
    "        batch_slice = np.random.choice(len(X_train), \n",
    "                                       replace = False, \n",
    "                                       size = self.batchSize) \n",
    "        X_batch = X_train.iloc[batch_slice]\n",
    "        y_batch = y_train.iloc[batch_slice]\n",
    "        \n",
    "        # convert to numpy 2D array\n",
    "        X_batch = X_batch.to_numpy()\n",
    "        self.y_batch = y_batch.to_numpy()\n",
    "        \n",
    "        \n",
    "        if self.iters == 0:\n",
    "            # initialize weights\n",
    "            M = X_batch.shape[1] # M = number of features\n",
    "            self.initialize_weights(M)\n",
    "            \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            print(\"X\", X)\n",
    "            print(\"weights\", weights)\n",
    "            h_layer = X.dot(weights)\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            h_layer_activated = activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer_activated\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        self.y_pred = X.dot(weights)\n",
    "        \n",
    "        l2_loss = squared_loss(self.y_pred, y_batch)\n",
    "        \n",
    "        self.layer_values[self.layers + 1] = l2_loss\n",
    "        \n",
    "        self.iters += 1\n",
    "        \n",
    "        return self.activations\n",
    "    \n",
    "    def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.z, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "\n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.activations[1].T\n",
    "        J_w2 = x_t.dot(J)\n",
    "        print(J_w2.shape)\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w2_t = self.weights[1].T\n",
    "        print(w2_t.shape)\n",
    "        J = np.dot(J, w2_t)\n",
    "        \n",
    "        # update jacobian at activation layer\n",
    "        inplace_relu_derivative(activations[1], J)\n",
    "        \n",
    "        # hidden layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = activations[0].T\n",
    "        J_w1 = x_t.dot(J)\n",
    "        \n",
    "        # update weights\n",
    "        self.weights[1] = self.weights[1] - self.lr * J_w2\n",
    "        self.weights[0] = self.weights[0] - self.lr * J_w1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        self.backprop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.75290e-01 0.00000e+00 6.20000e+00 0.00000e+00 5.07000e-01 8.33700e+00\n",
      "  7.33000e+01 3.83840e+00 8.00000e+00 3.07000e+02 1.74000e+01 3.85910e+02\n",
      "  2.47000e+00 1.00000e+00]\n",
      " [1.55757e+01 0.00000e+00 1.81000e+01 0.00000e+00 5.80000e-01 5.92600e+00\n",
      "  7.10000e+01 2.90840e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.68740e+02\n",
      "  1.81300e+01 1.00000e+00]\n",
      " [2.14918e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 5.70900e+00\n",
      "  9.85000e+01 1.62320e+00 5.00000e+00 4.03000e+02 1.47000e+01 2.61950e+02\n",
      "  1.57900e+01 1.00000e+00]\n",
      " [9.51200e-02 0.00000e+00 1.28300e+01 0.00000e+00 4.37000e-01 6.28600e+00\n",
      "  4.50000e+01 4.50260e+00 5.00000e+00 3.98000e+02 1.87000e+01 3.83230e+02\n",
      "  8.94000e+00 1.00000e+00]\n",
      " [1.20830e-01 0.00000e+00 2.89000e+00 0.00000e+00 4.45000e-01 8.06900e+00\n",
      "  7.60000e+01 3.49520e+00 2.00000e+00 2.76000e+02 1.80000e+01 3.96900e+02\n",
      "  4.21000e+00 1.00000e+00]\n",
      " [7.75223e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.30100e+00\n",
      "  8.37000e+01 2.78310e+00 2.40000e+01 6.66000e+02 2.02000e+01 2.72210e+02\n",
      "  1.62300e+01 1.00000e+00]\n",
      " [1.83377e+00 0.00000e+00 1.95800e+01 1.00000e+00 6.05000e-01 7.80200e+00\n",
      "  9.82000e+01 2.04070e+00 5.00000e+00 4.03000e+02 1.47000e+01 3.89610e+02\n",
      "  1.92000e+00 1.00000e+00]\n",
      " [1.51772e+01 0.00000e+00 1.81000e+01 0.00000e+00 7.40000e-01 6.15200e+00\n",
      "  1.00000e+02 1.91420e+00 2.40000e+01 6.66000e+02 2.02000e+01 9.32000e+00\n",
      "  2.64500e+01 1.00000e+00]\n",
      " [7.01300e-02 0.00000e+00 1.38900e+01 0.00000e+00 5.50000e-01 6.64200e+00\n",
      "  8.51000e+01 3.42110e+00 5.00000e+00 2.76000e+02 1.64000e+01 3.92780e+02\n",
      "  9.69000e+00 1.00000e+00]\n",
      " [2.11240e-01 1.25000e+01 7.87000e+00 0.00000e+00 5.24000e-01 5.63100e+00\n",
      "  1.00000e+02 6.08210e+00 5.00000e+00 3.11000e+02 1.52000e+01 3.86630e+02\n",
      "  2.99300e+01 1.00000e+00]\n",
      " [3.32105e+00 0.00000e+00 1.95800e+01 1.00000e+00 8.71000e-01 5.40300e+00\n",
      "  1.00000e+02 1.32160e+00 5.00000e+00 4.03000e+02 1.47000e+01 3.96900e+02\n",
      "  2.68200e+01 1.00000e+00]\n",
      " [8.38700e-02 0.00000e+00 1.28300e+01 0.00000e+00 4.37000e-01 5.87400e+00\n",
      "  3.66000e+01 4.50260e+00 5.00000e+00 3.98000e+02 1.87000e+01 3.96060e+02\n",
      "  9.10000e+00 1.00000e+00]\n",
      " [2.44668e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 5.27200e+00\n",
      "  9.40000e+01 1.73640e+00 5.00000e+00 4.03000e+02 1.47000e+01 8.86300e+01\n",
      "  1.61400e+01 1.00000e+00]\n",
      " [1.05740e-01 0.00000e+00 2.77400e+01 0.00000e+00 6.09000e-01 5.98300e+00\n",
      "  9.88000e+01 1.86810e+00 4.00000e+00 7.11000e+02 2.01000e+01 3.90110e+02\n",
      "  1.80700e+01 1.00000e+00]\n",
      " [2.28760e-01 0.00000e+00 8.56000e+00 0.00000e+00 5.20000e-01 6.40500e+00\n",
      "  8.54000e+01 2.71470e+00 5.00000e+00 3.84000e+02 2.09000e+01 7.08000e+01\n",
      "  1.06300e+01 1.00000e+00]\n",
      " [3.04900e-02 5.50000e+01 3.78000e+00 0.00000e+00 4.84000e-01 6.87400e+00\n",
      "  2.81000e+01 6.46540e+00 5.00000e+00 3.70000e+02 1.76000e+01 3.87970e+02\n",
      "  4.61000e+00 1.00000e+00]\n",
      " [1.05393e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 5.93500e+00\n",
      "  2.93000e+01 4.49860e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.86850e+02\n",
      "  6.58000e+00 1.00000e+00]\n",
      " [1.91330e-01 2.20000e+01 5.86000e+00 0.00000e+00 4.31000e-01 5.60500e+00\n",
      "  7.02000e+01 7.95490e+00 7.00000e+00 3.30000e+02 1.91000e+01 3.89130e+02\n",
      "  1.84600e+01 1.00000e+00]\n",
      " [6.80117e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.08100e+00\n",
      "  8.44000e+01 2.71750e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.96900e+02\n",
      "  1.47000e+01 1.00000e+00]\n",
      " [3.58400e-02 8.00000e+01 3.37000e+00 0.00000e+00 3.98000e-01 6.29000e+00\n",
      "  1.78000e+01 6.61150e+00 4.00000e+00 3.37000e+02 1.61000e+01 3.96900e+02\n",
      "  4.67000e+00 1.00000e+00]\n",
      " [1.06718e+01 0.00000e+00 1.81000e+01 0.00000e+00 7.40000e-01 6.45900e+00\n",
      "  9.48000e+01 1.98790e+00 2.40000e+01 6.66000e+02 2.02000e+01 4.30600e+01\n",
      "  2.39800e+01 1.00000e+00]\n",
      " [8.30800e-02 0.00000e+00 2.46000e+00 0.00000e+00 4.88000e-01 5.60400e+00\n",
      "  8.98000e+01 2.98790e+00 3.00000e+00 1.93000e+02 1.78000e+01 3.91000e+02\n",
      "  1.39800e+01 1.00000e+00]\n",
      " [3.25430e-01 0.00000e+00 2.18900e+01 0.00000e+00 6.24000e-01 6.43100e+00\n",
      "  9.88000e+01 1.81250e+00 4.00000e+00 4.37000e+02 2.12000e+01 3.96900e+02\n",
      "  1.53900e+01 1.00000e+00]\n",
      " [3.53700e-02 3.40000e+01 6.09000e+00 0.00000e+00 4.33000e-01 6.59000e+00\n",
      "  4.04000e+01 5.49170e+00 7.00000e+00 3.29000e+02 1.61000e+01 3.95750e+02\n",
      "  9.50000e+00 1.00000e+00]\n",
      " [5.50070e-01 2.00000e+01 3.97000e+00 0.00000e+00 6.47000e-01 7.20600e+00\n",
      "  9.16000e+01 1.93010e+00 5.00000e+00 2.64000e+02 1.30000e+01 3.87890e+02\n",
      "  8.10000e+00 1.00000e+00]\n",
      " [3.42700e-02 0.00000e+00 5.19000e+00 0.00000e+00 5.15000e-01 5.86900e+00\n",
      "  4.63000e+01 5.23110e+00 5.00000e+00 2.24000e+02 2.02000e+01 3.96900e+02\n",
      "  9.80000e+00 1.00000e+00]\n",
      " [5.57780e-01 0.00000e+00 2.18900e+01 0.00000e+00 6.24000e-01 6.33500e+00\n",
      "  9.82000e+01 2.11070e+00 4.00000e+00 4.37000e+02 2.12000e+01 3.94670e+02\n",
      "  1.69600e+01 1.00000e+00]\n",
      " [3.61500e-02 8.00000e+01 4.95000e+00 0.00000e+00 4.11000e-01 6.63000e+00\n",
      "  2.34000e+01 5.11670e+00 4.00000e+00 2.45000e+02 1.92000e+01 3.96900e+02\n",
      "  4.70000e+00 1.00000e+00]\n",
      " [1.10270e-01 2.50000e+01 5.13000e+00 0.00000e+00 4.53000e-01 6.45600e+00\n",
      "  6.78000e+01 7.22550e+00 8.00000e+00 2.84000e+02 1.97000e+01 3.96900e+02\n",
      "  6.73000e+00 1.00000e+00]\n",
      " [6.44405e+00 0.00000e+00 1.81000e+01 0.00000e+00 5.84000e-01 6.42500e+00\n",
      "  7.48000e+01 2.20040e+00 2.40000e+01 6.66000e+02 2.02000e+01 9.79500e+01\n",
      "  1.20300e+01 1.00000e+00]\n",
      " [1.02330e+01 0.00000e+00 1.81000e+01 0.00000e+00 6.14000e-01 6.18500e+00\n",
      "  9.67000e+01 2.17050e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.79700e+02\n",
      "  1.80300e+01 1.00000e+00]\n",
      " [1.31580e-01 0.00000e+00 1.00100e+01 0.00000e+00 5.47000e-01 6.17600e+00\n",
      "  7.25000e+01 2.73010e+00 6.00000e+00 4.32000e+02 1.78000e+01 3.93300e+02\n",
      "  1.20400e+01 1.00000e+00]\n",
      " [3.76800e-02 8.00000e+01 1.52000e+00 0.00000e+00 4.04000e-01 7.27400e+00\n",
      "  3.83000e+01 7.30900e+00 2.00000e+00 3.29000e+02 1.26000e+01 3.92200e+02\n",
      "  6.62000e+00 1.00000e+00]\n",
      " [2.43938e+01 0.00000e+00 1.81000e+01 0.00000e+00 7.00000e-01 4.65200e+00\n",
      "  1.00000e+02 1.46720e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.96900e+02\n",
      "  2.82800e+01 1.00000e+00]\n",
      " [1.58760e-01 0.00000e+00 1.08100e+01 0.00000e+00 4.13000e-01 5.96100e+00\n",
      "  1.75000e+01 5.28730e+00 4.00000e+00 3.05000e+02 1.92000e+01 3.76940e+02\n",
      "  9.88000e+00 1.00000e+00]\n",
      " [2.00900e-02 9.50000e+01 2.68000e+00 0.00000e+00 4.16100e-01 8.03400e+00\n",
      "  3.19000e+01 5.11800e+00 4.00000e+00 2.24000e+02 1.47000e+01 3.90550e+02\n",
      "  2.88000e+00 1.00000e+00]\n",
      " [1.20482e+01 0.00000e+00 1.81000e+01 0.00000e+00 6.14000e-01 5.64800e+00\n",
      "  8.76000e+01 1.95120e+00 2.40000e+01 6.66000e+02 2.02000e+01 2.91550e+02\n",
      "  1.41000e+01 1.00000e+00]\n",
      " [4.30100e-02 8.00000e+01 1.91000e+00 0.00000e+00 4.13000e-01 5.66300e+00\n",
      "  2.19000e+01 1.05857e+01 4.00000e+00 3.34000e+02 2.20000e+01 3.82800e+02\n",
      "  8.05000e+00 1.00000e+00]\n",
      " [1.44208e+01 0.00000e+00 1.81000e+01 0.00000e+00 7.40000e-01 6.46100e+00\n",
      "  9.33000e+01 2.00260e+00 2.40000e+01 6.66000e+02 2.02000e+01 2.74900e+01\n",
      "  1.80500e+01 1.00000e+00]\n",
      " [9.59571e+00 0.00000e+00 1.81000e+01 0.00000e+00 6.93000e-01 6.40400e+00\n",
      "  1.00000e+02 1.63900e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.76110e+02\n",
      "  2.03100e+01 1.00000e+00]\n",
      " [3.29820e-01 0.00000e+00 2.18900e+01 0.00000e+00 6.24000e-01 5.82200e+00\n",
      "  9.54000e+01 2.46990e+00 4.00000e+00 4.37000e+02 2.12000e+01 3.88690e+02\n",
      "  1.50300e+01 1.00000e+00]\n",
      " [1.49632e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 5.40400e+00\n",
      "  1.00000e+02 1.59160e+00 5.00000e+00 4.03000e+02 1.47000e+01 3.41600e+02\n",
      "  1.32800e+01 1.00000e+00]\n",
      " [5.26930e-01 0.00000e+00 6.20000e+00 0.00000e+00 5.04000e-01 8.72500e+00\n",
      "  8.30000e+01 2.89440e+00 8.00000e+00 3.07000e+02 1.74000e+01 3.82000e+02\n",
      "  4.63000e+00 1.00000e+00]\n",
      " [9.51363e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.13000e-01 6.72800e+00\n",
      "  9.41000e+01 2.49610e+00 2.40000e+01 6.66000e+02 2.02000e+01 6.68000e+00\n",
      "  1.87100e+01 1.00000e+00]\n",
      " [3.55100e-02 2.50000e+01 4.86000e+00 0.00000e+00 4.26000e-01 6.16700e+00\n",
      "  4.67000e+01 5.40070e+00 4.00000e+00 2.81000e+02 1.90000e+01 3.90640e+02\n",
      "  7.51000e+00 1.00000e+00]\n",
      " [2.17700e-02 8.25000e+01 2.03000e+00 0.00000e+00 4.15000e-01 7.61000e+00\n",
      "  1.57000e+01 6.27000e+00 2.00000e+00 3.48000e+02 1.47000e+01 3.95380e+02\n",
      "  3.11000e+00 1.00000e+00]\n",
      " [3.47428e+00 0.00000e+00 1.81000e+01 1.00000e+00 7.18000e-01 8.78000e+00\n",
      "  8.29000e+01 1.90470e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.54550e+02\n",
      "  5.29000e+00 1.00000e+00]\n",
      " [7.02259e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.18000e-01 6.00600e+00\n",
      "  9.53000e+01 1.87460e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.19980e+02\n",
      "  1.57000e+01 1.00000e+00]\n",
      " [8.26725e+00 0.00000e+00 1.81000e+01 1.00000e+00 6.68000e-01 5.87500e+00\n",
      "  8.96000e+01 1.12960e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.47880e+02\n",
      "  8.88000e+00 1.00000e+00]\n",
      " [1.19294e+00 0.00000e+00 2.18900e+01 0.00000e+00 6.24000e-01 6.32600e+00\n",
      "  9.77000e+01 2.27100e+00 4.00000e+00 4.37000e+02 2.12000e+01 3.96900e+02\n",
      "  1.22600e+01 1.00000e+00]] [[ 0.53  1.59 -0.2  -0.54]\n",
      " [ 0.58  0.06  0.12  0.61]\n",
      " [-1.42  0.25 -1.94 -1.85]\n",
      " [-0.9  -1.17  1.27 -0.43]\n",
      " [ 0.22  0.3   0.12 -1.7 ]\n",
      " [ 0.21  0.36  0.07  0.35]\n",
      " [-0.29  0.24 -1.27  0.78]\n",
      " [-0.73  0.07  1.1  -0.48]\n",
      " [ 0.13 -0.62 -1.21 -1.45]\n",
      " [ 1.33 -0.75 -0.05  0.51]\n",
      " [ 0.28  0.05 -1.01 -0.84]\n",
      " [-0.2  -0.71  0.49  1.08]\n",
      " [-1.28 -0.69 -1.19  1.38]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "X None\n",
      "weights 0      24.0\n",
      "1      21.6\n",
      "2      34.7\n",
      "3      33.4\n",
      "4      36.2\n",
      "       ... \n",
      "501    22.4\n",
      "502    20.6\n",
      "503    23.9\n",
      "504    22.0\n",
      "505    11.9\n",
      "Name: MEDV, Length: 506, dtype: float64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9cd2d6e97b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-7219e8465982>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0ml2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquared_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dot'"
     ]
    }
   ],
   "source": [
    "layers = 1\n",
    "nodes = [50, 4, 1]\n",
    "activations = [\"relu\"]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "NN.forward_pass(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.21100e-02, 4.00000e+01, 1.25000e+00, 0.00000e+00, 4.29000e-01,\n",
       "        6.49000e+00, 4.44000e+01, 8.79210e+00, 1.00000e+00, 3.35000e+02,\n",
       "        1.97000e+01, 3.96900e+02, 5.98000e+00, 1.00000e+00],\n",
       "       [1.91860e-01, 0.00000e+00, 7.38000e+00, 0.00000e+00, 4.93000e-01,\n",
       "        6.43100e+00, 1.47000e+01, 5.41590e+00, 5.00000e+00, 2.87000e+02,\n",
       "        1.96000e+01, 3.93680e+02, 5.08000e+00, 1.00000e+00],\n",
       "       [1.06590e-01, 8.00000e+01, 1.91000e+00, 0.00000e+00, 4.13000e-01,\n",
       "        5.93600e+00, 1.95000e+01, 1.05857e+01, 4.00000e+00, 3.34000e+02,\n",
       "        2.20000e+01, 3.76040e+02, 5.57000e+00, 1.00000e+00],\n",
       "       [1.11604e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.40000e-01,\n",
       "        6.62900e+00, 9.46000e+01, 2.12470e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 1.09850e+02, 2.32700e+01, 1.00000e+00],\n",
       "       [1.62864e+00, 0.00000e+00, 2.18900e+01, 0.00000e+00, 6.24000e-01,\n",
       "        5.01900e+00, 1.00000e+02, 1.43940e+00, 4.00000e+00, 4.37000e+02,\n",
       "        2.12000e+01, 3.96900e+02, 3.44100e+01, 1.00000e+00],\n",
       "       [1.06718e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.40000e-01,\n",
       "        6.45900e+00, 9.48000e+01, 1.98790e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 4.30600e+01, 2.39800e+01, 1.00000e+00],\n",
       "       [5.82115e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
       "        6.51300e+00, 8.99000e+01, 2.80160e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.93820e+02, 1.02900e+01, 1.00000e+00],\n",
       "       [5.56100e-02, 7.00000e+01, 2.24000e+00, 0.00000e+00, 4.00000e-01,\n",
       "        7.04100e+00, 1.00000e+01, 7.82780e+00, 5.00000e+00, 3.58000e+02,\n",
       "        1.48000e+01, 3.71580e+02, 4.74000e+00, 1.00000e+00],\n",
       "       [3.76800e-02, 8.00000e+01, 1.52000e+00, 0.00000e+00, 4.04000e-01,\n",
       "        7.27400e+00, 3.83000e+01, 7.30900e+00, 2.00000e+00, 3.29000e+02,\n",
       "        1.26000e+01, 3.92200e+02, 6.62000e+00, 1.00000e+00],\n",
       "       [5.82401e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 5.32000e-01,\n",
       "        6.24200e+00, 6.47000e+01, 3.42420e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 1.07400e+01, 1.00000e+00],\n",
       "       [2.63630e-01, 0.00000e+00, 8.56000e+00, 0.00000e+00, 5.20000e-01,\n",
       "        6.22900e+00, 9.12000e+01, 2.54510e+00, 5.00000e+00, 3.84000e+02,\n",
       "        2.09000e+01, 3.91230e+02, 1.55500e+01, 1.00000e+00],\n",
       "       [2.17190e-01, 0.00000e+00, 1.05900e+01, 1.00000e+00, 4.89000e-01,\n",
       "        5.80700e+00, 5.38000e+01, 3.65260e+00, 4.00000e+00, 2.77000e+02,\n",
       "        1.86000e+01, 3.90940e+02, 1.60300e+01, 1.00000e+00],\n",
       "       [8.01400e-02, 0.00000e+00, 5.96000e+00, 0.00000e+00, 4.99000e-01,\n",
       "        5.85000e+00, 4.15000e+01, 3.93420e+00, 5.00000e+00, 2.79000e+02,\n",
       "        1.92000e+01, 3.96900e+02, 8.77000e+00, 1.00000e+00],\n",
       "       [5.26930e-01, 0.00000e+00, 6.20000e+00, 0.00000e+00, 5.04000e-01,\n",
       "        8.72500e+00, 8.30000e+01, 2.89440e+00, 8.00000e+00, 3.07000e+02,\n",
       "        1.74000e+01, 3.82000e+02, 4.63000e+00, 1.00000e+00],\n",
       "       [1.31100e-02, 9.00000e+01, 1.22000e+00, 0.00000e+00, 4.03000e-01,\n",
       "        7.24900e+00, 2.19000e+01, 8.69660e+00, 5.00000e+00, 2.26000e+02,\n",
       "        1.79000e+01, 3.95930e+02, 4.81000e+00, 1.00000e+00],\n",
       "       [3.18270e-01, 0.00000e+00, 9.90000e+00, 0.00000e+00, 5.44000e-01,\n",
       "        5.91400e+00, 8.32000e+01, 3.99860e+00, 4.00000e+00, 3.04000e+02,\n",
       "        1.84000e+01, 3.90700e+02, 1.83300e+01, 1.00000e+00],\n",
       "       [5.08300e-02, 0.00000e+00, 5.19000e+00, 0.00000e+00, 5.15000e-01,\n",
       "        6.31600e+00, 3.81000e+01, 6.45840e+00, 5.00000e+00, 2.24000e+02,\n",
       "        2.02000e+01, 3.89710e+02, 5.68000e+00, 1.00000e+00],\n",
       "       [1.35222e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,\n",
       "        3.86300e+00, 1.00000e+02, 1.51060e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 1.31420e+02, 1.33300e+01, 1.00000e+00],\n",
       "       [1.64390e-01, 2.20000e+01, 5.86000e+00, 0.00000e+00, 4.31000e-01,\n",
       "        6.43300e+00, 4.91000e+01, 7.82650e+00, 7.00000e+00, 3.30000e+02,\n",
       "        1.91000e+01, 3.74710e+02, 9.52000e+00, 1.00000e+00],\n",
       "       [9.82349e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.71000e-01,\n",
       "        6.79400e+00, 9.88000e+01, 1.35800e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 2.12400e+01, 1.00000e+00],\n",
       "       [1.96091e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.71000e-01,\n",
       "        7.31300e+00, 9.79000e+01, 1.31630e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 1.34400e+01, 1.00000e+00],\n",
       "       [1.04690e-01, 4.00000e+01, 6.41000e+00, 1.00000e+00, 4.47000e-01,\n",
       "        7.26700e+00, 4.90000e+01, 4.78720e+00, 4.00000e+00, 2.54000e+02,\n",
       "        1.76000e+01, 3.89250e+02, 6.05000e+00, 1.00000e+00],\n",
       "       [1.51902e+00, 0.00000e+00, 1.95800e+01, 1.00000e+00, 6.05000e-01,\n",
       "        8.37500e+00, 9.39000e+01, 2.16200e+00, 5.00000e+00, 4.03000e+02,\n",
       "        1.47000e+01, 3.88450e+02, 3.32000e+00, 1.00000e+00],\n",
       "       [3.54800e-02, 8.00000e+01, 3.64000e+00, 0.00000e+00, 3.92000e-01,\n",
       "        5.87600e+00, 1.91000e+01, 9.22030e+00, 1.00000e+00, 3.15000e+02,\n",
       "        1.64000e+01, 3.95180e+02, 9.25000e+00, 1.00000e+00],\n",
       "       [1.83377e+00, 0.00000e+00, 1.95800e+01, 1.00000e+00, 6.05000e-01,\n",
       "        7.80200e+00, 9.82000e+01, 2.04070e+00, 5.00000e+00, 4.03000e+02,\n",
       "        1.47000e+01, 3.89610e+02, 1.92000e+00, 1.00000e+00],\n",
       "       [6.28807e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.40000e-01,\n",
       "        6.34100e+00, 9.64000e+01, 2.07200e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.18010e+02, 1.77900e+01, 1.00000e+00],\n",
       "       [4.26131e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.70000e-01,\n",
       "        6.11200e+00, 8.13000e+01, 2.50910e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.90740e+02, 1.26700e+01, 1.00000e+00],\n",
       "       [1.58603e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.79000e-01,\n",
       "        5.89600e+00, 9.54000e+01, 1.90960e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 7.68000e+00, 2.43900e+01, 1.00000e+00],\n",
       "       [5.51500e-02, 3.30000e+01, 2.18000e+00, 0.00000e+00, 4.72000e-01,\n",
       "        7.23600e+00, 4.11000e+01, 4.02200e+00, 7.00000e+00, 2.22000e+02,\n",
       "        1.84000e+01, 3.93680e+02, 6.93000e+00, 1.00000e+00],\n",
       "       [1.14320e-01, 0.00000e+00, 8.56000e+00, 0.00000e+00, 5.20000e-01,\n",
       "        6.78100e+00, 7.13000e+01, 2.85610e+00, 5.00000e+00, 3.84000e+02,\n",
       "        2.09000e+01, 3.95580e+02, 7.67000e+00, 1.00000e+00],\n",
       "       [6.27390e-01, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.83400e+00, 5.65000e+01, 4.49860e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.95620e+02, 8.47000e+00, 1.00000e+00],\n",
       "       [1.95390e-01, 0.00000e+00, 1.08100e+01, 0.00000e+00, 4.13000e-01,\n",
       "        6.24500e+00, 6.20000e+00, 5.28730e+00, 4.00000e+00, 3.05000e+02,\n",
       "        1.92000e+01, 3.77170e+02, 7.54000e+00, 1.00000e+00],\n",
       "       [1.10874e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.18000e-01,\n",
       "        6.41100e+00, 1.00000e+02, 1.85890e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.18750e+02, 1.50200e+01, 1.00000e+00],\n",
       "       [3.75780e-01, 0.00000e+00, 1.05900e+01, 1.00000e+00, 4.89000e-01,\n",
       "        5.40400e+00, 8.86000e+01, 3.66500e+00, 4.00000e+00, 2.77000e+02,\n",
       "        1.86000e+01, 3.95240e+02, 2.39800e+01, 1.00000e+00],\n",
       "       [3.53700e-02, 3.40000e+01, 6.09000e+00, 0.00000e+00, 4.33000e-01,\n",
       "        6.59000e+00, 4.04000e+01, 5.49170e+00, 7.00000e+00, 3.29000e+02,\n",
       "        1.61000e+01, 3.95750e+02, 9.50000e+00, 1.00000e+00],\n",
       "       [2.77974e+00, 0.00000e+00, 1.95800e+01, 0.00000e+00, 8.71000e-01,\n",
       "        4.90300e+00, 9.78000e+01, 1.34590e+00, 5.00000e+00, 4.03000e+02,\n",
       "        1.47000e+01, 3.96900e+02, 2.92900e+01, 1.00000e+00],\n",
       "       [3.25430e-01, 0.00000e+00, 2.18900e+01, 0.00000e+00, 6.24000e-01,\n",
       "        6.43100e+00, 9.88000e+01, 1.81250e+00, 4.00000e+00, 4.37000e+02,\n",
       "        2.12000e+01, 3.96900e+02, 1.53900e+01, 1.00000e+00],\n",
       "       [3.57800e-02, 2.00000e+01, 3.33000e+00, 0.00000e+00, 4.42900e-01,\n",
       "        7.82000e+00, 6.45000e+01, 4.69470e+00, 5.00000e+00, 2.16000e+02,\n",
       "        1.49000e+01, 3.87310e+02, 3.76000e+00, 1.00000e+00],\n",
       "       [5.09017e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
       "        6.29700e+00, 9.18000e+01, 2.36820e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.85090e+02, 1.72700e+01, 1.00000e+00],\n",
       "       [1.68118e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.00000e-01,\n",
       "        5.27700e+00, 9.81000e+01, 1.42610e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 3.08100e+01, 1.00000e+00],\n",
       "       [4.75470e-01, 0.00000e+00, 9.90000e+00, 0.00000e+00, 5.44000e-01,\n",
       "        6.11300e+00, 5.88000e+01, 4.00190e+00, 4.00000e+00, 3.04000e+02,\n",
       "        1.84000e+01, 3.96230e+02, 1.27300e+01, 1.00000e+00],\n",
       "       [1.42502e+00, 0.00000e+00, 1.95800e+01, 0.00000e+00, 8.71000e-01,\n",
       "        6.51000e+00, 1.00000e+02, 1.76590e+00, 5.00000e+00, 4.03000e+02,\n",
       "        1.47000e+01, 3.64310e+02, 7.39000e+00, 1.00000e+00],\n",
       "       [5.66998e+00, 0.00000e+00, 1.81000e+01, 1.00000e+00, 6.31000e-01,\n",
       "        6.68300e+00, 9.68000e+01, 1.35670e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.75330e+02, 3.73000e+00, 1.00000e+00],\n",
       "       [2.01019e+00, 0.00000e+00, 1.95800e+01, 0.00000e+00, 6.05000e-01,\n",
       "        7.92900e+00, 9.62000e+01, 2.04590e+00, 5.00000e+00, 4.03000e+02,\n",
       "        1.47000e+01, 3.69300e+02, 3.70000e+00, 1.00000e+00],\n",
       "       [9.88430e-01, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.81300e+00, 1.00000e+02, 4.09520e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.94540e+02, 1.98800e+01, 1.00000e+00],\n",
       "       [2.19770e-01, 0.00000e+00, 6.91000e+00, 0.00000e+00, 4.48000e-01,\n",
       "        5.60200e+00, 6.20000e+01, 6.08770e+00, 3.00000e+00, 2.33000e+02,\n",
       "        1.79000e+01, 3.96900e+02, 1.62000e+01, 1.00000e+00],\n",
       "       [3.56868e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 5.80000e-01,\n",
       "        6.43700e+00, 7.50000e+01, 2.89650e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.93370e+02, 1.43600e+01, 1.00000e+00],\n",
       "       [1.05740e-01, 0.00000e+00, 2.77400e+01, 0.00000e+00, 6.09000e-01,\n",
       "        5.98300e+00, 9.88000e+01, 1.86810e+00, 4.00000e+00, 7.11000e+02,\n",
       "        2.01000e+01, 3.90110e+02, 1.80700e+01, 1.00000e+00],\n",
       "       [1.11081e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.68000e-01,\n",
       "        4.90600e+00, 1.00000e+02, 1.17420e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 3.47700e+01, 1.00000e+00],\n",
       "       [2.39120e-01, 0.00000e+00, 9.69000e+00, 0.00000e+00, 5.85000e-01,\n",
       "        6.01900e+00, 6.53000e+01, 2.40910e+00, 6.00000e+00, 3.91000e+02,\n",
       "        1.92000e+01, 3.96900e+02, 1.29200e+01, 1.00000e+00]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.layer_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 1\n",
      "nodes: [50, 4, 1]\n",
      "activations: [None, 'relu', None]\n",
      "activationFn: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51467.04361299818"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 1\n",
    "nodes = [50, 4, 1]\n",
    "activations = [None, \"relu\", None]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "print(\"layers:\", NN.layers)\n",
    "print(\"nodes:\", NN.nodes)\n",
    "print(\"activations:\", NN.activations)\n",
    "print(\"activationFn:\", NN.activationFn)\n",
    "\n",
    "activations = NN.forward_pass(X, y)\n",
    "#print(\"dimensitons:\", z.shape)\n",
    "# for i in range(len(NN.weights)):\n",
    "#     print(i, \":\", NN.weights[i])\n",
    "activations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Class for Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with Multiple layers\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, batchSize=50, activationFn=\"relu\", lr=.1):\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[i] for i in range(len(nodes))]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward_pass(self, X_train, y_train):\n",
    "        \n",
    "        \"\"\" activations : list, length = n_layers - 1\n",
    "             The ith element of the list holds the values of the ith layer.\n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        self.weights = []\n",
    "        \n",
    "        \n",
    "        response = y_train.name\n",
    "        X_train[response] = y_train\n",
    "        \n",
    "        X_train_batch = X_train.sample(self.batchSize) # get batch\n",
    "        X_batch = X_train_batch.drop(response, axis=1)\n",
    "        X_batch[\"ones\"] = 1 # add ones for bias\n",
    "        y_batch = X_train_batch[response]\n",
    "        \n",
    "        X_batch = X_batch.to_numpy()\n",
    "        self.y_batch = y_batch.to_numpy()\n",
    "        \n",
    "        self.activations.append(X_batch)\n",
    "        \n",
    "        M = X_batch.shape[1] # M = number of features\n",
    "        \n",
    "        for i in range(layers):\n",
    "            N = nodes[1 + i] # N = number of nodes in hidden layer\n",
    "\n",
    "            # weights = M * N\n",
    "            w1 = np.random.normal(size=(M, N)) # initalize weights\n",
    "            w1 = np.round(w1, 2)\n",
    "            w1[M-1:] = 0 # initialize biases to 0\n",
    "            self.weights.append(w1)\n",
    "\n",
    "            h1 = X_batch.dot(w1) # first hidden layer\n",
    "            h1_activation_function = ACTIVATIONS[self.activationFn]\n",
    "            h1_activation_function(h1) # h1 is now \"activated\"\n",
    "\n",
    "            self.activations.append(h1)\n",
    "\n",
    "        w2 = np.random.normal(size=N) # initialize weights\n",
    "        w2 = np.round(w2, 2)\n",
    "        w2[N - 1] = 0 # initialize bias to 0\n",
    "\n",
    "        self.z = h1.dot(w2) # z = predictions\n",
    "\n",
    "        w2 = np.reshape(w2, (N, 1))\n",
    "        self.weights.append(w2)\n",
    "\n",
    "        loss = squared_loss(self.z, y_batch)\n",
    "\n",
    "        self.activations.append(loss)\n",
    "\n",
    "        return self.activations\n",
    "    \n",
    "    def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.z, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "\n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.activations[len(activations)-2].T\n",
    "        J_w2 = x_t.dot(J)\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w2_t = self.weights[len(self.weights)-1].T\n",
    "        J = np.dot(J, w2_t)\n",
    "        \n",
    "        #update this weight\n",
    "        self.weights[len(self.weights)-1] = self.weights[len(self.weights)-1] - self.lr * J_w2\n",
    "        \n",
    "        for i in range(layers, 0, -1):\n",
    "            # update jacobian at activation layer\n",
    "            inplace_relu_derivative(activations[i], J)\n",
    "\n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = activations[0].T\n",
    "            J_w1 = x_t.dot(J)\n",
    "        \n",
    "            # update weights\n",
    "            self.weights[i-1] = self.weights[i-1] - self.lr * J_w1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        self.backprop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 2\n",
      "nodes: [50, 4, 4, 1]\n",
      "activations: [None, 'relu', None]\n",
      "activationFn: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2579.4050879514343"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 2\n",
    "nodes = [50, 4, 4, 1]\n",
    "activations = [None, \"relu\", None]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "print(\"layers:\", NN.layers)\n",
    "print(\"nodes:\", NN.nodes)\n",
    "print(\"activations:\", NN.activations)\n",
    "print(\"activationFn:\", NN.activationFn)\n",
    "\n",
    "activations = NN.forward_pass(X, y)\n",
    "#print(\"dimensitons:\", z.shape)\n",
    "activations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [[-0.89  1.6  -0.08  1.55]\n",
      " [-0.09  1.03  0.57  0.5 ]\n",
      " [ 0.62  1.57  0.32  2.23]\n",
      " [-0.02  0.35 -0.89  0.4 ]\n",
      " [ 0.1   0.44  0.95 -0.73]\n",
      " [ 0.27  0.19  0.57 -0.41]\n",
      " [ 0.05  1.48  2.66 -1.48]\n",
      " [-0.13  0.21  0.05  0.39]\n",
      " [-1.56  1.24  1.31 -1.87]\n",
      " [ 0.54 -0.04 -1.08  0.76]\n",
      " [-0.32 -1.34  1.43 -0.55]\n",
      " [ 1.73  0.75 -1.27  0.77]\n",
      " [ 0.56  0.12 -1.28 -0.34]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "1 : [[-1.48  0.27 -0.33 -0.84]\n",
      " [ 1.48 -1.94  0.81 -1.79]\n",
      " [ 0.86 -0.47  0.87 -0.75]\n",
      " [-1.63  1.08 -0.13  1.6 ]\n",
      " [ 0.07  0.15 -0.38 -1.51]\n",
      " [-0.27 -0.04 -1.24 -0.92]\n",
      " [-0.96  0.45  1.24  0.24]\n",
      " [-0.78 -1.25 -0.62 -0.46]\n",
      " [ 1.4  -0.94  0.1   0.1 ]\n",
      " [-1.04 -0.67  0.25 -0.9 ]\n",
      " [ 0.04 -0.58  0.46 -0.62]\n",
      " [ 0.23 -0.1  -1.05 -1.41]\n",
      " [ 0.32  0.27 -0.5  -1.58]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "2 : [[ 0.14]\n",
      " [-1.56]\n",
      " [-0.74]\n",
      " [ 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NN.weights)):\n",
    "    print(i, \":\", NN.weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [[-0.89  1.6  -0.08  1.55]\n",
      " [-0.09  1.03  0.57  0.5 ]\n",
      " [ 0.62  1.57  0.32  2.23]\n",
      " [-0.02  0.35 -0.89  0.4 ]\n",
      " [ 0.1   0.44  0.95 -0.73]\n",
      " [ 0.27  0.19  0.57 -0.41]\n",
      " [ 0.05  1.48  2.66 -1.48]\n",
      " [-0.13  0.21  0.05  0.39]\n",
      " [-1.56  1.24  1.31 -1.87]\n",
      " [ 0.54 -0.04 -1.08  0.76]\n",
      " [-0.32 -1.34  1.43 -0.55]\n",
      " [ 1.73  0.75 -1.27  0.77]\n",
      " [ 0.56  0.12 -1.28 -0.34]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "1 : [[-1.48000000e+00  2.70000000e-01 -2.13911649e+01 -8.40000000e-01]\n",
      " [ 1.48000000e+00 -1.94000000e+00  8.10000000e-01 -1.79000000e+00]\n",
      " [ 8.60000000e-01 -4.70000000e-01 -3.20591325e+01 -7.50000000e-01]\n",
      " [-1.63000000e+00  1.08000000e+00 -1.30000000e-01  1.60000000e+00]\n",
      " [ 7.00000000e-02  1.50000000e-01 -1.58499858e+00 -1.51000000e+00]\n",
      " [-2.70000000e-01 -4.00000000e-02 -1.24566055e+01 -9.20000000e-01]\n",
      " [-9.60000000e-01  4.50000000e-01 -1.62647873e+02  2.40000000e-01]\n",
      " [-7.80000000e-01 -1.25000000e+00 -4.37203725e+00 -4.60000000e-01]\n",
      " [ 1.40000000e+00 -9.40000000e-01 -4.35629382e+01  1.00000000e-01]\n",
      " [-1.04000000e+00 -6.70000000e-01 -1.21139653e+03 -9.00000000e-01]\n",
      " [ 4.00000000e-02 -5.80000000e-01 -3.62896396e+01 -6.20000000e-01]\n",
      " [ 2.30000000e-01 -1.00000000e-01 -7.51249836e+01 -1.41000000e+00]\n",
      " [ 3.20000000e-01  2.70000000e-01 -3.86411302e+01 -1.58000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.81928909e+00  0.00000000e+00]]\n",
      "2 : [[ 1.4000000e-01]\n",
      " [-1.5600000e+00]\n",
      " [ 5.8785437e+02]\n",
      " [ 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NN.weights)):\n",
    "    print(i, \":\", NN.weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the weights updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = NN.weights[len(NN.weights)-1]\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = np.reshape(w2, (4, 1))\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
