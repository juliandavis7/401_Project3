{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"housing/boston_fixed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"MEDV\",axis=1)\n",
    "y = df[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital Class for 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, batchSize=50, activationFn=\"relu\", lr=.01):\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[0], nodes[1], nodes[2]]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                print(\"hi\")\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "        \n",
    "    def forward_pass(self, X_train, y_train):\n",
    "        \n",
    "        # add ones for bias\n",
    "        X_train[\"ones\"] = 1 \n",
    "        \n",
    "        # get batch from the data\n",
    "        batch_slice = np.random.choice(len(X_train), \n",
    "                                       replace = False, \n",
    "                                       size = self.batchSize) \n",
    "        X_batch = X_train.iloc[batch_slice]\n",
    "        y_batch = y_train.iloc[batch_slice]\n",
    "        \n",
    "        # convert to numpy arrays\n",
    "        X_batch = X_batch.to_numpy()\n",
    "        self.y_batch = y_batch.to_numpy()\n",
    "        \n",
    "        \n",
    "        if self.iters == 0:\n",
    "            # initialize weights\n",
    "            M = X_batch.shape[1] # M = number of features\n",
    "            self.weights = self.initialize_weights(M)\n",
    "            \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        self.y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = squared_loss(self.y_pred, y_batch)\n",
    "        self.layer_values[self.layers + 1] = l2_loss\n",
    "        self.iters += 1\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.y_pred, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        J = np.dot(J, w_t)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.weights[i] -= self.lr * J_weights[i]\n",
    "        \n",
    "    def predict(self, X_train, y_train):\n",
    "        l2_loss = self.forward_pass(X_train, y_train)\n",
    "        print(\"1st loss\", l2_loss)\n",
    "        \n",
    "        self.backprop()\n",
    "        l2_loss = self.forward_pass(X_train, y_train)\n",
    "        print(\"2nd loss\", l2_loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "1st loss 1623850.3268792834\n",
      "2nd loss 2.940878990410559e+39\n"
     ]
    }
   ],
   "source": [
    "layers = 3\n",
    "nodes = [50, 4, 4, 4, 1]\n",
    "activations = [\"relu\", \"relu\", \"relu\"]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "NN.predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.y_pred, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "\n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_w_last = x_t.dot(J)\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        J = np.dot(J, w_t)\n",
    "        \n",
    "        \n",
    "        # update jacobian at activation layer\n",
    "        inplace_relu_derivative(self.layer_values[self.layers], J)\n",
    "        \n",
    "        # hidden layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[0].T\n",
    "        J_w1 = x_t.dot(J)\n",
    "        \n",
    "        # update weights\n",
    "        self.weights[1] = self.weights[1] - self.lr * J_w2\n",
    "        self.weights[0] = self.weights[0] - self.lr * J_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 1\n",
      "nodes: [50, 4, 1]\n",
      "activations: [None, 'relu', None]\n",
      "activationFn: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51467.04361299818"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 1\n",
    "nodes = [50, 4, 1]\n",
    "activations = [None, \"relu\", None]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "print(\"layers:\", NN.layers)\n",
    "print(\"nodes:\", NN.nodes)\n",
    "print(\"activations:\", NN.activations)\n",
    "print(\"activationFn:\", NN.activationFn)\n",
    "\n",
    "NN.forward_pass(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Class for Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with Multiple layers\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, batchSize=50, activationFn=\"relu\", lr=.1):\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[i] for i in range(len(nodes))]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward_pass(self, X_train, y_train):\n",
    "        \n",
    "        \"\"\" activations : list, length = n_layers - 1\n",
    "             The ith element of the list holds the values of the ith layer.\n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        self.weights = []\n",
    "        \n",
    "        \n",
    "        response = y_train.name\n",
    "        X_train[response] = y_train\n",
    "        \n",
    "        X_train_batch = X_train.sample(self.batchSize) # get batch\n",
    "        X_batch = X_train_batch.drop(response, axis=1)\n",
    "        X_batch[\"ones\"] = 1 # add ones for bias\n",
    "        y_batch = X_train_batch[response]\n",
    "        \n",
    "        X_batch = X_batch.to_numpy()\n",
    "        self.y_batch = y_batch.to_numpy()\n",
    "        \n",
    "        self.activations.append(X_batch)\n",
    "        \n",
    "        M = X_batch.shape[1] # M = number of features\n",
    "        \n",
    "        for i in range(layers):\n",
    "            N = nodes[1 + i] # N = number of nodes in hidden layer\n",
    "\n",
    "            # weights = M * N\n",
    "            w1 = np.random.normal(size=(M, N)) # initalize weights\n",
    "            w1 = np.round(w1, 2)\n",
    "            w1[M-1:] = 0 # initialize biases to 0\n",
    "            self.weights.append(w1)\n",
    "\n",
    "            h1 = X_batch.dot(w1) # first hidden layer\n",
    "            h1_activation_function = ACTIVATIONS[self.activationFn]\n",
    "            h1_activation_function(h1) # h1 is now \"activated\"\n",
    "\n",
    "            self.activations.append(h1)\n",
    "\n",
    "        w2 = np.random.normal(size=N) # initialize weights\n",
    "        w2 = np.round(w2, 2)\n",
    "        w2[N - 1] = 0 # initialize bias to 0\n",
    "\n",
    "        self.z = h1.dot(w2) # z = predictions\n",
    "\n",
    "        w2 = np.reshape(w2, (N, 1))\n",
    "        self.weights.append(w2)\n",
    "\n",
    "        loss = squared_loss(self.z, y_batch)\n",
    "\n",
    "        self.activations.append(loss)\n",
    "\n",
    "        return self.activations\n",
    "    \n",
    "    def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.z, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "\n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.activations[len(activations)-2].T\n",
    "        J_w2 = x_t.dot(J)\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w2_t = self.weights[len(self.weights)-1].T\n",
    "        J = np.dot(J, w2_t)\n",
    "        \n",
    "        #update this weight\n",
    "        self.weights[len(self.weights)-1] = self.weights[len(self.weights)-1] - self.lr * J_w2\n",
    "        \n",
    "        for i in range(layers, 0, -1):\n",
    "            # update jacobian at activation layer\n",
    "            inplace_relu_derivative(activations[i], J)\n",
    "\n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = activations[0].T\n",
    "            J_w1 = x_t.dot(J)\n",
    "        \n",
    "            # update weights\n",
    "            self.weights[i-1] = self.weights[i-1] - self.lr * J_w1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        self.backprop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 2\n",
      "nodes: [50, 4, 4, 1]\n",
      "activations: [None, 'relu', None]\n",
      "activationFn: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2579.4050879514343"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = 2\n",
    "nodes = [50, 4, 4, 1]\n",
    "activations = [None, \"relu\", None]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "print(\"layers:\", NN.layers)\n",
    "print(\"nodes:\", NN.nodes)\n",
    "print(\"activations:\", NN.activations)\n",
    "print(\"activationFn:\", NN.activationFn)\n",
    "\n",
    "activations = NN.forward_pass(X, y)\n",
    "#print(\"dimensitons:\", z.shape)\n",
    "activations[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [[-0.89  1.6  -0.08  1.55]\n",
      " [-0.09  1.03  0.57  0.5 ]\n",
      " [ 0.62  1.57  0.32  2.23]\n",
      " [-0.02  0.35 -0.89  0.4 ]\n",
      " [ 0.1   0.44  0.95 -0.73]\n",
      " [ 0.27  0.19  0.57 -0.41]\n",
      " [ 0.05  1.48  2.66 -1.48]\n",
      " [-0.13  0.21  0.05  0.39]\n",
      " [-1.56  1.24  1.31 -1.87]\n",
      " [ 0.54 -0.04 -1.08  0.76]\n",
      " [-0.32 -1.34  1.43 -0.55]\n",
      " [ 1.73  0.75 -1.27  0.77]\n",
      " [ 0.56  0.12 -1.28 -0.34]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "1 : [[-1.48  0.27 -0.33 -0.84]\n",
      " [ 1.48 -1.94  0.81 -1.79]\n",
      " [ 0.86 -0.47  0.87 -0.75]\n",
      " [-1.63  1.08 -0.13  1.6 ]\n",
      " [ 0.07  0.15 -0.38 -1.51]\n",
      " [-0.27 -0.04 -1.24 -0.92]\n",
      " [-0.96  0.45  1.24  0.24]\n",
      " [-0.78 -1.25 -0.62 -0.46]\n",
      " [ 1.4  -0.94  0.1   0.1 ]\n",
      " [-1.04 -0.67  0.25 -0.9 ]\n",
      " [ 0.04 -0.58  0.46 -0.62]\n",
      " [ 0.23 -0.1  -1.05 -1.41]\n",
      " [ 0.32  0.27 -0.5  -1.58]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "2 : [[ 0.14]\n",
      " [-1.56]\n",
      " [-0.74]\n",
      " [ 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NN.weights)):\n",
    "    print(i, \":\", NN.weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [[-0.89  1.6  -0.08  1.55]\n",
      " [-0.09  1.03  0.57  0.5 ]\n",
      " [ 0.62  1.57  0.32  2.23]\n",
      " [-0.02  0.35 -0.89  0.4 ]\n",
      " [ 0.1   0.44  0.95 -0.73]\n",
      " [ 0.27  0.19  0.57 -0.41]\n",
      " [ 0.05  1.48  2.66 -1.48]\n",
      " [-0.13  0.21  0.05  0.39]\n",
      " [-1.56  1.24  1.31 -1.87]\n",
      " [ 0.54 -0.04 -1.08  0.76]\n",
      " [-0.32 -1.34  1.43 -0.55]\n",
      " [ 1.73  0.75 -1.27  0.77]\n",
      " [ 0.56  0.12 -1.28 -0.34]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "1 : [[-1.48000000e+00  2.70000000e-01 -2.13911649e+01 -8.40000000e-01]\n",
      " [ 1.48000000e+00 -1.94000000e+00  8.10000000e-01 -1.79000000e+00]\n",
      " [ 8.60000000e-01 -4.70000000e-01 -3.20591325e+01 -7.50000000e-01]\n",
      " [-1.63000000e+00  1.08000000e+00 -1.30000000e-01  1.60000000e+00]\n",
      " [ 7.00000000e-02  1.50000000e-01 -1.58499858e+00 -1.51000000e+00]\n",
      " [-2.70000000e-01 -4.00000000e-02 -1.24566055e+01 -9.20000000e-01]\n",
      " [-9.60000000e-01  4.50000000e-01 -1.62647873e+02  2.40000000e-01]\n",
      " [-7.80000000e-01 -1.25000000e+00 -4.37203725e+00 -4.60000000e-01]\n",
      " [ 1.40000000e+00 -9.40000000e-01 -4.35629382e+01  1.00000000e-01]\n",
      " [-1.04000000e+00 -6.70000000e-01 -1.21139653e+03 -9.00000000e-01]\n",
      " [ 4.00000000e-02 -5.80000000e-01 -3.62896396e+01 -6.20000000e-01]\n",
      " [ 2.30000000e-01 -1.00000000e-01 -7.51249836e+01 -1.41000000e+00]\n",
      " [ 3.20000000e-01  2.70000000e-01 -3.86411302e+01 -1.58000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.81928909e+00  0.00000000e+00]]\n",
      "2 : [[ 1.4000000e-01]\n",
      " [-1.5600000e+00]\n",
      " [ 5.8785437e+02]\n",
      " [ 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NN.weights)):\n",
    "    print(i, \":\", NN.weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the weights updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = NN.weights[len(NN.weights)-1]\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = np.reshape(w2, (4, 1))\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
