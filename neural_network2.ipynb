{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data to Test Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"housing/boston_fixed.csv\")\n",
    "X = df.drop(\"MEDV\",axis=1)\n",
    "y = df[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# # add ones to numpy array\n",
    "# ones = [1] * len(X_train)\n",
    "# ones = np.reshape(ones, (len(X_train), 1))\n",
    "# X_train = np.append(X_train, ones, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, nodes, activations, batchSize=50, activationFn=\"relu\", lr=.01):\n",
    "        \n",
    "        self.layers = layers # total number of hidden layers\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        \n",
    "        self.nnodes = [nodes[0], nodes[1], nodes[2]]\n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        \n",
    "        self.activations = activations\n",
    "        # activations[0] and activations[Layers + 1] are left unused\n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "        \n",
    "    def forward_pass(self, X_train, y_train):\n",
    "        \n",
    "        # add ones for bias\n",
    "        X_train[\"ones\"] = 1\n",
    "        \n",
    "        # get batch from the data\n",
    "        batch_slice = np.random.choice(len(X_train), \n",
    "                                       replace = False, \n",
    "                                       size = self.batchSize) \n",
    "        X_batch = X_train.iloc[batch_slice]\n",
    "        y_batch = y_train.iloc[batch_slice]\n",
    "        \n",
    "        # convert to numpy arrays\n",
    "        X_batch = X_batch.to_numpy()\n",
    "        self.y_batch = y_batch.to_numpy()\n",
    "        \n",
    "        \n",
    "        if self.iters == 0:\n",
    "            # initialize weights\n",
    "            M = X_batch.shape[1] # M = number of features\n",
    "            self.weights = self.initialize_weights(M)\n",
    "            \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        self.y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = squared_loss(self.y_pred, y_batch)\n",
    "        self.layer_values[self.layers + 1] = l2_loss\n",
    "        self.iters += 1\n",
    "        \n",
    "        return l2_loss\n",
    "    \n",
    "    def backprop(self):\n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(self.y_pred, self.y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        J = np.dot(J, w_t)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.weights[i] -= self.lr * J_weights[i]\n",
    "        \n",
    "    def predict(self, X_train, y_train):\n",
    "        l2_loss = self.forward_pass(X_train, y_train)\n",
    "        print(\"1st loss\", l2_loss)\n",
    "        \n",
    "        self.backprop()\n",
    "        l2_loss = self.forward_pass(X_train, y_train)\n",
    "        print(\"2nd loss\", l2_loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Nueral Network on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st loss 85918.81227906587\n",
      "2nd loss 49441047135029.0\n"
     ]
    }
   ],
   "source": [
    "layers = 3\n",
    "nodes = [50, 4, 4, 4, 1]\n",
    "activations = [\"relu\", \"relu\", \"relu\"]\n",
    "\n",
    "NN = NeuralNetwork(layers, nodes, activations)\n",
    "NN.predict(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
