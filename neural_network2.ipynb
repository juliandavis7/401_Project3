{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data to Test Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 9, 16]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sq = x ** 2\n",
    "x_sq\n",
    "np.sum(x_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers=None, nodes=None, nnodes=None, \n",
    "                 activations=[], activationFn=\"relu\", batchSize=50, \n",
    "                 lr=.001, lr_type=\"constant\", power_t=.5,\n",
    "                 annealing_rate=.999, max_epoch=200, momentum=.9, \n",
    "                 tol=0.0001, alpha=.0001, shuffle=False, \n",
    "                 early_stopping=False, num_epochs_stop=50):\n",
    "        \n",
    "        if layers != None:\n",
    "            self.layers = layers # total number of hidden layers\n",
    "        else:\n",
    "            self.layers = len(nodes)\n",
    "\n",
    "        # an int array of size [0, ..., Layers + 1]\n",
    "        # Nodes[0] shall represent the input size (typically 50)\n",
    "        # Nodes[Layers + 1] shall represent the output size (typically 1)\n",
    "        # all other Nodes represent the number of nodes (or width) in the hidden layer i\n",
    "        self.nodes = nodes\n",
    "        if nodes != None:\n",
    "            self.nodes.insert(0, batchSize)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # alternative to nodes where each hidden layer of the nueral network is the same size\n",
    "        self.nnodes = nnodes\n",
    "        if nnodes != None:\n",
    "            self.nodes = []\n",
    "            self.nodes.append(batchSize)\n",
    "            for i in range(layers):\n",
    "                self.nodes.append(nnodes)\n",
    "            self.nodes.append(1)\n",
    "        \n",
    "        # activations[i] values are labels indicating the activation function used in layer i\n",
    "        self.activations = activations\n",
    "        self.activationFn = activationFn\n",
    "        if activationFn != \"\":\n",
    "            self.activations = [activationFn] * self.layers\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "        self.lr_type = lr_type\n",
    "        self.power_t = power_t\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.mu = momentum\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if early_stopping == False:\n",
    "            self.num_epochs_stop = max_epoch\n",
    "        else:\n",
    "            self.num_epochs_stop = num_epochs_stop\n",
    "    \n",
    "        self.layer_values = [None] * (self.layers + 2)\n",
    "        self.iters = 0\n",
    "        self.epochs = 0\n",
    "                \n",
    "    def validateHyperParams(self):\n",
    "        \n",
    "        if self.layers != (len(self.nodes) - 2):\n",
    "            raise ValueError(\"layers must be equal to the number of hidden layers, got %s.\" % self.layers)\n",
    "        if self.nnodes != None and self.nnodes <= 0:\n",
    "            raise ValueError(\"nnodes must be > 0, got %s.\" % self.nnodes)\n",
    "        if self.lr <= 0 or self.lr > 1:\n",
    "            raise ValueError(\"lr must be in (0, 1], got %s.\" % self.lr)\n",
    "            \n",
    "        if self.lr_type not in [\"constant\", \"invscaling\", \"annealing\", \"adaptive\"]:\n",
    "            raise ValueError(\"lr_type is not valid\" % self.lr_type\n",
    "                            + \"\\nAvailable lr types: constant, invscaling, adaptive\")\n",
    "            \n",
    "        if self.max_epoch <= 0:\n",
    "            raise ValueError(\"max_iter must be > 0, got %s.\" % self.max_epoch)\n",
    "               \n",
    "        activation_functions = list(ACTIVATIONS.keys())\n",
    "        if self.activationFn != \"\":\n",
    "            if self.activationFn not in activation_functions:\n",
    "                raise ValueError(\"%s is not an activation function\" % self.activationFn\n",
    "                                + \"\\nAvailable activation functions: relu, leaky_relu, sigmoid, tanh\")\n",
    "    \n",
    "    def initialize_weights(self, M):\n",
    "        weights = []\n",
    "        \n",
    "        for i in range(self.layers + 1):\n",
    "            if i == 0:\n",
    "                input_size = M # special case for w1\n",
    "            else:\n",
    "                input_size = self.nodes[i]\n",
    "            output_size = self.nodes[i + 1]\n",
    "            \n",
    "            # Xavier (Glorot) Initialization\n",
    "            if self.activationFn == \"tanh\":\n",
    "                target_variance = 2 / (input_size + output_size)\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # He Initialization\n",
    "            elif self.activationFn == \"relu\":\n",
    "                target_variance = 2 / input_size\n",
    "                w_i = np.random.normal(loc= 0, scale = np.sqrt(target_variance), size=(input_size, output_size))\n",
    "            # Random Uniform\n",
    "            else:\n",
    "                w_i = np.random.uniform(-1/np.sqrt(input_size), 1/np.sqrt(input_size))\n",
    "                #w_i = np.random.normal(size=(input_size, output_size))\n",
    "            w_i = np.round(w_i, 2)\n",
    "            w_i[input_size - 1:] = 0 # initialize bias to 0\n",
    "            weights.append(w_i)\n",
    "        return weights\n",
    "    \n",
    "    # returns the weight term for L2 regularization\n",
    "    def get_weight_term(self):\n",
    "        weight_term = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_term = np.sum(self.weights[i] ** 2)\n",
    "        return weight_term\n",
    "        \n",
    "    def forward_pass(self, X_batch, y_batch):\n",
    "        \n",
    "        self.layer_values[0] = X_batch\n",
    "        \n",
    "        # calculate hidden layers\n",
    "        for i in range(self.layers):\n",
    "            X = self.layer_values[i]\n",
    "            weights = self.weights[i]\n",
    "            h_layer = X.dot(weights)\n",
    "            \n",
    "            # apply activation function\n",
    "            activation_fn = ACTIVATIONS[self.activations[i]]\n",
    "            activation_fn(h_layer)\n",
    "            self.layer_values[i + 1] = h_layer\n",
    "            \n",
    "        \n",
    "        # calculate predictions\n",
    "        X = self.layer_values[self.layers] # values in last hidden layer\n",
    "        weights = self.weights[self.layers]\n",
    "        y_pred = X.dot(weights)\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # calculate the l2 loss\n",
    "        l2_loss = 0\n",
    "        # only need predictions once we have fit the data\n",
    "        if isinstance(y_batch, np.ndarray): \n",
    "            l2_loss = squared_loss(y_pred, y_batch) # l2\n",
    "            weight_term = self.get_weight_term()\n",
    "            l2_loss += self.alpha * weight_term # l2 regularization\n",
    "            self.layer_values[self.layers + 1] = l2_loss\n",
    "        \n",
    "        return l2_loss, y_pred\n",
    "    \n",
    "    \n",
    "    def backward_pass(self, y_pred, y_batch):\n",
    "        \n",
    "        # loss layer\n",
    "        J = squared_loss_derivative(y_pred, y_batch, self.batchSize)\n",
    "        J = np.reshape(J, (len(J), 1))\n",
    "        \n",
    "        J_weights = [None] * (self.layers + 1)\n",
    "        \n",
    "        # output layer\n",
    "        # jacobian w.r.t. weights\n",
    "        x_t = self.layer_values[self.layers].T\n",
    "        J_wi = x_t.dot(J)\n",
    "        J_weights[self.layers] = J_wi\n",
    "        \n",
    "        # update jacobian at output layer\n",
    "        w_t = self.weights[self.layers].T\n",
    "        w_t = np.delete(w_t, w_t.shape[1] - 1, 1) # take out the bias\n",
    "        J = np.dot(J, w_t)\n",
    "        zeros = [0] * len(J)\n",
    "        zeros = np.reshape(zeros, (len(J), 1))\n",
    "        J = np.append(J, zeros, axis=1)\n",
    "        \n",
    "        # iterate through hidden layers backwards\n",
    "        for i in range(self.layers, 0 , -1):\n",
    "            # update jacobian at activation layer\n",
    "            d_activation_fn = DERIVATIVES[self.activations[i - 1]]\n",
    "            d_activation_fn(self.layer_values[i], J)\n",
    "            \n",
    "            # hidden layer\n",
    "            # jacobian w.r.t. weights\n",
    "            x_t = self.layer_values[i - 1].T\n",
    "            J_wi = x_t.dot(J)\n",
    "            J_weights[i - 1] = J_wi\n",
    "            \n",
    "            # jacobian w.r.t. inputs\n",
    "            w_t = self.weights[i - 1].T\n",
    "            w_t = np.delete(w_t, w_t.shape[1] - 1, 1)\n",
    "            J = np.dot(J, w_t)\n",
    "            zeros = [0] * len(J)\n",
    "            zeros = np.reshape(zeros, (len(J), 1))\n",
    "            J = np.append(J, zeros, axis=1)\n",
    "            \n",
    "            \n",
    "        # initialize velocity to 0\n",
    "        if self.epochs == 0 and self.iters == 0:\n",
    "            self.velocity = []\n",
    "            for i in range(len(J_weights)):\n",
    "                n_rows = J_weights[i].shape[0]\n",
    "                n_cols = J_weights[i].shape[1]\n",
    "                vel_i = np.zeros((n_rows, n_cols))\n",
    "                self.velocity.append(vel_i)\n",
    "        \n",
    "        for i in range(len(J_weights)):\n",
    "            self.velocity[i] = self.mu * self.velocity[i] - self.lr * J_weights[i]\n",
    "            self.weights[i] += self.velocity[i]\n",
    "      \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.validateHyperParams()\n",
    "        # convert to numpy arrays\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.to_numpy()\n",
    "            \n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.to_numpy()\n",
    "            \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_train)\n",
    "        ones = np.reshape(ones, (len(X_train), 1))\n",
    "        X_train = np.append(X_train, ones, axis=1)\n",
    "        \n",
    "        # save 10% for validation\n",
    "        val_rows = round(len(X_train) * .1)\n",
    "        X_val = X_train[:val_rows, :]\n",
    "        y_val = y_train[:val_rows]\n",
    "        \n",
    "        X_train = X_train[val_rows:, :]\n",
    "        y_train = y_train[val_rows:]\n",
    "        \n",
    "        # initalize weights on first iteration\n",
    "        M = X_train.shape[1] # M = number of features\n",
    "        self.weights = self.initialize_weights(M)\n",
    "        \n",
    "        best_v_loss = np.inf\n",
    "        n_epoch_no_change = 0 \n",
    "        while (self.epochs < self.max_epoch and n_epoch_no_change <= self.num_epochs_stop):\n",
    "            # ONE EPOCH \n",
    "            last_idx = 0\n",
    "            if self.shuffle == True: # shuffle data after every epoch, if specified \n",
    "                np.random.shuffle(X_train)\n",
    "            while (last_idx < len(X_train)):\n",
    "                first_idx = self.iters * self.batchSize\n",
    "                remaining_rows = len(X_train) - first_idx\n",
    "                last_idx = first_idx + min(self.batchSize, remaining_rows)\n",
    "                X_batch = X_train[first_idx: last_idx, :]\n",
    "                y_batch = y_train[first_idx: last_idx]\n",
    "\n",
    "                loss, y_pred = self.forward_pass(X_batch, y_batch)\n",
    "                self.backward_pass(y_pred, y_batch)\n",
    "                self.iters += 1\n",
    "            \n",
    "            # trainig and validation loss after one epoch\n",
    "            t_loss, y_pred = self.forward_pass(X_train, y_train)\n",
    "            v_loss, y_pred = self.forward_pass(X_val, y_val)\n",
    "            print(\"epoch:\", self.epochs)\n",
    "            print(\"training loss:\", t_loss)\n",
    "            print(\"validation loss:\", v_loss)\n",
    "            \n",
    "            self.iters = 0 # start over, next epoch\n",
    "            self.epochs += 1\n",
    "            \n",
    "            # decrease the learning rate by one of three methods, if specified\n",
    "            if self.lr_type == \"invscaling\":\n",
    "                self.lr = self.lr/pow(self.epochs, self.power_t)\n",
    "            elif self.lr_type == \"annealing\":\n",
    "                self.lr = self.lr * self.annealing_rate\n",
    "            elif self.lr_type == \"adaptive\":\n",
    "                if n_epoch_no_change >= 2: \n",
    "                    self.lr = self.lr/5\n",
    "                \n",
    "            # stops when validation loss doesn't improve for num_epochs_stop\n",
    "            if best_v_loss - v_loss < self.tol: \n",
    "                n_epoch_no_change += 1\n",
    "            else:\n",
    "                n_epoch_no_change = 0\n",
    "            # update best_v_loss\n",
    "            if v_loss < best_v_loss:\n",
    "                best_v_loss = v_loss\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # convert to numpy array\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.to_numpy()\n",
    "        \n",
    "        # add ones for bias\n",
    "        ones = [1] * len(X_test)\n",
    "        ones = np.reshape(ones, (len(X_test), 1))\n",
    "        X_test = np.append(X_test, ones, axis=1)\n",
    "        \n",
    "        loss, y_pred = self.forward_pass(X_test, None)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Nueral Network on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 258.96249394151096\n",
      "validation loss: 179.58621755267373\n",
      "epoch: 1\n",
      "training loss: 158.49421743843573\n",
      "validation loss: 116.93733145476338\n",
      "epoch: 2\n",
      "training loss: 65.69780579409617\n",
      "validation loss: 33.178636085198576\n",
      "epoch: 3\n",
      "training loss: 36.100177093504975\n",
      "validation loss: 17.984436413962502\n",
      "epoch: 4\n",
      "training loss: 26.77662338261293\n",
      "validation loss: 21.317553976781113\n",
      "epoch: 5\n",
      "training loss: 25.03125842963376\n",
      "validation loss: 22.34995920865861\n",
      "epoch: 6\n",
      "training loss: 24.854632593841064\n",
      "validation loss: 7.220059998571435\n",
      "epoch: 7\n",
      "training loss: 25.437037047808857\n",
      "validation loss: 15.511320959313739\n",
      "epoch: 8\n",
      "training loss: 17.72269073318845\n",
      "validation loss: 7.297130710859488\n",
      "epoch: 9\n",
      "training loss: 12.36527579117617\n",
      "validation loss: 6.759317625314018\n",
      "epoch: 10\n",
      "training loss: 10.522415157017361\n",
      "validation loss: 2.504496143144288\n",
      "epoch: 11\n",
      "training loss: 10.659100459490784\n",
      "validation loss: 5.389246004547442\n",
      "epoch: 12\n",
      "training loss: 13.535812871665229\n",
      "validation loss: 6.580664989407846\n",
      "epoch: 13\n",
      "training loss: 14.534450956944287\n",
      "validation loss: 7.67049181179028\n",
      "epoch: 14\n",
      "training loss: 11.904326916721217\n",
      "validation loss: 7.439640298335645\n",
      "epoch: 15\n",
      "training loss: 10.299327879709374\n",
      "validation loss: 4.645638710763957\n",
      "epoch: 16\n",
      "training loss: 9.863999550027208\n",
      "validation loss: 7.19913742399851\n",
      "epoch: 17\n",
      "training loss: 8.301119113417277\n",
      "validation loss: 3.2240146903680817\n",
      "epoch: 18\n",
      "training loss: 7.38674516673305\n",
      "validation loss: 4.470558053924491\n",
      "epoch: 19\n",
      "training loss: 7.246913476806529\n",
      "validation loss: 3.6338972639509026\n",
      "epoch: 20\n",
      "training loss: 7.115734756276141\n",
      "validation loss: 4.637413927079731\n",
      "epoch: 21\n",
      "training loss: 6.477428476840138\n",
      "validation loss: 3.943183931773021\n",
      "epoch: 22\n",
      "training loss: 6.051197387587036\n",
      "validation loss: 4.362227515016459\n",
      "epoch: 23\n",
      "training loss: 6.194433616093534\n",
      "validation loss: 4.731856199108063\n",
      "epoch: 24\n",
      "training loss: 6.864686148227121\n",
      "validation loss: 4.759812928071124\n",
      "epoch: 25\n",
      "training loss: 7.3778944094051635\n",
      "validation loss: 5.778766650646149\n",
      "epoch: 26\n",
      "training loss: 6.8416417987636295\n",
      "validation loss: 4.966893853019479\n",
      "epoch: 27\n",
      "training loss: 5.775846305425245\n",
      "validation loss: 5.628823644367784\n",
      "epoch: 28\n",
      "training loss: 4.861279769379521\n",
      "validation loss: 4.703276491203183\n",
      "epoch: 29\n",
      "training loss: 5.248909628489679\n",
      "validation loss: 5.194521030822106\n",
      "epoch: 30\n",
      "training loss: 7.394252206840926\n",
      "validation loss: 6.6351752156645905\n",
      "epoch: 31\n",
      "training loss: 10.759946380707376\n",
      "validation loss: 9.271127691355936\n",
      "epoch: 32\n",
      "training loss: 10.432330445639892\n",
      "validation loss: 8.465838668588011\n",
      "epoch: 33\n",
      "training loss: 8.596510031298788\n",
      "validation loss: 9.208356535378135\n",
      "epoch: 34\n",
      "training loss: 8.406270029870889\n",
      "validation loss: 5.7558684775030216\n",
      "epoch: 35\n",
      "training loss: 7.697742993526604\n",
      "validation loss: 9.475346307912835\n",
      "epoch: 36\n",
      "training loss: 5.936847959316888\n",
      "validation loss: 4.2909949829046035\n",
      "epoch: 37\n",
      "training loss: 4.55209434747988\n",
      "validation loss: 4.757738569345764\n",
      "epoch: 38\n",
      "training loss: 4.2600550934230474\n",
      "validation loss: 4.376052802784716\n",
      "epoch: 39\n",
      "training loss: 4.164935462578663\n",
      "validation loss: 4.750411219349976\n",
      "epoch: 40\n",
      "training loss: 4.114867394945126\n",
      "validation loss: 4.778095437827683\n",
      "epoch: 41\n",
      "training loss: 4.05653834512111\n",
      "validation loss: 4.569578982974421\n",
      "epoch: 42\n",
      "training loss: 3.9145607191288003\n",
      "validation loss: 4.859931671643941\n",
      "epoch: 43\n",
      "training loss: 3.77346996734961\n",
      "validation loss: 4.554959181503407\n",
      "epoch: 44\n",
      "training loss: 3.5807901365858723\n",
      "validation loss: 4.63352052743161\n",
      "epoch: 45\n",
      "training loss: 3.4859614459890227\n",
      "validation loss: 4.637047703372217\n",
      "epoch: 46\n",
      "training loss: 3.5895026211212637\n",
      "validation loss: 4.436856172887715\n",
      "epoch: 47\n",
      "training loss: 3.8249511973683124\n",
      "validation loss: 5.327403073144833\n",
      "epoch: 48\n",
      "training loss: 4.511483116334521\n",
      "validation loss: 4.89567280158574\n",
      "epoch: 49\n",
      "training loss: 5.11585368013521\n",
      "validation loss: 6.747867392426739\n",
      "epoch: 50\n",
      "training loss: 5.837062407028071\n",
      "validation loss: 5.791737560287483\n",
      "epoch: 51\n",
      "training loss: 6.5114628486513\n",
      "validation loss: 7.864674438876623\n",
      "epoch: 52\n",
      "training loss: 6.835066198703742\n",
      "validation loss: 5.992597653541707\n",
      "epoch: 53\n",
      "training loss: 5.775870283228563\n",
      "validation loss: 7.934568501866275\n",
      "epoch: 54\n",
      "training loss: 5.731790983298034\n",
      "validation loss: 4.927422653370406\n",
      "epoch: 55\n",
      "training loss: 4.276383091736547\n",
      "validation loss: 6.862813447058706\n",
      "epoch: 56\n",
      "training loss: 4.083596348288086\n",
      "validation loss: 4.75719901845506\n",
      "epoch: 57\n",
      "training loss: 3.55301747998765\n",
      "validation loss: 6.410633740943583\n",
      "epoch: 58\n",
      "training loss: 3.353971010942282\n",
      "validation loss: 4.1138186456997134\n",
      "epoch: 59\n",
      "training loss: 3.0986612572908117\n",
      "validation loss: 5.0021772143502865\n",
      "epoch: 60\n",
      "training loss: 3.054086356025555\n",
      "validation loss: 4.447780451620043\n",
      "epoch: 61\n",
      "training loss: 2.9548056293635034\n",
      "validation loss: 4.894021839908928\n",
      "epoch: 62\n",
      "training loss: 2.9346275856075907\n",
      "validation loss: 4.328213421239477\n",
      "epoch: 63\n",
      "training loss: 2.843815220646604\n",
      "validation loss: 4.662839947331559\n",
      "epoch: 64\n",
      "training loss: 2.817689596056862\n",
      "validation loss: 4.3418842124527846\n",
      "epoch: 65\n",
      "training loss: 2.745243817379217\n",
      "validation loss: 4.493685252013237\n",
      "epoch: 66\n",
      "training loss: 2.7153978222085153\n",
      "validation loss: 4.420357852675723\n",
      "epoch: 67\n",
      "training loss: 2.7332865580845014\n",
      "validation loss: 4.302020685549902\n",
      "epoch: 68\n",
      "training loss: 2.7554092754177337\n",
      "validation loss: 4.692056578913796\n",
      "epoch: 69\n",
      "training loss: 2.9784744983127713\n",
      "validation loss: 4.2897010351294025\n",
      "epoch: 70\n",
      "training loss: 3.1487775556729956\n",
      "validation loss: 5.2935478709007695\n",
      "epoch: 71\n",
      "training loss: 3.6155793868664934\n",
      "validation loss: 4.604851116500164\n",
      "epoch: 72\n",
      "training loss: 3.939435834603826\n",
      "validation loss: 6.156607498798153\n",
      "epoch: 73\n",
      "training loss: 4.324419387375763\n",
      "validation loss: 5.089388717037181\n",
      "epoch: 74\n",
      "training loss: 4.373718347234386\n",
      "validation loss: 6.496152456161983\n",
      "epoch: 75\n",
      "training loss: 4.73575009040004\n",
      "validation loss: 4.88677142035036\n",
      "epoch: 76\n",
      "training loss: 4.033632009134677\n",
      "validation loss: 6.86320120970184\n",
      "epoch: 77\n",
      "training loss: 3.880825641504045\n",
      "validation loss: 4.252761430526805\n",
      "epoch: 78\n",
      "training loss: 3.1815718531242436\n",
      "validation loss: 6.151821823212992\n",
      "epoch: 79\n",
      "training loss: 2.944090825832153\n",
      "validation loss: 4.255074117989618\n",
      "epoch: 80\n",
      "training loss: 2.716413680378396\n",
      "validation loss: 5.293384617315448\n",
      "epoch: 81\n",
      "training loss: 2.6626944047778083\n",
      "validation loss: 4.257234747040064\n",
      "epoch: 82\n",
      "training loss: 2.572577571125971\n",
      "validation loss: 4.848253925032915\n",
      "epoch: 83\n",
      "training loss: 2.567196199936418\n",
      "validation loss: 4.2865437363048615\n",
      "epoch: 84\n",
      "training loss: 2.4940679272791564\n",
      "validation loss: 4.750873118279565\n",
      "epoch: 85\n",
      "training loss: 2.487853387235958\n",
      "validation loss: 4.2848258132043\n",
      "epoch: 86\n",
      "training loss: 2.4136858646481385\n",
      "validation loss: 4.599871898960485\n",
      "epoch: 87\n",
      "training loss: 2.394904599917796\n",
      "validation loss: 4.317414691533111\n",
      "epoch: 88\n",
      "training loss: 2.335504832651271\n",
      "validation loss: 4.420688628965727\n",
      "epoch: 89\n",
      "training loss: 2.315705349203825\n",
      "validation loss: 4.425476981271965\n",
      "epoch: 90\n",
      "training loss: 2.3490884970950985\n",
      "validation loss: 4.2562147505178825\n",
      "epoch: 91\n",
      "training loss: 2.410176614337169\n",
      "validation loss: 4.767327011785751\n",
      "epoch: 92\n",
      "training loss: 2.6963194899328142\n",
      "validation loss: 4.332372167159157\n",
      "epoch: 93\n",
      "training loss: 3.009967940444159\n",
      "validation loss: 5.603629317810032\n",
      "epoch: 94\n",
      "training loss: 3.6410706359906087\n",
      "validation loss: 4.879625360959503\n",
      "epoch: 95\n",
      "training loss: 4.326819443084229\n",
      "validation loss: 6.999045536059311\n",
      "epoch: 96\n",
      "training loss: 4.891045582413539\n",
      "validation loss: 5.654320685969264\n",
      "epoch: 97\n",
      "training loss: 5.1607306563720075\n",
      "validation loss: 7.806863304721516\n",
      "epoch: 98\n",
      "training loss: 5.749845555336874\n",
      "validation loss: 5.397899455083782\n",
      "epoch: 99\n",
      "training loss: 4.941771049322727\n",
      "validation loss: 8.827566719675646\n",
      "epoch: 100\n",
      "training loss: 4.574443506806345\n",
      "validation loss: 4.886494564369794\n",
      "epoch: 101\n",
      "training loss: 3.6252170729811293\n",
      "validation loss: 7.630484688502316\n",
      "epoch: 102\n",
      "training loss: 3.2932797095124973\n",
      "validation loss: 4.554108979604125\n",
      "epoch: 103\n",
      "training loss: 3.1130376020763575\n",
      "validation loss: 6.476574176973989\n",
      "epoch: 104\n",
      "training loss: 3.1613104727151953\n",
      "validation loss: 4.651954594236216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 105\n",
      "training loss: 3.162801253743333\n",
      "validation loss: 6.45104525394246\n",
      "epoch: 106\n",
      "training loss: 3.2099505490888514\n",
      "validation loss: 4.634431664532554\n",
      "epoch: 107\n",
      "training loss: 3.1115018527936025\n",
      "validation loss: 6.453214046098934\n",
      "epoch: 108\n",
      "training loss: 3.0782749835114087\n",
      "validation loss: 4.535663393601894\n",
      "epoch: 109\n",
      "training loss: 2.8901297080506185\n",
      "validation loss: 6.166347157517038\n",
      "epoch: 110\n",
      "training loss: 2.8116969450004383\n",
      "validation loss: 4.367649856263496\n",
      "epoch: 111\n",
      "training loss: 2.602331034016876\n",
      "validation loss: 5.724474107870559\n",
      "epoch: 112\n",
      "training loss: 2.512408081441212\n",
      "validation loss: 4.251242783782784\n",
      "epoch: 113\n",
      "training loss: 2.34348109883928\n",
      "validation loss: 5.245894998356762\n",
      "epoch: 114\n",
      "training loss: 2.275859907094574\n",
      "validation loss: 4.221642360899642\n",
      "epoch: 115\n",
      "training loss: 2.1616153539260354\n",
      "validation loss: 4.84993510956824\n",
      "epoch: 116\n",
      "training loss: 2.1170470935580914\n",
      "validation loss: 4.253891667768144\n",
      "epoch: 117\n",
      "training loss: 2.0514538690399773\n",
      "validation loss: 4.563041253771768\n",
      "epoch: 118\n",
      "training loss: 2.0266044144685997\n",
      "validation loss: 4.3459282898286125\n",
      "epoch: 119\n",
      "training loss: 2.0139067121991308\n",
      "validation loss: 4.3834246795141985\n",
      "epoch: 120\n",
      "training loss: 2.014553627387262\n",
      "validation loss: 4.519069383192483\n",
      "epoch: 121\n",
      "training loss: 2.066467342623664\n",
      "validation loss: 4.3273678604398675\n",
      "epoch: 122\n",
      "training loss: 2.0990629428279135\n",
      "validation loss: 4.781103728939733\n",
      "epoch: 123\n",
      "training loss: 2.205110075758446\n",
      "validation loss: 4.405697213823821\n",
      "epoch: 124\n",
      "training loss: 2.247180894758895\n",
      "validation loss: 5.053568642906084\n",
      "epoch: 125\n",
      "training loss: 2.323531341289561\n",
      "validation loss: 4.559999545015818\n",
      "epoch: 126\n",
      "training loss: 2.294290382011384\n",
      "validation loss: 5.097052237542394\n",
      "epoch: 127\n",
      "training loss: 2.2493614597397418\n",
      "validation loss: 4.587192198313832\n",
      "epoch: 128\n",
      "training loss: 2.1330154630224873\n",
      "validation loss: 4.8763255043575695\n",
      "epoch: 129\n",
      "training loss: 2.0515426775215753\n",
      "validation loss: 4.290716263607213\n",
      "epoch: 130\n",
      "training loss: 1.9378726063046394\n",
      "validation loss: 4.814668853749857\n",
      "epoch: 131\n",
      "training loss: 1.9328635246732202\n",
      "validation loss: 4.45446055105392\n",
      "epoch: 132\n",
      "training loss: 2.038735395614191\n",
      "validation loss: 4.586822795103032\n",
      "epoch: 133\n",
      "training loss: 2.226285217108992\n",
      "validation loss: 5.192807526599052\n",
      "epoch: 134\n",
      "training loss: 2.50736511467802\n",
      "validation loss: 4.82109685264973\n",
      "epoch: 135\n",
      "training loss: 2.7779466606631735\n",
      "validation loss: 5.998602064178556\n",
      "epoch: 136\n",
      "training loss: 2.9558603648254627\n",
      "validation loss: 5.158895770271314\n",
      "epoch: 137\n",
      "training loss: 3.0692381448124713\n",
      "validation loss: 6.431738733591228\n",
      "epoch: 138\n",
      "training loss: 3.0324575292668046\n",
      "validation loss: 5.054750675397933\n",
      "epoch: 139\n",
      "training loss: 2.8182298703960393\n",
      "validation loss: 6.364749397492957\n",
      "epoch: 140\n",
      "training loss: 2.6181348377161275\n",
      "validation loss: 4.474309724368175\n",
      "epoch: 141\n",
      "training loss: 2.2626573610708065\n",
      "validation loss: 6.072711009280488\n",
      "epoch: 142\n",
      "training loss: 2.046318355621337\n",
      "validation loss: 4.468360356538737\n",
      "epoch: 143\n",
      "training loss: 1.881053406505343\n",
      "validation loss: 5.133719352337444\n",
      "epoch: 144\n",
      "training loss: 1.8195793655902461\n",
      "validation loss: 4.544856595360251\n",
      "epoch: 145\n",
      "training loss: 1.767539154639139\n",
      "validation loss: 4.7703141079245786\n",
      "epoch: 146\n",
      "training loss: 1.7427418381741129\n",
      "validation loss: 4.5958429551918085\n",
      "epoch: 147\n",
      "training loss: 1.7235334945072898\n",
      "validation loss: 4.656499787356507\n",
      "epoch: 148\n",
      "training loss: 1.7154712477457943\n",
      "validation loss: 4.69031584515406\n",
      "epoch: 149\n",
      "training loss: 1.7231294292458925\n",
      "validation loss: 4.643292510537052\n",
      "epoch: 150\n",
      "training loss: 1.731359165738092\n",
      "validation loss: 4.831804533660061\n",
      "epoch: 151\n",
      "training loss: 1.75996450456921\n",
      "validation loss: 4.703625156246859\n",
      "epoch: 152\n",
      "training loss: 1.7786277497291179\n",
      "validation loss: 4.966189763567796\n",
      "epoch: 153\n",
      "training loss: 1.8042155629861398\n",
      "validation loss: 4.814228222003495\n",
      "epoch: 154\n",
      "training loss: 1.8129317362061428\n",
      "validation loss: 5.027180681958967\n",
      "epoch: 155\n",
      "training loss: 1.7945458035931825\n",
      "validation loss: 4.919468687468549\n",
      "epoch: 156\n",
      "training loss: 1.764414620778668\n",
      "validation loss: 4.951648471074101\n",
      "epoch: 157\n",
      "training loss: 1.7038552760447843\n",
      "validation loss: 4.934485603502948\n",
      "epoch: 158\n",
      "training loss: 1.647760558750255\n",
      "validation loss: 4.857900356042945\n",
      "epoch: 159\n",
      "training loss: 1.6196621233741106\n",
      "validation loss: 4.901411288213963\n",
      "epoch: 160\n",
      "training loss: 1.6466768402917948\n",
      "validation loss: 4.910788523104448\n",
      "epoch: 161\n",
      "training loss: 1.7792236671069404\n",
      "validation loss: 5.174818104261603\n",
      "epoch: 162\n",
      "training loss: 2.0002688125598143\n",
      "validation loss: 5.1938871223171175\n",
      "epoch: 163\n",
      "training loss: 2.3410489977236217\n",
      "validation loss: 5.833400337774681\n",
      "epoch: 164\n",
      "training loss: 2.5985636565945356\n",
      "validation loss: 5.675615753804917\n",
      "epoch: 165\n",
      "training loss: 3.0025398917758865\n",
      "validation loss: 6.57839667679273\n",
      "epoch: 166\n",
      "training loss: 2.9345864287545735\n",
      "validation loss: 5.919862015515475\n",
      "epoch: 167\n",
      "training loss: 2.989905312022646\n",
      "validation loss: 6.747957699895807\n",
      "epoch: 168\n",
      "training loss: 2.7303002687621256\n",
      "validation loss: 5.344495847988715\n",
      "epoch: 169\n",
      "training loss: 2.4368598891195807\n",
      "validation loss: 6.647955049364214\n",
      "epoch: 170\n",
      "training loss: 2.107116280855711\n",
      "validation loss: 4.756267799616954\n",
      "epoch: 171\n",
      "training loss: 1.7936091086633268\n",
      "validation loss: 5.8034891436234455\n",
      "epoch: 172\n",
      "training loss: 1.6414832569964732\n",
      "validation loss: 4.799416705301093\n",
      "epoch: 173\n",
      "training loss: 1.5602802646190184\n",
      "validation loss: 5.1108779553197605\n",
      "epoch: 174\n",
      "training loss: 1.5292549993490054\n",
      "validation loss: 4.854369969266874\n",
      "epoch: 175\n",
      "training loss: 1.5136206159287533\n",
      "validation loss: 4.950591157574784\n",
      "epoch: 176\n",
      "training loss: 1.5122306767933986\n",
      "validation loss: 4.965038054552204\n",
      "epoch: 177\n",
      "training loss: 1.5081556412497645\n",
      "validation loss: 4.977870114384168\n",
      "epoch: 178\n",
      "training loss: 1.5127292363379534\n",
      "validation loss: 5.047847082319078\n",
      "epoch: 179\n",
      "training loss: 1.5037623658589776\n",
      "validation loss: 5.040976982316537\n",
      "epoch: 180\n",
      "training loss: 1.5015173660952172\n",
      "validation loss: 5.099976576690149\n",
      "epoch: 181\n",
      "training loss: 1.478697447683853\n",
      "validation loss: 5.103776849700762\n",
      "epoch: 182\n",
      "training loss: 1.464058339741413\n",
      "validation loss: 5.1019426660053675\n",
      "epoch: 183\n",
      "training loss: 1.4345666684219285\n",
      "validation loss: 5.150107570282503\n",
      "epoch: 184\n",
      "training loss: 1.4174328353175483\n",
      "validation loss: 5.0975079792083475\n",
      "epoch: 185\n",
      "training loss: 1.4074440690782228\n",
      "validation loss: 5.198188062159324\n",
      "epoch: 186\n",
      "training loss: 1.4110411073556208\n",
      "validation loss: 5.156097974440218\n",
      "epoch: 187\n",
      "training loss: 1.4435587437138142\n",
      "validation loss: 5.278058081031581\n",
      "epoch: 188\n",
      "training loss: 1.4738124102400496\n",
      "validation loss: 5.30094997642497\n",
      "epoch: 189\n",
      "training loss: 1.5356386010580712\n",
      "validation loss: 5.371989586467222\n",
      "epoch: 190\n",
      "training loss: 1.5508396244548028\n",
      "validation loss: 5.464020848769246\n",
      "epoch: 191\n",
      "training loss: 1.5955316837414666\n",
      "validation loss: 5.390528764166729\n",
      "epoch: 192\n",
      "training loss: 1.5424089433171149\n",
      "validation loss: 5.554659361385884\n",
      "epoch: 193\n",
      "training loss: 1.5319356432077293\n",
      "validation loss: 5.2763882953161065\n",
      "epoch: 194\n",
      "training loss: 1.4338638353051465\n",
      "validation loss: 5.53084453097691\n",
      "epoch: 195\n",
      "training loss: 1.3895743504691476\n",
      "validation loss: 5.1848130018242164\n",
      "epoch: 196\n",
      "training loss: 1.3629959927378392\n",
      "validation loss: 5.4129886236935425\n",
      "epoch: 197\n",
      "training loss: 1.3856608399763903\n",
      "validation loss: 5.360490588864738\n",
      "epoch: 198\n",
      "training loss: 1.5060321198036517\n",
      "validation loss: 5.468654801866204\n",
      "epoch: 199\n",
      "training loss: 1.6192390021881513\n",
      "validation loss: 5.688152173990274\n",
      "epoch: 200\n",
      "training loss: 1.7947861609187206\n",
      "validation loss: 5.67394865749592\n",
      "epoch: 201\n",
      "training loss: 1.8002880826433283\n",
      "validation loss: 5.904544523381015\n",
      "epoch: 202\n",
      "training loss: 1.8825880732212998\n",
      "validation loss: 5.6794923776261195\n",
      "epoch: 203\n",
      "training loss: 1.691919960501685\n",
      "validation loss: 5.841357532646213\n",
      "epoch: 204\n",
      "training loss: 1.620116868157173\n",
      "validation loss: 5.424176497792557\n",
      "epoch: 205\n",
      "training loss: 1.416848018624451\n",
      "validation loss: 5.47969399918677\n",
      "epoch: 206\n",
      "training loss: 1.3260120211146798\n",
      "validation loss: 5.330753957036307\n",
      "epoch: 207\n",
      "training loss: 1.3551927165572013\n",
      "validation loss: 5.228054613605321\n",
      "epoch: 208\n",
      "training loss: 1.4838725711752143\n",
      "validation loss: 5.625885681645292\n",
      "epoch: 209\n",
      "training loss: 1.7628178183843186\n",
      "validation loss: 5.616004960575686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 210\n",
      "training loss: 1.905105650348697\n",
      "validation loss: 6.031308919921277\n",
      "epoch: 211\n",
      "training loss: 2.1145541492143565\n",
      "validation loss: 5.965204511927546\n",
      "epoch: 212\n",
      "training loss: 1.9504560281695804\n",
      "validation loss: 6.052540312953463\n",
      "epoch: 213\n",
      "training loss: 1.9176568761697212\n",
      "validation loss: 5.845859460287438\n",
      "epoch: 214\n",
      "training loss: 1.6125793655808296\n",
      "validation loss: 5.624071122122277\n",
      "epoch: 215\n",
      "training loss: 1.445436557244531\n",
      "validation loss: 5.504776566107526\n",
      "epoch: 216\n",
      "training loss: 1.3027677677527096\n",
      "validation loss: 5.103099160715209\n",
      "epoch: 217\n",
      "training loss: 1.2898209732248151\n",
      "validation loss: 5.480320396226983\n",
      "epoch: 218\n",
      "training loss: 1.4465165556809436\n",
      "validation loss: 5.275756301659673\n",
      "epoch: 219\n",
      "training loss: 1.6052389333921107\n",
      "validation loss: 5.766783958338969\n",
      "epoch: 220\n",
      "training loss: 1.835921111104104\n",
      "validation loss: 5.726025065787502\n",
      "epoch: 221\n",
      "training loss: 1.8120410405874257\n",
      "validation loss: 5.963041182733669\n",
      "epoch: 222\n",
      "training loss: 1.8494129698272581\n",
      "validation loss: 5.840843230255642\n",
      "epoch: 223\n",
      "training loss: 1.6217031962416513\n",
      "validation loss: 5.731778528792714\n",
      "epoch: 224\n",
      "training loss: 1.4990183637917174\n",
      "validation loss: 5.582121156782283\n",
      "epoch: 225\n",
      "training loss: 1.3093407577847986\n",
      "validation loss: 5.294284897625649\n",
      "epoch: 226\n",
      "training loss: 1.226360362404799\n",
      "validation loss: 5.430394397898096\n",
      "epoch: 227\n",
      "training loss: 1.2484919242420092\n",
      "validation loss: 5.175315134977904\n",
      "epoch: 228\n",
      "training loss: 1.333356645732124\n",
      "validation loss: 5.575434470325382\n",
      "epoch: 229\n",
      "training loss: 1.5081247356939869\n",
      "validation loss: 5.482382027790942\n",
      "epoch: 230\n",
      "training loss: 1.5680033950188388\n",
      "validation loss: 5.829701533800275\n",
      "epoch: 231\n",
      "training loss: 1.6579091901675356\n",
      "validation loss: 5.691151347503979\n",
      "epoch: 232\n",
      "training loss: 1.5350866884407992\n",
      "validation loss: 5.792181593326227\n",
      "epoch: 233\n",
      "training loss: 1.4742596901328686\n",
      "validation loss: 5.582809991039036\n",
      "epoch: 234\n",
      "training loss: 1.297445931435369\n",
      "validation loss: 5.497772699288043\n",
      "epoch: 235\n",
      "training loss: 1.2080850936445198\n",
      "validation loss: 5.426388939167166\n",
      "epoch: 236\n",
      "training loss: 1.1689900360460905\n",
      "validation loss: 5.274130606208303\n",
      "epoch: 237\n",
      "training loss: 1.2029337093742596\n",
      "validation loss: 5.530974333857425\n",
      "epoch: 238\n",
      "training loss: 1.3312672627129085\n",
      "validation loss: 5.425286539495714\n",
      "epoch: 239\n",
      "training loss: 1.4146199141284468\n",
      "validation loss: 5.790254997832622\n",
      "epoch: 240\n",
      "training loss: 1.5257278416443043\n",
      "validation loss: 5.645510452128854\n",
      "epoch: 241\n",
      "training loss: 1.463114136884028\n",
      "validation loss: 5.8506565415493315\n",
      "epoch: 242\n",
      "training loss: 1.4385072566007089\n",
      "validation loss: 5.608258168981176\n",
      "epoch: 243\n",
      "training loss: 1.2782145094695487\n",
      "validation loss: 5.637052930951871\n",
      "epoch: 244\n",
      "training loss: 1.1941205443494987\n",
      "validation loss: 5.457568263690115\n",
      "epoch: 245\n",
      "training loss: 1.1272237554296085\n",
      "validation loss: 5.397270895275454\n",
      "epoch: 246\n",
      "training loss: 1.1345548841708133\n",
      "validation loss: 5.534350927276339\n",
      "epoch: 247\n",
      "training loss: 1.2335656004192277\n",
      "validation loss: 5.448973421655493\n",
      "epoch: 248\n",
      "training loss: 1.3178150270049787\n",
      "validation loss: 5.792113844279888\n",
      "epoch: 249\n",
      "training loss: 1.432523852501111\n",
      "validation loss: 5.654437326874536\n",
      "epoch: 250\n",
      "training loss: 1.398842630937901\n",
      "validation loss: 5.8935070777194225\n",
      "epoch: 251\n",
      "training loss: 1.3900832584636205\n",
      "validation loss: 5.638876528606144\n",
      "epoch: 252\n",
      "training loss: 1.2440142884225638\n",
      "validation loss: 5.7228072528346985\n",
      "epoch: 253\n",
      "training loss: 1.1663399730253514\n",
      "validation loss: 5.4940611296998885\n",
      "epoch: 254\n",
      "training loss: 1.0923297895108661\n",
      "validation loss: 5.4950285067569995\n",
      "epoch: 255\n",
      "training loss: 1.0909339634995594\n",
      "validation loss: 5.559894310716872\n",
      "epoch: 256\n",
      "training loss: 1.1759282419293542\n",
      "validation loss: 5.506364787621073\n",
      "epoch: 257\n",
      "training loss: 1.2541341147135259\n",
      "validation loss: 5.81508176806669\n",
      "epoch: 258\n",
      "training loss: 1.361293793089351\n",
      "validation loss: 5.68448889416687\n",
      "epoch: 259\n",
      "training loss: 1.3345219336228245\n",
      "validation loss: 5.92041781126576\n",
      "epoch: 260\n",
      "training loss: 1.3254650287161833\n",
      "validation loss: 5.660752048560684\n",
      "epoch: 261\n",
      "training loss: 1.1908681608200682\n",
      "validation loss: 5.768648704062486\n",
      "epoch: 262\n",
      "training loss: 1.1197157071889863\n",
      "validation loss: 5.523994563712006\n",
      "epoch: 263\n",
      "training loss: 1.0559477781260846\n",
      "validation loss: 5.568813750147534\n",
      "epoch: 264\n",
      "training loss: 1.0592197578394464\n",
      "validation loss: 5.603826604228838\n",
      "epoch: 265\n",
      "training loss: 1.1420535609868177\n",
      "validation loss: 5.581015478830085\n",
      "epoch: 266\n",
      "training loss: 1.2106526197422556\n",
      "validation loss: 5.852693050695565\n",
      "epoch: 267\n",
      "training loss: 1.3001543468658097\n",
      "validation loss: 5.723149393588478\n",
      "epoch: 268\n",
      "training loss: 1.2637996600039554\n",
      "validation loss: 5.933010577214979\n",
      "epoch: 269\n",
      "training loss: 1.2432186706368944\n",
      "validation loss: 5.669105051624619\n",
      "epoch: 270\n",
      "training loss: 1.1212590200784986\n",
      "validation loss: 5.789515863481886\n",
      "epoch: 271\n",
      "training loss: 1.0599934524040229\n",
      "validation loss: 5.551676806176015\n",
      "epoch: 272\n",
      "training loss: 1.0214936439339646\n",
      "validation loss: 5.634722353742317\n",
      "epoch: 273\n",
      "training loss: 1.037733821824037\n",
      "validation loss: 5.667512466684129\n",
      "epoch: 274\n",
      "training loss: 1.1217160154862662\n",
      "validation loss: 5.667806767808017\n",
      "epoch: 275\n",
      "training loss: 1.1728010280459695\n",
      "validation loss: 5.895094255174039\n",
      "epoch: 276\n",
      "training loss: 1.2334045515719836\n",
      "validation loss: 5.757806452702367\n",
      "epoch: 277\n",
      "training loss: 1.1788192799326442\n",
      "validation loss: 5.930883716027457\n",
      "epoch: 278\n",
      "training loss: 1.1438592699114876\n",
      "validation loss: 5.665915801642532\n",
      "epoch: 279\n",
      "training loss: 1.044010970292904\n",
      "validation loss: 5.8044650316069415\n",
      "epoch: 280\n",
      "training loss: 1.0008975041337567\n",
      "validation loss: 5.592426319073532\n",
      "epoch: 281\n",
      "training loss: 0.9968360085920822\n",
      "validation loss: 5.712847213043156\n",
      "epoch: 282\n",
      "training loss: 1.0246288531286403\n",
      "validation loss: 5.748222208759995\n",
      "epoch: 283\n",
      "training loss: 1.099605337454717\n",
      "validation loss: 5.75876723294983\n",
      "epoch: 284\n",
      "training loss: 1.1204583246881783\n",
      "validation loss: 5.925510658349866\n",
      "epoch: 285\n",
      "training loss: 1.1442312848003335\n",
      "validation loss: 5.7761729574417995\n",
      "epoch: 286\n",
      "training loss: 1.0775686325461766\n",
      "validation loss: 5.920350506621536\n",
      "epoch: 287\n",
      "training loss: 1.0379681754566705\n",
      "validation loss: 5.667923301765976\n",
      "epoch: 288\n",
      "training loss: 0.9759581335384632\n",
      "validation loss: 5.839870865421918\n",
      "epoch: 289\n",
      "training loss: 0.9580604064904518\n",
      "validation loss: 5.663597298355802\n",
      "epoch: 290\n",
      "training loss: 0.981792272430086\n",
      "validation loss: 5.814299759625092\n",
      "epoch: 291\n",
      "training loss: 1.004787704040031\n",
      "validation loss: 5.827138508337287\n",
      "epoch: 292\n",
      "training loss: 1.0513348732079393\n",
      "validation loss: 5.836694123500216\n",
      "epoch: 293\n",
      "training loss: 1.0375396009624902\n",
      "validation loss: 5.932210962111494\n",
      "epoch: 294\n",
      "training loss: 1.0316648253018856\n",
      "validation loss: 5.786390895699572\n",
      "epoch: 295\n",
      "training loss: 0.9766637751512889\n",
      "validation loss: 5.9246657556673465\n",
      "epoch: 296\n",
      "training loss: 0.9504596184175141\n",
      "validation loss: 5.710220751031443\n",
      "epoch: 297\n",
      "training loss: 0.9316615273165009\n",
      "validation loss: 5.915278506983468\n",
      "epoch: 298\n",
      "training loss: 0.9304001676432382\n",
      "validation loss: 5.761868223944064\n",
      "epoch: 299\n",
      "training loss: 0.9554163183659594\n",
      "validation loss: 5.923733572785174\n",
      "epoch: 300\n",
      "training loss: 0.9550481582385851\n",
      "validation loss: 5.87765373375284\n",
      "epoch: 301\n",
      "training loss: 0.9674304401208285\n",
      "validation loss: 5.899688668030808\n",
      "epoch: 302\n",
      "training loss: 0.9400713126769389\n",
      "validation loss: 5.932761022798335\n",
      "epoch: 303\n",
      "training loss: 0.9287508288546245\n",
      "validation loss: 5.837814719074277\n",
      "epoch: 304\n",
      "training loss: 0.9049407487553447\n",
      "validation loss: 5.963406434856181\n",
      "epoch: 305\n",
      "training loss: 0.896585139299701\n",
      "validation loss: 5.817213299117955\n",
      "epoch: 306\n",
      "training loss: 0.8988086323848942\n",
      "validation loss: 6.0024301972570235\n",
      "epoch: 307\n",
      "training loss: 0.8946724852493884\n",
      "validation loss: 5.863773957557808\n",
      "epoch: 308\n",
      "training loss: 0.902834433882405\n",
      "validation loss: 6.012968383511615\n",
      "epoch: 309\n",
      "training loss: 0.8898159554684457\n",
      "validation loss: 5.918131857853984\n",
      "epoch: 310\n",
      "training loss: 0.8890242960435947\n",
      "validation loss: 5.989431477657984\n",
      "epoch: 311\n",
      "training loss: 0.8742430483732689\n",
      "validation loss: 5.959566027595444\n",
      "epoch: 312\n",
      "training loss: 0.8703243611102097\n",
      "validation loss: 5.970555632503344\n",
      "epoch: 313\n",
      "training loss: 0.864318834787137\n",
      "validation loss: 6.001035286455433\n",
      "epoch: 314\n",
      "training loss: 0.8604729060276842\n",
      "validation loss: 5.9778903282511076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 315\n",
      "training loss: 0.8598697366406598\n",
      "validation loss: 6.033981498384876\n",
      "epoch: 316\n",
      "training loss: 0.8542596896152657\n",
      "validation loss: 6.0006613293915505\n",
      "epoch: 317\n",
      "training loss: 0.8533564614852497\n",
      "validation loss: 6.051803785133269\n",
      "epoch: 318\n",
      "training loss: 0.8473831058623703\n",
      "validation loss: 6.025760050696484\n",
      "epoch: 319\n",
      "training loss: 0.8453121368778138\n",
      "validation loss: 6.064738109320017\n",
      "epoch: 320\n",
      "training loss: 0.840913764671134\n",
      "validation loss: 6.048257484606348\n",
      "epoch: 321\n",
      "training loss: 0.8381170546813442\n",
      "validation loss: 6.081614046433825\n",
      "epoch: 322\n",
      "training loss: 0.8348467587629377\n",
      "validation loss: 6.065515407422839\n",
      "epoch: 323\n",
      "training loss: 0.8317228751002668\n",
      "validation loss: 6.10300495993467\n",
      "epoch: 324\n",
      "training loss: 0.8285818959671946\n",
      "validation loss: 6.07735728016246\n",
      "epoch: 325\n",
      "training loss: 0.8257064059161854\n",
      "validation loss: 6.126979099182169\n",
      "epoch: 326\n",
      "training loss: 0.8222953208990632\n",
      "validation loss: 6.085100268905269\n",
      "epoch: 327\n",
      "training loss: 0.820000806898431\n",
      "validation loss: 6.153094809632431\n",
      "epoch: 328\n",
      "training loss: 0.8163197935139165\n",
      "validation loss: 6.088266004291123\n",
      "epoch: 329\n",
      "training loss: 0.8147479776809985\n",
      "validation loss: 6.183555515759868\n",
      "epoch: 330\n",
      "training loss: 0.8109835676801423\n",
      "validation loss: 6.0836508563977025\n",
      "epoch: 331\n",
      "training loss: 0.810566163314145\n",
      "validation loss: 6.223562216840112\n",
      "epoch: 332\n",
      "training loss: 0.8071341648668853\n",
      "validation loss: 6.065706932692047\n",
      "epoch: 333\n",
      "training loss: 0.8091756144690059\n",
      "validation loss: 6.282058968762144\n",
      "epoch: 334\n",
      "training loss: 0.8072828943722942\n",
      "validation loss: 6.0266261676372554\n",
      "epoch: 335\n",
      "training loss: 0.8151050327050311\n",
      "validation loss: 6.373337639724315\n",
      "epoch: 336\n",
      "training loss: 0.8182547668157346\n",
      "validation loss: 5.958739134362075\n",
      "epoch: 337\n",
      "training loss: 0.8388694080004201\n",
      "validation loss: 6.513460946707968\n",
      "epoch: 338\n",
      "training loss: 0.854363455151414\n",
      "validation loss: 5.869080138757499\n",
      "epoch: 339\n",
      "training loss: 0.8928038556861198\n",
      "validation loss: 6.683017993092065\n",
      "epoch: 340\n",
      "training loss: 0.9241863385310823\n",
      "validation loss: 5.804882429655159\n",
      "epoch: 341\n",
      "training loss: 0.9541304291927503\n",
      "validation loss: 6.7441955496025905\n",
      "epoch: 342\n",
      "training loss: 0.9730674270238846\n",
      "validation loss: 5.79134380318717\n",
      "epoch: 343\n",
      "training loss: 0.9525562679790668\n",
      "validation loss: 6.589462506762444\n",
      "epoch: 344\n",
      "training loss: 0.9276091542929666\n",
      "validation loss: 5.759904493625494\n",
      "epoch: 345\n",
      "training loss: 0.8813741808536287\n",
      "validation loss: 6.376426221569767\n",
      "epoch: 346\n",
      "training loss: 0.8435197228358815\n",
      "validation loss: 5.7536073924305615\n",
      "epoch: 347\n",
      "training loss: 0.8282037794431193\n",
      "validation loss: 6.22364498145737\n",
      "epoch: 348\n",
      "training loss: 0.8113071327661409\n",
      "validation loss: 5.839843047844693\n",
      "epoch: 349\n",
      "training loss: 0.8210459682262348\n",
      "validation loss: 6.153934082752816\n",
      "epoch: 350\n",
      "training loss: 0.8002901005817493\n",
      "validation loss: 5.9334290334799755\n",
      "epoch: 351\n",
      "training loss: 0.796277070722305\n",
      "validation loss: 6.09445150818149\n",
      "epoch: 352\n",
      "training loss: 0.7717167656123225\n",
      "validation loss: 5.985292082977558\n",
      "epoch: 353\n",
      "training loss: 0.7657208721129647\n",
      "validation loss: 6.057856382204737\n",
      "epoch: 354\n",
      "training loss: 0.7584756385993735\n",
      "validation loss: 6.033961040940623\n",
      "epoch: 355\n",
      "training loss: 0.7554183787933875\n",
      "validation loss: 6.073891591996464\n",
      "epoch: 356\n",
      "training loss: 0.7540370757167362\n",
      "validation loss: 6.063643062437032\n",
      "epoch: 357\n",
      "training loss: 0.7496493845828692\n",
      "validation loss: 6.109822857286066\n",
      "epoch: 358\n",
      "training loss: 0.7470629093854596\n",
      "validation loss: 6.073095811128071\n",
      "epoch: 359\n",
      "training loss: 0.7442525156707583\n",
      "validation loss: 6.144059496312287\n",
      "epoch: 360\n",
      "training loss: 0.7408659085328528\n",
      "validation loss: 6.081479741493209\n",
      "epoch: 361\n",
      "training loss: 0.7394428959673817\n",
      "validation loss: 6.172581247671853\n",
      "epoch: 362\n",
      "training loss: 0.7357916869861315\n",
      "validation loss: 6.088015028985576\n",
      "epoch: 363\n",
      "training loss: 0.7349728149209016\n",
      "validation loss: 6.202699134148893\n",
      "epoch: 364\n",
      "training loss: 0.7316643317664926\n",
      "validation loss: 6.084220081602573\n",
      "epoch: 365\n",
      "training loss: 0.7319141282306149\n",
      "validation loss: 6.244069958732405\n",
      "epoch: 366\n",
      "training loss: 0.7292994466279595\n",
      "validation loss: 6.064131237611737\n",
      "epoch: 367\n",
      "training loss: 0.7320758601626323\n",
      "validation loss: 6.30452509726957\n",
      "epoch: 368\n",
      "training loss: 0.7313029808054179\n",
      "validation loss: 6.022295531773017\n",
      "epoch: 369\n",
      "training loss: 0.7398405691303486\n",
      "validation loss: 6.395299108245289\n",
      "epoch: 370\n",
      "training loss: 0.7443110450523762\n",
      "validation loss: 5.952556228780705\n",
      "epoch: 371\n",
      "training loss: 0.765150882108771\n",
      "validation loss: 6.529823886575431\n",
      "epoch: 372\n",
      "training loss: 0.781283039628528\n",
      "validation loss: 5.86038015490375\n",
      "epoch: 373\n",
      "training loss: 0.8197442636713643\n",
      "validation loss: 6.6923364456791035\n",
      "epoch: 374\n",
      "training loss: 0.8500215348067381\n",
      "validation loss: 5.7861290197156725\n",
      "epoch: 375\n",
      "training loss: 0.8843251612939178\n",
      "validation loss: 6.767595275739198\n",
      "epoch: 376\n",
      "training loss: 0.9039317893809368\n",
      "validation loss: 5.763317921198125\n",
      "epoch: 377\n",
      "training loss: 0.8896042321066703\n",
      "validation loss: 6.629563647653933\n",
      "epoch: 378\n",
      "training loss: 0.8627383809204634\n",
      "validation loss: 5.736071026526257\n",
      "epoch: 379\n",
      "training loss: 0.8183049511590925\n",
      "validation loss: 6.390917363609746\n",
      "epoch: 380\n",
      "training loss: 0.7773364456915393\n",
      "validation loss: 5.731364205852468\n",
      "epoch: 381\n",
      "training loss: 0.7629791486457276\n",
      "validation loss: 6.214861343583315\n",
      "epoch: 382\n",
      "training loss: 0.7441063483297958\n",
      "validation loss: 5.817307205733437\n",
      "epoch: 383\n",
      "training loss: 0.7485665175595922\n",
      "validation loss: 6.1263622473586254\n",
      "epoch: 384\n",
      "training loss: 0.7245236792222389\n",
      "validation loss: 5.905223081413991\n",
      "epoch: 385\n",
      "training loss: 0.7169617336774391\n",
      "validation loss: 6.061084855956038\n",
      "epoch: 386\n",
      "training loss: 0.697771277675823\n",
      "validation loss: 5.959719351802679\n",
      "epoch: 387\n",
      "training loss: 0.6938138361915278\n",
      "validation loss: 6.042835585633012\n",
      "epoch: 388\n",
      "training loss: 0.6897305231571508\n",
      "validation loss: 6.002912130132145\n",
      "epoch: 389\n",
      "training loss: 0.6871160740798994\n",
      "validation loss: 6.073113670894109\n",
      "epoch: 390\n",
      "training loss: 0.6846029817862813\n",
      "validation loss: 6.018897633358714\n",
      "epoch: 391\n",
      "training loss: 0.6823571358031357\n",
      "validation loss: 6.112210314827295\n",
      "epoch: 392\n",
      "training loss: 0.6790705491853177\n",
      "validation loss: 6.025874123123125\n",
      "epoch: 393\n",
      "training loss: 0.678262970658769\n",
      "validation loss: 6.142982945486667\n",
      "epoch: 394\n",
      "training loss: 0.6747040640888076\n",
      "validation loss: 6.033231166369788\n",
      "epoch: 395\n",
      "training loss: 0.6744865942987683\n",
      "validation loss: 6.171199513730799\n",
      "epoch: 396\n",
      "training loss: 0.6713636580463685\n",
      "validation loss: 6.031784981085825\n",
      "epoch: 397\n",
      "training loss: 0.6721096341962098\n",
      "validation loss: 6.209327582093436\n",
      "epoch: 398\n",
      "training loss: 0.6696070072479251\n",
      "validation loss: 6.01538219948537\n",
      "epoch: 399\n",
      "training loss: 0.6724064063216786\n",
      "validation loss: 6.262453087689228\n",
      "epoch: 400\n",
      "training loss: 0.6711375948102855\n",
      "validation loss: 5.982085498365745\n",
      "epoch: 401\n",
      "training loss: 0.6779580378619383\n",
      "validation loss: 6.33522873047074\n",
      "epoch: 402\n",
      "training loss: 0.6797963419977182\n",
      "validation loss: 5.928642206514948\n",
      "epoch: 403\n",
      "training loss: 0.6942066862200827\n",
      "validation loss: 6.434859558265875\n",
      "epoch: 404\n",
      "training loss: 0.70224618982104\n",
      "validation loss: 5.856411610267572\n",
      "epoch: 405\n",
      "training loss: 0.7277060575015073\n",
      "validation loss: 6.55466509827598\n",
      "epoch: 406\n",
      "training loss: 0.7434594865146098\n",
      "validation loss: 5.784616047651446\n",
      "epoch: 407\n",
      "training loss: 0.7733059263162145\n",
      "validation loss: 6.6423626616132445\n",
      "epoch: 408\n",
      "training loss: 0.7878463213260948\n",
      "validation loss: 5.743226608457491\n",
      "epoch: 409\n",
      "training loss: 0.7980676415398356\n",
      "validation loss: 6.611693881814575\n",
      "epoch: 410\n",
      "training loss: 0.7903558075470485\n",
      "validation loss: 5.7258038010059344\n",
      "epoch: 411\n",
      "training loss: 0.7714954247183167\n",
      "validation loss: 6.462598931360074\n",
      "epoch: 412\n",
      "training loss: 0.7415141065765437\n",
      "validation loss: 5.7164398551665805\n",
      "epoch: 413\n",
      "training loss: 0.7211097435061812\n",
      "validation loss: 6.295913892318331\n",
      "epoch: 414\n",
      "training loss: 0.6949059004703607\n",
      "validation loss: 5.749743505933408\n",
      "epoch: 415\n",
      "training loss: 0.6896301690708193\n",
      "validation loss: 6.180913876056825\n",
      "epoch: 416\n",
      "training loss: 0.6698290204049538\n",
      "validation loss: 5.81555918297298\n",
      "epoch: 417\n",
      "training loss: 0.6664154094617308\n",
      "validation loss: 6.110986714135308\n",
      "epoch: 418\n",
      "training loss: 0.6494868477877025\n",
      "validation loss: 5.870535562929983\n",
      "epoch: 419\n",
      "training loss: 0.6469418419148929\n",
      "validation loss: 6.081424692171473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 420\n",
      "training loss: 0.6385879194247289\n",
      "validation loss: 5.90873312427484\n",
      "epoch: 421\n",
      "training loss: 0.6382347327811188\n",
      "validation loss: 6.094236382542136\n",
      "epoch: 422\n",
      "training loss: 0.6336350962183401\n",
      "validation loss: 5.926827612196892\n",
      "epoch: 423\n",
      "training loss: 0.6340915650951026\n",
      "validation loss: 6.124876414549368\n",
      "epoch: 424\n",
      "training loss: 0.6299177066298308\n",
      "validation loss: 5.932646906983209\n",
      "epoch: 425\n",
      "training loss: 0.6312163626244133\n",
      "validation loss: 6.154971944794799\n",
      "epoch: 426\n",
      "training loss: 0.6273544725867621\n",
      "validation loss: 5.932577691202224\n",
      "epoch: 427\n",
      "training loss: 0.6295991349335801\n",
      "validation loss: 6.186827253653306\n",
      "epoch: 428\n",
      "training loss: 0.6265456086461004\n",
      "validation loss: 5.922675966785823\n",
      "epoch: 429\n",
      "training loss: 0.6304149592923852\n",
      "validation loss: 6.228574433587841\n",
      "epoch: 430\n",
      "training loss: 0.6283599313547508\n",
      "validation loss: 5.900449338858287\n",
      "epoch: 431\n",
      "training loss: 0.6347671824795169\n",
      "validation loss: 6.28083835032336\n",
      "epoch: 432\n",
      "training loss: 0.6341599525212138\n",
      "validation loss: 5.867297551962031\n",
      "epoch: 433\n",
      "training loss: 0.6440810411993368\n",
      "validation loss: 6.34099249522018\n",
      "epoch: 434\n",
      "training loss: 0.6455203869737972\n",
      "validation loss: 5.825984235316264\n",
      "epoch: 435\n",
      "training loss: 0.65923437435877\n",
      "validation loss: 6.402805036263632\n",
      "epoch: 436\n",
      "training loss: 0.6622656599935043\n",
      "validation loss: 5.783274477814426\n",
      "epoch: 437\n",
      "training loss: 0.6773801320618962\n",
      "validation loss: 6.448512015683008\n",
      "epoch: 438\n",
      "training loss: 0.6790954795417935\n",
      "validation loss: 5.749245223637616\n",
      "epoch: 439\n",
      "training loss: 0.6898669709246146\n",
      "validation loss: 6.453601960427523\n",
      "epoch: 440\n",
      "training loss: 0.6854104225159988\n",
      "validation loss: 5.729347257659642\n",
      "epoch: 441\n",
      "training loss: 0.6873833966966642\n",
      "validation loss: 6.408818676166951\n",
      "epoch: 442\n",
      "training loss: 0.6746710589663171\n",
      "validation loss: 5.7230833040137306\n",
      "epoch: 443\n",
      "training loss: 0.6703743405071101\n",
      "validation loss: 6.33255422451795\n",
      "epoch: 444\n",
      "training loss: 0.6534599708231863\n",
      "validation loss: 5.732158733030932\n",
      "epoch: 445\n",
      "training loss: 0.6489425395451119\n",
      "validation loss: 6.254663150194941\n",
      "epoch: 446\n",
      "training loss: 0.6328443263544004\n",
      "validation loss: 5.756336042961755\n",
      "epoch: 447\n",
      "training loss: 0.6304712300071854\n",
      "validation loss: 6.19455757412821\n",
      "epoch: 448\n",
      "training loss: 0.6170660369490613\n",
      "validation loss: 5.786520555304029\n",
      "epoch: 449\n",
      "training loss: 0.6164388041036952\n",
      "validation loss: 6.158652086788053\n",
      "epoch: 450\n",
      "training loss: 0.6062647510670265\n",
      "validation loss: 5.812860764155428\n",
      "epoch: 451\n",
      "training loss: 0.6070161514231328\n",
      "validation loss: 6.1463044553270905\n",
      "epoch: 452\n",
      "training loss: 0.5995077349227024\n",
      "validation loss: 5.830597916592392\n",
      "epoch: 453\n",
      "training loss: 0.6012406740504402\n",
      "validation loss: 6.150717551433084\n",
      "epoch: 454\n",
      "training loss: 0.5952710876386135\n",
      "validation loss: 5.8396558651379555\n",
      "epoch: 455\n",
      "training loss: 0.5977437922428604\n",
      "validation loss: 6.163867899178785\n",
      "epoch: 456\n",
      "training loss: 0.5926982463497987\n",
      "validation loss: 5.841641699706399\n",
      "epoch: 457\n",
      "training loss: 0.5959358425452677\n",
      "validation loss: 6.182221914707827\n",
      "epoch: 458\n",
      "training loss: 0.5916262631843257\n",
      "validation loss: 5.83711146531747\n",
      "epoch: 459\n",
      "training loss: 0.595805192591317\n",
      "validation loss: 6.205270673004559\n",
      "epoch: 460\n",
      "training loss: 0.5921144656564217\n",
      "validation loss: 5.826393042991638\n",
      "epoch: 461\n",
      "training loss: 0.597403052189341\n",
      "validation loss: 6.2317315602657155\n",
      "epoch: 462\n",
      "training loss: 0.5942251405237169\n",
      "validation loss: 5.810612130346863\n",
      "epoch: 463\n",
      "training loss: 0.6006688551690733\n",
      "validation loss: 6.259160784805448\n",
      "epoch: 464\n",
      "training loss: 0.5978397577205852\n",
      "validation loss: 5.7914677484397705\n",
      "epoch: 465\n",
      "training loss: 0.6052111559702603\n",
      "validation loss: 6.284275095347665\n",
      "epoch: 466\n",
      "training loss: 0.6023274094627866\n",
      "validation loss: 5.77128469101064\n",
      "epoch: 467\n",
      "training loss: 0.6099644259941241\n",
      "validation loss: 6.302432448305433\n",
      "epoch: 468\n",
      "training loss: 0.6063255337713972\n",
      "validation loss: 5.752907630169525\n",
      "epoch: 469\n",
      "training loss: 0.6132121102659053\n",
      "validation loss: 6.308751058702489\n",
      "epoch: 470\n",
      "training loss: 0.6080247221617422\n",
      "validation loss: 5.7388634785112425\n",
      "epoch: 471\n",
      "training loss: 0.6132513567236404\n",
      "validation loss: 6.300713450902213\n",
      "epoch: 472\n",
      "training loss: 0.6060679934090349\n",
      "validation loss: 5.730728399883159\n",
      "epoch: 473\n",
      "training loss: 0.6093845916485119\n",
      "validation loss: 6.279866352295927\n",
      "epoch: 474\n",
      "training loss: 0.6004750810429614\n",
      "validation loss: 5.729067682936916\n",
      "epoch: 475\n",
      "training loss: 0.6023909222140675\n",
      "validation loss: 6.25141641727568\n",
      "epoch: 476\n",
      "training loss: 0.5926178039859722\n",
      "validation loss: 5.733362441511414\n",
      "epoch: 477\n",
      "training loss: 0.5939403797533438\n",
      "validation loss: 6.221928010300946\n",
      "epoch: 478\n",
      "training loss: 0.5842425825221567\n",
      "validation loss: 5.741894493813513\n",
      "epoch: 479\n",
      "training loss: 0.5855914284836286\n",
      "validation loss: 6.196738764949385\n",
      "epoch: 480\n",
      "training loss: 0.5765977740282799\n",
      "validation loss: 5.752170599619332\n",
      "epoch: 481\n",
      "training loss: 0.5782798856500151\n",
      "validation loss: 6.178705618642395\n",
      "epoch: 482\n",
      "training loss: 0.5702382530318015\n",
      "validation loss: 5.761816877441945\n",
      "epoch: 483\n",
      "training loss: 0.5723489296463892\n",
      "validation loss: 6.168323507624231\n",
      "epoch: 484\n",
      "training loss: 0.5652329636048986\n",
      "validation loss: 5.769241813069982\n",
      "epoch: 485\n",
      "training loss: 0.5677759935232761\n",
      "validation loss: 6.164591493003405\n",
      "epoch: 486\n",
      "training loss: 0.5614373210716126\n",
      "validation loss: 5.773703840198012\n",
      "epoch: 487\n",
      "training loss: 0.5644011958797148\n",
      "validation loss: 6.166031205161829\n",
      "epoch: 488\n",
      "training loss: 0.5586741851124851\n",
      "validation loss: 5.775019406431094\n",
      "epoch: 489\n",
      "training loss: 0.5620577385213646\n",
      "validation loss: 6.171265845407581\n",
      "epoch: 490\n",
      "training loss: 0.5567929998192391\n",
      "validation loss: 5.77331652852104\n",
      "epoch: 491\n",
      "training loss: 0.5605968573687529\n",
      "validation loss: 6.179066387115099\n",
      "epoch: 492\n",
      "training loss: 0.5556599318754415\n",
      "validation loss: 5.768945207505742\n",
      "epoch: 493\n",
      "training loss: 0.5598662764341192\n",
      "validation loss: 6.188238896159047\n",
      "epoch: 494\n",
      "training loss: 0.5551264526250383\n",
      "validation loss: 5.7624417010069635\n",
      "epoch: 495\n",
      "training loss: 0.5596780856271115\n",
      "validation loss: 6.197561221245139\n",
      "epoch: 496\n",
      "training loss: 0.554994008498428\n",
      "validation loss: 5.754501809564086\n",
      "epoch: 497\n",
      "training loss: 0.5597786639585474\n",
      "validation loss: 6.205763170110886\n",
      "epoch: 498\n",
      "training loss: 0.5549918247246854\n",
      "validation loss: 5.745947082951579\n",
      "epoch: 499\n",
      "training loss: 0.5598425831901845\n",
      "validation loss: 6.211602695752561\n",
      "epoch: 500\n",
      "training loss: 0.5547886183416227\n",
      "validation loss: 5.737647340322797\n",
      "epoch: 501\n",
      "training loss: 0.5595094942376575\n",
      "validation loss: 6.214067594373168\n",
      "epoch: 502\n",
      "training loss: 0.554048323018732\n",
      "validation loss: 5.730406584650862\n",
      "epoch: 503\n",
      "training loss: 0.5584616109480546\n",
      "validation loss: 6.212617432628011\n",
      "epoch: 504\n",
      "training loss: 0.5525177998845313\n",
      "validation loss: 5.724848814874362\n",
      "epoch: 505\n",
      "training loss: 0.5565143140139719\n",
      "validation loss: 6.207356877052179\n",
      "epoch: 506\n",
      "training loss: 0.5501073007114698\n",
      "validation loss: 5.7213278728416235\n",
      "epoch: 507\n",
      "training loss: 0.5536732463400068\n",
      "validation loss: 6.19904660642532\n",
      "epoch: 508\n",
      "training loss: 0.546916151836629\n",
      "validation loss: 5.719881319546479\n",
      "epoch: 509\n",
      "training loss: 0.5501223262791822\n",
      "validation loss: 6.188911594183008\n",
      "epoch: 510\n",
      "training loss: 0.5431867966165828\n",
      "validation loss: 5.720243653336748\n",
      "epoch: 511\n",
      "training loss: 0.5461504923238354\n",
      "validation loss: 6.1783219132699365\n",
      "epoch: 512\n",
      "training loss: 0.5392164345842329\n",
      "validation loss: 5.721919354371684\n",
      "epoch: 513\n",
      "training loss: 0.5420603790873423\n",
      "validation loss: 6.1684839962967\n",
      "epoch: 514\n",
      "training loss: 0.5352745115088757\n",
      "validation loss: 5.72429925718707\n",
      "epoch: 515\n",
      "training loss: 0.538100647407998\n",
      "validation loss: 6.160244716599167\n",
      "epoch: 516\n",
      "training loss: 0.5315568384910764\n",
      "validation loss: 5.72678466307722\n",
      "epoch: 517\n",
      "training loss: 0.5344379510098943\n",
      "validation loss: 6.154039223537982\n",
      "epoch: 518\n",
      "training loss: 0.5281778522804389\n",
      "validation loss: 5.728879664032624\n",
      "epoch: 519\n",
      "training loss: 0.5311605284867043\n",
      "validation loss: 6.1499488705157415\n",
      "epoch: 520\n",
      "training loss: 0.5251851902079007\n",
      "validation loss: 5.730234694480752\n",
      "epoch: 521\n",
      "training loss: 0.528296263199689\n",
      "validation loss: 6.147804337654574\n",
      "epoch: 522\n",
      "training loss: 0.5225798784167632\n",
      "validation loss: 5.730650697771321\n",
      "epoch: 523\n",
      "training loss: 0.5258316506383472\n",
      "validation loss: 6.147282621843784\n",
      "epoch: 524\n",
      "training loss: 0.5203326954090413\n",
      "validation loss: 5.730062369823364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 525\n",
      "training loss: 0.5237256189367039\n",
      "validation loss: 6.14797786261535\n",
      "epoch: 526\n",
      "training loss: 0.5183942365959223\n",
      "validation loss: 5.728514657336024\n",
      "epoch: 527\n",
      "training loss: 0.5219176317267104\n",
      "validation loss: 6.1494472191945375\n",
      "epoch: 528\n",
      "training loss: 0.516700084626703\n",
      "validation loss: 5.72613930549866\n",
      "epoch: 529\n",
      "training loss: 0.5203321818151168\n",
      "validation loss: 6.151241740276402\n",
      "epoch: 530\n",
      "training loss: 0.5151738584723958\n",
      "validation loss: 5.723132408091905\n",
      "epoch: 531\n",
      "training loss: 0.5188824954598545\n",
      "validation loss: 6.152932855126489\n",
      "epoch: 532\n",
      "training loss: 0.5137307868883839\n",
      "validation loss: 5.719731510356127\n",
      "epoch: 533\n",
      "training loss: 0.5174757343708195\n",
      "validation loss: 6.154140582773331\n",
      "epoch: 534\n",
      "training loss: 0.5122835501995416\n",
      "validation loss: 5.716191258032578\n",
      "epoch: 535\n",
      "training loss: 0.5160208012777544\n",
      "validation loss: 6.154563952514389\n",
      "epoch: 536\n",
      "training loss: 0.5107507701320563\n",
      "validation loss: 5.712757946572525\n",
      "epoch: 537\n",
      "training loss: 0.5144383054145455\n",
      "validation loss: 6.1540095042335405\n",
      "epoch: 538\n",
      "training loss: 0.5090669211989161\n",
      "validation loss: 5.709644855786331\n",
      "epoch: 539\n",
      "training loss: 0.5126706832444843\n",
      "validation loss: 6.152410580722241\n",
      "epoch: 540\n",
      "training loss: 0.5071911536966734\n",
      "validation loss: 5.707011432885735\n",
      "epoch: 541\n",
      "training loss: 0.5106896257549826\n",
      "validation loss: 6.1498302281363\n",
      "epoch: 542\n",
      "training loss: 0.5051122531268127\n",
      "validation loss: 5.704949487798552\n",
      "epoch: 543\n",
      "training loss: 0.5084983935651247\n",
      "validation loss: 6.146444366583394\n",
      "epoch: 544\n",
      "training loss: 0.5028480012285359\n",
      "validation loss: 5.703478649293843\n",
      "epoch: 545\n",
      "training loss: 0.5061282026502208\n",
      "validation loss: 6.142507601580616\n",
      "epoch: 546\n",
      "training loss: 0.5004391253755027\n",
      "validation loss: 5.70255172933283\n",
      "epoch: 547\n",
      "training loss: 0.503629877738244\n",
      "validation loss: 6.138309259914741\n",
      "epoch: 548\n",
      "training loss: 0.49793983671027636\n",
      "validation loss: 5.7020686307192125\n",
      "epoch: 549\n",
      "training loss: 0.5010633679013227\n",
      "validation loss: 6.134129685345582\n",
      "epoch: 550\n",
      "training loss: 0.4954077522167344\n",
      "validation loss: 5.7018956729916415\n",
      "epoch: 551\n",
      "training loss: 0.4984878893156583\n",
      "validation loss: 6.130205470152502\n",
      "epoch: 552\n",
      "training loss: 0.4928955652518854\n",
      "validation loss: 5.701886386536129\n",
      "epoch: 553\n",
      "training loss: 0.49595458790446095\n",
      "validation loss: 6.126708327766379\n",
      "epoch: 554\n",
      "training loss: 0.4904456691596205\n",
      "validation loss: 5.701900152045231\n",
      "epoch: 555\n",
      "training loss: 0.4935023636257186\n",
      "validation loss: 6.123737886492116\n",
      "epoch: 556\n",
      "training loss: 0.48808775909233093\n",
      "validation loss: 5.701816308246392\n",
      "epoch: 557\n",
      "training loss: 0.4911564881663983\n",
      "validation loss: 6.121325477346982\n",
      "epoch: 558\n",
      "training loss: 0.48583870815879476\n",
      "validation loss: 5.701542905489137\n",
      "epoch: 559\n",
      "training loss: 0.4889291650205264\n",
      "validation loss: 6.1194447527013685\n",
      "epoch: 560\n",
      "training loss: 0.4837038178237982\n",
      "validation loss: 5.701020501438391\n",
      "epoch: 561\n",
      "training loss: 0.48682116779314627\n",
      "validation loss: 6.118025384204619\n",
      "epoch: 562\n",
      "training loss: 0.48167870224466136\n",
      "validation loss: 5.7002220205596945\n",
      "epoch: 563\n",
      "training loss: 0.48482392911664707\n",
      "validation loss: 6.116967252679803\n",
      "epoch: 564\n",
      "training loss: 0.47975135553473686\n",
      "validation loss: 5.699149819447263\n",
      "epoch: 565\n",
      "training loss: 0.4829217392892597\n",
      "validation loss: 6.1161537433240705\n",
      "epoch: 566\n",
      "training loss: 0.4779042106161129\n",
      "validation loss: 5.6978309249680095\n",
      "epoch: 567\n",
      "training loss: 0.4810939322917721\n",
      "validation loss: 6.1154636129933575\n",
      "epoch: 568\n",
      "training loss: 0.4761161561411185\n",
      "validation loss: 5.696311165308325\n",
      "epoch: 569\n",
      "training loss: 0.47931704531190605\n",
      "validation loss: 6.114781287437647\n",
      "epoch: 570\n",
      "training loss: 0.4743645284581017\n",
      "validation loss: 5.694648731008919\n",
      "epoch: 571\n",
      "training loss: 0.47756694934700944\n",
      "validation loss: 6.114005496877879\n",
      "epoch: 572\n",
      "training loss: 0.4726270671195843\n",
      "validation loss: 5.692907607741643\n",
      "epoch: 573\n",
      "training loss: 0.47582089866221283\n",
      "validation loss: 6.113056050502475\n",
      "epoch: 574\n",
      "training loss: 0.4708837547472498\n",
      "validation loss: 5.691151296075917\n",
      "epoch: 575\n",
      "training loss: 0.47405937740649917\n",
      "validation loss: 6.111878429579579\n",
      "epoch: 576\n",
      "training loss: 0.46911839731082494\n",
      "validation loss: 5.689437234607352\n",
      "epoch: 577\n",
      "training loss: 0.4722675735858894\n",
      "validation loss: 6.1104458702819295\n",
      "epoch: 578\n",
      "training loss: 0.4673197735245887\n",
      "validation loss: 5.687812319350006\n",
      "epoch: 579\n",
      "training loss: 0.47043631182809037\n",
      "validation loss: 6.108758764584654\n",
      "epoch: 580\n",
      "training loss: 0.4654822096634203\n",
      "validation loss: 5.686309836892027\n",
      "epoch: 581\n",
      "training loss: 0.46856233382838924\n",
      "validation loss: 6.106841496658247\n",
      "epoch: 582\n",
      "training loss: 0.4636055150929307\n",
      "validation loss: 5.684947997131942\n",
      "epoch: 583\n",
      "training loss: 0.46664791483571166\n",
      "validation loss: 6.104737177826514\n",
      "epoch: 584\n",
      "training loss: 0.4616943203365927\n",
      "validation loss: 5.683730075063407\n",
      "epoch: 585\n",
      "training loss: 0.4646999149453584\n",
      "validation loss: 6.102501041461469\n",
      "epoch: 586\n",
      "training loss: 0.45975695911674247\n",
      "validation loss: 5.682645984234064\n",
      "epoch: 587\n",
      "training loss: 0.46272845054112427\n",
      "validation loss: 6.1001934136814135\n",
      "epoch: 588\n",
      "training loss: 0.45780409865997473\n",
      "validation loss: 5.6816749473629775\n",
      "epoch: 589\n",
      "training loss: 0.4607454108241051\n",
      "validation loss: 6.0978731496123695\n",
      "epoch: 590\n",
      "training loss: 0.4558473344869877\n",
      "validation loss: 5.68078883201013\n",
      "epoch: 591\n",
      "training loss: 0.458763032168795\n",
      "validation loss: 6.095592238827385\n",
      "epoch: 592\n",
      "training loss: 0.45389793158205366\n",
      "validation loss: 5.679955697820976\n",
      "epoch: 593\n",
      "training loss: 0.45679269138102796\n",
      "validation loss: 6.093391999071474\n",
      "epoch: 594\n",
      "training loss: 0.45196583172266097\n",
      "validation loss: 5.67914315228539\n",
      "epoch: 595\n",
      "training loss: 0.4548440096782441\n",
      "validation loss: 6.09130097832632\n",
      "epoch: 596\n",
      "training loss: 0.4500589789323294\n",
      "validation loss: 5.678321210877005\n",
      "epoch: 597\n",
      "training loss: 0.4529242932103223\n",
      "validation loss: 6.089334438931821\n",
      "epoch: 598\n",
      "training loss: 0.44818295863278174\n",
      "validation loss: 5.677464476745975\n",
      "epoch: 599\n",
      "training loss: 0.451038286505485\n",
      "validation loss: 6.08749513726086\n",
      "epoch: 600\n",
      "training loss: 0.4463409095119054\n",
      "validation loss: 5.6765535689680995\n",
      "epoch: 601\n",
      "training loss: 0.4491881868399851\n",
      "validation loss: 6.085775043126099\n",
      "epoch: 602\n",
      "training loss: 0.44453365040347526\n",
      "validation loss: 5.675575818866729\n",
      "epoch: 603\n",
      "training loss: 0.4473738574437942\n",
      "validation loss: 6.084157645040287\n",
      "epoch: 604\n",
      "training loss: 0.442759962495826\n",
      "validation loss: 5.674525315571493\n",
      "epoch: 605\n",
      "training loss: 0.44559317945407406\n",
      "validation loss: 6.082620531127049\n",
      "epoch: 606\n",
      "training loss: 0.44101697342942614\n",
      "validation loss: 5.6734024169782\n",
      "epoch: 607\n",
      "training loss: 0.44384249033273226\n",
      "validation loss: 6.0811379964097565\n",
      "epoch: 608\n",
      "training loss: 0.43930059871536076\n",
      "validation loss: 5.672212856454391\n",
      "epoch: 609\n",
      "training loss: 0.44211706530494854\n",
      "validation loss: 6.079683487694305\n",
      "epoch: 610\n",
      "training loss: 0.4376060039494135\n",
      "validation loss: 5.6709665765912485\n",
      "epoch: 611\n",
      "training loss: 0.44041160573866633\n",
      "validation loss: 6.078231748393147\n",
      "epoch: 612\n",
      "training loss: 0.43592805727557316\n",
      "validation loss: 5.6696764146666165\n",
      "epoch: 613\n",
      "training loss: 0.4387207038251742\n",
      "validation loss: 6.0767605673121405\n",
      "epoch: 614\n",
      "training loss: 0.4342617457728943\n",
      "validation loss: 5.668356753306336\n",
      "epoch: 615\n",
      "training loss: 0.437039257154036\n",
      "validation loss: 6.075252070255558\n",
      "epoch: 616\n",
      "training loss: 0.43260253304085466\n",
      "validation loss: 5.667022235916041\n",
      "epoch: 617\n",
      "training loss: 0.43536281102053226\n",
      "validation loss: 6.07369352473979\n",
      "epoch: 618\n",
      "training loss: 0.4309466395018318\n",
      "validation loss: 5.665686630000121\n",
      "epoch: 619\n",
      "training loss: 0.4336878116588436\n",
      "validation loss: 6.072077659409132\n",
      "epoch: 620\n",
      "training loss: 0.4292912326059404\n",
      "validation loss: 5.664361902213564\n",
      "epoch: 621\n",
      "training loss: 0.4320117604906192\n",
      "validation loss: 6.070402531024323\n",
      "epoch: 622\n",
      "training loss: 0.42763452131127316\n",
      "validation loss: 5.6630575474269\n",
      "epoch: 623\n",
      "training loss: 0.4303332676378284\n",
      "validation loss: 6.0686710012126905\n",
      "epoch: 624\n",
      "training loss: 0.4259757573249478\n",
      "validation loss: 5.661780191048944\n",
      "epoch: 625\n",
      "training loss: 0.42865201150061844\n",
      "validation loss: 6.066889909840763\n",
      "epoch: 626\n",
      "training loss: 0.4243151535259579\n",
      "validation loss: 5.660533460821983\n",
      "epoch: 627\n",
      "training loss: 0.42696861891432575\n",
      "validation loss: 6.065069048294201\n",
      "epoch: 628\n",
      "training loss: 0.4226537365540498\n",
      "validation loss: 5.659318103445577\n",
      "epoch: 629\n",
      "training loss: 0.42528448613817305\n",
      "validation loss: 6.063220041730976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 630\n",
      "training loss: 0.42099315478308996\n",
      "validation loss: 5.658132304539509\n",
      "epoch: 631\n",
      "training loss: 0.4236015639506988\n",
      "validation loss: 6.061355244182353\n",
      "epoch: 632\n",
      "training loss: 0.41933546431854546\n",
      "validation loss: 5.656972159055782\n",
      "epoch: 633\n",
      "training loss: 0.4219221302185132\n",
      "validation loss: 6.0594867352540795\n",
      "epoch: 634\n",
      "training loss: 0.41768291437092125\n",
      "validation loss: 5.655832234130544\n",
      "epoch: 635\n",
      "training loss: 0.42024857082516887\n",
      "validation loss: 6.057625485007321\n",
      "epoch: 636\n",
      "training loss: 0.41603774992986686\n",
      "validation loss: 5.654706167253846\n",
      "epoch: 637\n",
      "training loss: 0.4185831855471302\n",
      "validation loss: 6.055780728202166\n",
      "epoch: 638\n",
      "training loss: 0.4144020449294249\n",
      "validation loss: 5.653587248569169\n",
      "epoch: 639\n",
      "training loss: 0.4169280302401971\n",
      "validation loss: 6.0539595638807\n",
      "epoch: 640\n",
      "training loss: 0.4127775739446531\n",
      "validation loss: 5.65246894571177\n",
      "epoch: 641\n",
      "training loss: 0.4152848014156389\n",
      "validation loss: 6.052166774241866\n",
      "epoch: 642\n",
      "training loss: 0.41116572562823933\n",
      "validation loss: 5.6513453411110515\n",
      "epoch: 643\n",
      "training loss: 0.41365476459120487\n",
      "validation loss: 6.050404839831511\n",
      "epoch: 644\n",
      "training loss: 0.40956745706865394\n",
      "validation loss: 5.650211463587437\n",
      "epoch: 645\n",
      "training loss: 0.41203872406951736\n",
      "validation loss: 6.0486741167582885\n",
      "epoch: 646\n",
      "training loss: 0.40798328526288463\n",
      "validation loss: 5.64906350718985\n",
      "epoch: 647\n",
      "training loss: 0.4104370291599513\n",
      "validation loss: 6.046973135789273\n",
      "epoch: 648\n",
      "training loss: 0.40641330995053615\n",
      "validation loss: 5.647898939653363\n",
      "epoch: 649\n",
      "training loss: 0.40884961025647454\n",
      "validation loss: 6.045298981940902\n",
      "epoch: 650\n",
      "training loss: 0.4048572610184576\n",
      "validation loss: 5.6467165102238415\n",
      "epoch: 651\n",
      "training loss: 0.407276037438275\n",
      "validation loss: 6.043647715309411\n",
      "epoch: 652\n",
      "training loss: 0.4033145633664585\n",
      "validation loss: 5.6455161718283655\n",
      "epoch: 653\n",
      "training loss: 0.40571559417336317\n",
      "validation loss: 6.042014798345617\n",
      "epoch: 654\n",
      "training loss: 0.40178441232107215\n",
      "validation loss: 5.644298935744222\n",
      "epoch: 655\n",
      "training loss: 0.4041673590833192\n",
      "validation loss: 6.04039550059488\n",
      "epoch: 656\n",
      "training loss: 0.40026585323113684\n",
      "validation loss: 5.6430666783290055\n",
      "epoch: 657\n",
      "training loss: 0.40263028941601864\n",
      "validation loss: 6.038785258289849\n",
      "epoch: 658\n",
      "training loss: 0.39875785965828564\n",
      "validation loss: 5.641821919327504\n",
      "epoch: 659\n",
      "training loss: 0.4011033007692339\n",
      "validation loss: 6.037179972682063\n",
      "epoch: 660\n",
      "training loss: 0.3972594055000538\n",
      "validation loss: 5.640567590032088\n",
      "epoch: 661\n",
      "training loss: 0.3995853386341201\n",
      "validation loss: 6.035576237255534\n",
      "epoch: 662\n",
      "training loss: 0.3957695274017115\n",
      "validation loss: 5.63930680746512\n",
      "epoch: 663\n",
      "training loss: 0.3980754384309096\n",
      "validation loss: 6.033971489718233\n",
      "epoch: 664\n",
      "training loss: 0.39428737488348026\n",
      "validation loss: 5.638042668018107\n",
      "epoch: 665\n",
      "training loss: 0.39657277184691936\n",
      "validation loss: 6.032364089825048\n",
      "epoch: 666\n",
      "training loss: 0.3928122466861724\n",
      "validation loss: 5.636778070837314\n",
      "epoch: 667\n",
      "training loss: 0.3950766784081662\n",
      "validation loss: 6.030753328457231\n",
      "epoch: 668\n",
      "training loss: 0.39134361287835395\n",
      "validation loss: 5.63551557794114\n",
      "epoch: 669\n",
      "training loss: 0.3935866822704913\n",
      "validation loss: 6.02913937684852\n",
      "epoch: 670\n",
      "training loss: 0.38988112322016993\n",
      "validation loss: 5.634257314774579\n",
      "epoch: 671\n",
      "training loss: 0.3921024951523187\n",
      "validation loss: 6.027523187367848\n",
      "epoch: 672\n",
      "training loss: 0.3884246030902833\n",
      "validation loss: 5.633004911829013\n",
      "epoch: 673\n",
      "training loss: 0.39062400709766687\n",
      "validation loss: 6.025906358743036\n",
      "epoch: 674\n",
      "training loss: 0.38697403891522414\n",
      "validation loss: 5.631759485267324\n",
      "epoch: 675\n",
      "training loss: 0.38915126732020305\n",
      "validation loss: 6.024290979069463\n",
      "epoch: 676\n",
      "training loss: 0.3855295554641167\n",
      "validation loss: 5.6305216522958785\n",
      "epoch: 677\n",
      "training loss: 0.3876844577172663\n",
      "validation loss: 6.022679459498422\n",
      "epoch: 678\n",
      "training loss: 0.38409138757634176\n",
      "validation loss: 5.629291575402292\n",
      "epoch: 679\n",
      "training loss: 0.3862238617557415\n",
      "validation loss: 6.0210743702336105\n",
      "epoch: 680\n",
      "training loss: 0.3826598488893194\n",
      "validation loss: 5.62806902857944\n",
      "epoch: 681\n",
      "training loss: 0.3847698313416925\n",
      "validation loss: 6.019478288604373\n",
      "epoch: 682\n",
      "training loss: 0.38123529995242583\n",
      "validation loss: 5.62685347824455\n",
      "epoch: 683\n",
      "training loss: 0.38332275402734867\n",
      "validation loss: 6.017893666752804\n",
      "epoch: 684\n",
      "training loss: 0.3798181177928222\n",
      "validation loss: 5.6256441716916985\n",
      "epoch: 685\n",
      "training loss: 0.3818830225276072\n",
      "validation loss: 6.016322724055709\n",
      "epoch: 686\n",
      "training loss: 0.3784086685886374\n",
      "validation loss: 5.624440226502185\n",
      "epoch: 687\n",
      "training loss: 0.38045100806462095\n",
      "validation loss: 6.014767367041692\n",
      "epoch: 688\n",
      "training loss: 0.377007284647843\n",
      "validation loss: 5.623240715250793\n",
      "epoch: 689\n",
      "training loss: 0.37902703857870906\n",
      "validation loss: 6.013229137411341\n",
      "epoch: 690\n",
      "training loss: 0.37561424643230534\n",
      "validation loss: 5.622044740985559\n",
      "epoch: 691\n",
      "training loss: 0.37761138237736747\n",
      "validation loss: 6.0117091869261206\n",
      "epoch: 692\n",
      "training loss: 0.37422976994010343\n",
      "validation loss: 5.620851500206012\n",
      "epoch: 693\n",
      "training loss: 0.3762042373731192\n",
      "validation loss: 6.0102082765037075\n",
      "epoch: 694\n",
      "training loss: 0.37285399938634983\n",
      "validation loss: 5.6196603313083475\n",
      "epoch: 695\n",
      "training loss: 0.37480572570434956\n",
      "validation loss: 6.008726795843864\n",
      "epoch: 696\n",
      "training loss: 0.37148700482136365\n",
      "validation loss: 5.618470747639897\n",
      "epoch: 697\n",
      "training loss: 0.373415893252842\n",
      "validation loss: 6.00726479929551\n",
      "epoch: 698\n",
      "training loss: 0.3701287840989984\n",
      "validation loss: 5.617282455338066\n",
      "epoch: 699\n",
      "training loss: 0.3720347133701908\n",
      "validation loss: 6.005822053446424\n",
      "epoch: 700\n",
      "training loss: 0.3687792684547828\n",
      "validation loss: 5.616095356980177\n",
      "epoch: 701\n",
      "training loss: 0.3706620939980991\n",
      "validation loss: 6.00439809198479\n",
      "epoch: 702\n",
      "training loss: 0.36743833087077094\n",
      "validation loss: 5.6149095427316205\n",
      "epoch: 703\n",
      "training loss: 0.36929788730854424\n",
      "validation loss: 6.002992273695577\n",
      "epoch: 704\n",
      "training loss: 0.36610579637975843\n",
      "validation loss: 5.613725271135455\n",
      "epoch: 705\n",
      "training loss: 0.36794190098826074\n",
      "validation loss: 6.001603839957213\n",
      "epoch: 706\n",
      "training loss: 0.36478145348624125\n",
      "validation loss: 5.612542941949908\n",
      "epoch: 707\n",
      "training loss: 0.36659391033700844\n",
      "validation loss: 6.000231968708396\n",
      "epoch: 708\n",
      "training loss: 0.3634650659469835\n",
      "validation loss: 5.611363063536571\n",
      "epoch: 709\n",
      "training loss: 0.3652536704306415\n",
      "validation loss: 5.998875822528764\n",
      "epoch: 710\n",
      "training loss: 0.3621563842464404\n",
      "validation loss: 5.610186217242995\n",
      "epoch: 711\n",
      "training loss: 0.3639209277068177\n",
      "validation loss: 5.997534589168182\n",
      "epoch: 712\n",
      "training loss: 0.3608551562148054\n",
      "validation loss: 5.609013021046298\n",
      "epoch: 713\n",
      "training loss: 0.36259543045425446\n",
      "validation loss: 5.996207513515736\n",
      "epoch: 714\n",
      "training loss: 0.3595611363607527\n",
      "validation loss: 5.607844094457074\n",
      "epoch: 715\n",
      "training loss: 0.3612769378175731\n",
      "validation loss: 5.994893920614804\n",
      "epoch: 716\n",
      "training loss: 0.3582740936165993\n",
      "validation loss: 5.6066800263461625\n",
      "epoch: 717\n",
      "training loss: 0.35996522706067546\n",
      "validation loss: 5.99359322986655\n",
      "epoch: 718\n",
      "training loss: 0.3569938173177661\n",
      "validation loss: 5.605521346987476\n",
      "epoch: 719\n",
      "training loss: 0.35866009895681233\n",
      "validation loss: 5.992304961005309\n",
      "epoch: 720\n",
      "training loss: 0.3557201213535871\n",
      "validation loss: 5.604368505227219\n",
      "epoch: 721\n",
      "training loss: 0.35736138128742223\n",
      "validation loss: 5.991028732783807\n",
      "epoch: 722\n",
      "training loss: 0.35445284652725995\n",
      "validation loss: 5.6032218513114564\n",
      "epoch: 723\n",
      "training loss: 0.3560689305301329\n",
      "validation loss: 5.989764255550936\n",
      "epoch: 724\n",
      "training loss: 0.3531918612493126\n",
      "validation loss: 5.602081625557258\n",
      "epoch: 725\n",
      "training loss: 0.3547826318970259\n",
      "validation loss: 5.988511319052732\n",
      "epoch: 726\n",
      "training loss: 0.3519370607556569\n",
      "validation loss: 5.600947952741678\n",
      "epoch: 727\n",
      "training loss: 0.35350239794502064\n",
      "validation loss: 5.987269776851522\n",
      "epoch: 728\n",
      "training loss: 0.3506883650886621\n",
      "validation loss: 5.599820841821548\n",
      "epoch: 729\n",
      "training loss: 0.35222816602099605\n",
      "validation loss: 5.98603952873029\n",
      "epoch: 730\n",
      "training loss: 0.3494457161097595\n",
      "validation loss: 5.598700190394664\n",
      "epoch: 731\n",
      "training loss: 0.3509598948261108\n",
      "validation loss: 5.984820502361529\n",
      "epoch: 732\n",
      "training loss: 0.3482090738220403\n",
      "validation loss: 5.597585793164166\n",
      "epoch: 733\n",
      "training loss: 0.3496975603875006\n",
      "validation loss: 5.9836126353791705\n",
      "epoch: 734\n",
      "training loss: 0.3469784122768835\n",
      "validation loss: 5.596477353577917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 735\n",
      "training loss: 0.3484411517144071\n",
      "validation loss: 5.982415858805721\n",
      "epoch: 736\n",
      "training loss: 0.3457537153213518\n",
      "validation loss: 5.595374497779277\n",
      "epoch: 737\n",
      "training loss: 0.3471906663922385\n",
      "validation loss: 5.98123008258818\n",
      "epoch: 738\n",
      "training loss: 0.34453497241347086\n",
      "validation loss: 5.594276790012783\n",
      "epoch: 739\n",
      "training loss: 0.34594610633490686\n",
      "validation loss: 5.980055183782647\n",
      "epoch: 740\n",
      "training loss: 0.34332217469808873\n",
      "validation loss: 5.593183748679441\n",
      "epoch: 741\n",
      "training loss: 0.3447074738770901\n",
      "validation loss: 5.978890997716228\n",
      "epoch: 742\n",
      "training loss: 0.3421153114959405\n",
      "validation loss: 5.59209486231649\n",
      "epoch: 743\n",
      "training loss: 0.34347476834573226\n",
      "validation loss: 5.977737312266159\n",
      "epoch: 744\n",
      "training loss: 0.34091436731629743\n",
      "validation loss: 5.591009604877429\n",
      "epoch: 745\n",
      "training loss: 0.3422479832074067\n",
      "validation loss: 5.9765938652184385\n",
      "epoch: 746\n",
      "training loss: 0.3397193194644542\n",
      "validation loss: 5.589927449806966\n",
      "epoch: 747\n",
      "training loss: 0.3410271038475589\n",
      "validation loss: 5.97546034452447\n",
      "epoch: 748\n",
      "training loss: 0.33853013627625966\n",
      "validation loss: 5.588847882526743\n",
      "epoch: 749\n",
      "training loss: 0.3398121059999154\n",
      "validation loss: 5.974336391161434\n",
      "epoch: 750\n",
      "training loss: 0.3373467759783279\n",
      "validation loss: 5.587770411069323\n",
      "epoch: 751\n",
      "training loss: 0.3386029548119721\n",
      "validation loss: 5.973221604213018\n",
      "epoch: 752\n",
      "training loss: 0.3361691861450046\n",
      "validation loss: 5.5866945747152945\n",
      "epoch: 753\n",
      "training loss: 0.33739960450549217\n",
      "validation loss: 5.972115547737055\n",
      "epoch: 754\n",
      "training loss: 0.3349973036985857\n",
      "validation loss: 5.585619950591014\n",
      "epoch: 755\n",
      "training loss: 0.33620199856956845\n",
      "validation loss: 5.97101775895993\n",
      "epoch: 756\n",
      "training loss: 0.3338310553835071\n",
      "validation loss: 5.584546158276609\n",
      "epoch: 757\n",
      "training loss: 0.3350100704091333\n",
      "validation loss: 5.969927757332902\n",
      "epoch: 758\n",
      "training loss: 0.33267035863360944\n",
      "validation loss: 5.583472862549253\n",
      "epoch: 759\n",
      "training loss: 0.3338237443621169\n",
      "validation loss: 5.9688450540103215\n",
      "epoch: 760\n",
      "training loss: 0.3315151227443638\n",
      "validation loss: 5.582399774443316\n",
      "epoch: 761\n",
      "training loss: 0.3326429369943028\n",
      "validation loss: 5.96776916134141\n",
      "epoch: 762\n",
      "training loss: 0.33036525026235686\n",
      "validation loss: 5.581326650852464\n",
      "epoch: 763\n",
      "training loss: 0.33146755858166527\n",
      "validation loss: 5.966699602017044\n",
      "epoch: 764\n",
      "training loss: 0.3292206385055005\n",
      "validation loss: 5.58025329292335\n",
      "epoch: 765\n",
      "training loss: 0.33029751469375346\n",
      "validation loss: 5.965635917572554\n",
      "epoch: 766\n",
      "training loss: 0.3280811811336895\n",
      "validation loss: 5.579179543501197\n",
      "epoch: 767\n",
      "training loss: 0.3291327077994353\n",
      "validation loss: 5.964577676004309\n",
      "epoch: 768\n",
      "training loss: 0.3269467696993085\n",
      "validation loss: 5.578105283887384\n",
      "epoch: 769\n",
      "training loss: 0.32797303882584067\n",
      "validation loss: 5.963524478325145\n",
      "epoch: 770\n",
      "training loss: 0.3258172951152991\n",
      "validation loss: 5.577030430154997\n",
      "epoch: 771\n",
      "training loss: 0.32681840861221934\n",
      "validation loss: 5.962475963943626\n",
      "epoch: 772\n",
      "training loss: 0.3246926489917035\n",
      "validation loss: 5.575954929248138\n",
      "epoch: 773\n",
      "training loss: 0.32566871921299584\n",
      "validation loss: 5.961431814807426\n",
      "epoch: 774\n",
      "training loss: 0.3235727248030455\n",
      "validation loss: 5.574878755064073\n",
      "epoch: 775\n",
      "training loss: 0.3245238750160045\n",
      "validation loss: 5.960391758307847\n",
      "epoch: 776\n",
      "training loss: 0.3224574188595149\n",
      "validation loss: 5.573801904684005\n",
      "epoch: 777\n",
      "training loss: 0.32338378365402\n",
      "validation loss: 5.959355568983033\n",
      "epoch: 778\n",
      "training loss: 0.3213466310685522\n",
      "validation loss: 5.572724394886354\n",
      "epoch: 779\n",
      "training loss: 0.32224835669916196\n",
      "validation loss: 5.958323069096742\n",
      "epoch: 780\n",
      "training loss: 0.3202402654815032\n",
      "validation loss: 5.571646259041026\n",
      "epoch: 781\n",
      "training loss: 0.3211175101391519\n",
      "validation loss: 5.957294128200826\n",
      "epoch: 782\n",
      "training loss: 0.31913823063004093\n",
      "validation loss: 5.570567544449581\n",
      "epoch: 783\n",
      "training loss: 0.31999116464387917\n",
      "validation loss: 5.956268661806419\n",
      "epoch: 784\n",
      "training loss: 0.3180404396658262\n",
      "validation loss: 5.569488310166361\n",
      "epoch: 785\n",
      "training loss: 0.3188692456375537\n",
      "validation loss: 5.955246629307377\n",
      "epoch: 786\n",
      "training loss: 0.31694681032048144\n",
      "validation loss: 5.568408625306396\n",
      "epoch: 787\n",
      "training loss: 0.3177516831971879\n",
      "validation loss: 5.954228031303455\n",
      "epoch: 788\n",
      "training loss: 0.31585726471039993\n",
      "validation loss: 5.567328567822297\n",
      "epoch: 789\n",
      "training loss: 0.3166384118029554\n",
      "validation loss: 5.953212906470145\n",
      "epoch: 790\n",
      "training loss: 0.3147717290125297\n",
      "validation loss: 5.56624822371351\n",
      "epoch: 791\n",
      "training loss: 0.3155293699675439\n",
      "validation loss: 5.952201328120665\n",
      "epoch: 792\n",
      "training loss: 0.31369013303822263\n",
      "validation loss: 5.56516768661414\n",
      "epoch: 793\n",
      "training loss: 0.31442449977347214\n",
      "validation loss: 5.951193400591171\n",
      "epoch: 794\n",
      "training loss: 0.31261240973542137\n",
      "validation loss: 5.5640870576965105\n",
      "epoch: 795\n",
      "training loss: 0.31332374634718035\n",
      "validation loss: 5.950189255570473\n",
      "epoch: 796\n",
      "training loss: 0.31153849464521\n",
      "validation loss: 5.563006445820309\n",
      "epoch: 797\n",
      "training loss: 0.31222705729677974\n",
      "validation loss: 5.9491890484798\n",
      "epoch: 798\n",
      "training loss: 0.3104683253395084\n",
      "validation loss: 5.5619259678536315\n",
      "epoch: 799\n",
      "training loss: 0.3111343821394962\n",
      "validation loss: 5.948192954987887\n",
      "epoch: 800\n",
      "training loss: 0.3094018408643532\n",
      "validation loss: 5.560845749094779\n",
      "epoch: 801\n",
      "training loss: 0.31004567174102776\n",
      "validation loss: 5.9472011677340335\n",
      "epoch: 802\n",
      "training loss: 0.3083389812074667\n",
      "validation loss: 5.559765923725471\n",
      "epoch: 803\n",
      "training loss: 0.3089608777861893\n",
      "validation loss: 5.946213893309337\n",
      "epoch: 804\n",
      "training loss: 0.3072796868097899\n",
      "validation loss: 5.558686635232745\n",
      "epoch: 805\n",
      "training loss: 0.30787995229739196\n",
      "validation loss: 5.945231349531608\n",
      "epoch: 806\n",
      "training loss: 0.30622389813348844\n",
      "validation loss: 5.557608036745469\n",
      "epoch: 807\n",
      "training loss: 0.30680284721244644\n",
      "validation loss: 5.944253763034904\n",
      "epoch: 808\n",
      "training loss: 0.3051715552964929\n",
      "validation loss: 5.556530291238246\n",
      "epoch: 809\n",
      "training loss: 0.30572951403124027\n",
      "validation loss: 5.943281367176556\n",
      "epoch: 810\n",
      "training loss: 0.3041225977825911\n",
      "validation loss: 5.555453571567096\n",
      "epoch: 811\n",
      "training loss: 0.3046599035364857\n",
      "validation loss: 5.9423144002569686\n",
      "epoch: 812\n",
      "training loss: 0.3030769642279708\n",
      "validation loss: 5.554378060310409\n",
      "epoch: 813\n",
      "training loss: 0.30359396559032986\n",
      "validation loss: 5.941353104033972\n",
      "epoch: 814\n",
      "training loss: 0.3020345922874409\n",
      "validation loss: 5.55330394939709\n",
      "epoch: 815\n",
      "training loss: 0.302531649007359\n",
      "validation loss: 5.940397722505485\n",
      "epoch: 816\n",
      "training loss: 0.30099541857814655\n",
      "validation loss: 5.552231439515397\n",
      "epoch: 817\n",
      "training loss: 0.3014729014999465\n",
      "validation loss: 5.939448500930905\n",
      "epoch: 818\n",
      "training loss: 0.29995937869526984\n",
      "validation loss: 5.551160739301804\n",
      "epoch: 819\n",
      "training loss: 0.30041766969172673\n",
      "validation loss: 5.938505685052621\n",
      "epoch: 820\n",
      "training loss: 0.2989264072975713\n",
      "validation loss: 5.550092064317066\n",
      "epoch: 821\n",
      "training loss: 0.29936589919292644\n",
      "validation loss: 5.9375695204809915\n",
      "epoch: 822\n",
      "training loss: 0.2978964382524086\n",
      "validation loss: 5.5490256358245516\n",
      "epoch: 823\n",
      "training loss: 0.2983175347288654\n",
      "validation loss: 5.9366402522024915\n",
      "epoch: 824\n",
      "training loss: 0.2968694048343778\n",
      "validation loss: 5.547961679387086\n",
      "epoch: 825\n",
      "training loss: 0.2972725203148552\n",
      "validation loss: 5.935718124170888\n",
      "epoch: 826\n",
      "training loss: 0.29584523997001105\n",
      "validation loss: 5.546900423305668\n",
      "epoch: 827\n",
      "training loss: 0.29623079946740816\n",
      "validation loss: 5.934803378945282\n",
      "epoch: 828\n",
      "training loss: 0.2948238765164574\n",
      "validation loss: 5.54584209692437\n",
      "epoch: 829\n",
      "training loss: 0.2951923154433916\n",
      "validation loss: 5.933896257336995\n",
      "epoch: 830\n",
      "training loss: 0.2938052475702645\n",
      "validation loss: 5.544786928824952\n",
      "epoch: 831\n",
      "training loss: 0.2941570114993758\n",
      "validation loss: 5.932996998034262\n",
      "epoch: 832\n",
      "training loss: 0.29278928679402605\n",
      "validation loss: 5.543735144939781\n",
      "epoch: 833\n",
      "training loss: 0.29312483116114413\n",
      "validation loss: 5.932105837174146\n",
      "epoch: 834\n",
      "training loss: 0.29177592875401126\n",
      "validation loss: 5.542686966605275\n",
      "epoch: 835\n",
      "training loss: 0.29209571849802374\n",
      "validation loss: 5.93122300783508\n",
      "epoch: 836\n",
      "training loss: 0.2907651092640237\n",
      "validation loss: 5.541642608579881\n",
      "epoch: 837\n",
      "training loss: 0.2910696183937759\n",
      "validation loss: 5.930348739429174\n",
      "epoch: 838\n",
      "training loss: 0.28975676572325754\n",
      "validation loss: 5.540602277049968\n",
      "epoch: 839\n",
      "training loss: 0.2900464768080899\n",
      "validation loss: 5.929483256972707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 840\n",
      "training loss: 0.2887508374483181\n",
      "validation loss: 5.5395661676378625\n",
      "epoch: 841\n",
      "training loss: 0.28902624102539265\n",
      "validation loss: 5.928626780221971\n",
      "epoch: 842\n",
      "training loss: 0.28774726599009587\n",
      "validation loss: 5.538534463432885\n",
      "epoch: 843\n",
      "training loss: 0.28800885988390784\n",
      "validation loss: 5.927779522660207\n",
      "epoch: 844\n",
      "training loss: 0.2867459954306824\n",
      "validation loss: 5.5375073330552675\n",
      "epoch: 845\n",
      "training loss: 0.2869942839846596\n",
      "validation loss: 5.926941690327027\n",
      "epoch: 846\n",
      "training loss: 0.28574697266146654\n",
      "validation loss: 5.536484928762226\n",
      "epoch: 847\n",
      "training loss: 0.28598246587644743\n",
      "validation loss: 5.92611348048568\n",
      "epoch: 848\n",
      "training loss: 0.28475014763115547\n",
      "validation loss: 5.535467384608501\n",
      "epoch: 849\n",
      "training loss: 0.28497336021449193\n",
      "validation loss: 5.92529508012134\n",
      "epoch: 850\n",
      "training loss: 0.28375547356898273\n",
      "validation loss: 5.534454814658934\n",
      "epoch: 851\n",
      "training loss: 0.28396692389493167\n",
      "validation loss: 5.924486664274573\n",
      "epoch: 852\n",
      "training loss: 0.28276290717702346\n",
      "validation loss: 5.5334473112626394\n",
      "epoch: 853\n",
      "training loss: 0.28296311616057795\n",
      "validation loss: 5.923688394207982\n",
      "epoch: 854\n",
      "training loss: 0.28177240878799575\n",
      "validation loss: 5.532444943388039\n",
      "epoch: 855\n",
      "training loss: 0.28196189868219723\n",
      "validation loss: 5.92290041541324\n",
      "epoch: 856\n",
      "training loss: 0.280783942495613\n",
      "validation loss: 5.531447755015556\n",
      "epoch: 857\n",
      "training loss: 0.28096323561445824\n",
      "validation loss: 5.922122855468405\n",
      "epoch: 858\n",
      "training loss: 0.2797974762454167\n",
      "validation loss: 5.53045576359813\n",
      "epoch: 859\n",
      "training loss: 0.2799670936259998\n",
      "validation loss: 5.921355821751298\n",
      "epoch: 860\n",
      "training loss: 0.2788129818959368\n",
      "validation loss: 5.529468958578873\n",
      "epoch: 861\n",
      "training loss: 0.2789734419100003\n",
      "validation loss: 5.920599399032691\n",
      "epoch: 862\n",
      "training loss: 0.27783043524602236\n",
      "validation loss: 5.528487299977019\n",
      "epoch: 863\n",
      "training loss: 0.2779822521704211\n",
      "validation loss: 5.919853646959476\n",
      "epoch: 864\n",
      "training loss: 0.2768498160243813\n",
      "validation loss: 5.5275107170452165\n",
      "epoch: 865\n",
      "training loss: 0.27699349859121203\n",
      "validation loss: 5.91911859745614\n",
      "epoch: 866\n",
      "training loss: 0.2758711078543478\n",
      "validation loss: 5.526539106996143\n",
      "epoch: 867\n",
      "training loss: 0.2760071577883495\n",
      "validation loss: 5.918394252073494\n",
      "epoch: 868\n",
      "training loss: 0.27489429817871225\n",
      "validation loss: 5.525572333824556\n",
      "epoch: 869\n",
      "training loss: 0.2750232087434528\n",
      "validation loss: 5.917680579306911\n",
      "epoch: 870\n",
      "training loss: 0.2739193781595677\n",
      "validation loss: 5.524610227219149\n",
      "epoch: 871\n",
      "training loss: 0.2740416327282902\n",
      "validation loss: 5.916977511935794\n",
      "epoch: 872\n",
      "training loss: 0.27294634254959427\n",
      "validation loss: 5.523652581593059\n",
      "epoch: 873\n",
      "training loss: 0.2730624132119718\n",
      "validation loss: 5.916284944407857\n",
      "epoch: 874\n",
      "training loss: 0.27197518952955013\n",
      "validation loss: 5.522699155253063\n",
      "epoch: 875\n",
      "training loss: 0.2720855357603192\n",
      "validation loss: 5.915602730322792\n",
      "epoch: 876\n",
      "training loss: 0.2710059205317842\n",
      "validation loss: 5.521749669714818\n",
      "epoch: 877\n",
      "training loss: 0.27111098792564264\n",
      "validation loss: 5.914930680062647\n",
      "epoch: 878\n",
      "training loss: 0.27003854002863853\n",
      "validation loss: 5.520803809216825\n",
      "epoch: 879\n",
      "training loss: 0.2701387591232026\n",
      "validation loss: 5.9142685586019335\n",
      "epoch: 880\n",
      "training loss: 0.26907305530760367\n",
      "validation loss: 5.519861220431412\n",
      "epoch: 881\n",
      "training loss: 0.26916884050632855\n",
      "validation loss: 5.9136160835730305\n",
      "epoch: 882\n",
      "training loss: 0.2681094762276585\n",
      "validation loss: 5.518921512420491\n",
      "epoch: 883\n",
      "training loss: 0.2682012248255674\n",
      "validation loss: 5.912972923604017\n",
      "epoch: 884\n",
      "training loss: 0.26714781494966244\n",
      "validation loss: 5.517984256865478\n",
      "epoch: 885\n",
      "training loss: 0.26723590628477145\n",
      "validation loss: 5.912338696996625\n",
      "epoch: 886\n",
      "training loss: 0.26618808566818314\n",
      "validation loss: 5.517048988573344\n",
      "epoch: 887\n",
      "training loss: 0.2662728803884495\n",
      "validation loss: 5.911712970781105\n",
      "epoch: 888\n",
      "training loss: 0.26523030431292544\n",
      "validation loss: 5.516115206326542\n",
      "epoch: 889\n",
      "training loss: 0.26531214377422885\n",
      "validation loss: 5.911095260161493\n",
      "epoch: 890\n",
      "training loss: 0.26427448825194644\n",
      "validation loss: 5.515182374049208\n",
      "epoch: 891\n",
      "training loss: 0.26435369404634707\n",
      "validation loss: 5.9104850284216655\n",
      "epoch: 892\n",
      "training loss: 0.2633206559838505\n",
      "validation loss: 5.514249922337605\n",
      "epoch: 893\n",
      "training loss: 0.26339752958641405\n",
      "validation loss: 5.9098816872558935\n",
      "epoch: 894\n",
      "training loss: 0.26236882681069856\n",
      "validation loss: 5.51331725036102\n",
      "epoch: 895\n",
      "training loss: 0.2624436493622765\n",
      "validation loss: 5.909284597577281\n",
      "epoch: 896\n",
      "training loss: 0.2614190205269641\n",
      "validation loss: 5.512383728097985\n",
      "epoch: 897\n",
      "training loss: 0.2614920527224963\n",
      "validation loss: 5.908693070782954\n",
      "epoch: 898\n",
      "training loss: 0.2604712570759723\n",
      "validation loss: 5.511448698968647\n",
      "epoch: 899\n",
      "training loss: 0.26054273917096216\n",
      "validation loss: 5.9081063704376025\n",
      "epoch: 900\n",
      "training loss: 0.259525556223426\n",
      "validation loss: 5.510511482769143\n",
      "epoch: 901\n",
      "training loss: 0.25959570814339156\n",
      "validation loss: 5.90752371441534\n",
      "epoch: 902\n",
      "training loss: 0.25858193721903994\n",
      "validation loss: 5.509571378946662\n",
      "epoch: 903\n",
      "training loss: 0.2586509587500994\n",
      "validation loss: 5.906944277371082\n",
      "epoch: 904\n",
      "training loss: 0.2576404184426616\n",
      "validation loss: 5.508627670167757\n",
      "epoch: 905\n",
      "training loss: 0.25770848952166703\n",
      "validation loss: 5.906367193585913\n",
      "epoch: 906\n",
      "training loss: 0.2567010170770546\n",
      "validation loss: 5.507679626098833\n",
      "epoch: 907\n",
      "training loss: 0.25676829813171476\n",
      "validation loss: 5.905791560078472\n",
      "epoch: 908\n",
      "training loss: 0.25576374873639174\n",
      "validation loss: 5.50672650745649\n",
      "epoch: 909\n",
      "training loss: 0.25583038109910883\n",
      "validation loss: 5.905216439910866\n",
      "epoch: 910\n",
      "training loss: 0.2548286271297551\n",
      "validation loss: 5.50576757015018\n",
      "epoch: 911\n",
      "training loss: 0.25489473349582964\n",
      "validation loss: 5.904640865710736\n",
      "epoch: 912\n",
      "training loss: 0.2538956636989722\n",
      "validation loss: 5.504802069586414\n",
      "epoch: 913\n",
      "training loss: 0.25396134860963504\n",
      "validation loss: 5.904063843193418\n",
      "epoch: 914\n",
      "training loss: 0.2529648672469588\n",
      "validation loss: 5.5038292650247005\n",
      "epoch: 915\n",
      "training loss: 0.25303021762400424\n",
      "validation loss: 5.903484354785831\n",
      "epoch: 916\n",
      "training loss: 0.25203624359720794\n",
      "validation loss: 5.50284842390281\n",
      "epoch: 917\n",
      "training loss: 0.25210132926170215\n",
      "validation loss: 5.902901363149069\n",
      "epoch: 918\n",
      "training loss: 0.25110979518887067\n",
      "validation loss: 5.501858826218226\n",
      "epoch: 919\n",
      "training loss: 0.25117466941629185\n",
      "validation loss: 5.902313814580064\n",
      "epoch: 920\n",
      "training loss: 0.25018552073155625\n",
      "validation loss: 5.500859768716465\n",
      "epoch: 921\n",
      "training loss: 0.2502502207908799\n",
      "validation loss: 5.901720642314543\n",
      "epoch: 922\n",
      "training loss: 0.24926341480095496\n",
      "validation loss: 5.4998505690702135\n",
      "epoch: 923\n",
      "training loss: 0.24932796247797084\n",
      "validation loss: 5.901120769465718\n",
      "epoch: 924\n",
      "training loss: 0.24834346744200309\n",
      "validation loss: 5.498830569857537\n",
      "epoch: 925\n",
      "training loss: 0.2484078695779359\n",
      "validation loss: 5.90051311184373\n",
      "epoch: 926\n",
      "training loss: 0.24742566379094125\n",
      "validation loss: 5.49779914234652\n",
      "epoch: 927\n",
      "training loss: 0.24748991275031187\n",
      "validation loss: 5.899896580316884\n",
      "epoch: 928\n",
      "training loss: 0.2465099836078107\n",
      "validation loss: 5.496755690207063\n",
      "epoch: 929\n",
      "training loss: 0.24657405777212585\n",
      "validation loss: 5.899270082875016\n",
      "epoch: 930\n",
      "training loss: 0.24559640089683632\n",
      "validation loss: 5.495699652848753\n",
      "epoch: 931\n",
      "training loss: 0.24566026508411898\n",
      "validation loss: 5.898632526355465\n",
      "epoch: 932\n",
      "training loss: 0.24468488340156747\n",
      "validation loss: 5.494630508768249\n",
      "epoch: 933\n",
      "training loss: 0.24474848926037168\n",
      "validation loss: 5.89798281758085\n",
      "epoch: 934\n",
      "training loss: 0.24377539214535687\n",
      "validation loss: 5.493547778551622\n",
      "epoch: 935\n",
      "training loss: 0.2438386785319839\n",
      "validation loss: 5.897319864326408\n",
      "epoch: 936\n",
      "training loss: 0.24286788093504097\n",
      "validation loss: 5.4924510277656\n",
      "epoch: 937\n",
      "training loss: 0.2429307741796934\n",
      "validation loss: 5.896642575543603\n",
      "epoch: 938\n",
      "training loss: 0.24196229575667777\n",
      "validation loss: 5.4913398697927525\n",
      "epoch: 939\n",
      "training loss: 0.24202470996562\n",
      "validation loss: 5.895949861344395\n",
      "epoch: 940\n",
      "training loss: 0.24105857427011118\n",
      "validation loss: 5.490213968319399\n",
      "epoch: 941\n",
      "training loss: 0.24112041148166313\n",
      "validation loss: 5.895240632426275\n",
      "epoch: 942\n",
      "training loss: 0.24015664507003937\n",
      "validation loss: 5.4890730400866445\n",
      "epoch: 943\n",
      "training loss: 0.24021779541047142\n",
      "validation loss: 5.894513798870176\n",
      "epoch: 944\n",
      "training loss: 0.2392564270584602\n",
      "validation loss: 5.487916857257623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 945\n",
      "training loss: 0.23931676882073546\n",
      "validation loss: 5.89376826877516\n",
      "epoch: 946\n",
      "training loss: 0.23835782863751995\n",
      "validation loss: 5.486745250069577\n",
      "epoch: 947\n",
      "training loss: 0.23841722822976516\n",
      "validation loss: 5.893002945858348\n",
      "epoch: 948\n",
      "training loss: 0.2374607468151908\n",
      "validation loss: 5.485558109495865\n",
      "epoch: 949\n",
      "training loss: 0.23751905874783333\n",
      "validation loss: 5.892216727043145\n",
      "epoch: 950\n",
      "training loss: 0.23656506634767474\n",
      "validation loss: 5.484355389863971\n",
      "epoch: 951\n",
      "training loss: 0.23662213297500642\n",
      "validation loss: 5.891408499043224\n",
      "epoch: 952\n",
      "training loss: 0.2356706585144138\n",
      "validation loss: 5.483137112130055\n",
      "epoch: 953\n",
      "training loss: 0.23572630983633372\n",
      "validation loss: 5.890577134470609\n",
      "epoch: 954\n",
      "training loss: 0.23477738008008142\n",
      "validation loss: 5.481903366826644\n",
      "epoch: 955\n",
      "training loss: 0.2348314333300127\n",
      "validation loss: 5.889721487514517\n",
      "epoch: 956\n",
      "training loss: 0.2338850717977829\n",
      "validation loss: 5.480654318035224\n",
      "epoch: 957\n",
      "training loss: 0.23393733093016073\n",
      "validation loss: 5.888840388253431\n",
      "epoch: 958\n",
      "training loss: 0.23299355693352153\n",
      "validation loss: 5.479390207342059\n",
      "epoch: 959\n",
      "training loss: 0.23304381208058966\n",
      "validation loss: 5.887932637113642\n",
      "epoch: 960\n",
      "training loss: 0.23210263957357488\n",
      "validation loss: 5.478111358524784\n",
      "epoch: 961\n",
      "training loss: 0.23215066614777077\n",
      "validation loss: 5.886996997393917\n",
      "epoch: 962\n",
      "training loss: 0.2312121024796612\n",
      "validation loss: 5.476818183237759\n",
      "epoch: 963\n",
      "training loss: 0.23125766039957502\n",
      "validation loss: 5.886032187663313\n",
      "epoch: 964\n",
      "training loss: 0.23032170510292413\n",
      "validation loss: 5.475511186755519\n",
      "epoch: 965\n",
      "training loss: 0.23036453754605885\n",
      "validation loss: 5.885036872634677\n",
      "epoch: 966\n",
      "training loss: 0.2294311807146426\n",
      "validation loss: 5.474190975818643\n",
      "epoch: 967\n",
      "training loss: 0.2294710128877618\n",
      "validation loss: 5.884009652468443\n",
      "epoch: 968\n",
      "training loss: 0.2285402337538605\n",
      "validation loss: 5.472858266396483\n",
      "epoch: 969\n",
      "training loss: 0.22857677136559404\n",
      "validation loss: 5.882949051672507\n",
      "epoch: 970\n",
      "training loss: 0.22764853632759266\n",
      "validation loss: 5.471513893826754\n",
      "epoch: 971\n",
      "training loss: 0.22768146366880967\n",
      "validation loss: 5.881853504598021\n",
      "epoch: 972\n",
      "training loss: 0.22675572436130256\n",
      "validation loss: 5.470158824045634\n",
      "epoch: 973\n",
      "training loss: 0.22678470243686155\n",
      "validation loss: 5.88072134104347\n",
      "epoch: 974\n",
      "training loss: 0.22586139345750222\n",
      "validation loss: 5.468794166358922\n",
      "epoch: 975\n",
      "training loss: 0.2258860572756098\n",
      "validation loss: 5.879550767676336\n",
      "epoch: 976\n",
      "training loss: 0.22496509343268575\n",
      "validation loss: 5.4674211895634235\n",
      "epoch: 977\n",
      "training loss: 0.22498504948378092\n",
      "validation loss: 5.8783398480503575\n",
      "epoch: 978\n",
      "training loss: 0.2240663231067391\n",
      "validation loss: 5.466041338631234\n",
      "epoch: 979\n",
      "training loss: 0.22408114591048423\n",
      "validation loss: 5.877086479487245\n",
      "epoch: 980\n",
      "training loss: 0.2231645230976245\n",
      "validation loss: 5.46465625673541\n",
      "epoch: 981\n",
      "training loss: 0.22317375151979085\n",
      "validation loss: 5.875788364932655\n",
      "epoch: 982\n",
      "training loss: 0.22225906860558187\n",
      "validation loss: 5.4632678084692\n",
      "epoch: 983\n",
      "training loss: 0.2222622016719142\n",
      "validation loss: 5.87444298343979\n",
      "epoch: 984\n",
      "training loss: 0.22134926045357908\n",
      "validation loss: 5.4618781087037584\n",
      "epoch: 985\n",
      "training loss: 0.22134575202059942\n",
      "validation loss: 5.873047551730077\n",
      "epoch: 986\n",
      "training loss: 0.22043431468664973\n",
      "validation loss: 5.460489555984403\n",
      "epoch: 987\n",
      "training loss: 0.22042356819592607\n",
      "validation loss: 5.871598984103633\n",
      "epoch: 988\n",
      "training loss: 0.21951335145326684\n",
      "validation loss: 5.45910487023736\n",
      "epoch: 989\n",
      "training loss: 0.2194947127021809\n",
      "validation loss: 5.87009384181161\n",
      "epoch: 990\n",
      "training loss: 0.21858538043608827\n",
      "validation loss: 5.457727140262927\n",
      "epoch: 991\n",
      "training loss: 0.2185581304398705\n",
      "validation loss: 5.868528276004461\n",
      "epoch: 992\n",
      "training loss: 0.2176492862533715\n",
      "validation loss: 5.456359874859645\n",
      "epoch: 993\n",
      "training loss: 0.21761263204061831\n",
      "validation loss: 5.866897961623467\n",
      "epoch: 994\n",
      "training loss: 0.21670380912082451\n",
      "validation loss: 5.455007068376862\n",
      "epoch: 995\n",
      "training loss: 0.21665687350414212\n",
      "validation loss: 5.865198015819518\n",
      "epoch: 996\n",
      "training loss: 0.21574752443156556\n",
      "validation loss: 5.453673273235942\n",
      "epoch: 997\n",
      "training loss: 0.2156893345413992\n",
      "validation loss: 5.863422909116291\n",
      "epoch: 998\n",
      "training loss: 0.2147788180214867\n",
      "validation loss: 5.452363688761613\n",
      "epoch: 999\n",
      "training loss: 0.21470829081603807\n",
      "validation loss: 5.861566351316082\n",
      "epoch: 1000\n",
      "training loss: 0.2137958570542901\n",
      "validation loss: 5.451084266337706\n",
      "epoch: 1001\n",
      "training loss: 0.21371178456992743\n",
      "validation loss: 5.859621166623829\n",
      "epoch: 1002\n",
      "training loss: 0.2127965583104514\n",
      "validation loss: 5.449841830837477\n",
      "epoch: 1003\n",
      "training loss: 0.21269758811316897\n",
      "validation loss: 5.8575791374717765\n",
      "epoch: 1004\n",
      "training loss: 0.21177854774977858\n",
      "validation loss: 5.448644232445578\n",
      "epoch: 1005\n",
      "training loss: 0.21166316289323525\n",
      "validation loss: 5.8554308237266\n",
      "epoch: 1006\n",
      "training loss: 0.21073911845103746\n",
      "validation loss: 5.4475005177351585\n",
      "epoch: 1007\n",
      "training loss: 0.21060561237922967\n",
      "validation loss: 5.853165349328951\n",
      "epoch: 1008\n",
      "training loss: 0.2096751769257479\n",
      "validation loss: 5.446421145902425\n",
      "epoch: 1009\n",
      "training loss: 0.20952162550318207\n",
      "validation loss: 5.850770140019623\n",
      "epoch: 1010\n",
      "training loss: 0.20858318561122857\n",
      "validation loss: 5.445418237415835\n",
      "epoch: 1011\n",
      "training loss: 0.20840741577750313\n",
      "validation loss: 5.848230626330685\n",
      "epoch: 1012\n",
      "training loss: 0.20745909448281108\n",
      "validation loss: 5.4445058801587605\n",
      "epoch: 1013\n",
      "training loss: 0.2072586461018748\n",
      "validation loss: 5.845529868985502\n",
      "epoch: 1014\n",
      "training loss: 0.2062982628700188\n",
      "validation loss: 5.443700496624528\n",
      "epoch: 1015\n",
      "training loss: 0.2060703497727444\n",
      "validation loss: 5.842648135294436\n",
      "epoch: 1016\n",
      "training loss: 0.205095375046514\n",
      "validation loss: 5.4430212808985035\n",
      "epoch: 1017\n",
      "training loss: 0.20483683627371635\n",
      "validation loss: 5.839562373841981\n",
      "epoch: 1018\n",
      "training loss: 0.20384433940944183\n",
      "validation loss: 5.44249074067342\n",
      "epoch: 1019\n",
      "training loss: 0.2035515913116464\n",
      "validation loss: 5.836245603669338\n",
      "epoch: 1020\n",
      "training loss: 0.20253818841011817\n",
      "validation loss: 5.442135331371917\n",
      "epoch: 1021\n",
      "training loss: 0.20220716862000143\n",
      "validation loss: 5.832666187662177\n",
      "epoch: 1022\n",
      "training loss: 0.20116896230043982\n",
      "validation loss: 5.441986248872528\n",
      "epoch: 1023\n",
      "training loss: 0.20079507501735283\n",
      "validation loss: 5.8287869624691355\n",
      "epoch: 1024\n",
      "training loss: 0.1997276027039001\n",
      "validation loss: 5.442080361066422\n",
      "epoch: 1025\n",
      "training loss: 0.19930566616475898\n",
      "validation loss: 5.824564247336994\n",
      "epoch: 1026\n",
      "training loss: 0.19820384858020812\n",
      "validation loss: 5.44246135307808\n",
      "epoch: 1027\n",
      "training loss: 0.1977280475148868\n",
      "validation loss: 5.819946651519933\n",
      "epoch: 1028\n",
      "training loss: 0.19658616103703136\n",
      "validation loss: 5.443181090850797\n",
      "epoch: 1029\n",
      "training loss: 0.19605002490581844\n",
      "validation loss: 5.814873764902392\n",
      "epoch: 1030\n",
      "training loss: 0.19486170434576816\n",
      "validation loss: 5.444301245815447\n",
      "epoch: 1031\n",
      "training loss: 0.19425811005809154\n",
      "validation loss: 5.809274642101936\n",
      "epoch: 1032\n",
      "training loss: 0.1930164103760581\n",
      "validation loss: 5.44589523402107\n",
      "epoch: 1033\n",
      "training loss: 0.1923376567254663\n",
      "validation loss: 5.803066217150319\n",
      "epoch: 1034\n",
      "training loss: 0.1910352172604333\n",
      "validation loss: 5.448050431887816\n",
      "epoch: 1035\n",
      "training loss: 0.19027318307742078\n",
      "validation loss: 5.796151666151773\n",
      "epoch: 1036\n",
      "training loss: 0.18890253576043103\n",
      "validation loss: 5.4508707421832066\n",
      "epoch: 1037\n",
      "training loss: 0.1880489985771989\n",
      "validation loss: 5.788418920682989\n",
      "epoch: 1038\n",
      "training loss: 0.1866031251220497\n",
      "validation loss: 5.454479325216137\n",
      "epoch: 1039\n",
      "training loss: 0.18565029822123374\n",
      "validation loss: 5.779739660067155\n",
      "epoch: 1040\n",
      "training loss: 0.18412351561688173\n",
      "validation loss: 5.45902143490513\n",
      "epoch: 1041\n",
      "training loss: 0.18306491109391665\n",
      "validation loss: 5.769969175747103\n",
      "epoch: 1042\n",
      "training loss: 0.18145426460390923\n",
      "validation loss: 5.464666870532368\n",
      "epoch: 1043\n",
      "training loss: 0.18028601455234813\n",
      "validation loss: 5.758948045495327\n",
      "epoch: 1044\n",
      "training loss: 0.17859331350495816\n",
      "validation loss: 5.471611450118443\n",
      "epoch: 1045\n",
      "training loss: 0.17731606576288508\n",
      "validation loss: 5.746506517739705\n",
      "epoch: 1046\n",
      "training loss: 0.1755507561450831\n",
      "validation loss: 5.480076290989531\n",
      "epoch: 1047\n",
      "training loss: 0.17417229721405048\n",
      "validation loss: 5.732473467373203\n",
      "epoch: 1048\n",
      "training loss: 0.17235524551280998\n",
      "validation loss: 5.490302966737342\n",
      "epoch: 1049\n",
      "training loss: 0.17089381456876906\n",
      "validation loss: 5.716691653048461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1050\n",
      "training loss: 0.16906187186984992\n",
      "validation loss: 5.502541749468663\n",
      "epoch: 1051\n",
      "training loss: 0.16754998811505467\n",
      "validation loss: 5.699041813294482\n",
      "epoch: 1052\n",
      "training loss: 0.16576075687864958\n",
      "validation loss: 5.517028610626569\n",
      "epoch: 1053\n",
      "training loss: 0.16424878984410937\n",
      "validation loss: 5.679477333384151\n",
      "epoch: 1054\n",
      "training loss: 0.16258414935465662\n",
      "validation loss: 5.53394601397254\n",
      "epoch: 1055\n",
      "training loss: 0.16114221208444643\n",
      "validation loss: 5.658070014204006\n",
      "epoch: 1056\n",
      "training loss: 0.15970810542976469\n",
      "validation loss: 5.5533615012429856\n",
      "epoch: 1057\n",
      "training loss: 0.15842379470358825\n",
      "validation loss: 5.63506420509557\n",
      "epoch: 1058\n",
      "training loss: 0.1573424887514124\n",
      "validation loss: 5.575140619760916\n",
      "epoch: 1059\n",
      "training loss: 0.15631128558807955\n",
      "validation loss: 5.610931054230473\n",
      "epoch: 1060\n",
      "training loss: 0.15570208030867588\n",
      "validation loss: 5.598835570863429\n",
      "epoch: 1061\n",
      "training loss: 0.15500746046696318\n",
      "validation loss: 5.586409481736497\n",
      "epoch: 1062\n",
      "training loss: 0.15495357081006128\n",
      "validation loss: 5.623563045437993\n",
      "epoch: 1063\n",
      "training loss: 0.15463639368775617\n",
      "validation loss: 5.562516245556721\n",
      "epoch: 1064\n",
      "training loss: 0.1551412803609422\n",
      "validation loss: 5.6479002587296225\n",
      "epoch: 1065\n",
      "training loss: 0.1551636451321102\n",
      "validation loss: 5.540513913012803\n",
      "epoch: 1066\n",
      "training loss: 0.15610888311541027\n",
      "validation loss: 5.669845002957657\n",
      "epoch: 1067\n",
      "training loss: 0.15632562133860115\n",
      "validation loss: 5.521839744081251\n",
      "epoch: 1068\n",
      "training loss: 0.15745148699789482\n",
      "validation loss: 5.686892578938505\n",
      "epoch: 1069\n",
      "training loss: 0.15760965535320226\n",
      "validation loss: 5.508022898152552\n",
      "epoch: 1070\n",
      "training loss: 0.1585429845035626\n",
      "validation loss: 5.696270092242898\n",
      "epoch: 1071\n",
      "training loss: 0.1583312615665718\n",
      "validation loss: 5.50062672216081\n",
      "epoch: 1072\n",
      "training loss: 0.15867707521278912\n",
      "validation loss: 5.695337441452246\n",
      "epoch: 1073\n",
      "training loss: 0.15783698513716637\n",
      "validation loss: 5.501234655703777\n",
      "epoch: 1074\n",
      "training loss: 0.15732821034442387\n",
      "validation loss: 5.6821172996799\n",
      "epoch: 1075\n",
      "training loss: 0.15581411697214717\n",
      "validation loss: 5.511434512033695\n",
      "epoch: 1076\n",
      "training loss: 0.154480049144915\n",
      "validation loss: 5.655875281711419\n",
      "epoch: 1077\n",
      "training loss: 0.152614402569523\n",
      "validation loss: 5.532675357638063\n",
      "epoch: 1078\n",
      "training loss: 0.15089240911998125\n",
      "validation loss: 5.617630546546051\n",
      "epoch: 1079\n",
      "training loss: 0.14941966415497193\n",
      "validation loss: 5.565820137188975\n",
      "epoch: 1080\n",
      "training loss: 0.14811578648242812\n",
      "validation loss: 5.570450504496925\n",
      "epoch: 1081\n",
      "training loss: 0.1480395219266563\n",
      "validation loss: 5.6102723412928315\n",
      "epoch: 1082\n",
      "training loss: 0.14806459554618248\n",
      "validation loss: 5.519341078131365\n",
      "epoch: 1083\n",
      "training loss: 0.1502070110321701\n",
      "validation loss: 5.662780310497738\n",
      "epoch: 1084\n",
      "training loss: 0.15210499227351226\n",
      "validation loss: 5.470554122881383\n",
      "epoch: 1085\n",
      "training loss: 0.15650072596550402\n",
      "validation loss: 5.716408005771826\n",
      "epoch: 1086\n",
      "training loss: 0.1599448303939109\n",
      "validation loss: 5.430331909792153\n",
      "epoch: 1087\n",
      "training loss: 0.16541076488987053\n",
      "validation loss: 5.760491653086891\n",
      "epoch: 1088\n",
      "training loss: 0.16897955149578944\n",
      "validation loss: 5.403635954983101\n",
      "epoch: 1089\n",
      "training loss: 0.17326021145240877\n",
      "validation loss: 5.7822274463771\n",
      "epoch: 1090\n",
      "training loss: 0.17479503630485466\n",
      "validation loss: 5.393856058472222\n",
      "epoch: 1091\n",
      "training loss: 0.17547127626623174\n",
      "validation loss: 5.769828538358518\n",
      "epoch: 1092\n",
      "training loss: 0.17319438592423672\n",
      "validation loss: 5.404136544666663\n",
      "epoch: 1093\n",
      "training loss: 0.1692441233662851\n",
      "validation loss: 5.7166160844693445\n",
      "epoch: 1094\n",
      "training loss: 0.16350616080963437\n",
      "validation loss: 5.439483939790141\n",
      "epoch: 1095\n",
      "training loss: 0.1568849796389189\n",
      "validation loss: 5.625161825851608\n",
      "epoch: 1096\n",
      "training loss: 0.15151644481421303\n",
      "validation loss: 5.506933134924162\n",
      "epoch: 1097\n",
      "training loss: 0.1473189912115081\n",
      "validation loss: 5.509676042010066\n",
      "epoch: 1098\n",
      "training loss: 0.14860640578966441\n",
      "validation loss: 5.610607776115432\n",
      "epoch: 1099\n",
      "training loss: 0.15250699569393872\n",
      "validation loss: 5.393783039260912\n",
      "epoch: 1100\n",
      "training loss: 0.16470814820717034\n",
      "validation loss: 5.742060570260801\n",
      "epoch: 1101\n",
      "training loss: 0.17852952841779032\n",
      "validation loss: 5.301592830731843\n",
      "epoch: 1102\n",
      "training loss: 0.19890304447708249\n",
      "validation loss: 5.873605285726305\n",
      "epoch: 1103\n",
      "training loss: 0.21815066553274332\n",
      "validation loss: 5.245217196663505\n",
      "epoch: 1104\n",
      "training loss: 0.23731622831616953\n",
      "validation loss: 5.965447537046131\n",
      "epoch: 1105\n",
      "training loss: 0.2531470066119512\n",
      "validation loss: 5.219572270053252\n",
      "epoch: 1106\n",
      "training loss: 0.2603635218963355\n",
      "validation loss: 5.9840480915954855\n",
      "epoch: 1107\n",
      "training loss: 0.2636340443767411\n",
      "validation loss: 5.212197671266497\n",
      "epoch: 1108\n",
      "training loss: 0.25272785294906314\n",
      "validation loss: 5.916003545832602\n",
      "epoch: 1109\n",
      "training loss: 0.24013029423144952\n",
      "validation loss: 5.221239436895054\n",
      "epoch: 1110\n",
      "training loss: 0.21572698953536884\n",
      "validation loss: 5.7752952299263205\n",
      "epoch: 1111\n",
      "training loss: 0.1949362399179548\n",
      "validation loss: 5.263769457558614\n",
      "epoch: 1112\n",
      "training loss: 0.17081774377691838\n",
      "validation loss: 5.600157915846067\n",
      "epoch: 1113\n",
      "training loss: 0.1557370809263659\n",
      "validation loss: 5.360145309440515\n",
      "epoch: 1114\n",
      "training loss: 0.14446222315515053\n",
      "validation loss: 5.4317428124488165\n",
      "epoch: 1115\n",
      "training loss: 0.1454026058484756\n",
      "validation loss: 5.511496412241596\n",
      "epoch: 1116\n",
      "training loss: 0.15211799243530516\n",
      "validation loss: 5.297043006488949\n",
      "epoch: 1117\n",
      "training loss: 0.17000907994699357\n",
      "validation loss: 5.692388165029092\n",
      "epoch: 1118\n",
      "training loss: 0.1909073450113961\n",
      "validation loss: 5.207585397329976\n",
      "epoch: 1119\n",
      "training loss: 0.21598447134174178\n",
      "validation loss: 5.8567417934981965\n",
      "epoch: 1120\n",
      "training loss: 0.24018844206187748\n",
      "validation loss: 5.158564137738419\n",
      "epoch: 1121\n",
      "training loss: 0.2581163920586332\n",
      "validation loss: 5.95815532609602\n",
      "epoch: 1122\n",
      "training loss: 0.2741383924310156\n",
      "validation loss: 5.133430197849923\n",
      "epoch: 1123\n",
      "training loss: 0.27559994236132096\n",
      "validation loss: 5.975255770769188\n",
      "epoch: 1124\n",
      "training loss: 0.2775751001546791\n",
      "validation loss: 5.118999299135175\n",
      "epoch: 1125\n",
      "training loss: 0.2632852252679566\n",
      "validation loss: 5.919814981820813\n",
      "epoch: 1126\n",
      "training loss: 0.25354715208204986\n",
      "validation loss: 5.116665516138976\n",
      "epoch: 1127\n",
      "training loss: 0.23210883244422784\n",
      "validation loss: 5.825237138725032\n",
      "epoch: 1128\n",
      "training loss: 0.21817564292838423\n",
      "validation loss: 5.134852031740612\n",
      "epoch: 1129\n",
      "training loss: 0.19817880581616948\n",
      "validation loss: 5.726322729096451\n",
      "epoch: 1130\n",
      "training loss: 0.1858187056037734\n",
      "validation loss: 5.174474028675982\n",
      "epoch: 1131\n",
      "training loss: 0.1706792811867366\n",
      "validation loss: 5.641267104203414\n",
      "epoch: 1132\n",
      "training loss: 0.16169906760280922\n",
      "validation loss: 5.2278468018489965\n",
      "epoch: 1133\n",
      "training loss: 0.15141582889315683\n",
      "validation loss: 5.572727383698131\n",
      "epoch: 1134\n",
      "training loss: 0.14550423781939398\n",
      "validation loss: 5.286662574575166\n",
      "epoch: 1135\n",
      "training loss: 0.13893790690023125\n",
      "validation loss: 5.516132436802326\n",
      "epoch: 1136\n",
      "training loss: 0.13540201133829532\n",
      "validation loss: 5.346983410581192\n",
      "epoch: 1137\n",
      "training loss: 0.1317081935579634\n",
      "validation loss: 5.466678826592253\n",
      "epoch: 1138\n",
      "training loss: 0.1302400791508156\n",
      "validation loss: 5.407381370556886\n",
      "epoch: 1139\n",
      "training loss: 0.12887963398704363\n",
      "validation loss: 5.421702034731726\n",
      "epoch: 1140\n",
      "training loss: 0.12933524667910906\n",
      "validation loss: 5.467413211835869\n",
      "epoch: 1141\n",
      "training loss: 0.12996435131991566\n",
      "validation loss: 5.380431577670676\n",
      "epoch: 1142\n",
      "training loss: 0.1321403913424234\n",
      "validation loss: 5.5246794581103424\n",
      "epoch: 1143\n",
      "training loss: 0.13423792536957768\n",
      "validation loss: 5.344411565762261\n",
      "epoch: 1144\n",
      "training loss: 0.13754769079278986\n",
      "validation loss: 5.5753825905031125\n",
      "epoch: 1145\n",
      "training loss: 0.14030002644151068\n",
      "validation loss: 5.315223444174597\n",
      "epoch: 1146\n",
      "training loss: 0.1436023444389189\n",
      "validation loss: 5.61362017496924\n",
      "epoch: 1147\n",
      "training loss: 0.1457834816156689\n",
      "validation loss: 5.295706426901384\n",
      "epoch: 1148\n",
      "training loss: 0.14782389887394293\n",
      "validation loss: 5.633448077981994\n",
      "epoch: 1149\n",
      "training loss: 0.1484064860879539\n",
      "validation loss: 5.287317281413133\n",
      "epoch: 1150\n",
      "training loss: 0.1480945207241868\n",
      "validation loss: 5.6305040319641355\n",
      "epoch: 1151\n",
      "training loss: 0.14627071595477684\n",
      "validation loss: 5.2929658177005345\n",
      "epoch: 1152\n",
      "training loss: 0.14346354461691357\n",
      "validation loss: 5.602699819568543\n",
      "epoch: 1153\n",
      "training loss: 0.13976318910952384\n",
      "validation loss: 5.314465773667811\n",
      "epoch: 1154\n",
      "training loss: 0.13537857528189925\n",
      "validation loss: 5.55273014485024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1155\n",
      "training loss: 0.1310385811982018\n",
      "validation loss: 5.3553371304390165\n",
      "epoch: 1156\n",
      "training loss: 0.12695553000287682\n",
      "validation loss: 5.484557023516166\n",
      "epoch: 1157\n",
      "training loss: 0.12457997297947326\n",
      "validation loss: 5.416665221189416\n",
      "epoch: 1158\n",
      "training loss: 0.12323501438991778\n",
      "validation loss: 5.407868557726934\n",
      "epoch: 1159\n",
      "training loss: 0.12517652692791723\n",
      "validation loss: 5.497406047759851\n",
      "epoch: 1160\n",
      "training loss: 0.12834472761563626\n",
      "validation loss: 5.33090019296642\n",
      "epoch: 1161\n",
      "training loss: 0.13562117839643223\n",
      "validation loss: 5.588757605248134\n",
      "epoch: 1162\n",
      "training loss: 0.14297333643753585\n",
      "validation loss: 5.26647073427719\n",
      "epoch: 1163\n",
      "training loss: 0.15355591378702912\n",
      "validation loss: 5.6734624798093245\n",
      "epoch: 1164\n",
      "training loss: 0.16242334393376773\n",
      "validation loss: 5.219921115380912\n",
      "epoch: 1165\n",
      "training loss: 0.17110189904898884\n",
      "validation loss: 5.729139276647871\n",
      "epoch: 1166\n",
      "training loss: 0.17600897222392245\n",
      "validation loss: 5.197045538256634\n",
      "epoch: 1167\n",
      "training loss: 0.17774853555609232\n",
      "validation loss: 5.734056250674124\n",
      "epoch: 1168\n",
      "training loss: 0.1760685337857317\n",
      "validation loss: 5.1964749564034145\n",
      "epoch: 1169\n",
      "training loss: 0.1691904426838558\n",
      "validation loss: 5.683192216922439\n",
      "epoch: 1170\n",
      "training loss: 0.15976350845048565\n",
      "validation loss: 5.224731784755114\n",
      "epoch: 1171\n",
      "training loss: 0.14745466256898207\n",
      "validation loss: 5.5792978358663525\n",
      "epoch: 1172\n",
      "training loss: 0.13697642055705722\n",
      "validation loss: 5.288501601337307\n",
      "epoch: 1173\n",
      "training loss: 0.1272893588318998\n",
      "validation loss: 5.447666613913705\n",
      "epoch: 1174\n",
      "training loss: 0.12336023275864953\n",
      "validation loss: 5.399080703518986\n",
      "epoch: 1175\n",
      "training loss: 0.12415961721944067\n",
      "validation loss: 5.306670643644979\n",
      "epoch: 1176\n",
      "training loss: 0.13517290891032327\n",
      "validation loss: 5.552499884315356\n",
      "epoch: 1177\n",
      "training loss: 0.15045584305434628\n",
      "validation loss: 5.1918510910038105\n",
      "epoch: 1178\n",
      "training loss: 0.1753232736222444\n",
      "validation loss: 5.72281974680284\n",
      "epoch: 1179\n",
      "training loss: 0.2008583229308104\n",
      "validation loss: 5.111327731472341\n",
      "epoch: 1180\n",
      "training loss: 0.225819612297429\n",
      "validation loss: 5.859980335332185\n",
      "epoch: 1181\n",
      "training loss: 0.24564805429930597\n",
      "validation loss: 5.071939120614431\n",
      "epoch: 1182\n",
      "training loss: 0.2559321285801678\n",
      "validation loss: 5.91077561492772\n",
      "epoch: 1183\n",
      "training loss: 0.2643933702097161\n",
      "validation loss: 5.051005373258599\n",
      "epoch: 1184\n",
      "training loss: 0.25445408550511683\n",
      "validation loss: 5.873788209346533\n",
      "epoch: 1185\n",
      "training loss: 0.24313854173050792\n",
      "validation loss: 5.048783927115899\n",
      "epoch: 1186\n",
      "training loss: 0.21857311706819724\n",
      "validation loss: 5.758887395927602\n",
      "epoch: 1187\n",
      "training loss: 0.20217787462216447\n",
      "validation loss: 5.068085068980444\n",
      "epoch: 1188\n",
      "training loss: 0.17833384226217364\n",
      "validation loss: 5.630937458592601\n",
      "epoch: 1189\n",
      "training loss: 0.15943062785158885\n",
      "validation loss: 5.122262956825225\n",
      "epoch: 1190\n",
      "training loss: 0.13911387515720844\n",
      "validation loss: 5.491112587704845\n",
      "epoch: 1191\n",
      "training loss: 0.12819551168595028\n",
      "validation loss: 5.209681307086117\n",
      "epoch: 1192\n",
      "training loss: 0.11959103882938794\n",
      "validation loss: 5.381745819079169\n",
      "epoch: 1193\n",
      "training loss: 0.11650575079712247\n",
      "validation loss: 5.320522174159988\n",
      "epoch: 1194\n",
      "training loss: 0.11623108628785705\n",
      "validation loss: 5.271403168033503\n",
      "epoch: 1195\n",
      "training loss: 0.1230907324730594\n",
      "validation loss: 5.456686167396012\n",
      "epoch: 1196\n",
      "training loss: 0.13195416043177713\n",
      "validation loss: 5.192747228068429\n",
      "epoch: 1197\n",
      "training loss: 0.14617227370446326\n",
      "validation loss: 5.581650890607267\n",
      "epoch: 1198\n",
      "training loss: 0.16368131113654363\n",
      "validation loss: 5.123011180471083\n",
      "epoch: 1199\n",
      "training loss: 0.18258689591204755\n",
      "validation loss: 5.708992602054325\n",
      "epoch: 1200\n",
      "training loss: 0.19514999591169058\n",
      "validation loss: 5.081564297891681\n",
      "epoch: 1201\n",
      "training loss: 0.2030754220774353\n",
      "validation loss: 5.7611135778974605\n",
      "epoch: 1202\n",
      "training loss: 0.21586997537086236\n",
      "validation loss: 5.054640988596355\n",
      "epoch: 1203\n",
      "training loss: 0.223155375834545\n",
      "validation loss: 5.797537790955462\n",
      "epoch: 1204\n",
      "training loss: 0.2217041188316275\n",
      "validation loss: 5.038739017788613\n",
      "epoch: 1205\n",
      "training loss: 0.2068517913693008\n",
      "validation loss: 5.74291891426832\n",
      "epoch: 1206\n",
      "training loss: 0.20243293879783922\n",
      "validation loss: 5.044135633796161\n",
      "epoch: 1207\n",
      "training loss: 0.19792987749234947\n",
      "validation loss: 5.705816667379239\n",
      "epoch: 1208\n",
      "training loss: 0.19105457232787787\n",
      "validation loss: 5.047582061147042\n",
      "epoch: 1209\n",
      "training loss: 0.17054060650571498\n",
      "validation loss: 5.624050764746712\n",
      "epoch: 1210\n",
      "training loss: 0.15963457231397282\n",
      "validation loss: 5.086705378787978\n",
      "epoch: 1211\n",
      "training loss: 0.1546857456680034\n",
      "validation loss: 5.572547901065325\n",
      "epoch: 1212\n",
      "training loss: 0.15350741066973897\n",
      "validation loss: 5.099568172240235\n",
      "epoch: 1213\n",
      "training loss: 0.13944920848957088\n",
      "validation loss: 5.521456024496523\n",
      "epoch: 1214\n",
      "training loss: 0.12883874824509373\n",
      "validation loss: 5.1594342866416625\n",
      "epoch: 1215\n",
      "training loss: 0.12457418926742238\n",
      "validation loss: 5.461696113897923\n",
      "epoch: 1216\n",
      "training loss: 0.12710315616431297\n",
      "validation loss: 5.171459914791744\n",
      "epoch: 1217\n",
      "training loss: 0.12120062373116802\n",
      "validation loss: 5.447120907955169\n",
      "epoch: 1218\n",
      "training loss: 0.11200381074224056\n",
      "validation loss: 5.23902484688196\n",
      "epoch: 1219\n",
      "training loss: 0.10918354983371832\n",
      "validation loss: 5.363697497722856\n",
      "epoch: 1220\n",
      "training loss: 0.10986046842780149\n",
      "validation loss: 5.260082812951836\n",
      "epoch: 1221\n",
      "training loss: 0.11158537708367061\n",
      "validation loss: 5.37777181439819\n",
      "epoch: 1222\n",
      "training loss: 0.10579134351568555\n",
      "validation loss: 5.325430029985142\n",
      "epoch: 1223\n",
      "training loss: 0.10961630129880591\n",
      "validation loss: 5.266071686006667\n",
      "epoch: 1224\n",
      "training loss: 0.1065172865950831\n",
      "validation loss: 5.376270005041873\n",
      "epoch: 1225\n",
      "training loss: 0.11087001025343918\n",
      "validation loss: 5.3006502239885025\n",
      "epoch: 1226\n",
      "training loss: 0.11167965765202065\n",
      "validation loss: 5.417802519192312\n",
      "epoch: 1227\n",
      "training loss: 0.12835028337149043\n",
      "validation loss: 5.1757042715778\n",
      "epoch: 1228\n",
      "training loss: 0.12818048696939865\n",
      "validation loss: 5.5172604697149525\n",
      "epoch: 1229\n",
      "training loss: 0.12076526437512258\n",
      "validation loss: 5.222520658679259\n",
      "epoch: 1230\n",
      "training loss: 0.12692404734178309\n",
      "validation loss: 5.487780086985474\n",
      "epoch: 1231\n",
      "training loss: 0.15233827411325807\n",
      "validation loss: 5.113979652409583\n",
      "epoch: 1232\n",
      "training loss: 0.17273158765354762\n",
      "validation loss: 5.643868521585723\n",
      "epoch: 1233\n",
      "training loss: 0.1338041026707836\n",
      "validation loss: 5.160238207654122\n",
      "epoch: 1234\n",
      "training loss: 0.1391458058155669\n",
      "validation loss: 5.4854821901558655\n",
      "epoch: 1235\n",
      "training loss: 0.14956506070074724\n",
      "validation loss: 5.101233260333241\n",
      "epoch: 1236\n",
      "training loss: 0.21151944906464393\n",
      "validation loss: 5.6948458561571185\n",
      "epoch: 1237\n",
      "training loss: 0.139817871227744\n",
      "validation loss: 5.128467554316391\n",
      "epoch: 1238\n",
      "training loss: 0.14340774708424006\n",
      "validation loss: 5.393031103530526\n",
      "epoch: 1239\n",
      "training loss: 0.11914722311442447\n",
      "validation loss: 5.16864481302911\n",
      "epoch: 1240\n",
      "training loss: 0.20867454439196834\n",
      "validation loss: 5.6247055639004975\n",
      "epoch: 1241\n",
      "training loss: 0.1556880826520018\n",
      "validation loss: 5.150240056473128\n",
      "epoch: 1242\n",
      "training loss: 0.14640852531439077\n",
      "validation loss: 5.23870170759986\n",
      "epoch: 1243\n",
      "training loss: 0.14277683606745492\n",
      "validation loss: 5.386697161435003\n",
      "epoch: 1244\n",
      "training loss: 0.1525009307044981\n",
      "validation loss: 5.393163793685637\n",
      "epoch: 1245\n",
      "training loss: 0.2283805759845978\n",
      "validation loss: 5.268950277393322\n",
      "epoch: 1246\n",
      "training loss: 0.15359669237883916\n",
      "validation loss: 5.069887436808416\n",
      "epoch: 1247\n",
      "training loss: 0.35117048289307895\n",
      "validation loss: 5.812859252429826\n",
      "epoch: 1248\n",
      "training loss: 0.15388770419002226\n",
      "validation loss: 5.009402307344481\n",
      "epoch: 1249\n",
      "training loss: 0.3397790269499952\n",
      "validation loss: 5.4755501250786125\n",
      "epoch: 1250\n",
      "training loss: 0.21034623659869925\n",
      "validation loss: 5.042915905444488\n",
      "epoch: 1251\n",
      "training loss: 0.5689749838721518\n",
      "validation loss: 6.113113931122275\n",
      "epoch: 1252\n",
      "training loss: 0.5924992703871411\n",
      "validation loss: 4.821787802968989\n",
      "epoch: 1253\n",
      "training loss: 0.27127927496642745\n",
      "validation loss: 5.596919401743474\n",
      "epoch: 1254\n",
      "training loss: 0.4530084369467778\n",
      "validation loss: 5.2422805306386575\n",
      "epoch: 1255\n",
      "training loss: 0.2881072076934571\n",
      "validation loss: 5.579744528755859\n",
      "epoch: 1256\n",
      "training loss: 0.7976631543763353\n",
      "validation loss: 4.836248292261947\n",
      "epoch: 1257\n",
      "training loss: 0.7687160513727752\n",
      "validation loss: 6.286647244179994\n",
      "epoch: 1258\n",
      "training loss: 0.313964541277965\n",
      "validation loss: 4.538060606096013\n",
      "epoch: 1259\n",
      "training loss: 0.8229106464959842\n",
      "validation loss: 5.690063599560302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1260\n",
      "training loss: 0.6574086498921382\n",
      "validation loss: 5.546497687363475\n",
      "epoch: 1261\n",
      "training loss: 0.3706746043197684\n",
      "validation loss: 5.317189196334301\n",
      "epoch: 1262\n",
      "training loss: 0.8466956515598552\n",
      "validation loss: 4.6923813487633845\n",
      "epoch: 1263\n",
      "training loss: 0.675653509159704\n",
      "validation loss: 5.884443141622956\n",
      "epoch: 1264\n",
      "training loss: 0.23310470996146215\n",
      "validation loss: 4.47496054305915\n",
      "epoch: 1265\n",
      "training loss: 0.7731278586568164\n",
      "validation loss: 5.237147659203494\n",
      "epoch: 1266\n",
      "training loss: 1.1822902180664472\n",
      "validation loss: 6.459893580725258\n",
      "epoch: 1267\n",
      "training loss: 0.5689910412606759\n",
      "validation loss: 4.936868066528156\n",
      "epoch: 1268\n",
      "training loss: 0.18633754450754858\n",
      "validation loss: 4.7949756018753895\n",
      "epoch: 1269\n",
      "training loss: 0.5006116139979637\n",
      "validation loss: 5.365723058207181\n",
      "epoch: 1270\n",
      "training loss: 0.6732720113034751\n",
      "validation loss: 4.910713888788172\n",
      "epoch: 1271\n",
      "training loss: 0.2866725094430984\n",
      "validation loss: 5.330978226177993\n",
      "epoch: 1272\n",
      "training loss: 0.1465284468743987\n",
      "validation loss: 4.686064356783397\n",
      "epoch: 1273\n",
      "training loss: 0.40169642690818774\n",
      "validation loss: 4.86471000771781\n",
      "epoch: 1274\n",
      "training loss: 0.4918164154614167\n",
      "validation loss: 5.6550971386257896\n",
      "epoch: 1275\n",
      "training loss: 0.29203369985702604\n",
      "validation loss: 4.659652387772106\n",
      "epoch: 1276\n",
      "training loss: 0.1346394204900209\n",
      "validation loss: 4.926863204786636\n",
      "epoch: 1277\n",
      "training loss: 0.25818566288294226\n",
      "validation loss: 5.155865340036722\n",
      "epoch: 1278\n",
      "training loss: 0.3645445821049446\n",
      "validation loss: 4.795678690285812\n",
      "epoch: 1279\n",
      "training loss: 0.2049401161051551\n",
      "validation loss: 5.2479759290524495\n",
      "epoch: 1280\n",
      "training loss: 0.12187963482810903\n",
      "validation loss: 4.78436408602448\n",
      "epoch: 1281\n",
      "training loss: 0.22390879333444683\n",
      "validation loss: 4.845522250063432\n",
      "epoch: 1282\n",
      "training loss: 0.2513482663982917\n",
      "validation loss: 5.3210267009793215\n",
      "epoch: 1283\n",
      "training loss: 0.16025607243188592\n",
      "validation loss: 4.751716547327112\n",
      "epoch: 1284\n",
      "training loss: 0.12493813391729032\n",
      "validation loss: 4.937157914948332\n",
      "epoch: 1285\n",
      "training loss: 0.1888857443568122\n",
      "validation loss: 5.16119007795734\n",
      "epoch: 1286\n",
      "training loss: 0.18885704245423005\n",
      "validation loss: 4.816244315324221\n",
      "epoch: 1287\n",
      "training loss: 0.11505995864979865\n",
      "validation loss: 5.0628363767335545\n",
      "epoch: 1288\n",
      "training loss: 0.13094496974307424\n",
      "validation loss: 5.018011725858537\n",
      "epoch: 1289\n",
      "training loss: 0.17317092176172616\n",
      "validation loss: 4.8749047265265215\n",
      "epoch: 1290\n",
      "training loss: 0.1289013704267485\n",
      "validation loss: 5.14062801651447\n",
      "epoch: 1291\n",
      "training loss: 0.10878957764152263\n",
      "validation loss: 4.954006097211391\n",
      "epoch: 1292\n",
      "training loss: 0.1455564505563259\n",
      "validation loss: 4.92231247769393\n",
      "epoch: 1293\n",
      "training loss: 0.13446016052056212\n",
      "validation loss: 5.164732756862902\n",
      "epoch: 1294\n",
      "training loss: 0.10537971730865132\n",
      "validation loss: 4.9406240852511925\n",
      "epoch: 1295\n",
      "training loss: 0.12462292317508317\n",
      "validation loss: 4.96377801907885\n",
      "epoch: 1296\n",
      "training loss: 0.13170972461201413\n",
      "validation loss: 5.163391556002154\n",
      "epoch: 1297\n",
      "training loss: 0.10665090176534534\n",
      "validation loss: 4.947594283637881\n",
      "epoch: 1298\n",
      "training loss: 0.1121189536363283\n",
      "validation loss: 4.998449966180014\n",
      "epoch: 1299\n",
      "training loss: 0.12612962000978045\n",
      "validation loss: 5.155310234723765\n",
      "epoch: 1300\n",
      "training loss: 0.10771719457834483\n",
      "validation loss: 4.9599535200600995\n",
      "epoch: 1301\n",
      "training loss: 0.10514539853743561\n",
      "validation loss: 5.025235749160322\n",
      "epoch: 1302\n",
      "training loss: 0.12069588498624513\n",
      "validation loss: 5.147887451676822\n",
      "epoch: 1303\n",
      "training loss: 0.10776486647648482\n",
      "validation loss: 4.972184046634558\n",
      "epoch: 1304\n",
      "training loss: 0.10116298073045525\n",
      "validation loss: 5.044623476023257\n",
      "epoch: 1305\n",
      "training loss: 0.11632722313964482\n",
      "validation loss: 5.143162248454498\n",
      "epoch: 1306\n",
      "training loss: 0.10703024281570613\n",
      "validation loss: 4.982702837995481\n",
      "epoch: 1307\n",
      "training loss: 0.09869801364503214\n",
      "validation loss: 5.05777132945401\n",
      "epoch: 1308\n",
      "training loss: 0.11310550403074746\n",
      "validation loss: 5.14131475531666\n",
      "epoch: 1309\n",
      "training loss: 0.1058285171368315\n",
      "validation loss: 4.991263062056056\n",
      "epoch: 1310\n",
      "training loss: 0.09701829472965369\n",
      "validation loss: 5.065865179333034\n",
      "epoch: 1311\n",
      "training loss: 0.11088318280379166\n",
      "validation loss: 5.142017888295018\n",
      "epoch: 1312\n",
      "training loss: 0.10435673830728803\n",
      "validation loss: 4.99807066057366\n",
      "epoch: 1313\n",
      "training loss: 0.09580659497118627\n",
      "validation loss: 5.0698589048807605\n",
      "epoch: 1314\n",
      "training loss: 0.10948844199789226\n",
      "validation loss: 5.1448744841826235\n",
      "epoch: 1315\n",
      "training loss: 0.10269722260753855\n",
      "validation loss: 5.003481458480622\n",
      "epoch: 1316\n",
      "training loss: 0.09497343554640557\n",
      "validation loss: 5.07045206174616\n",
      "epoch: 1317\n",
      "training loss: 0.10877034505174907\n",
      "validation loss: 5.149496353953246\n",
      "epoch: 1318\n",
      "training loss: 0.10085814184738358\n",
      "validation loss: 5.00792338972029\n",
      "epoch: 1319\n",
      "training loss: 0.09456633529010619\n",
      "validation loss: 5.068137657583968\n",
      "epoch: 1320\n",
      "training loss: 0.10858688678854865\n",
      "validation loss: 5.155480886861002\n",
      "epoch: 1321\n",
      "training loss: 0.09881612122012345\n",
      "validation loss: 5.011869533367858\n",
      "epoch: 1322\n",
      "training loss: 0.09472737494603943\n",
      "validation loss: 5.063293687453847\n",
      "epoch: 1323\n",
      "training loss: 0.10877332929590258\n",
      "validation loss: 5.162334781690759\n",
      "epoch: 1324\n",
      "training loss: 0.0965609157365196\n",
      "validation loss: 5.015852881941341\n",
      "epoch: 1325\n",
      "training loss: 0.09566545790163472\n",
      "validation loss: 5.056263593104098\n",
      "epoch: 1326\n",
      "training loss: 0.10910660011355805\n",
      "validation loss: 5.169397315299111\n",
      "epoch: 1327\n",
      "training loss: 0.0941476197200195\n",
      "validation loss: 5.0204818568995515\n",
      "epoch: 1328\n",
      "training loss: 0.09761870337013623\n",
      "validation loss: 5.047430097614551\n",
      "epoch: 1329\n",
      "training loss: 0.10927749951967952\n",
      "validation loss: 5.175765298881184\n",
      "epoch: 1330\n",
      "training loss: 0.09175697306872777\n",
      "validation loss: 5.026467666817503\n",
      "epoch: 1331\n",
      "training loss: 0.10078885316473572\n",
      "validation loss: 5.0372512544184636\n",
      "epoch: 1332\n",
      "training loss: 0.10888855932980462\n",
      "validation loss: 5.180261446229154\n",
      "epoch: 1333\n",
      "training loss: 0.08975413552729741\n",
      "validation loss: 5.034634498641434\n",
      "epoch: 1334\n",
      "training loss: 0.1052382534020549\n",
      "validation loss: 5.026261318575335\n",
      "epoch: 1335\n",
      "training loss: 0.10749993016680962\n",
      "validation loss: 5.181467012213397\n",
      "epoch: 1336\n",
      "training loss: 0.088722454271736\n",
      "validation loss: 5.045908799491742\n",
      "epoch: 1337\n",
      "training loss: 0.11075932421007353\n",
      "validation loss: 5.015018999417636\n",
      "epoch: 1338\n",
      "training loss: 0.10474617707962483\n",
      "validation loss: 5.177865850772491\n",
      "epoch: 1339\n",
      "training loss: 0.08943833783228337\n",
      "validation loss: 5.061249356463003\n",
      "epoch: 1340\n",
      "training loss: 0.11675492462248971\n",
      "validation loss: 5.0040282148755395\n",
      "epoch: 1341\n",
      "training loss: 0.1005295879782934\n",
      "validation loss: 5.168104935761037\n",
      "epoch: 1342\n",
      "training loss: 0.09275519477073002\n",
      "validation loss: 5.0815040745827185\n",
      "epoch: 1343\n",
      "training loss: 0.12219975402506872\n",
      "validation loss: 4.993657478194639\n",
      "epoch: 1344\n",
      "training loss: 0.09526166428345495\n",
      "validation loss: 5.151352794040728\n",
      "epoch: 1345\n",
      "training loss: 0.09938683336629865\n",
      "validation loss: 5.107167902631551\n",
      "epoch: 1346\n",
      "training loss: 0.12576235075943024\n",
      "validation loss: 4.9841195199195285\n",
      "epoch: 1347\n",
      "training loss: 0.09007576466413864\n",
      "validation loss: 5.127658804941676\n",
      "epoch: 1348\n",
      "training loss: 0.10962195507844075\n",
      "validation loss: 5.138063107878833\n",
      "epoch: 1349\n",
      "training loss: 0.1261358434446875\n",
      "validation loss: 4.975541668349871\n",
      "epoch: 1350\n",
      "training loss: 0.08689446731488579\n",
      "validation loss: 5.09818864018867\n",
      "epoch: 1351\n",
      "training loss: 0.12304589905384866\n",
      "validation loss: 5.172986309338246\n",
      "epoch: 1352\n",
      "training loss: 0.12254172749338177\n",
      "validation loss: 4.9681296861985\n",
      "epoch: 1353\n",
      "training loss: 0.08822384994393759\n",
      "validation loss: 5.065190156004131\n",
      "epoch: 1354\n",
      "training loss: 0.13838369919373694\n",
      "validation loss: 5.209454828899468\n",
      "epoch: 1355\n",
      "training loss: 0.11525933466654475\n",
      "validation loss: 4.962339181075556\n",
      "epoch: 1356\n",
      "training loss: 0.09659120751966602\n",
      "validation loss: 5.031611259273651\n",
      "epoch: 1357\n",
      "training loss: 0.1536027330140091\n",
      "validation loss: 5.243761732494281\n",
      "epoch: 1358\n",
      "training loss: 0.10592289386562298\n",
      "validation loss: 4.958918566064964\n",
      "epoch: 1359\n",
      "training loss: 0.1136794004176174\n",
      "validation loss: 5.00038795861625\n",
      "epoch: 1360\n",
      "training loss: 0.16640233005086066\n",
      "validation loss: 5.271621340931833\n",
      "epoch: 1361\n",
      "training loss: 0.09728864213062248\n",
      "validation loss: 4.958654448161081\n",
      "epoch: 1362\n",
      "training loss: 0.13947061668026245\n",
      "validation loss: 4.97362511525147\n",
      "epoch: 1363\n",
      "training loss: 0.17507690550052962\n",
      "validation loss: 5.289508490391758\n",
      "epoch: 1364\n",
      "training loss: 0.0923231623623656\n",
      "validation loss: 4.961777230723972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1365\n",
      "training loss: 0.1719681349954828\n",
      "validation loss: 4.952070858713665\n",
      "epoch: 1366\n",
      "training loss: 0.17943367066756954\n",
      "validation loss: 5.296327040113943\n",
      "epoch: 1367\n",
      "training loss: 0.09288393307300662\n",
      "validation loss: 4.9672054213403545\n",
      "epoch: 1368\n",
      "training loss: 0.20793013798429988\n",
      "validation loss: 4.935258999351009\n",
      "epoch: 1369\n",
      "training loss: 0.1812126527594314\n",
      "validation loss: 5.2945343677599235\n",
      "epoch: 1370\n",
      "training loss: 0.09868768575847621\n",
      "validation loss: 4.971998104787605\n",
      "epoch: 1371\n",
      "training loss: 0.24428879281822596\n",
      "validation loss: 4.922218683547529\n",
      "epoch: 1372\n",
      "training loss: 0.18374251669043667\n",
      "validation loss: 5.2900443340103935\n",
      "epoch: 1373\n",
      "training loss: 0.10717444831044859\n",
      "validation loss: 4.97124867412996\n",
      "epoch: 1374\n",
      "training loss: 0.27909714840299626\n",
      "validation loss: 4.912126152680788\n",
      "epoch: 1375\n",
      "training loss: 0.19127329694443088\n",
      "validation loss: 5.2911021195768075\n",
      "epoch: 1376\n",
      "training loss: 0.1143071571856185\n",
      "validation loss: 4.958501448547024\n",
      "epoch: 1377\n",
      "training loss: 0.3108848199269655\n",
      "validation loss: 4.904194476445202\n",
      "epoch: 1378\n",
      "training loss: 0.2087079142070601\n",
      "validation loss: 5.306843947323186\n",
      "epoch: 1379\n",
      "training loss: 0.11617377020291089\n",
      "validation loss: 4.927098037042062\n",
      "epoch: 1380\n",
      "training loss: 0.3361792271013772\n",
      "validation loss: 4.8965579304866\n",
      "epoch: 1381\n",
      "training loss: 0.24156374335506947\n",
      "validation loss: 5.345321438157849\n",
      "epoch: 1382\n",
      "training loss: 0.11191689230722629\n",
      "validation loss: 4.8737551668377215\n",
      "epoch: 1383\n",
      "training loss: 0.3461503212050734\n",
      "validation loss: 4.884780148095569\n",
      "epoch: 1384\n",
      "training loss: 0.29398011240455574\n",
      "validation loss: 5.408701339459814\n",
      "epoch: 1385\n",
      "training loss: 0.10863318209937672\n",
      "validation loss: 4.8055588241043266\n",
      "epoch: 1386\n",
      "training loss: 0.3256484063886953\n",
      "validation loss: 4.8628928637923075\n",
      "epoch: 1387\n",
      "training loss: 0.36076436742330076\n",
      "validation loss: 5.482110980906214\n",
      "epoch: 1388\n",
      "training loss: 0.12535153887986197\n",
      "validation loss: 4.747037889979449\n",
      "epoch: 1389\n",
      "training loss: 0.26198069027619925\n",
      "validation loss: 4.833027495376284\n",
      "epoch: 1390\n",
      "training loss: 0.41359796320768794\n",
      "validation loss: 5.518565215919714\n",
      "epoch: 1391\n",
      "training loss: 0.1849586591451114\n",
      "validation loss: 4.733560189420163\n",
      "epoch: 1392\n",
      "training loss: 0.16871942949570118\n",
      "validation loss: 4.824124860759399\n",
      "epoch: 1393\n",
      "training loss: 0.4010761164725893\n",
      "validation loss: 5.444497193646274\n",
      "epoch: 1394\n",
      "training loss: 0.2830570495899133\n",
      "validation loss: 4.7771016979032845\n",
      "epoch: 1395\n",
      "training loss: 0.10254223625915714\n",
      "validation loss: 4.891034445550279\n",
      "epoch: 1396\n",
      "training loss: 0.2971997085738931\n",
      "validation loss: 5.2251123943287805\n",
      "epoch: 1397\n",
      "training loss: 0.35992278666447414\n",
      "validation loss: 4.832894200039701\n",
      "epoch: 1398\n",
      "training loss: 0.12106174058146904\n",
      "validation loss: 5.052975229488368\n",
      "epoch: 1399\n",
      "training loss: 0.1622500037947069\n",
      "validation loss: 4.944890630400613\n",
      "epoch: 1400\n",
      "training loss: 0.3440519956051191\n",
      "validation loss: 4.839206013946422\n",
      "epoch: 1401\n",
      "training loss: 0.20310007034530597\n",
      "validation loss: 5.228996357931832\n",
      "epoch: 1402\n",
      "training loss: 0.09373720892693199\n",
      "validation loss: 4.74892070665147\n",
      "epoch: 1403\n",
      "training loss: 0.24233597966260717\n",
      "validation loss: 4.806045431563729\n",
      "epoch: 1404\n",
      "training loss: 0.25966307329018745\n",
      "validation loss: 5.296242390070138\n",
      "epoch: 1405\n",
      "training loss: 0.1148226623703942\n",
      "validation loss: 4.690827005325095\n",
      "epoch: 1406\n",
      "training loss: 0.13507013364967327\n",
      "validation loss: 4.8087823286627005\n",
      "epoch: 1407\n",
      "training loss: 0.23813088796214543\n",
      "validation loss: 5.214211660999285\n",
      "epoch: 1408\n",
      "training loss: 0.1689256963342546\n",
      "validation loss: 4.716753088592541\n",
      "epoch: 1409\n",
      "training loss: 0.0860311808358609\n",
      "validation loss: 4.884404315860281\n",
      "epoch: 1410\n",
      "training loss: 0.16787865887379827\n",
      "validation loss: 5.050242910125932\n",
      "epoch: 1411\n",
      "training loss: 0.19483858850705496\n",
      "validation loss: 4.7558288930399435\n",
      "epoch: 1412\n",
      "training loss: 0.09481766309799737\n",
      "validation loss: 4.9927229638733\n",
      "epoch: 1413\n",
      "training loss: 0.10671117235290743\n",
      "validation loss: 4.8999036874128405\n",
      "epoch: 1414\n",
      "training loss: 0.17785019776705357\n",
      "validation loss: 4.780073210667013\n",
      "epoch: 1415\n",
      "training loss: 0.1228094211482925\n",
      "validation loss: 5.073390677582995\n",
      "epoch: 1416\n",
      "training loss: 0.08120199924126376\n",
      "validation loss: 4.80996188329388\n",
      "epoch: 1417\n",
      "training loss: 0.13971880257480834\n",
      "validation loss: 4.799995056414383\n",
      "epoch: 1418\n",
      "training loss: 0.13930446070614444\n",
      "validation loss: 5.100113136235853\n",
      "epoch: 1419\n",
      "training loss: 0.08378546091421785\n",
      "validation loss: 4.775212788772764\n",
      "epoch: 1420\n",
      "training loss: 0.10450755890887904\n",
      "validation loss: 4.829794652954271\n",
      "epoch: 1421\n",
      "training loss: 0.13714224709632933\n",
      "validation loss: 5.0814038442223515\n",
      "epoch: 1422\n",
      "training loss: 0.0972054059853429\n",
      "validation loss: 4.772669573780004\n",
      "epoch: 1423\n",
      "training loss: 0.08308398363437693\n",
      "validation loss: 4.871097993701094\n",
      "epoch: 1424\n",
      "training loss: 0.12284898552712269\n",
      "validation loss: 5.0379750835387265\n",
      "epoch: 1425\n",
      "training loss: 0.10916799196432717\n",
      "validation loss: 4.783504131907614\n",
      "epoch: 1426\n",
      "training loss: 0.07543545282549986\n",
      "validation loss: 4.916868791764607\n",
      "epoch: 1427\n",
      "training loss: 0.1051629373339383\n",
      "validation loss: 4.988077509238529\n",
      "epoch: 1428\n",
      "training loss: 0.11472144647182433\n",
      "validation loss: 4.797641211829294\n",
      "epoch: 1429\n",
      "training loss: 0.07691155354758883\n",
      "validation loss: 4.959237085000804\n",
      "epoch: 1430\n",
      "training loss: 0.08988658399876073\n",
      "validation loss: 4.9427258819229065\n",
      "epoch: 1431\n",
      "training loss: 0.11383627148127169\n",
      "validation loss: 4.811547959784006\n",
      "epoch: 1432\n",
      "training loss: 0.08256338989549641\n",
      "validation loss: 4.993284495513313\n",
      "epoch: 1433\n",
      "training loss: 0.07935447008744224\n",
      "validation loss: 4.906460207887743\n",
      "epoch: 1434\n",
      "training loss: 0.10853738949757044\n",
      "validation loss: 4.824805144193907\n",
      "epoch: 1435\n",
      "training loss: 0.08892750777393939\n",
      "validation loss: 5.017431489142639\n",
      "epoch: 1436\n",
      "training loss: 0.07364168060857545\n",
      "validation loss: 4.879740103098422\n",
      "epoch: 1437\n",
      "training loss: 0.10107405690389755\n",
      "validation loss: 4.837955331533109\n",
      "epoch: 1438\n",
      "training loss: 0.09417262231066405\n",
      "validation loss: 5.032230881583331\n",
      "epoch: 1439\n",
      "training loss: 0.07181957369166307\n",
      "validation loss: 4.861218101580527\n",
      "epoch: 1440\n",
      "training loss: 0.09317042634428531\n",
      "validation loss: 4.851404181346771\n",
      "epoch: 1441\n",
      "training loss: 0.09763753136252375\n",
      "validation loss: 5.039276070188625\n",
      "epoch: 1442\n",
      "training loss: 0.07273278132488921\n",
      "validation loss: 4.848982502883963\n",
      "epoch: 1443\n",
      "training loss: 0.08589273845804492\n",
      "validation loss: 4.865274450520722\n",
      "epoch: 1444\n",
      "training loss: 0.09932125240058036\n",
      "validation loss: 5.040371564800495\n",
      "epoch: 1445\n",
      "training loss: 0.07536322669841992\n",
      "validation loss: 4.841267352419228\n",
      "epoch: 1446\n",
      "training loss: 0.07977520605997156\n",
      "validation loss: 4.879408672971584\n",
      "epoch: 1447\n",
      "training loss: 0.09952168390119424\n",
      "validation loss: 5.0372020054928095\n",
      "epoch: 1448\n",
      "training loss: 0.07894723591962986\n",
      "validation loss: 4.836595414090452\n",
      "epoch: 1449\n",
      "training loss: 0.07499375868125953\n",
      "validation loss: 4.893596544579136\n",
      "epoch: 1450\n",
      "training loss: 0.09861480645687565\n",
      "validation loss: 5.031101550064239\n",
      "epoch: 1451\n",
      "training loss: 0.08297263973210135\n",
      "validation loss: 4.833899120286122\n",
      "epoch: 1452\n",
      "training loss: 0.07152319695605024\n",
      "validation loss: 4.9076017728881505\n",
      "epoch: 1453\n",
      "training loss: 0.09695227803878513\n",
      "validation loss: 5.023100130555343\n",
      "epoch: 1454\n",
      "training loss: 0.08713313415588311\n",
      "validation loss: 4.832380085973037\n",
      "epoch: 1455\n",
      "training loss: 0.06924416192481288\n",
      "validation loss: 4.921302055488141\n",
      "epoch: 1456\n",
      "training loss: 0.09481354765600805\n",
      "validation loss: 5.013876940999001\n",
      "epoch: 1457\n",
      "training loss: 0.09126881913669858\n",
      "validation loss: 4.831536837729176\n",
      "epoch: 1458\n",
      "training loss: 0.06801451400914466\n",
      "validation loss: 4.9346182943394234\n",
      "epoch: 1459\n",
      "training loss: 0.09240532410027553\n",
      "validation loss: 5.003902200998073\n",
      "epoch: 1460\n",
      "training loss: 0.09532032796826878\n",
      "validation loss: 4.8309916301325035\n",
      "epoch: 1461\n",
      "training loss: 0.06770677656637689\n",
      "validation loss: 4.947603573234581\n",
      "epoch: 1462\n",
      "training loss: 0.08986253150201079\n",
      "validation loss: 4.993394639246468\n",
      "epoch: 1463\n",
      "training loss: 0.09928430366616653\n",
      "validation loss: 4.830551958769178\n",
      "epoch: 1464\n",
      "training loss: 0.06822944140004904\n",
      "validation loss: 4.9603293130335535\n",
      "epoch: 1465\n",
      "training loss: 0.08726999601566073\n",
      "validation loss: 4.982476426114015\n",
      "epoch: 1466\n",
      "training loss: 0.10319116691896052\n",
      "validation loss: 4.83004267565567\n",
      "epoch: 1467\n",
      "training loss: 0.06953633575716521\n",
      "validation loss: 4.972983150825225\n",
      "epoch: 1468\n",
      "training loss: 0.0846674494739655\n",
      "validation loss: 4.971085583718636\n",
      "epoch: 1469\n",
      "training loss: 0.10707473441930697\n",
      "validation loss: 4.829421085989091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1470\n",
      "training loss: 0.07163042650551349\n",
      "validation loss: 4.985733550950584\n",
      "epoch: 1471\n",
      "training loss: 0.08207281881413804\n",
      "validation loss: 4.959144953948324\n",
      "epoch: 1472\n",
      "training loss: 0.11096537762284865\n",
      "validation loss: 4.828595702010807\n",
      "epoch: 1473\n",
      "training loss: 0.07456881433148886\n",
      "validation loss: 4.998862018890866\n",
      "epoch: 1474\n",
      "training loss: 0.07948407084062203\n",
      "validation loss: 4.946418485343417\n",
      "epoch: 1475\n",
      "training loss: 0.11486025617198597\n",
      "validation loss: 4.8275994427050435\n",
      "epoch: 1476\n",
      "training loss: 0.0784614355427824\n",
      "validation loss: 5.012594221880318\n",
      "epoch: 1477\n",
      "training loss: 0.07690637808503306\n",
      "validation loss: 4.932724185455358\n",
      "epoch: 1478\n",
      "training loss: 0.1187217853152987\n",
      "validation loss: 4.826365098089769\n",
      "epoch: 1479\n",
      "training loss: 0.08347925530249009\n",
      "validation loss: 5.027276685066137\n",
      "epoch: 1480\n",
      "training loss: 0.07435778066952381\n",
      "validation loss: 4.917727182406724\n",
      "epoch: 1481\n",
      "training loss: 0.12243513190283635\n",
      "validation loss: 4.824972958056059\n",
      "epoch: 1482\n",
      "training loss: 0.08984414469291677\n",
      "validation loss: 5.043138698696223\n",
      "epoch: 1483\n",
      "training loss: 0.0719129779261923\n",
      "validation loss: 4.90124864543455\n",
      "epoch: 1484\n",
      "training loss: 0.1258061505842184\n",
      "validation loss: 4.823348888593654\n",
      "epoch: 1485\n",
      "training loss: 0.09783301979538982\n",
      "validation loss: 5.060514118639912\n",
      "epoch: 1486\n",
      "training loss: 0.0697296682630847\n",
      "validation loss: 4.882985105205365\n",
      "epoch: 1487\n",
      "training loss: 0.1284975381806139\n",
      "validation loss: 4.821628034627035\n",
      "epoch: 1488\n",
      "training loss: 0.10773067481035925\n",
      "validation loss: 5.079460670544617\n",
      "epoch: 1489\n",
      "training loss: 0.06812937288755189\n",
      "validation loss: 4.86299973917936\n",
      "epoch: 1490\n",
      "training loss: 0.13003588779857972\n",
      "validation loss: 4.81974157240697\n",
      "epoch: 1491\n",
      "training loss: 0.11979109592280765\n",
      "validation loss: 5.100003002142108\n",
      "epoch: 1492\n",
      "training loss: 0.06766504560619276\n",
      "validation loss: 4.841342875153837\n",
      "epoch: 1493\n",
      "training loss: 0.12974760972736138\n",
      "validation loss: 4.817992456553571\n",
      "epoch: 1494\n",
      "training loss: 0.134070752860074\n",
      "validation loss: 5.1214651110555725\n",
      "epoch: 1495\n",
      "training loss: 0.06923890743488498\n",
      "validation loss: 4.818842896389778\n",
      "epoch: 1496\n",
      "training loss: 0.12685075206299284\n",
      "validation loss: 4.816521433667642\n",
      "epoch: 1497\n",
      "training loss: 0.15024976695542838\n",
      "validation loss: 5.142678725612751\n",
      "epoch: 1498\n",
      "training loss: 0.07416761046897878\n",
      "validation loss: 4.796502254039428\n",
      "epoch: 1499\n",
      "training loss: 0.12052974642956053\n",
      "validation loss: 4.816299518894272\n",
      "epoch: 1500\n",
      "training loss: 0.16721419805805346\n",
      "validation loss: 5.16084969116345\n",
      "epoch: 1501\n",
      "training loss: 0.08415995829017814\n",
      "validation loss: 4.776601560572754\n",
      "epoch: 1502\n",
      "training loss: 0.11036553205548508\n",
      "validation loss: 4.818496701882798\n",
      "epoch: 1503\n",
      "training loss: 0.18271965829666542\n",
      "validation loss: 5.1718784891330705\n",
      "epoch: 1504\n",
      "training loss: 0.10100910648199934\n",
      "validation loss: 4.761310109668642\n",
      "epoch: 1505\n",
      "training loss: 0.09687346849769092\n",
      "validation loss: 4.826099337255252\n",
      "epoch: 1506\n",
      "training loss: 0.19299041527414731\n",
      "validation loss: 5.169135412875004\n",
      "epoch: 1507\n",
      "training loss: 0.12574787123250963\n",
      "validation loss: 4.753648411247272\n",
      "epoch: 1508\n",
      "training loss: 0.08245407691838072\n",
      "validation loss: 4.842766246476942\n",
      "epoch: 1509\n",
      "training loss: 0.19323055170317657\n",
      "validation loss: 5.145432628289715\n",
      "epoch: 1510\n",
      "training loss: 0.1572355558044339\n",
      "validation loss: 4.754104833759793\n",
      "epoch: 1511\n",
      "training loss: 0.0719064587385813\n",
      "validation loss: 4.874159417639915\n",
      "epoch: 1512\n",
      "training loss: 0.1791597909084321\n",
      "validation loss: 5.094006018823196\n",
      "epoch: 1513\n",
      "training loss: 0.19041295696196048\n",
      "validation loss: 4.761577383261396\n",
      "epoch: 1514\n",
      "training loss: 0.071882554562663\n",
      "validation loss: 4.923902406813328\n",
      "epoch: 1515\n",
      "training loss: 0.15043573538664085\n",
      "validation loss: 5.014907199307888\n",
      "epoch: 1516\n",
      "training loss: 0.21594624213623098\n",
      "validation loss: 4.769465623609046\n",
      "epoch: 1517\n",
      "training loss: 0.0879246013668529\n",
      "validation loss: 4.992148839597383\n",
      "epoch: 1518\n",
      "training loss: 0.11358099437109859\n",
      "validation loss: 4.916852749536199\n",
      "epoch: 1519\n",
      "training loss: 0.2227731449925345\n",
      "validation loss: 4.772323653530946\n",
      "epoch: 1520\n",
      "training loss: 0.11983560372196468\n",
      "validation loss: 5.06868320088348\n",
      "epoch: 1521\n",
      "training loss: 0.08182800254807583\n",
      "validation loss: 4.8201208520340515\n",
      "epoch: 1522\n",
      "training loss: 0.2042172396568811\n",
      "validation loss: 4.76639703975344\n",
      "epoch: 1523\n",
      "training loss: 0.15845637999362844\n",
      "validation loss: 5.135083781076188\n",
      "epoch: 1524\n",
      "training loss: 0.06913326929093061\n",
      "validation loss: 4.7450485942396625\n",
      "epoch: 1525\n",
      "training loss: 0.16369517590518196\n",
      "validation loss: 4.760665268936566\n",
      "epoch: 1526\n",
      "training loss: 0.18778933812174015\n",
      "validation loss: 5.166875239075736\n",
      "epoch: 1527\n",
      "training loss: 0.08184693012566606\n",
      "validation loss: 4.7066094273329355\n",
      "epoch: 1528\n",
      "training loss: 0.11537857306968667\n",
      "validation loss: 4.767743042273433\n",
      "epoch: 1529\n",
      "training loss: 0.19303873377010342\n",
      "validation loss: 5.147847848664017\n",
      "epoch: 1530\n",
      "training loss: 0.11348599110990114\n",
      "validation loss: 4.700602204057172\n",
      "epoch: 1531\n",
      "training loss: 0.07794798267995222\n",
      "validation loss: 4.803029388615339\n",
      "epoch: 1532\n",
      "training loss: 0.17020057781074288\n",
      "validation loss: 5.076683693301128\n",
      "epoch: 1533\n",
      "training loss: 0.14722428343517138\n",
      "validation loss: 4.715459507054085\n",
      "epoch: 1534\n",
      "training loss: 0.06447998850911044\n",
      "validation loss: 4.864980110030956\n",
      "epoch: 1535\n",
      "training loss: 0.12958501344244328\n",
      "validation loss: 4.974643353606568\n",
      "epoch: 1536\n",
      "training loss: 0.16521621523381233\n",
      "validation loss: 4.730330964951317\n",
      "epoch: 1537\n",
      "training loss: 0.07524036899176201\n",
      "validation loss: 4.941638494667167\n",
      "epoch: 1538\n",
      "training loss: 0.08984225701457144\n",
      "validation loss: 4.869822730891339\n",
      "epoch: 1539\n",
      "training loss: 0.15951805707520017\n",
      "validation loss: 4.741495007699483\n",
      "epoch: 1540\n",
      "training loss: 0.09873209527441092\n",
      "validation loss: 5.007712217260256\n",
      "epoch: 1541\n",
      "training loss: 0.06591660257709846\n",
      "validation loss: 4.789996863075209\n",
      "epoch: 1542\n",
      "training loss: 0.13464312950050944\n",
      "validation loss: 4.747036113114526\n",
      "epoch: 1543\n",
      "training loss: 0.11983288258463942\n",
      "validation loss: 5.046919272931169\n",
      "epoch: 1544\n",
      "training loss: 0.062251451330797576\n",
      "validation loss: 4.740599181041341\n",
      "epoch: 1545\n",
      "training loss: 0.10298230014117259\n",
      "validation loss: 4.76248796617608\n",
      "epoch: 1546\n",
      "training loss: 0.12855384402839176\n",
      "validation loss: 5.048514736549763\n",
      "epoch: 1547\n",
      "training loss: 0.0736534678629764\n",
      "validation loss: 4.723748080149299\n",
      "epoch: 1548\n",
      "training loss: 0.0760371460704224\n",
      "validation loss: 4.78729760321549\n",
      "epoch: 1549\n",
      "training loss: 0.12234635850288315\n",
      "validation loss: 5.019874536337615\n",
      "epoch: 1550\n",
      "training loss: 0.09041776948840873\n",
      "validation loss: 4.721030998710031\n",
      "epoch: 1551\n",
      "training loss: 0.06088342646383373\n",
      "validation loss: 4.829176932348025\n",
      "epoch: 1552\n",
      "training loss: 0.1059300630379145\n",
      "validation loss: 4.9677493779032975\n",
      "epoch: 1553\n",
      "training loss: 0.10414383203507266\n",
      "validation loss: 4.732889806482158\n",
      "epoch: 1554\n",
      "training loss: 0.05822794710054494\n",
      "validation loss: 4.873408291130985\n",
      "epoch: 1555\n",
      "training loss: 0.08583867656521518\n",
      "validation loss: 4.912157531744541\n",
      "epoch: 1556\n",
      "training loss: 0.10928963549043264\n",
      "validation loss: 4.738967901307483\n",
      "epoch: 1557\n",
      "training loss: 0.06467415311184012\n",
      "validation loss: 4.921502755281012\n",
      "epoch: 1558\n",
      "training loss: 0.06897316910742736\n",
      "validation loss: 4.855014577387051\n",
      "epoch: 1559\n",
      "training loss: 0.10606917870067413\n",
      "validation loss: 4.75489998920745\n",
      "epoch: 1560\n",
      "training loss: 0.07539832945591068\n",
      "validation loss: 4.9549279556044485\n",
      "epoch: 1561\n",
      "training loss: 0.05854954140422652\n",
      "validation loss: 4.816604040653712\n",
      "epoch: 1562\n",
      "training loss: 0.09562665061465031\n",
      "validation loss: 4.759233066898887\n",
      "epoch: 1563\n",
      "training loss: 0.08506795877724231\n",
      "validation loss: 4.982325972786505\n",
      "epoch: 1564\n",
      "training loss: 0.05567297771814982\n",
      "validation loss: 4.779429922068455\n",
      "epoch: 1565\n",
      "training loss: 0.0830367162313022\n",
      "validation loss: 4.781936430544788\n",
      "epoch: 1566\n",
      "training loss: 0.09189392460230705\n",
      "validation loss: 4.985787261896083\n",
      "epoch: 1567\n",
      "training loss: 0.05923200139455069\n",
      "validation loss: 4.771190884001343\n",
      "epoch: 1568\n",
      "training loss: 0.06991874902455125\n",
      "validation loss: 4.7892485275714565\n",
      "epoch: 1569\n",
      "training loss: 0.092594492305535\n",
      "validation loss: 4.987419091557611\n",
      "epoch: 1570\n",
      "training loss: 0.06598877632921307\n",
      "validation loss: 4.748260136111168\n",
      "epoch: 1571\n",
      "training loss: 0.06062315586026991\n",
      "validation loss: 4.825603984656612\n",
      "epoch: 1572\n",
      "training loss: 0.0903958253594131\n",
      "validation loss: 4.960867652733405\n",
      "epoch: 1573\n",
      "training loss: 0.07548836396703246\n",
      "validation loss: 4.766467758435497\n",
      "epoch: 1574\n",
      "training loss: 0.054748409861434644\n",
      "validation loss: 4.834534927185482\n"
     ]
    }
   ],
   "source": [
    "nodes = [100, 50, 100] # use to specify a number of hidden nodes per layer\n",
    "activations = [] # use if you want a diff activationFn per layer\n",
    "\n",
    "nn = NeuralNetwork(layers=3, nnodes=100, batchSize=50, \n",
    "                   activationFn=\"tanh\", lr=.001, lr_type=\"constant\", \n",
    "                   max_epoch=2000, momentum=0.9, early_stopping=False)\n",
    "nn.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error of Housing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y, nn.predict(X_std))\n",
    "print('Mean absolute error: $%0.2f'%(mae*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to results to those in nn_tuning_example.ipynb.  Goal: Get MAE Under $1000 with our NN.  Then, we know our NN is working well and can use it on the dataset for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # create vector of ones...\n",
    "        ones = np.ones(shape=len(X_train))[..., None]\n",
    "        #...and add to feature matrix\n",
    "        X = np.concatenate((ones, X_train), 1)\n",
    "        #calculate coefficients using closed-form solution\n",
    "        self.coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        ones = np.ones(shape=len(X_test))[..., None]\n",
    "        X_test = np.concatenate((ones, X_test), 1)\n",
    "        y_hat = X_test.dot(self.coeffs)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: $17885.89\n"
     ]
    }
   ],
   "source": [
    "lr = LR()\n",
    "lr.fit(X, y)\n",
    "mae = mean_absolute_error(y, lr.predict(X_std))\n",
    "print('Mean absolute error: $%0.2f'%(mae*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
